{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "\n",
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from tensorflow.contrib import learn\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.metrics import f1_score\n",
    "from IPython.display import clear_output, Image, display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### Do not modify here ###### \n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = \"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = graph_def\n",
    "    #strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))\n",
    "###### Do not modify  here ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data_and_labels(train_data_file, test_data_file):\n",
    "    \"\"\"\n",
    "    Loads MR polarity data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "    # Load data from files\n",
    "    train_data = pd.read_csv(train_data_file, sep='\\t', quoting=csv.QUOTE_NONE, header=None, names=['id', 'language', 'label', 'text'])\n",
    "    test_data = pd.read_csv(test_data_file, sep='\\t', quoting=csv.QUOTE_NONE, header=None, names=['id', 'language', 'label', 'text'])\n",
    "    \n",
    "    x_train = train_data['text'].tolist()\n",
    "    y_train = train_data['label'].tolist()\n",
    "\n",
    "    x_test = test_data['text'].tolist()\n",
    "    y_test = test_data['label'].tolist()\n",
    "    \n",
    "    x_train = [s.strip() for s in x_train]\n",
    "    x_test = [s.strip() for s in x_test]\n",
    "    \n",
    "    label_encoding = {'neutral':0, 'positive':1, 'negative':2}\n",
    "    \n",
    "    y_train_encoding = [label_encoding[label] for label in y_train]    \n",
    "    y_test_encoding = [label_encoding[label] for label in y_test]\n",
    "\n",
    "    \n",
    "    return [x_train, y_train_encoding, x_test, y_test_encoding]\n",
    "\n",
    "def transform_data_and_labels(data):\n",
    "    \"\"\"\n",
    "    Loads MR polarity data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    x = np.array(data['text'].tolist())\n",
    "    y = data['label'].tolist()\n",
    "    \n",
    "    # encoding label\n",
    "    label_encoding = {'neutral':0, 'positive':1, 'negative':2}\n",
    "    y = [label_encoding[label] for label in y]    \n",
    "    \n",
    "    \n",
    "    y = np.array(y)\n",
    "    \n",
    "    # Randomly shuffle data\n",
    "    np.random.seed(10)\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "    x_shuffled = x[shuffle_indices]\n",
    "    y_shuffled = y[shuffle_indices]\n",
    "\n",
    "    # Split train/test set\n",
    "    # maybe we can use cross-validation to improve\n",
    "    dev_sample_index = -1 * int(0.1 * float(len(y)))\n",
    "    x_train, x_test = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "    y_train, y_test = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "    print(\"Train/Test split: {:d}/{:d}\".format(len(y_train), len(y_test)))\n",
    "    \n",
    "    return [x_train, y_train, x_test, y_test]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    This function assumes that the last word in the word embedding is a zero vector, and will use it as padding.\n",
    "    The input 'num_voc' equals to the shape[0] of the word embedding.\n",
    "\"\"\"\n",
    "def process_tweet(train_tweets, test_tweets, num_voc):\n",
    "    # max_document_length = max([len(x.split(\" \")) for x in x_train_sentence])\n",
    "    ppl_re = re.compile(r'@\\S*')\n",
    "    url_re = re.compile(r'http\\S+')\n",
    "    tknzr = TweetTokenizer()\n",
    "    # tknzr = TweetTokenizer(reduce_len=True)\n",
    "    \n",
    "    tokenized_tweets_all = []\n",
    "    max_document_length = 0\n",
    "    \n",
    "    for tweets in [train_tweets, test_tweets]:\n",
    "        tweets = [url_re.sub('URLTOK', ppl_re.sub('USRTOK', tweet.lower())) for tweet in tweets]\n",
    "        tokenized_tweets = [tknzr.tokenize(tweet) for tweet in tweets]\n",
    "        tokenized_tweets_all.append(tokenized_tweets)\n",
    "        max_document_length = max(max_document_length, max([len(tweet) for tweet in tokenized_tweets]))\n",
    "    print(max_document_length)\n",
    "    \n",
    "    x = []\n",
    "    \n",
    "    for tokenized_tweets in tokenized_tweets_all:\n",
    "        x_curr = []\n",
    "        for tokenized_tweet in tokenized_tweets:\n",
    "            if len(tokenized_tweet) == max_document_length:\n",
    "                print(tokenized_tweet)\n",
    "            \"\"\"Not sure if original paper does this, but since index 0 means USRTOK, padding should be a number\n",
    "            higher than total word count, so tf.nn.embedding_lookup will return a tensor of 0 insted of USRTOK.\"\"\"\n",
    "        #     temp = np.zeros(max_document_length, dtype=np.int).tolist()\n",
    "            temp = (np.ones(max_document_length, dtype=np.int)*(num_voc-1)).tolist()\n",
    "\n",
    "            for index, word in enumerate(tokenized_tweet):\n",
    "                if word in word_dict:\n",
    "#                     temp[index] = word_dict[word][0]\n",
    "                    temp[index] = word_dict[word]\n",
    "            x_curr.append(temp)\n",
    "        x_curr = np.array(x_curr)\n",
    "        x.append(x_curr)\n",
    "    \n",
    "    return x[0], x[1]\n",
    "\n",
    "\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Current epoch: \", epoch)\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pre-train word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# final_embeddings = np.load('./data/embed_tweets_en_200M_200D/embedding_matrix.npy')\n",
    "# word_dict = {}\n",
    "# with open('./data/embed_tweets_en_200M_200D/vocabulary.pickle', 'rb') as myfile:\n",
    "#     word_dict = pickle.load(myfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_embeddings = np.load('./data/embed_tweets_en_590M_52D_data/en_word2vec_52.npy')\n",
    "word_dict = {}\n",
    "with open('./data/embed_tweets_en_590M_52D_data/vocabulary_dict_52.pickle', 'rb') as myfile:\n",
    "    word_dict = pickle.load(myfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76.44 94\n"
     ]
    }
   ],
   "source": [
    "# shit\n",
    "for key, val in word_dict.items():\n",
    "    if val == 94:\n",
    "        print(key, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'@lauren_hp': 3568330,\n",
       " 'hadcore': 1145459,\n",
       " '@oobeyme__': 2899054,\n",
       " '@literallylayna': 5024063,\n",
       " '@daisbond_': 372284,\n",
       " '@that_mann_mike': 3085518,\n",
       " '@liveasy10': 1524849,\n",
       " '@bikinimowing': 3013104,\n",
       " '@kianaparrr': 4325737,\n",
       " '@duhhasian18': 3054969,\n",
       " '#nayariveraontheellenshow': 203198,\n",
       " '@niditriyant': 3489693,\n",
       " '@solene_ldn': 4085061,\n",
       " '@moonmunch': 3419030,\n",
       " '@logical_saario': 4210626,\n",
       " '@tylermcflurry': 382946,\n",
       " '@ashleigh_kwele_': 3124617,\n",
       " '@joecrizzydchill': 1304721,\n",
       " '@josidschultz': 1920915,\n",
       " '#koanteykoti': 2494613,\n",
       " '@ms_baum': 3784082,\n",
       " '@mikejacques1': 1809177,\n",
       " '@kidlyznick1020': 1434302,\n",
       " '@cheyannemarieee': 5967408,\n",
       " '@yesigalano': 4104663,\n",
       " '@nabillasaurs': 3283274,\n",
       " '@shaylamarieee_': 3285170,\n",
       " '@dookie92': 2101334,\n",
       " \"buzz'n\": 667825,\n",
       " '@rockingstep': 830543,\n",
       " '@weerapreya': 3941575,\n",
       " '@le0nthedon': 5789832,\n",
       " 'it.lots': 6085389,\n",
       " '@marcussskwan': 3511959,\n",
       " '@seto_balyan': 1060388,\n",
       " '@okikawasa': 3140443,\n",
       " '@xalisonskiba': 212137,\n",
       " '#kidrauhlliveson': 3104926,\n",
       " '@ohmymuslimmalik': 4988244,\n",
       " '@drewdouglasx': 1291153,\n",
       " '@minutemaid_us': 52841,\n",
       " '@alii_huberr': 5685799,\n",
       " '@jinxman': 1809885,\n",
       " '#kyaaa': 4518908,\n",
       " '@jla_ox': 1300788,\n",
       " '@pacs_life': 524814,\n",
       " '@knowsleymammag': 984828,\n",
       " '@xxkusjedaniek': 574500,\n",
       " '@alyssakirch': 4459054,\n",
       " '@sherryyyl': 1696157,\n",
       " '@nikitaxgriffin': 1519742,\n",
       " '@hadabad_dayln': 462800,\n",
       " '@sambo_turner': 2999079,\n",
       " '@tesshahah': 611978,\n",
       " '6462004945': 1948877,\n",
       " '@bc_mjmonkey': 1396788,\n",
       " '#foodp': 383453,\n",
       " '@kushn_highheels': 5559680,\n",
       " '@emilylitton4': 1437437,\n",
       " '@littlefaux': 310519,\n",
       " 'maddened': 722446,\n",
       " 'auto-triggered': 4809158,\n",
       " '@ltalaricowltx': 447265,\n",
       " '@alfons0_o': 471794,\n",
       " '@ifashionistaa': 5502958,\n",
       " '@supportbeau': 5994300,\n",
       " 'hytner': 4952233,\n",
       " '@m_jharvey': 2687438,\n",
       " 'caocao': 5474012,\n",
       " '@iam_too_live': 997098,\n",
       " '@izyannn': 4037833,\n",
       " '@trey_111412': 5045559,\n",
       " '@ohkatieanderson': 1081301,\n",
       " '@scottybahm': 3141170,\n",
       " '@minabrckovic': 4472597,\n",
       " '@_itsallbella': 4293870,\n",
       " '@jessmaex': 4380225,\n",
       " '@anfalalfahad': 4654137,\n",
       " '@kayystackkz': 1597982,\n",
       " '@racheldubs': 4584692,\n",
       " '@pnkdiamonz': 1582144,\n",
       " '@sofia_vaz123': 1706945,\n",
       " '@_roseyyyy_': 3415244,\n",
       " '@realjaejae': 6029818,\n",
       " '@nathanarnold35': 3118623,\n",
       " '@feeberz': 5613635,\n",
       " '@paulsreece': 5822238,\n",
       " '@idespise_love': 5193675,\n",
       " '@jossaye': 2687061,\n",
       " 'blunn': 938641,\n",
       " 'faizaan': 3676059,\n",
       " '@sierrasafox': 1709517,\n",
       " '@_willywacker': 692817,\n",
       " '#followmydreams': 1198565,\n",
       " '@thekidbiebs': 102164,\n",
       " '@z_vantall': 6042939,\n",
       " 'desconectes': 1068558,\n",
       " '@ain_adiya': 4821062,\n",
       " '@twin_or_lose': 2986180,\n",
       " '@_thisjustinn': 3345796,\n",
       " '@queenalicia_ok': 2514512,\n",
       " '@the_queen_teen': 119038,\n",
       " '@asc18': 3603032,\n",
       " '@markstanley62': 789293,\n",
       " '@mariaflorea1997': 1065337,\n",
       " '#sleepingbags': 5570795,\n",
       " '@monet_ware': 2161537,\n",
       " '@kaybeesc': 5739320,\n",
       " '@_yogirljazzie': 2039158,\n",
       " '@013bbuckalew': 2260424,\n",
       " '@katiekarnif': 5635137,\n",
       " '@babyha93': 2054134,\n",
       " '#familyparties': 1520605,\n",
       " '@yamilly1': 4834953,\n",
       " '@_jade_leigh_x': 1427913,\n",
       " 'b8350': 4066189,\n",
       " 'perviously': 3704825,\n",
       " 'marcdproduction@aol.com': 4253906,\n",
       " '@bakasekaikun': 5182300,\n",
       " 'pathiye': 4832876,\n",
       " '@grantsome2': 2259217,\n",
       " '@cydneycurran': 1508792,\n",
       " '@chelcfarrell': 4194468,\n",
       " '@ab_fr3sco': 5529517,\n",
       " '@heartzxc_m': 254438,\n",
       " 'u120h': 5084683,\n",
       " '290.80': 3725167,\n",
       " '@emmagkeller': 2076375,\n",
       " 'makaganti': 4194600,\n",
       " '@corninyahole': 4659351,\n",
       " '<`)>': 209236,\n",
       " '@haley_burg17': 641025,\n",
       " '@endepee': 153292,\n",
       " 'fabrt': 3437528,\n",
       " '@cutesy__': 2859351,\n",
       " '@_taylor_1d': 4900444,\n",
       " 'culinair': 901593,\n",
       " '@blancaeb3': 897449,\n",
       " '@malyssa_dunn': 2101395,\n",
       " '@nicolasminaj': 2549813,\n",
       " 'kiloo': 53144,\n",
       " '@parrish_12': 5872790,\n",
       " '@nelxson': 1817760,\n",
       " 'happennnnnnnn': 3956966,\n",
       " '@nikkkkicondon': 2565516,\n",
       " '@ayebaybayy13': 2947835,\n",
       " '@taslimreza1': 767361,\n",
       " '@saaheba_bedi': 891255,\n",
       " '@kppollinger': 1049100,\n",
       " '@dannyelaaah': 3041194,\n",
       " '@itstbennett': 345728,\n",
       " '@magdalen_alyssa': 184997,\n",
       " '@beckieeehill': 1821067,\n",
       " '@onedremone': 3817351,\n",
       " '@juicyylv': 4442091,\n",
       " '@king_jiff': 1477183,\n",
       " 'meramianwali': 802361,\n",
       " '@nksiren': 4965051,\n",
       " '@heartheart22': 3377197,\n",
       " '@__tashee': 1721440,\n",
       " '@janel_a09': 3491586,\n",
       " '@lynunique': 4384540,\n",
       " '@xoxodedexoxo': 2843205,\n",
       " '@jaeehoney': 324026,\n",
       " '@lyddialst': 5687725,\n",
       " 'emyla': 3888026,\n",
       " '@1stephaniehelen': 650337,\n",
       " '@josenatics': 5615856,\n",
       " '@aricody1dzenny': 4887260,\n",
       " '@nnez_': 895987,\n",
       " '@sjcivi_': 3300623,\n",
       " '@worleygirl': 5523099,\n",
       " '@eli_smalls23': 3855894,\n",
       " '@mondli_ngema': 561726,\n",
       " '@anethbow': 190966,\n",
       " 'mandopony': 3443252,\n",
       " '@mintyziall': 6047494,\n",
       " '@damarishastiti': 4364529,\n",
       " '@alan1_ak': 3039545,\n",
       " '@llamariel': 2985196,\n",
       " '@sunitidamani': 4577780,\n",
       " '@bigdaddyclaudia': 915285,\n",
       " '@justtish13': 5026639,\n",
       " '@sophiehall3': 5284992,\n",
       " '@emilyheminuuuk': 2596506,\n",
       " '@lucastanahkao': 3161735,\n",
       " '@cuhhlexxis': 2843073,\n",
       " '@oh_chloeee': 3118409,\n",
       " '@shannynlouise21': 1749213,\n",
       " '@_nurainnajwa': 34968,\n",
       " '@carmenngarciaa': 4035464,\n",
       " 'megatrain': 4552218,\n",
       " '@swiftselenator1': 3902868,\n",
       " '@lucyjoanne94': 4777464,\n",
       " '@craicwhore_': 4545495,\n",
       " '@klubbuku': 1226538,\n",
       " '@iamazombiebro': 2544528,\n",
       " '@stacybadside16': 1916600,\n",
       " '@realjerrysotelo': 3571688,\n",
       " 'mandew': 4718142,\n",
       " '@champagnedelo': 2963637,\n",
       " '@nonacweety': 2699922,\n",
       " '@h0x0d': 4441409,\n",
       " '@bellind07': 1543867,\n",
       " '@jesshook395': 2120628,\n",
       " '@mrlivestrong': 5892756,\n",
       " '@blklion34': 5585176,\n",
       " '@liltunechiiii': 5921377,\n",
       " '@jng_ailee': 4780686,\n",
       " '@__itsnialler': 5503320,\n",
       " '@anesthesha': 2768034,\n",
       " '@agent_muffin': 2218696,\n",
       " '@mofomunkay': 2142746,\n",
       " '@lesbeehonesty': 3759859,\n",
       " '@exoticc_beautii': 2440368,\n",
       " '@china_edwards': 3918176,\n",
       " '@allysonfalls': 6021742,\n",
       " 'tummarow': 3407712,\n",
       " 'cutla': 4193819,\n",
       " '294.43': 4741481,\n",
       " 'brejon': 2735072,\n",
       " '@hannahdemel13': 3927470,\n",
       " '@_flxwer': 5343080,\n",
       " '@carolmalouf': 1859207,\n",
       " '@ephemera_me': 2960865,\n",
       " 'magbabait': 3507258,\n",
       " '@imfrostblooded': 4202498,\n",
       " '@hanadenouden': 864150,\n",
       " 'intellistage': 4077309,\n",
       " '@ffhumam': 4202366,\n",
       " '@lisamarkwell': 278976,\n",
       " '@ashley_baker05': 2469798,\n",
       " '3.move': 4408772,\n",
       " '@bysimply_daisia': 3571946,\n",
       " '@soulja': 900082,\n",
       " '@steph21rose': 5690615,\n",
       " '@lauramarie1227': 6020998,\n",
       " '@ra_craik': 5307883,\n",
       " '@_lovepaigeyyy': 4742494,\n",
       " '@jaureguisrascal': 2793702,\n",
       " '@ziallscream': 4124438,\n",
       " '@astriipn': 2812761,\n",
       " '@yeong_sheng': 4494895,\n",
       " '@marianaaa_c': 488577,\n",
       " '@supercutediidii': 2674214,\n",
       " 'alkout': 3412417,\n",
       " '@ikiabdurrh': 5976237,\n",
       " '@heartbeat2897': 1068473,\n",
       " '@dorkyyhass': 677698,\n",
       " '@greatambition_': 709650,\n",
       " '@cole_keeley15': 5799566,\n",
       " '@atomy__': 5559947,\n",
       " 'wwpj': 5147221,\n",
       " '@alyvico': 4012558,\n",
       " '@0_vidalll': 2326128,\n",
       " '@fauzanhakimii': 5053232,\n",
       " '@josieee_romano': 1579566,\n",
       " '@paige_wilson': 4255382,\n",
       " '@giolimjoco': 1766673,\n",
       " '@tommo7david': 5239393,\n",
       " '@randy_lewi': 1641921,\n",
       " '@tinadelara': 5126034,\n",
       " '@beicawnic_': 831649,\n",
       " '@faradiba31': 5608191,\n",
       " 'muxus': 3561856,\n",
       " '@ashleycronley': 4434108,\n",
       " 'maufuckas': 199748,\n",
       " 'islaa': 5195639,\n",
       " '1.208': 456418,\n",
       " '@barceyankee': 3922950,\n",
       " '@alimarie1428': 5255748,\n",
       " '@daisyjayne2': 2965105,\n",
       " '@gabby_gotem': 1746757,\n",
       " 'slippered': 3891765,\n",
       " '@gwencoco': 348207,\n",
       " '@mrwalz': 1404272,\n",
       " '@ohb_': 2666424,\n",
       " '#mrf': 275401,\n",
       " '@kaylah_jones17': 3259239,\n",
       " '@k3oni': 1823386,\n",
       " '@blanca_leidiana': 2813993,\n",
       " '@sambrammer': 5824508,\n",
       " '@jonigabrielle': 4283063,\n",
       " '@legendnika': 3403123,\n",
       " '2cali': 2459195,\n",
       " '@oldschoolbieber': 2037036,\n",
       " '@createdgorgeous': 4924176,\n",
       " '@anneaysh': 5032484,\n",
       " '@justsayingjose': 376969,\n",
       " '@_tracyvee': 1941631,\n",
       " '@chelseafontana': 1616918,\n",
       " '@stylesftmendes_': 4148054,\n",
       " '@housey95': 2005557,\n",
       " '@spicysorceress': 547507,\n",
       " '@gabmccarthy': 2390030,\n",
       " 'teamgalaxy': 3539716,\n",
       " '@dazzles_': 1997576,\n",
       " '@bellaballantyne': 960287,\n",
       " '@aye_yoofool': 2440100,\n",
       " '#alittlebetter': 4798748,\n",
       " '@travismorris69': 4275031,\n",
       " '@brook1hart': 807142,\n",
       " '#instasex': 5947260,\n",
       " '@jaybaby098': 4698759,\n",
       " 'winninqq': 1912187,\n",
       " '@gnarly_carlyy': 815160,\n",
       " '@adzrii': 5076560,\n",
       " '@laurameunierr': 1685012,\n",
       " 'larit': 3765897,\n",
       " '@tinman_jr': 2873475,\n",
       " '5792': 1624580,\n",
       " '@imeon050597': 607429,\n",
       " '@wbcb_punk': 991058,\n",
       " '@hanan991': 2762650,\n",
       " 'raagas': 5061764,\n",
       " '4,242': 4060866,\n",
       " '@xoxox_malika': 2458762,\n",
       " '@jae_roberts3': 3639920,\n",
       " '@dancrupt': 1133124,\n",
       " \"jaii'll\": 3564292,\n",
       " '@shehariah': 5188794,\n",
       " '@shawnamuah': 4374995,\n",
       " '@joee_whitworth': 6046854,\n",
       " '3040-2': 4029430,\n",
       " '@joannajay': 202958,\n",
       " '@_abreugetsup': 4341833,\n",
       " '@vickiprybell': 5742496,\n",
       " '#gnightt': 4858044,\n",
       " 'breedable': 5909824,\n",
       " '@vsvpnick': 3124506,\n",
       " '@rhona___': 3676575,\n",
       " '@parga_based_god': 42890,\n",
       " '@imerikasanmateo': 5579282,\n",
       " '@imjordandaniels': 5545506,\n",
       " '@alicemitchellox': 4820653,\n",
       " 'anytimmmme': 783636,\n",
       " '@_thirdeyeking': 422334,\n",
       " '@kristyr_': 519983,\n",
       " '@_itsraeeduhh': 4673917,\n",
       " '@mettaworld_quis': 2084474,\n",
       " '@wheresdecraic': 3460235,\n",
       " '@mizzieloore': 3590332,\n",
       " '@tiiqhatamie': 5447095,\n",
       " '@kchris2016': 2745809,\n",
       " '@farmermikayla': 1536325,\n",
       " '@tsgflowers': 4946078,\n",
       " '#immadatyou': 350072,\n",
       " '@haydenyvonnefan': 5007952,\n",
       " '@ezrahrah': 5224188,\n",
       " '#mcn': 1136454,\n",
       " '@jessie_janeee': 207667,\n",
       " '@camcrev': 1359657,\n",
       " '@nbrown10': 294358,\n",
       " '@vivianlugo09': 496630,\n",
       " '#trigeminalneuralgia': 1064920,\n",
       " 'photoeditor': 895229,\n",
       " '@_mace_face_': 5434242,\n",
       " '@juliedinh1': 4561485,\n",
       " '@taylor2733': 1598760,\n",
       " '@gensanquenatics': 3353579,\n",
       " '@iamravenruth': 624741,\n",
       " '@inmaculadahenar': 1915833,\n",
       " '@mindmymanners': 1474983,\n",
       " '@bethanyhamer1': 2560796,\n",
       " 'garuantee': 2188742,\n",
       " '@twitinno': 4540534,\n",
       " '#notoxenophobia': 2766280,\n",
       " '@qingkei': 283498,\n",
       " '@naddibrahim': 872885,\n",
       " '@harrical': 5791353,\n",
       " '@dunnieboi': 1458107,\n",
       " '@shendddy': 3386213,\n",
       " '@marionholly': 605066,\n",
       " '@__wavyjosh': 3136693,\n",
       " '@_redassbitch': 6044700,\n",
       " '@bapechaser2': 3685948,\n",
       " '@bigboy_vinny': 2675289,\n",
       " '@loadsofcrapp': 4964880,\n",
       " '@call_her_jay': 2696330,\n",
       " '@timmy__chan24': 3460262,\n",
       " '#instajokin': 1609043,\n",
       " 'tweeeeeeeeeeeeeeeeet': 598991,\n",
       " '#happybirthdaymmj': 6038372,\n",
       " 'nokuthula': 2895198,\n",
       " '@tehreemuk': 5697658,\n",
       " '@hannnnyforgg': 2256361,\n",
       " '@yvotran5': 3251096,\n",
       " '@kissdonghae': 3351392,\n",
       " '@am_buuurrr': 517832,\n",
       " '@nycnextlevel': 4443311,\n",
       " '@sema2103': 5080920,\n",
       " '@mindlesswaffles': 1470203,\n",
       " '@_smartmouthlynn': 5209251,\n",
       " '@tikimonroe': 5965594,\n",
       " 'ampah': 1994154,\n",
       " '@madi_sholtz': 2305658,\n",
       " '@mawtinn': 4866098,\n",
       " 'rayshonna': 2052290,\n",
       " 'calientan': 3901755,\n",
       " '@skylablaize': 6017873,\n",
       " '@followmecancun': 2396468,\n",
       " '@larrinv8a': 4651154,\n",
       " '@sheisangelea': 1123291,\n",
       " '@biznessiam': 3799452,\n",
       " '@jademysupport': 4397873,\n",
       " '@delfinabazan1': 5054645,\n",
       " '@denise_105': 2864558,\n",
       " '@nakiasideas': 4882924,\n",
       " '@anthonyenoch': 4867477,\n",
       " '@mink_deville': 815947,\n",
       " '@vannallope': 5464110,\n",
       " '@bbagnall8': 671987,\n",
       " '@azariaheng': 3048699,\n",
       " '@papivchris': 4025774,\n",
       " '@_illustri0us': 4099435,\n",
       " '23/32': 612276,\n",
       " '@josiemlow': 2453477,\n",
       " '@cho33donghyun': 1046007,\n",
       " '@naaziya289': 2740968,\n",
       " '@amona_3': 132448,\n",
       " '@ebonyhovard': 5576832,\n",
       " '@nomes3': 1280675,\n",
       " '@trippywicked': 1762769,\n",
       " '@24hourhiphop': 658360,\n",
       " 'jeydi': 37893,\n",
       " '#divergentfandom': 659844,\n",
       " '@scottjones7': 3894026,\n",
       " '@cricketupdates7': 3089989,\n",
       " 'kadupu': 748400,\n",
       " '@proudestgrandpa': 2387908,\n",
       " '@haneliswe': 3989409,\n",
       " '@tezzydoll': 4587660,\n",
       " '@hentaiqueen_': 1149394,\n",
       " '@sportybebe': 2239663,\n",
       " '@reyesrj_': 3105117,\n",
       " '@jenna_wolfson': 981551,\n",
       " 'halkett': 2518053,\n",
       " '@malissamcdaniel': 3766791,\n",
       " '@iorkara': 4994956,\n",
       " 'delayed': 6111628,\n",
       " '@beckysmall2': 3485552,\n",
       " '@meggie071': 2211115,\n",
       " '@annawrecksic': 5769564,\n",
       " '@session_diva': 4725862,\n",
       " '@manuela_arango': 4592199,\n",
       " '@chrisdjdl': 2757889,\n",
       " '@laurabrennan1d': 4345974,\n",
       " '@matthewnash23': 5428790,\n",
       " '@mrsonline_kecil': 2638640,\n",
       " '@metalmilitia7': 1478664,\n",
       " '@_notdhaaverage': 5022067,\n",
       " '@dj_fad3': 2545276,\n",
       " '@bnnnewslive': 2364413,\n",
       " '@jayycasian': 987714,\n",
       " 'colestream': 6099092,\n",
       " 'auratic': 3890743,\n",
       " '@dannykdr': 3790768,\n",
       " '@neveerstopdream': 3641987,\n",
       " '@littlemixnv': 1104555,\n",
       " '@cecewillliams': 2067539,\n",
       " '@_ttorii_': 1797378,\n",
       " '@aileensosavage': 3233317,\n",
       " '@maggiebend': 4481755,\n",
       " '@louisenikolai': 5956344,\n",
       " '@juli4_cl4re': 5033891,\n",
       " '@shaunster116': 5642958,\n",
       " '@licadoodles': 4492105,\n",
       " '@never_plainjane': 3531437,\n",
       " '@pelozocarla': 5306060,\n",
       " '@iam_jaydeer': 4379202,\n",
       " '@illmatic_rekah': 1922756,\n",
       " '@asapxstyles': 633454,\n",
       " '@agenk_bmx': 5741851,\n",
       " '@emilyriddle': 147457,\n",
       " '@ipacmandapussy': 705424,\n",
       " '@madisonness': 765066,\n",
       " 'sarza': 5848288,\n",
       " '@george_lee92': 4392945,\n",
       " '@yesparamore': 4038894,\n",
       " 'qkwkwk': 3733987,\n",
       " '@bullshetschool': 4715702,\n",
       " '#getmarriedalready': 2077571,\n",
       " '@katz_k': 5711932,\n",
       " '@finfinsharkfin': 4203010,\n",
       " '@liams123flickk': 1791496,\n",
       " '@parkjangstar': 2298224,\n",
       " 'denlee': 632423,\n",
       " '@isabella_aj': 1628327,\n",
       " '@picturestweets': 4789770,\n",
       " \"diss's\": 2607187,\n",
       " '@antlopdoe': 4014306,\n",
       " '@fuckdaniel666': 3246766,\n",
       " '@pur3dope': 3005204,\n",
       " '@alexis_kellam': 3988314,\n",
       " '@2hi2b': 230484,\n",
       " '@jlee_78': 2418824,\n",
       " '@lol_nope_': 77558,\n",
       " '#southtexasprobs': 1373818,\n",
       " '@teamdamonbrazil': 3109718,\n",
       " '@gisela_x3': 605511,\n",
       " '@abbieeeupton': 5477469,\n",
       " '@justdarylll': 3662100,\n",
       " '@ellie_jazmine': 2955920,\n",
       " '@chauntae1': 6020946,\n",
       " '@joelwarren': 5783065,\n",
       " '@daaaaaanixo': 2984204,\n",
       " '#lakynorlove': 4834994,\n",
       " '@can_be_both': 2170461,\n",
       " '@courtney_deeley': 1768269,\n",
       " '@melissa_price78': 2518521,\n",
       " '@heyyoofiona': 6040326,\n",
       " 'weifeng': 1564422,\n",
       " '@barmyluke23': 3885630,\n",
       " 'first-rounders': 1069144,\n",
       " 'ashler': 1410875,\n",
       " '@loutomlinsn': 153750,\n",
       " '@iamayoo': 259103,\n",
       " '@ilovejanay': 5578174,\n",
       " '@assh_lee': 4432584,\n",
       " '@short_cayke': 5459769,\n",
       " '@josiegohh': 2759559,\n",
       " '@clariceylovf': 797929,\n",
       " '@ames_10twins': 5568178,\n",
       " '#sc2012': 4800992,\n",
       " 'rightchoice': 456499,\n",
       " '@north_dairy': 5997589,\n",
       " '@alexiscrutch': 782848,\n",
       " '@carlenerd': 4200237,\n",
       " '@mell_lover': 905040,\n",
       " '@brookie171': 5446441,\n",
       " '@the__for': 4082586,\n",
       " '@5_fields_5': 3039519,\n",
       " '@dwinayp': 4346218,\n",
       " '@presgotups15': 3659457,\n",
       " '@savitrahman': 3140576,\n",
       " '@not_syd_helmick': 1411579,\n",
       " '@kanisha_reb': 3722962,\n",
       " 'tabeem': 3432896,\n",
       " '@krissa_barr': 2532017,\n",
       " '@adamr_94': 265008,\n",
       " '@kennedy_wojno': 871682,\n",
       " '@bdadyslexia': 2494569,\n",
       " '@lashaunarmani_': 4543904,\n",
       " '@gwooh_wooh': 4044047,\n",
       " '@sabriinaato': 2717985,\n",
       " 'bhsister': 5687830,\n",
       " '@shawtybad97': 952947,\n",
       " '@findingoscar': 511777,\n",
       " '@wowhoes': 6041682,\n",
       " '@rawrbaby92fl': 4642248,\n",
       " '@tweetwhileheeat': 5499769,\n",
       " '@scotthowland': 3182931,\n",
       " '@adindagn': 4546859,\n",
       " '@peacefulexistnc': 5493813,\n",
       " '@kerrigansk': 5328950,\n",
       " '@invinceabl': 189085,\n",
       " '@_aldo21': 263679,\n",
       " '@adaoraa': 5072683,\n",
       " '@galos_kate': 3369011,\n",
       " '@danni_smith23': 1587365,\n",
       " '@baboyannie': 2202685,\n",
       " '@m_dawg__': 2287160,\n",
       " \"ensign's\": 1776434,\n",
       " '@zarrywoah': 4156079,\n",
       " '@unreportedworld': 5042970,\n",
       " '@maura_jae': 2887243,\n",
       " '@jazmine_brewer2': 1816766,\n",
       " 'asriiii': 3427782,\n",
       " '@kenidykubitz': 2808681,\n",
       " '@lyssa_0n1': 3422193,\n",
       " '@hansa_1': 2366509,\n",
       " 'barofsky': 1236191,\n",
       " '@shane__whelan': 993089,\n",
       " '@lilbrav': 4474210,\n",
       " '@catalinasanch3z': 4975092,\n",
       " 'japppaaannnnn': 6139845,\n",
       " '@caseyoschmann': 4583011,\n",
       " '@biancadasilvax': 2563820,\n",
       " '@doncherryparody': 5330514,\n",
       " '@tweetgod__': 5822790,\n",
       " '@liddleb7': 5771240,\n",
       " '@kissable_kay': 3183397,\n",
       " '@naeanais': 468424,\n",
       " '@jordanholston': 2205431,\n",
       " 'godson': 6127070,\n",
       " '@shu___shu': 5520068,\n",
       " '@katmurphyy': 1976357,\n",
       " '@luizabfan': 1679714,\n",
       " '@ncad_dublin': 4498650,\n",
       " '@mjvarnsverry': 3038070,\n",
       " '@__sadai': 90478,\n",
       " '@racheyy': 2950354,\n",
       " '@_zajmxo': 2750584,\n",
       " '@juantheg': 5034196,\n",
       " '@lady_multham': 249257,\n",
       " '@kinghollay': 3367478,\n",
       " '@v_dylancook_v': 3051014,\n",
       " '@ytf_chicago': 137804,\n",
       " '@megchristinehun': 3861198,\n",
       " '@brandyjurevitz': 3478084,\n",
       " '@katiefeesey': 1376381,\n",
       " '@justjazlyynnn': 598282,\n",
       " '@christopher_lv': 4591086,\n",
       " '@sammigauthier': 6061937,\n",
       " '@esamulaan_': 1573378,\n",
       " '@xtinaalisaa_06': 5740500,\n",
       " '@audreymrc': 4856148,\n",
       " '@_rebekahohare': 2436052,\n",
       " '@aoife__kav': 1019245,\n",
       " '@bkvssidy_': 1949169,\n",
       " '@elizabetham__': 2301225,\n",
       " '@cacafacecindy': 3712154,\n",
       " '@1oliviarose': 5424081,\n",
       " '@jolizeines_xx': 5104562,\n",
       " '@comfortandadam': 2893293,\n",
       " '@sarahgraice': 3167677,\n",
       " '@emiillly_': 691911,\n",
       " '@omgitslibby': 2596784,\n",
       " '@xmarliessss': 4949478,\n",
       " '@stbridelibrary': 3418109,\n",
       " '@thisfaggot__': 5400619,\n",
       " '@shynebeats': 1597884,\n",
       " 'irressistible': 5634661,\n",
       " '@seminolereverie': 627507,\n",
       " '2poster': 2526628,\n",
       " 'maileen': 520184,\n",
       " '@pronnie2324': 3182638,\n",
       " '@marisseck': 3513730,\n",
       " '@taralobianco': 1353016,\n",
       " '@mccurrie_': 1477328,\n",
       " '#5sosarenumberoneparty': 50110,\n",
       " 'echauz': 1863688,\n",
       " '@unclemad': 2512674,\n",
       " '#truearianator': 2181178,\n",
       " '@jamrokjake': 232589,\n",
       " '#yourgirlsahoeif': 643417,\n",
       " '@bigmarc251': 66360,\n",
       " '@fabgoingham': 3773682,\n",
       " '@fun_sized_yo09': 3207688,\n",
       " '@twdarin': 2196471,\n",
       " '@xbettyfx': 3815330,\n",
       " '@saracakes_': 1176486,\n",
       " '@jodie_lunttw': 4650574,\n",
       " '@barbara_ann4497': 5423498,\n",
       " '@aksesorina': 5775397,\n",
       " '@lilzac34': 1660519,\n",
       " '@dgdizzy': 3469790,\n",
       " '@caiumhoocl': 346354,\n",
       " '@viaaneyyyy': 3070683,\n",
       " '@kayy_faith': 4726274,\n",
       " '@emilyshannon94': 1190203,\n",
       " '@andreaa_p96': 3644872,\n",
       " 'ngerengek': 2367701,\n",
       " '@natashapatel_tw': 2445894,\n",
       " '@aliyahmader': 3576116,\n",
       " '@lou_tomlemonson': 5108889,\n",
       " '@rylezra': 5716974,\n",
       " '#worddddd': 3274023,\n",
       " 'uhhgh': 2427659,\n",
       " '#torontostar': 4261095,\n",
       " '@kuranicole': 3730164,\n",
       " 'myugsoo': 3732966,\n",
       " '@leighemment': 2843357,\n",
       " '@emmyabe': 1391758,\n",
       " '@aimansyamim_': 3029619,\n",
       " '@naya_symone': 1722018,\n",
       " '#rugbyleaguelive2': 5425690,\n",
       " '@hollyskillings': 3699627,\n",
       " '@kaylee_wolff': 4909264,\n",
       " '@olivia_1414': 3254188,\n",
       " '#everythingis4preorder': 283377,\n",
       " '@sarah_seagull13': 2836359,\n",
       " '@arineeda_pencil': 2370047,\n",
       " '@macbarbielubber': 1969967,\n",
       " '@guyedgvvr': 4939939,\n",
       " '#faile': 5847443,\n",
       " '@afinefeline': 1956442,\n",
       " '@kaylaa_wardd': 440164,\n",
       " '@niarimadona': 3351899,\n",
       " '@ishipnarrry': 3699912,\n",
       " '@cheyenne_toon': 5828993,\n",
       " '@d4rkbr0therhood': 2384017,\n",
       " '@t_chillin4': 3901247,\n",
       " '@serdonmaria': 761821,\n",
       " '@wefallforgomez': 5014637,\n",
       " '@jakartadc': 2463453,\n",
       " '@caseythemodel': 335577,\n",
       " '@mga_wolf': 3450711,\n",
       " '@camgohamfuller': 2872844,\n",
       " '@mancboi1987': 5377899,\n",
       " '@asianniggguh': 779367,\n",
       " '@darinharvey': 4879202,\n",
       " '@syrenaica': 5565027,\n",
       " '@chickenjoe342': 3519765,\n",
       " '@xmanfa': 1633134,\n",
       " '@missapanavicius': 2224296,\n",
       " '@looc_si_nae': 3043283,\n",
       " '@irolar': 3067302,\n",
       " 'boards.ie': 1209158,\n",
       " '@danomccarthy121': 745328,\n",
       " '@mrsmartin4144': 4771164,\n",
       " '@amydickenx': 58353,\n",
       " '@courtstruly': 2824853,\n",
       " '@j_lapes': 2333424,\n",
       " '@ericapribila': 1770725,\n",
       " '@iamteddyrozay': 4021176,\n",
       " '@leilersss': 2004906,\n",
       " '@xyouaintdope': 1535406,\n",
       " '@sophiebellamy': 5815498,\n",
       " '#thecomeups': 455010,\n",
       " '@_jo_king_': 1153798,\n",
       " 'jodemeeeee': 1131243,\n",
       " '@molinasemple': 3157308,\n",
       " '@nialls_mofowife': 1114337,\n",
       " '@thaob_le': 5098155,\n",
       " '@samantha_diperi': 589604,\n",
       " '@valleouk': 5947517,\n",
       " '@dirtnddglitter': 4269100,\n",
       " 'gritch': 5924433,\n",
       " '@bockle': 2527690,\n",
       " '@jlrockefeller': 2471439,\n",
       " '@natashaoct': 1589594,\n",
       " '@maddie_shipman': 5831495,\n",
       " '#thugsdontdance': 1934617,\n",
       " '@to_financejob': 1583255,\n",
       " '#zombietime': 184268,\n",
       " '@taylorrminnick': 3448834,\n",
       " '@prettiblaque_': 4985699,\n",
       " '@xmileycyrusr': 5392186,\n",
       " '@the_algerbraic': 3394860,\n",
       " '@rompehuevos3': 5052586,\n",
       " '@mansakaur': 4354571,\n",
       " '@donetbh_': 2724020,\n",
       " '@kpzy': 3365917,\n",
       " '@charlie_angels_': 5346834,\n",
       " '@annamarie_1004': 1557047,\n",
       " '@specialistpsy': 1018055,\n",
       " '@t_summers76': 5806009,\n",
       " 'j-si': 3730673,\n",
       " '@lompss': 994875,\n",
       " 'colegioooooooo': 5327957,\n",
       " '@radyacntn': 5101245,\n",
       " '@oliviaruss1': 3133419,\n",
       " '@lenonooo': 4978391,\n",
       " '@mugasha_': 3579898,\n",
       " '@johnongoco': 5533766,\n",
       " 'tweenteen': 242934,\n",
       " '#youremyperson': 1911554,\n",
       " '@bradleyu': 1539581,\n",
       " '@austinmahboii': 2167171,\n",
       " '@_hmichelle': 2579852,\n",
       " '@shasha_loveso': 2279239,\n",
       " '@aquila_j': 6046252,\n",
       " '@tiger_cocainee': 5592516,\n",
       " '@airsoull': 4534557,\n",
       " '@orinndb': 5345009,\n",
       " '@justbrittney23': 2064724,\n",
       " '@1dlondonon': 1681969,\n",
       " '@lexii_louuuu': 99812,\n",
       " '@laiteux': 3236229,\n",
       " '@mjazmartinez608': 2441556,\n",
       " 'saimung': 2744362,\n",
       " '@rooseveltk': 3293348,\n",
       " '@camlikesbooty': 3425002,\n",
       " '@abbiestonehouse': 4352721,\n",
       " '@shawtyyletssgoo': 3984086,\n",
       " '#dontlistentothem': 4009009,\n",
       " '@midnightsmiles': 6002223,\n",
       " '@austinpollard4': 2537745,\n",
       " 'cilgintvtags': 4927826,\n",
       " '#happybirthdaytristanevans': 3094279,\n",
       " '@anotherlie': 4962362,\n",
       " '@i_beliebin1': 4648187,\n",
       " '@drew_north1': 3675084,\n",
       " '@shylaa_xo': 3975945,\n",
       " '@las2themee': 4258459,\n",
       " '@danceforever148': 5270409,\n",
       " '@blackgirlsrock5': 3576769,\n",
       " 'rediculouss': 4140218,\n",
       " '@gizzalon': 606029,\n",
       " '@kyleadlawan': 4008892,\n",
       " '@linnellcourtney': 5797730,\n",
       " '@jollyriffic': 1644814,\n",
       " '@brenda_booboo': 5383951,\n",
       " '@ethanlyttle': 4743835,\n",
       " '280.62': 2175646,\n",
       " '@fortedpark': 5134333,\n",
       " '@iamredgemoi17': 3493533,\n",
       " '@hxorsnhon': 862913,\n",
       " '@anony_mmis': 1263584,\n",
       " '@michaelwatson85': 870880,\n",
       " 'meeesssss': 1486986,\n",
       " '@mronyxreyes': 239371,\n",
       " '@gutierrezdeee': 835243,\n",
       " '@xosimplyamazing': 1663384,\n",
       " '@dgrog': 3257360,\n",
       " '@tsmooky': 1637128,\n",
       " '@jessrockstarrr': 1613961,\n",
       " '@shineeonew_fba': 4340148,\n",
       " '@i_neeed_moneey': 1599340,\n",
       " '@mc_girrr': 5776463,\n",
       " '@edentodope69': 3681122,\n",
       " '@mayaalyaaa': 5848333,\n",
       " '@aaronellis1': 737420,\n",
       " '#tvfail': 3357682,\n",
       " '@tatiana__nieves': 58856,\n",
       " '@idkemotional': 4387187,\n",
       " '@indahahehahe': 3520291,\n",
       " '@glogirl__': 1440152,\n",
       " '@milah___': 5164594,\n",
       " '@hayleybrock5': 3738798,\n",
       " '@jess_beltonx': 5058471,\n",
       " 'hoyah': 2913235,\n",
       " '@thongstrap': 1623169,\n",
       " '@laurennnf': 3457955,\n",
       " '@naomichristlike': 4519037,\n",
       " '@__leahbeah': 2845247,\n",
       " '@jade_flear': 897602,\n",
       " 'cachetito': 2614416,\n",
       " '@sarashayksbae': 3838601,\n",
       " '@caitlin_marie29': 5165394,\n",
       " '#gameoflove': 2323639,\n",
       " '@_asapslim': 2945516,\n",
       " '@taylorblubaugh': 4759932,\n",
       " '@kaaitliinn_': 561385,\n",
       " '@na5im': 1344450,\n",
       " '@nmrh_': 1253706,\n",
       " '@halakoura_': 2837229,\n",
       " '@beliebers324': 4620961,\n",
       " '@kim_cassidy': 2185733,\n",
       " '@tippmeover': 3648447,\n",
       " '@imperfectyna': 2408132,\n",
       " '#banvafg': 1350624,\n",
       " '@bbb_butta': 5066071,\n",
       " '#ghettobitches': 271362,\n",
       " '#ineedprayer': 3577388,\n",
       " '@tierasarena': 5119683,\n",
       " '@nicolebonhommie': 1454743,\n",
       " '@bingham_jordan': 5993517,\n",
       " '195x': 516043,\n",
       " '@onlybeauty_': 439153,\n",
       " '@olanrehwahju': 169043,\n",
       " '@ifyoudont_stfu': 2315633,\n",
       " '@eloquenteq': 5649136,\n",
       " '@heens_5': 1431168,\n",
       " '@liannaaa_': 1715610,\n",
       " '@_karen_mendez': 2624850,\n",
       " 'gullllll': 567836,\n",
       " '#august14th': 3666780,\n",
       " 'kaskusers': 3991065,\n",
       " '@mgcjdmx': 4672791,\n",
       " '@caoimh_oleary': 616120,\n",
       " '@saruuhdonald': 2638359,\n",
       " 'omeey': 19952,\n",
       " '@methocean': 2857556,\n",
       " '@thatkid_natalyy': 1230240,\n",
       " 'auriee': 1042,\n",
       " 'bedart': 3087936,\n",
       " 'angsle': 1844577,\n",
       " '@misslameface': 2309507,\n",
       " '@emchann': 5220281,\n",
       " '@charleyashmore': 2398238,\n",
       " '@cameroncisik': 4363126,\n",
       " 'hoays': 4867502,\n",
       " '@permescudi': 1453157,\n",
       " '@loveejasss': 2257972,\n",
       " '@nathanielholm': 1432046,\n",
       " '@deedashini': 1471200,\n",
       " '@jennnadestefano': 569357,\n",
       " '#ough': 5221659,\n",
       " 'buccament': 3622878,\n",
       " '#deepsea': 1189308,\n",
       " 'presidnet': 3711952,\n",
       " '@hopeella': 1702968,\n",
       " '@5secsofjaybird': 528646,\n",
       " '@indescribableee': 4566346,\n",
       " '@maddie_stack': 59496,\n",
       " '@pranumars': 4398434,\n",
       " '@katie_n_kaden': 1889306,\n",
       " '@artjourney': 2557339,\n",
       " '@kelliott4real': 4004381,\n",
       " '#kelena': 4849987,\n",
       " '@brittneycacharr': 1639318,\n",
       " '@fabianeloy_': 1623463,\n",
       " '@niallisabeaut20': 3810719,\n",
       " '@luv2dancexoxo': 225323,\n",
       " '@angelinacarochi': 926762,\n",
       " '@_samueldylan': 2714806,\n",
       " '#kissoff': 1303560,\n",
       " '@lukesbiebah': 2027148,\n",
       " '@pigeonboi': 118745,\n",
       " '@hunty_23': 3575260,\n",
       " '@abykhasby': 5658288,\n",
       " '@nalnoubee': 1172360,\n",
       " '@angelachewhs': 4795355,\n",
       " '@npa34': 2887216,\n",
       " '@t4keziall': 3963127,\n",
       " 'panggi': 2541614,\n",
       " '@jonathonio': 432226,\n",
       " '@jakedoganality': 1075921,\n",
       " '@eduardoalp_': 606199,\n",
       " '@hardyswagg4': 892234,\n",
       " '@arinsofia': 2955522,\n",
       " '@imsydirgibson': 5922451,\n",
       " 'benyedder': 5242231,\n",
       " '@kativarian': 475020,\n",
       " '@grace_brettle': 1139951,\n",
       " '@rizal_riyadi': 5533683,\n",
       " 'company': 6107126,\n",
       " 'models': 6110025,\n",
       " '@ali_topnotch': 1839120,\n",
       " '@leethuggin_': 704360,\n",
       " '@covochu': 153993,\n",
       " 'projecy': 956056,\n",
       " 'h0nest': 975285,\n",
       " '@jasmineiida': 5060653,\n",
       " '@zach_keith_': 3287465,\n",
       " '@royce_payne': 260086,\n",
       " '#michealjackaon': 1121587,\n",
       " '@socrazyxo_': 2368066,\n",
       " '@tw0cansam': 4555293,\n",
       " '@bougie__monee': 1277934,\n",
       " '@likethe_movie': 3003434,\n",
       " '@miyeah_': 5792307,\n",
       " '@adam__stratton': 1714180,\n",
       " '@marcus24sk': 6057695,\n",
       " '@umthat_bitch': 2438505,\n",
       " '@fauxoo': 987327,\n",
       " '@iamricosuaavve': 5153336,\n",
       " '@zmrej_': 1657748,\n",
       " '@jenniferwelker': 4925706,\n",
       " 'ersyad': 2788928,\n",
       " '#imsuchafailure': 5826429,\n",
       " '@iam_angimac': 4042757,\n",
       " '@paushinski': 1874377,\n",
       " '@xellabellax': 5083173,\n",
       " 'aikibatto': 5597635,\n",
       " '@ubfootball': 1997324,\n",
       " '@shingkwayla': 4957237,\n",
       " 'baaaaarely': 5468400,\n",
       " '@kmillr21': 1851451,\n",
       " '@murphybieberjb': 5275688,\n",
       " '@ricdodger1': 3910591,\n",
       " '@thatovoxo_x': 2261180,\n",
       " '@yoanieber': 1897544,\n",
       " '@saintnegro27': 2401924,\n",
       " 'kalkma': 5685053,\n",
       " 'mediatemple': 414012,\n",
       " '@owotaiye': 2268743,\n",
       " '@musefanspain': 1560184,\n",
       " '@shortiiejordiii': 3581092,\n",
       " '@lickziam': 5366635,\n",
       " '@cynncynna': 5440871,\n",
       " '@gabgab2016': 4027998,\n",
       " '@erriee_xx': 1492170,\n",
       " '@howtobesick': 1290060,\n",
       " '@ashleigh_hs': 5631265,\n",
       " 'engineers': 6122795,\n",
       " 'rakinah': 3845596,\n",
       " '@deeka_shanov': 4290155,\n",
       " '@jashonxiao': 4467733,\n",
       " '@marsslynn_': 4473775,\n",
       " 'nyseyh': 2283386,\n",
       " '@aye_itsaamandaa': 3916060,\n",
       " '@cpower14': 3550833,\n",
       " '@caseybarnes7': 1874434,\n",
       " '@tintooooy': 3513180,\n",
       " 'pobersito': 5417980,\n",
       " 'non-acoustic': 4613827,\n",
       " '@addit_up': 638896,\n",
       " '@masley_forever': 1371559,\n",
       " 'yourselffffffff': 1445492,\n",
       " '@bradfordjack': 4977900,\n",
       " '@jack_johnson91': 223339,\n",
       " '#happyfamilyday': 127388,\n",
       " '@acrazyrainbow': 3690754,\n",
       " '@sakynayusof': 1179660,\n",
       " '@broaddaywalker': 5144375,\n",
       " '@zavibabess': 4830009,\n",
       " '#especialbelievezonalivrebmbr': 3889941,\n",
       " '@dellajean': 895824,\n",
       " '@booms4c': 481946,\n",
       " '@xosaraxox': 5862715,\n",
       " '@hemmingzsempai': 22691,\n",
       " '@ellienero': 4608456,\n",
       " '@vale_va87': 5287686,\n",
       " '@nochand': 3091541,\n",
       " '@folarinurban_': 854380,\n",
       " \"jum'uah\": 5065930,\n",
       " '@1dlaav': 4886355,\n",
       " '@valadezjazmyn': 2866958,\n",
       " '@hiddenvalley69': 4626188,\n",
       " '@meechydoe': 6024977,\n",
       " '@tino_pham13': 2220857,\n",
       " '@kim_nunziato': 2772796,\n",
       " '@teairraxo': 2873906,\n",
       " '#lovebacklines': 3972865,\n",
       " '@_kaylababiee_': 987205,\n",
       " '@annakaymusic': 2467494,\n",
       " '@melvin5022': 4440061,\n",
       " ...}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6140853, 52)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distant Supervision phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distance_supervised_tweets = pd.read_csv('./data/distant_data/distance_supervised_tweets_corrected', names=[\"id\",\"lan\", \"label\", \"text\"], sep=\"\\t\", header=None, usecols=[\"lan\", \"label\", \"text\"])\n",
    "distance_supervised_tweets_2 = pd.read_csv('./data/distant_data/distance_supervised_tweets_2_corrected', names=[\"id\",\"lan\", \"label\", \"text\"], sep=\"\\t\", header=None, usecols=[\"lan\", \"label\", \"text\"])\n",
    "distance_supervised_tweets_3 = pd.read_csv('./data/distant_data/distance_supervised_tweets_3_corrected', names=[\"id\",\"lan\", \"label\", \"text\"], sep=\"\\t\", header=None, usecols=[\"lan\", \"label\", \"text\"])\n",
    "distance_supervised_tweets = distance_supervised_tweets.append(distance_supervised_tweets_2).append(distance_supervised_tweets_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lan</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>Boston Bruins Morning Thoughtefense Exceeding ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>Bol Bachchan!. #AeZindagiGaleLagale. Aata Majh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>(17) karena lagi sakit, aku lagi gelisah terus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>Telkomsel: Thary422 Terima kasih telah berpart...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>@parisa_khania آخر سر هم از جزوه ی محترم عکس گ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>kyristcl: XL123: frungnarikvnn Bisa ajak selai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>Telkomsel: daff_01 Terima kasih telah berparti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>@chris_randall Just the usual disclaimer that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>@Lin_Manuel Congrats from me and all my friend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>@nicaaaji hahahaha magtext ako beb. \"Hi lola! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>@emergiTEL launches their new website.  #Emegi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>Okulda iki kere üst üste kitli kalıp camdan at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>New Lyft Users get 10 free Lyft rides &amp;lt;&amp;lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>I want Thai food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>@LionArts we are checking with our music searc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>I actually lover Anna Faris https://t.co/sUAG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>latest podcast from Chop Suey Press The Battle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>MAJufri3 Terima kasih telah berpartisipasi. Ku...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>Telkomsel: fadilanurul955 Terima kasih telah b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>@AwaisAhmad718 thanks  ab theek hai medicine l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>Telkomsel: khai_dier Jika keluhan baru hari in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>@taykecare Hi! Saw you follow music and think ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>Telkomsel: InnekeVermarien Pagi, Mbak Inneke. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>@nreatherford Sounds good to us! We are always...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>#ملتقي_لحن_للابداع #قروب_طويق_للدعم تبون ريتويت</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>Yung feeling na di nyo hinahayaan matapos ang ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>Hello baby jane (Babeeh_Jane)  https://t.co/LC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>@kevinthewhippet @iggiesrule89 @bunniemommie i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>هام تم تعديل مسار خط باص ميسلون - جامعة بحيث ي...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>Helloo.  Alyssa (TheJoltaire) https://t.co/25L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160752</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>ANG CUTE CUTE CUTE MO  https:// twitter.com/Gi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160753</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>I’m willing to bet it’s a partial tear. Idk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160754</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>この笑顔はキュン死もの_´ཀ`」 ∠):_ pic.twitter.com/5iBTOampMh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160755</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>really? for real?  https:// twitter.com/readef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160756</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>え？_´ཀ`」 ∠): 良いもん食ってんのな https:// twitter.com/mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160757</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>And still beating us....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160758</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>Kolaborasi kuy  https:// twitter.com/amandasya...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160759</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>captain crunch..  https:// twitter.com/CAMILAH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160760</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>people should start selling fish rice or prawn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160761</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>http://htSomething needs to be done to prevent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160762</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>http://htSomething needs to be done to prevent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160763</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>http://htSomething needs to be done to prevent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160764</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>http://htSomething needs to be done to prevent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160765</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>http://htSomething needs to be done to prevent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160766</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>http://htSomething needs to be done to prevent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160767</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>http://htSomething needs to be done to prevent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160768</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>@Mbrower42 HAPPY BIRTHDAY, i love you mace!!!!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160769</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>http://htSomething needs to be done to prevent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160770</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>http://htSomething needs to be done to prevent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160771</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>https:// twitter.com/NFL/status/928 834503887...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160772</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>I'm so hungry but I'm so done eating goldfish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160773</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>i'm always stuck between wanting to do somethi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160774</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>Please sign &amp; share! {|} http:// fb.me/9qsBceTD1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160775</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>no b you deserve to be happy  don’t say that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160776</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>Kaylee is deadass sobbing her eyes out over ft...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160777</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>http://htSomething needs to be done to prevent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160778</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>Ugh. @DangeRussWilson 's getting no coverage! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160779</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>http://htSomething needs to be done to prevent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160780</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>http://htSomething needs to be done to prevent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160781</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>i want to eat hamburger and chicken</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>203512 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       lan     label                                               text\n",
       "0       en  positive  Boston Bruins Morning Thoughtefense Exceeding ...\n",
       "1       en  positive  Bol Bachchan!. #AeZindagiGaleLagale. Aata Majh...\n",
       "2       en  negative  (17) karena lagi sakit, aku lagi gelisah terus...\n",
       "3       en  positive  Telkomsel: Thary422 Terima kasih telah berpart...\n",
       "4       en  positive  @parisa_khania آخر سر هم از جزوه ی محترم عکس گ...\n",
       "5       en  positive  kyristcl: XL123: frungnarikvnn Bisa ajak selai...\n",
       "6       en  positive  Telkomsel: daff_01 Terima kasih telah berparti...\n",
       "7       en  positive  @chris_randall Just the usual disclaimer that ...\n",
       "8       en  positive  @Lin_Manuel Congrats from me and all my friend...\n",
       "9       en  negative  @nicaaaji hahahaha magtext ako beb. \"Hi lola! ...\n",
       "10      en  positive  @emergiTEL launches their new website.  #Emegi...\n",
       "11      en  negative  Okulda iki kere üst üste kitli kalıp camdan at...\n",
       "12      en  positive  New Lyft Users get 10 free Lyft rides &lt;&lt;...\n",
       "13      en  negative                                  I want Thai food \n",
       "14      en  positive  @LionArts we are checking with our music searc...\n",
       "15      en  negative   I actually lover Anna Faris https://t.co/sUAG...\n",
       "16      en  positive  latest podcast from Chop Suey Press The Battle...\n",
       "17      en  positive  MAJufri3 Terima kasih telah berpartisipasi. Ku...\n",
       "18      en  positive  Telkomsel: fadilanurul955 Terima kasih telah b...\n",
       "19      en  positive  @AwaisAhmad718 thanks  ab theek hai medicine l...\n",
       "20      en  positive  Telkomsel: khai_dier Jika keluhan baru hari in...\n",
       "21      en  positive  @taykecare Hi! Saw you follow music and think ...\n",
       "22      en  positive  Telkomsel: InnekeVermarien Pagi, Mbak Inneke. ...\n",
       "23      en  positive  @nreatherford Sounds good to us! We are always...\n",
       "24      en  positive   #ملتقي_لحن_للابداع #قروب_طويق_للدعم تبون ريتويت \n",
       "25      en  positive  Yung feeling na di nyo hinahayaan matapos ang ...\n",
       "26      en  positive  Hello baby jane (Babeeh_Jane)  https://t.co/LC...\n",
       "27      en  positive  @kevinthewhippet @iggiesrule89 @bunniemommie i...\n",
       "28      en  negative  هام تم تعديل مسار خط باص ميسلون - جامعة بحيث ي...\n",
       "29      en  positive  Helloo.  Alyssa (TheJoltaire) https://t.co/25L...\n",
       "...     ..       ...                                                ...\n",
       "160752  en  negative  ANG CUTE CUTE CUTE MO  https:// twitter.com/Gi...\n",
       "160753  en  negative       I’m willing to bet it’s a partial tear. Idk \n",
       "160754  en  negative   この笑顔はキュン死もの_´ཀ`」 ∠):_ pic.twitter.com/5iBTOampMh\n",
       "160755  en  negative  really? for real?  https:// twitter.com/readef...\n",
       "160756  en  negative  え？_´ཀ`」 ∠): 良いもん食ってんのな https:// twitter.com/mu...\n",
       "160757  en  negative                           And still beating us....\n",
       "160758  en  negative  Kolaborasi kuy  https:// twitter.com/amandasya...\n",
       "160759  en  negative  captain crunch..  https:// twitter.com/CAMILAH...\n",
       "160760  en  negative  people should start selling fish rice or prawn...\n",
       "160761  en  negative  http://htSomething needs to be done to prevent...\n",
       "160762  en  negative  http://htSomething needs to be done to prevent...\n",
       "160763  en  negative  http://htSomething needs to be done to prevent...\n",
       "160764  en  negative  http://htSomething needs to be done to prevent...\n",
       "160765  en  negative  http://htSomething needs to be done to prevent...\n",
       "160766  en  negative  http://htSomething needs to be done to prevent...\n",
       "160767  en  negative  http://htSomething needs to be done to prevent...\n",
       "160768  en  negative  @Mbrower42 HAPPY BIRTHDAY, i love you mace!!!!...\n",
       "160769  en  negative  http://htSomething needs to be done to prevent...\n",
       "160770  en  negative  http://htSomething needs to be done to prevent...\n",
       "160771  en  negative   https:// twitter.com/NFL/status/928 834503887...\n",
       "160772  en  negative     I'm so hungry but I'm so done eating goldfish \n",
       "160773  en  negative  i'm always stuck between wanting to do somethi...\n",
       "160774  en  negative   Please sign & share! {|} http:// fb.me/9qsBceTD1\n",
       "160775  en  negative       no b you deserve to be happy  don’t say that\n",
       "160776  en  negative  Kaylee is deadass sobbing her eyes out over ft...\n",
       "160777  en  negative  http://htSomething needs to be done to prevent...\n",
       "160778  en  negative  Ugh. @DangeRussWilson 's getting no coverage! ...\n",
       "160779  en  negative  http://htSomething needs to be done to prevent...\n",
       "160780  en  negative  http://htSomething needs to be done to prevent...\n",
       "160781  en  negative               i want to eat hamburger and chicken \n",
       "\n",
       "[203512 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_supervised_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Test split: 183161/20351\n"
     ]
    }
   ],
   "source": [
    "x_train_distance, y_train, x_test_distance, y_test = transform_data_and_labels(distance_supervised_tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20351\n",
      "2063\n",
      "['people', 'come', 'and', 'people', 'go', '...', \"that's\", 'life', '...', '#aldubmistakenidentity', '33877', 'en', 'positive', \"michiganon't\", 'sell', 'USRTOK', '100m', 'gallons', 'of', 'groundwater', 'for', '$', '200', 'and', '20', 'jobs', '.', \"that's\", 'bananas', '.', 'URLTOK', '33878', 'en', 'positive', 'jio', 'has', 'touched', 'the', 'hearts', 'of', '50', 'millions', 'users', 'with', 'their', 'network', 'really', 'happy', 'for', 'jio', '!', '#jio50million', '33879', 'en', 'positive', 'every', 'second', ',', 'minute', '&', 'hour', 'of', 'our', 'life', 'must', 'be', 'filled', 'with', 'passion', ',', 'dedication', 'and', 'restlessness', 'to', 'make', 'it', 'the', 'best', 'possible', 'life', 'ever', '.', '33880', 'en', 'positive', 'USRTOK', 'USRTOK', 'USRTOK', 'whys', 'this', 'yous', '33881', 'en', 'positive', 'thanks', 'for', 'the', 'recent', 'follow', 'USRTOK', 'USRTOK', 'USRTOK', 'happy', 'to', 'connect', 'have', 'a', 'great', 'tuesday', '.', '>', '>', 'URLTOK', '33882', 'en', 'positive', 'done', 'putting', 'in', 'effort', 'into', 'relationships', 'w', 'people', 'when', 'i', \"don't\", 'get', 'any', 'effort', 'back', ':', '33883', 'en', 'positive', 'USRTOK', 'because', 'of', 'i', 'will.put', 'none', 'option', 'then', 'most', 'of', 'people', 'will', 'vote', 'for', 'none', 'so', 'i', 'wanna', 'see', 'which', '1', 'of', 'them', 'both', 'r', 'popular', '..', '33884', 'en', 'negative', 'i', 'miss', 'my', 'phone', '.', '33885', 'en', 'negative', 'USRTOK', 'ahh', 'thank', 'you', 'ky', 'xoxo', '❣', '️', 'miss', 'you', 'too', '33886', 'en', 'positive', 'conflictenied', 'ops', '(', 'sony', 'playstation', '3', ',', '2008', ')', 'URLTOK', 'URLTOK', '33887', 'en', 'negative', 'lama', 'ndk', 'chat', 'bf', '33888', 'en', 'positive', 'scorching', 'hot', 'services', 'stocks', 'tapeish', 'network', 'corp', '.', '(', 'dish', ')', ',', '...', 'URLTOK', '#dishnetwork', '33889', 'en', 'positive', 'USRTOK', 'of', 'course', 'yeah', '.', 'thanks', 'man', '33890', 'en', 'negative', 'it', 'feels', 'like', 'the', 'world', \"doesn't\", 'want', 'u', 'just', 'because', 'u', 'have', 'this', 'pimples', 'in', 'your', 'face', '33891', 'en', 'positive', 'my', 'fave', 'lm', 'alcott', 'is', 'not', 'little', 'women', 'nor', \"jo's\", 'boys', 'nor', 'little', 'men', 'something', 'simpler', 'and', 'more', 'relatable', 'for', 'me', '33892', 'en', 'negative', 'sleepy', 'na', 'me', '33893', 'en', 'positive', 'jacket', 'reseller', 'very', 'wellcome', '-', 'size', 'm', 'fit', 'l', 'bbm', ':', 'pin', '37314c7', 'lineUSRTOK', ':', 'USRTOK', '(', 'pake', 'USRTOK', 'URLTOK', '33894', 'en', 'positive', 'thank', 'you', 'USRTOK', ':', '*', 'happy', 'birthday', 'kay', 'bby', 'brother', 'mo', '33895', 'en', 'negative', 'meis', 'home', ')', 'megoes', 'to', 'my', 'mom', ')', 'hi', 'mom', 'my', 'brothercomes', 'running', 'after', 'me', ')', 'she', 'met', 'her', 'crush', 'wonho', '33896', 'en', 'positive', '4days', 'drunk', 'saraaaapp', '\\\\', 'ud83d', '\\\\', 'ude', '02', '33897', 'en', 'positive', 'kinda', 'weird', 'but', 'still', 'cute', '33898', 'en', 'negative', \"didn't\", 'even', 'get', 'an', 'interview', 'for', 'ta', 'job', '.', 'apparently', 'i', \"don't\", 'have', 'the', 'skills', 'or', 'the', 'experience', 'i', 'guess', 'my', 'science', 'degree', 'means', 'nothing', '33899', 'en', 'positive', 'thanks', 'for', 'the', 'recent', 'follow', 'USRTOK', 'USRTOK', 'USRTOK', 'happy', 'to', 'connect', 'have', 'a', 'great', 'tuesday', '.', '>', '>', 'get', 'free', '?', 'URLTOK', '33900', 'en', 'positive', 'opiniononald', 'trump', 'and', 'the', 'nuclear', 'danger', 'URLTOK', 'URLTOK', '33901', 'en', 'negative', 'USRTOK', 'i', 'miss', 'you', 'too', 'po', 'ate', '33902', 'en', 'positive', 'USRTOK', 'so', 'true', 'but', 'the', 'beautiful', 'part', ';', 'for', 'ones', 'own', 'admonition', 'it', 'is', 'that', 'reality', 'only', 'confirms', 'the', 'scriptures', '.', '33903', 'en', 'positive', 'USRTOK', 'USRTOK', '4', 'bio', '5', 'icon', '5', 'layout', '5', '–', 'i', 'really', 'like', 'it', 'recent', 'tweets', '4', 'would', 'i', 'follow', ':', 'no', ',', 'but', 'good', 'acc', '33904', 'en', 'negative', 'USRTOK', 'tfw', 'you', \"can't\", 'see', 'your', 'feet', '33905', 'en', 'positive', 'USRTOK', 'ikr', '.', 'that', 'eyes', 'smile', '33906', 'en', 'positive', 'USRTOK', '3t', 'is', 'the', 'better', 'buy', '33907', 'en', 'positive', '#givingtuesdayca', 'is', 'under', 'way', 'at', 'go', 'stations', 'around', 'the', 'city', '!', 'be', 'sure', 'to', 'grab', 'a', '$', '2', 'scone', 'on', 'your', 'way', 'to', 'work', 'this', 'morning', '33908', 'en', 'positive', 'USRTOK', 'wonderful', '!', 'thanks', 'for', 'joining', 'the', '#skypeathon', '.', 'was', 'your', 'call', 'booked', 'through', 'the', 'site', '?', 'we', 'want', 'to', 'make', 'sure', 'your', 'miles', 'count', '!', '33909', 'en', 'positive', 'USRTOK', 'just', 'got', 'off', 'my', 'flight', 'to', 'munich', '33910', 'en', 'positive', 'USRTOK', 'advance', 'happy', 'birthday', 'maria', '!', 'love', 'you', 'queen', 'alwaysalexa', 'bukasna', '33911', 'en', 'negative', 'why', 'are', 'these', 'garbage', 'trucks', 'so', 'loud', '33912', 'en', 'positive', 'a', 'case', 'study', 'in', 'poor', 'portfolio', 'risk', 'decisionsallas', 'police', '&', 'fire', 'pension', '.', 'will', 'pension', 'boards', '/', 'advisors', 'learn', '?', 'URLTOK', '33913', 'en', 'negative', 'USRTOK', 'still', 'the', 'bully', 'huhu', 'thank', 'you', 'mom', '!', '!', 'i', 'love', 'youuu', '♡', '33914', 'en', 'positive', 'USRTOK', 'thanks', 'for', 'the', 'follow', '33915', 'en', 'positive', 'i', 'have', 'to', 'drop', 'a', 'class', 'asan', 'ang', 'sistema', 'isd', '.', 'asan', 'na', '.', 'paki', 'hanap', 'please', '.', '33916', 'en', 'positive', 'USRTOK', 'awesome', '-', 'i', 'wish', 'my', 'dogs', 'would', 'sleep', 'close', 'together', 'like', 'that', '33917', 'en', 'negative', 'USRTOK', 'o', 'omygod', 'i', 'love', 'you', 'i', 'love', 'va', 'i', 'love', 'art', 'yay', '33918', 'en', 'positive', 'USRTOK', 'black', '33919', 'en', 'positive', 'help', 'a', 'sister', 'out', '!', 'i', 'could', 'win', '$', '500', 'from', 'unbound', 'for', 'new', 'lingerie', 'URLTOK', '33920', 'en', 'positive', '#thanks', 'USRTOK', 'USRTOK', 'thanks', 'for', 'the', 'recent', 'follow', '.', 'much', 'appreciated', '>', '>', 'want', 'this', '?', \"it's\", 'free', '!', 'URLTOK', '33921', 'en', 'positive', 'USRTOK', 'and', 'USRTOK', 'support', 'for', 'echo', 'soon', '33922', 'en', 'positive', 'love', '#thegiftyougive', 'to', 'the', 'world', '33923', 'en', 'positive', 'new', 'videof', 'network', '-', 'extra', 'beautiful', 'fitness', 'milf', 'bangs', 'huge', 'cock', '#nsfw', '#xxx', '#sex', '#tube', '#porn', '33924', 'en', 'positive', 'not', 'a', 'bad', 'start', 'a', 'nice', '50/50', 'on', \"today's\", 'clients', 'wing', '#autocleandetailing', '…', 'URLTOK', '33925', 'en', 'positive', 'USRTOK', 'let', 'us', 'know', 'if', 'we', 'can', 'help', 'answer', 'any', 'of', 'your', 'plumbing', 'questions', '!', 'URLTOK', '33926', 'en', 'positive', 'bostonecember', 'is', 'almost', 'here', '.', 'make', 'the', 'most', 'of', 'it', 'and', 'get', 'to', 'one', '(', 'or', 'more', ')', 'of', 'these', 'events', 'around', 'boston', '.', '…', 'URLTOK', '33927', 'en', 'positive', 'good', 'morning', 'people', '!', '!', 'have', 'a', 'lovely', 'day', '#twitter', '#wedding', '#bride', '33928', 'en', 'positive', 'thanks', 'for', 'the', 'recent', 'follow', 'USRTOK', 'USRTOK', 'happy', 'to', 'connect', 'have', 'a', 'great', 'tuesday', '.', '(', 'want', 'this', '\\\\', 'ud83c', '\\\\', 'udd', '93', '?', '>', '>', 'URLTOK', '33929', 'en', 'positive', 'USRTOK', 'oh', 'hey', ',', 'after', 'seeing', 'the', 'picture', 'again', 'i', 'see', 'why', 'you', 'like', 'post-its', 'so', 'much', '.', 'make', 'a', 'breakfast', 'pizza', '!', 'then', 'u', \"won't\", 'b', 'hungry', '.', '33930', 'en', 'negative', 'USRTOK', 'then', 'on', 'his', 'drive', 'into', 'work', 'he', 'hit', 'a', 'big', 'deer', 'with', 'his', 'car', '.', 'let', 'the', 'christmas', 'season', 'celebrations', 'begin', '.', '33931', 'en', 'negative', 'USRTOK', 'its', 'worse', 'when', 'its', '30', 'and', 'over', '33932', 'en', 'positive', 'USRTOK', 'i', \"ain't\", 'actually', 'selling', 'dvds', 'man', 'x', ')', 'just', 'coding', 'a', 'website', 'capable', 'of', 'such', 'things', '.', 'finished', 'the', 'whole', 'registration', 'process', '33933', 'en', 'positive', 'child', 'i', 'can', 'do', 'everything', 'URLTOK', '33934', 'en', 'positive', 'son', 'has', 'no', 'legal', 'right', 'in', 'parents', '’', 'house', ',', 'can', 'stay', 'at', 'their', 'mercyelhi', '...', 'URLTOK', 'by', '#jantakareporter', 'via', 'USRTOK', '33935', 'en', 'positive', 'my', 'face', 'is', 'so', 'sunburn', '33936', 'en', 'positive', 'happy', 'sweet', 'sixteen', 'USRTOK', '\\\\', 'ud83d', '\\\\', 'ude', '1b', '\\\\', 'ud83c', '\\\\', 'udf', '89', '\\\\', 'ud83c', '\\\\', 'udf', '81i', 'hope', 'you', 'have', 'a', 'great', 'day', '33937', 'en', 'positive', 'hii', '.', 'a', '$', 'm', '(', 'youngam', '904', ')', 'URLTOK', '33938', 'en', 'positive', 'thanks', 'for', 'the', 'recent', 'follow', 'USRTOK', 'USRTOK', 'USRTOK', 'happy', 'to', 'connect', 'have', 'a', 'great', 'this', 'week', '33939', 'en', 'positive', 'watch', 'lesbian', 'movieisrobe', ',', 'lesson', 'about', 'to', 'commence', '▶', 'URLTOK', 'URLTOK', '33940', 'en', 'positive', 'URLTOK', 'pls', 'retweet', 'and', 'like', 'if', 'you', 'like', 'video', '33941', 'en', 'positive', 'USRTOK', 'wow', ',', 'how', '?', '33942', 'en', 'positive', 'pathfinder', 'battleseadly', 'foes', 'minis', 'now', 'available', 'URLTOK', '#tabletop', 'URLTOK', '33943', 'en', 'positive', 'this', 'is', 'the', 'day', 'i', 'have', 'been', 'waiting', 'for', '...', 'my', 'last', 'day', 'of', 'unemployment', '!', '!', '!', '33944', 'en', 'negative', 'USRTOK', '12:36', '12', 'from', 'parkway', 'centre', 'to', 'middlesbrough', 'never', 'turned', 'up', ',', 'had', 'to', 'wait', '10', 'minutes', 'when', 'i', 'had', 'a', 'train', 'to', 'catch', '33945', 'en', 'positive', 'USRTOK', 'thank', 'youuuu', '.', '33946', 'en', 'positive', 'we', 'are', 'hiring', 'a', '#marketingmanager', '!', 'ping', 'us', 'if', 'you', 'think', 'you', 'have', 'what', 'it', 'takes', 'URLTOK', '#ai', '#bots', '#marketing', '#jobs', '\\\\', 'ud83d', '\\\\', 'ude', '4c', '33947', 'en', 'positive', 'i', 'love', 'this', '#watch', '⌚', '️', 'dropped', 'hours', ',', 'grey', '#twt247', '\\\\', 'ud83d', '\\\\', 'udecd', '➡', '️', 'URLTOK', 'hours', ',', 'grey', 'URLTOK', '#gift', '#watch', '33948', 'en', 'positive', 'injuriesanny', 'trevathan', 'done', 'for', 'remainder', 'of', 'year', 'URLTOK', '33949', 'en', 'positive', 'german', 'union', 'leader', 'says', 'will', 'fight', 'for', 'plants', 'in', 'tata-thyssen', 'mergeruesseldorf', ',', 'germany', '…', 'URLTOK', '#news', '#reuters', '33950', 'en', 'positive', 'hello', '..', 'everyone', 'shut', 'up', '(', 'fatierza', ')', 'URLTOK', '33951', 'en', 'positive', 'im', 'not', 'ready', 'URLTOK', '33952', 'en', 'negative', 'am', 'i', 'going', 'to', 'have', 'to', 'turn', 'this', 'alarm', 'back', 'on', 'again', ':/', 'URLTOK', '33953', 'en', 'positive', '#ooc', \"you're\", 'welcome', '.', 'URLTOK', '33954', 'en', 'negative', 'USRTOK', 'USRTOK', 'district', 'in', 'aleppo', '33955', 'en', 'positive', 'thats', 'a', 'cool', 'shirt', '!', 'USRTOK', 'URLTOK', '33956', 'en', 'positive', 'i', 'love', 'waking', 'up', 'every', '20', 'minutes', 'throughout', 'the', 'night', '33957', 'en', 'positive', 'wonder', 'if', 'he', 'will', 'be', 'checking', 'with', 'his', 'mates', 'in', 'the', 'farc', 'rebels', '?', 'URLTOK', '33958', 'en', 'positive', 'state', 'to', 'implement', 'therapy', 'cuts', 'for', 'disabled', 'children', 'next', 'month', '-', 'houston', 'chronicleallas', '…', 'URLTOK', '#handicapped', '#love', '33959', 'en', 'negative', 'i', 'want', 'to', 'make', 'art', 'that', 'is', 'not', 'related', 'to', 'school', 'but', 'no', 'time', 'em', 'sad', '33960', 'en', 'negative', 'USRTOK', 'i', 'coukd', 'spend', 'all', 'my', 'day', 'at', 'home', ',', 'resting', 'and', 'still', 'feel', 'tired', '33961', 'en', 'positive', '“', 'opec', 'could', 'lose', 'market', 'share', 'in', 'a', 'world', 'awash', 'in', 'oilon', 'pittis', '”', 'URLTOK', '#energy', '#oil', '33962', 'en', 'positive', 'i', 'just', 'realized', 'that', \"sony's\", 'ffxv-themed', 'headphones', 'are', 'perfect', 'match', 'for', 'the', 'luna', 'edition', 'ps4', '!', '^', '_', '^', 'i', 'hope', 'it', 'comes', 'out', 'in', 'our', 'country', '!', '33963', 'en', 'negative', 'USRTOK', 'maybe', 'it', 'will', 'actually', 'let', 'me', 'post', ',', 'unlike', 'the', 'actual', 'twitter', 'client', '.', '33964', 'en', 'positive', 'love', 'it', 'mayward', 'twogetherinmasbate', 'URLTOK', '33965', 'en', 'positive', 'someone', 'pls', 'shoot', 'me', 'before', 'i', 'have', 'to', 'work', 'this', '6', 'hour', 'cash', 'shift', '33966', 'en', 'positive', 'manrepellerid', 'someone', 'say', 'pink', 'man', 'repeller', 'hats', '!', '?', 'URLTOK', 'URLTOK', '33967', 'en', 'negative', 'shooting', 'tomorrow', '!', '!', '33968', 'en', 'positive', 'USRTOK', 'fight', 'amongst', 'themselves', 'until', 'their', 'enemies', 'get', 'bored', 'of', 'killing', 'the', 'incompetent', 'mess', '?', 'sounds', 'like', 'skaven', 'business', 'as', 'usual', '33969', 'en', 'positive', 'tell', 'him', 'happy', 'birthday', 'too', 'bro', 'and', 'how', 'old', 'is', 'he', 'now', '?', 'URLTOK', '33970', 'en', 'positive', 'i', 'liked', 'a', 'USRTOK', 'video', 'from', 'USRTOK', 'URLTOK', 'quién', 'besa', 'mejor', '?', '-', 'con', 'sayoyyi', 'most', 'likely', 'to', 'tag', '33971', 'en', 'positive', 'USRTOK', 'sure', '!', '!', \"i'll\", 'try', 'to', 'get', 'to', 'it', 'this', 'week', '/', 'maybe', 'weekend', '33972', 'en', 'negative', 'sooooooo', 'USRTOK', 'and', 'i', 'probably', \"can't\", 'get', 'wifi', '/', 'phone', 'plans', 'until', 'after', 'the', 'new', 'year', 'lol', 'rip', 'i', 'hate', 'everything', '33973', 'en', 'positive', 'USRTOK', 'that', 'last', 'photo', 'thank', 'you', 'ave', 'but', 'i', 'love', 'you', 'so', 'much', '!', '!', '\\\\', 'ud83d', '\\\\', 'ude', '0b', '\\\\', 'ud83d', '\\\\', 'ude', '18', '33974', 'en', 'negative', 'fantastic', 'beasts', 'was', 'so', 'good', \"can't\", 'help', 'but', 'cry', 'whilst', 'watching', 'because', 'jk', \"rowling's\", 'imagination', 'is', 'overwhelming', '33975', 'en', 'positive', 'done', '33976', 'en', 'positive', 'thanks', 'for', 'the', 'recent', 'follow', 'USRTOK', 'USRTOK', 'USRTOK', 'happy', 'to', 'connect', 'have', 'a', 'great', 'tuesday', '.', '>', '>', 'URLTOK', '33977', 'en', 'positive', '#9eco', 'brothers', 'pan', 'organizer', 'rack', ',', 'bronzeeco', 'brothers', 'pan', 'organizer', 'rack', ',', 'bronze', 'by', 'deco', 'brothers', '…', 'URLTOK', '#storage', '33978', 'en', 'negative', 'i', 'thought', 'the', 'feeling', 'is', 'mutual', '33979', 'en', 'positive', 'USRTOK', 'the', 'reaper', 'is', 'your', 'friend', '33980', 'en', 'negative', 'had', '200-400', 'viewers', 'constantly', 'the', 'last', '3', 'days', 'i', 'best', 'get', 'a', 'sub', 'button', 'soon', '33981', 'en', 'negative', 'so', 'sad', 'URLTOK', '33982', 'en', 'positive', 'USRTOK', 'freudian', 'mishear', '33983', 'en', 'positive', 'USRTOK', 'USRTOK', '-', 'translation', '...', 'i', 'want', 'you', 'to', 'do', 'the', 'research', 'for', 'me', '...', 'liberalism', 'means', 'knowing', 'even', 'when', 'you', \"don't\", 'know', '33984', 'en', 'positive', 'how', 'cute', 'is', 'growlithe', '!', '!', '!', 'got', 'this', 'nice', 'little', 'poke', 'haul', 'at', 'the', 'kyoto', 'pokemon', 'centre', 'today', '#pokemon', '…', 'URLTOK', '33985', 'en', 'positive', 'thanks', 'for', 'the', 'recent', 'follow', 'USRTOK', 'USRTOK', 'happy', 'to', 'connect', 'have', 'a', 'great', 'tuesday', '.', '>', '>', 'want', 'this', '\\\\', 'ud83c', '\\\\', 'udd', '93', '?', 'URLTOK', '33986', 'en', 'positive', 'its', '6am', ',', 'my', 'classes', 'ends', 'at', '6pm', 'and', 'im', 'already', 'feeling', 'dead', 'inside', '33987', 'en', 'positive', 'stats', 'for', 'the', 'day', 'have', 'arrived', '.', '2', 'new', 'followers', 'and', 'no', 'unfollowers', 'via', 'URLTOK', '33988', 'en', 'negative', 'USRTOK', \"it's\", 'time', 'for', 'kamilah', 'and', 'helen', 'to', 'start', 'working', 'together', 'w', '/', 'o', 'kane', 'please', '33989', 'en', 'positive', 'this', 'is', 'a', 'plan', 'i', 'can', 'support', '.', 'URLTOK', '33990', 'en', 'positive', 'all', 'clean', 'new', 'brakes', 'just', 'wait', 'for', 'new', 'ignition', 'now', 'URLTOK', '33991', 'en', 'negative', 'USRTOK', 'thank', 'you', 'for', 'your', 'words', ',', 'they', 'will', 'never', 'be', 'forgotten', '33992', 'en', 'positive', 'popular', 'on', '500px', 'aily', 'shot', ':', 'the', 'first', 'flood', 'by', 'sergebcd', 'URLTOK', '33993', 'en', 'positive', \"that's\", 'all', 'we', 'need', '!', '#sold', 'URLTOK', '33994', 'en', 'positive', 'USRTOK', 'USRTOK', 'absolutely', '33995', 'en', 'positive', 'USRTOK', 'btw', ',', 'congratulations', 'aisha', '...', 'transition', 'from', 'neem', 'daacter', 'tou', 'complete', 'doctor', 'mubaarak', '.', 'may', 'you', 'get', 'more', 'success', '.', '33996', 'en', 'negative', 'USRTOK', 'USRTOK', '..', 'horrible', '33997', 'en', 'positive', 'rt', 'nythealthespite', 'the', 'problems', 'nightmares', 'can', 'cause', ',', 'sleeping', 'and', 'having', 'them', 'is', 'better', 'than', 'not', 'sleeping', 'URLTOK']\n"
     ]
    }
   ],
   "source": [
    "# TODO Load distant-supervised data\n",
    "# train it with two-layer CNN model\n",
    "# pass the weight to next two layer CNN model\n",
    "print(len(x_test_sentence))\n",
    "x_train_distance, x_test_distance = process_tweet(x_train_distance, x_test_distance, final_embeddings.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To save memory\n",
    "# del x_train_sentence\n",
    "# del x_test_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 2063)\n",
      "(?, 2063, 52, 1)\n",
      "CNN filter (4, 52, 1, 200)\n",
      "CNN filter (3, 1, 200, 200)\n",
      "(?, 1027, 1, 200)\n",
      "h_pool_flat (?, 200)\n",
      "scores (?, 3)\n",
      "predictions (?,)\n",
      "losses (?,)\n"
     ]
    }
   ],
   "source": [
    "# put Word2Vec on 590 million English Tweets using 52 dimensions.\n",
    "vocabulary_size = final_embeddings.shape[0]\n",
    "embedding_size = final_embeddings.shape[1]  # Dimension of the embedding vector.\n",
    "graph = tf.Graph()\n",
    "\n",
    "sequence_length=x_train_distance.shape[1]\n",
    "num_classes=3\n",
    "\n",
    "# filter_sizes: The number of words we want our convolutional filters to cover. \n",
    "# We will have num_filters for each size specified here. \n",
    "# For example, [3, 4, 5] means that we will have filters that slide over 3, 4 and 5 words respectively, for a total of 3 * num_filters filters.\n",
    "first_filter_sizes = [4]\n",
    "first_pool_window_sizes = [4]\n",
    "first_pool_strides = [2]\n",
    "\n",
    "\n",
    "second_filter_window_sizes = [3]\n",
    "num_filters = 200\n",
    "\n",
    "# No L2 norm\n",
    "l2_reg_lambda=0.0\n",
    "\n",
    "with graph.as_default():\n",
    "    input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "    input_y = tf.placeholder(tf.int64, [None], name=\"input_y\")\n",
    "    with tf.device('/cpu:0'):\n",
    "        embeddings = tf.Variable(tf.constant(0.0, shape=[vocabulary_size, embedding_size]),\n",
    "                        trainable=False, name=\"embedding\")\n",
    "\n",
    "        embedding_placeholder = tf.placeholder(tf.float32, [vocabulary_size, embedding_size], name='word_embedding_placeholder')\n",
    "        embedding_init = embeddings.assign(embedding_placeholder)  # assign exist word embeddings\n",
    "\n",
    "        embedded_chars = tf.nn.embedding_lookup(embeddings, input_x)\n",
    "    embedded_chars_expanded = tf.expand_dims(embedded_chars, -1)\n",
    "    print(input_x.shape)\n",
    "    print(embedded_chars_expanded.shape)\n",
    "    # Keeping track of l2 regularization loss (optional)\n",
    "    l2_loss = tf.constant(0.0)\n",
    "    \n",
    "     # Add variable initializer.\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    \n",
    "    # Create first cnn : a convolution + maxpool layer for each filter size    \n",
    "    # 1st Convolution Layer\n",
    "    for i, first_filter_size in enumerate(first_filter_sizes):\n",
    "        with tf.name_scope(\"conv-maxpool-1\"):\n",
    "            # Convolution Layer\n",
    "            filter_shape = [first_filter_size, embedding_size, 1, num_filters]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "            print(\"CNN filter\", W.shape)\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "            conv = tf.nn.conv2d(\n",
    "                embedded_chars_expanded,\n",
    "                W,\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding=\"VALID\",\n",
    "                name=\"conv\")\n",
    "            # Apply nonlinearity\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "            # Maxpooling over the outputs and transform for next layer, so the \"channel\" of convolution \n",
    "            # will become \"input_width\" for next layer\n",
    "#             pooled = tf.transpose(tf.nn.max_pool(\n",
    "#                 h,\n",
    "#                 ksize=[1, first_pool_window_sizes[i], 1, 1],\n",
    "#                 strides=[1, first_pool_strides[i], 1, 1],\n",
    "#                 padding='VALID',\n",
    "#                 name=\"pool\"), perm=[0, 1, 3, 2])\n",
    "            pooled = tf.nn.max_pool(\n",
    "                h,\n",
    "                ksize=[1, first_pool_window_sizes[i], 1, 1],\n",
    "                strides=[1, first_pool_strides[i], 1, 1],\n",
    "                padding='VALID',\n",
    "                name=\"pool\")\n",
    "#     print(\"conv1\", conv.shape)\n",
    "#     print(\"h1\", h.shape)\n",
    "#     print(\"pooled1\", pooled_1.shape)\n",
    "    \n",
    "    # 2nd Convolutional Layer\n",
    "#     for i, second_filter_size in enumerate(second_filter_window_sizes):\n",
    "#         with tf.name_scope(\"conv-maxpool-2\"):\n",
    "#             # Convolution Layer\n",
    "#             filter_shape = [second_filter_size, num_filters, 1, num_filters]\n",
    "#             W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "#             print(\"CNN filter\", W.shape)\n",
    "#             b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "#             conv = tf.nn.conv2d(\n",
    "#                 pooled,\n",
    "#                 W,\n",
    "#                 strides=[1, 1, 1, 1],\n",
    "#                 padding=\"VALID\",\n",
    "#                 name=\"conv\")\n",
    "#             # Apply nonlinearity\n",
    "#             h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "#             # Maxpooling over the outputs and transform for next layer, so the \"channel\" of convolution \n",
    "#             # will become \"input_width\" for next layer\n",
    "#             pooled = tf.nn.max_pool(\n",
    "#                 h,\n",
    "#                 ksize=[1, h.shape[1], 1, 1],\n",
    "#                 strides=[1, 1, 1, 1],\n",
    "#                 padding='VALID',\n",
    "#                 name=\"pool\")\n",
    "    for i, second_filter_size in enumerate(second_filter_window_sizes):\n",
    "        with tf.name_scope(\"conv-maxpool-2\"):\n",
    "            # Convolution Layer\n",
    "            filter_shape = [second_filter_size, 1, num_filters, num_filters]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "            print(\"CNN filter\", W.shape)\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "            conv = tf.nn.conv2d(\n",
    "                pooled,\n",
    "                W,\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding=\"VALID\",\n",
    "                name=\"conv\")\n",
    "            # Apply nonlinearity\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "            print(h.shape)\n",
    "            # Maxpooling over the outputs and transform for next layer, so the \"channel\" of convolution \n",
    "            # will become \"input_width\" for next layer\n",
    "            pooled = tf.nn.max_pool(\n",
    "                h,\n",
    "                ksize=[1, h.shape[1], 1, 1],\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding='VALID',\n",
    "                name=\"pool\")\n",
    " \n",
    "\n",
    "    h_pool_flat = tf.reshape(pooled, [-1, num_filters])  # flatten pooling layers\n",
    "    print(\"h_pool_flat\", h_pool_flat.shape)\n",
    "    \n",
    "    # Add dropout\n",
    "#     with tf.name_scope(\"dropout\"):\n",
    "#         self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "    # Final (unnormalized) scores and predictions\n",
    "    \n",
    "    # Fully connected hidden layer\n",
    "    with tf.name_scope(\"hidden\"):\n",
    "        with tf.variable_scope(\"hidden\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters, num_filters],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            out = tf.nn.relu(tf.nn.xw_plus_b(h_pool_flat, W, b))\n",
    "        \n",
    "    \n",
    "    with tf.name_scope(\"output\"):\n",
    "        with tf.variable_scope(\"output\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            scores = tf.nn.xw_plus_b(out, W, b, name=\"scores\")\n",
    "            print(\"scores\", scores.shape)\n",
    "            predictions = tf.argmax(scores, 1, name=\"predictions\")\n",
    "            print(\"predictions\", predictions.shape)\n",
    "\n",
    "\n",
    "    # Calculate mean cross-entropy loss\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=scores, labels=input_y)\n",
    "        print(\"losses\", losses.shape)\n",
    "        loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "    # Accuracy\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        correct_predictions = tf.equal(predictions, input_y)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-1/W:0/grad/hist is illegal; using conv-maxpool-1/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-1/W:0/grad/sparsity is illegal; using conv-maxpool-1/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-1/b:0/grad/hist is illegal; using conv-maxpool-1/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-1/b:0/grad/sparsity is illegal; using conv-maxpool-1/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/W:0/grad/hist is illegal; using conv-maxpool-2/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/W:0/grad/sparsity is illegal; using conv-maxpool-2/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/b:0/grad/hist is illegal; using conv-maxpool-2/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/b:0/grad/sparsity is illegal; using conv-maxpool-2/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name hidden/W:0/grad/hist is illegal; using hidden/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name hidden/W:0/grad/sparsity is illegal; using hidden/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name hidden/hidden/b:0/grad/hist is illegal; using hidden/hidden/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name hidden/hidden/b:0/grad/sparsity is illegal; using hidden/hidden/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/W:0/grad/hist is illegal; using output/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/W:0/grad/sparsity is illegal; using output/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/output/b:0/grad/hist is illegal; using output/output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/output/b:0/grad/sparsity is illegal; using output/output/b_0/grad/sparsity instead.\n",
      "Writing to /home/phejimlin/Documents/Machine-learning/Milestone2/runs/1511025828\n",
      "\n",
      "Current epoch:  0\n",
      "Current epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phejimlin/anaconda3/envs/tensorflow_3.5/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-19T01:24:24.238155: step 5, loss 0.469793, acc 0.835938, f1 0.761237\n",
      "2017-11-19T01:24:27.531674: step 10, loss 0.452523, acc 0.832031, f1 0.755747\n",
      "2017-11-19T01:24:30.652735: step 15, loss 0.430573, acc 0.849609, f1 0.780528\n",
      "2017-11-19T01:24:33.748674: step 20, loss 0.458488, acc 0.820312, f1 0.739337\n",
      "2017-11-19T01:24:36.863587: step 25, loss 0.417459, acc 0.863281, f1 0.799938\n",
      "2017-11-19T01:24:39.966193: step 30, loss 0.449762, acc 0.820312, f1 0.746475\n",
      "2017-11-19T01:24:43.064461: step 35, loss 0.436679, acc 0.820312, f1 0.746535\n",
      "2017-11-19T01:24:46.197001: step 40, loss 0.417404, acc 0.835938, f1 0.7684\n",
      "2017-11-19T01:24:49.323127: step 45, loss 0.431263, acc 0.830078, f1 0.753006\n",
      "2017-11-19T01:24:52.442455: step 50, loss 0.437058, acc 0.839844, f1 0.784143\n",
      "\n",
      "Evaluation:\n",
      "loss 0.441994, acc 0.831175, f1 0.754614\n",
      "\n",
      "2017-11-19T01:25:04.992139: step 55, loss 0.509859, acc 0.8125, f1 0.728448\n",
      "2017-11-19T01:25:08.146066: step 60, loss 0.392543, acc 0.851562, f1 0.78703\n",
      "2017-11-19T01:25:11.231423: step 65, loss 0.458524, acc 0.822266, f1 0.788314\n",
      "2017-11-19T01:25:14.332001: step 70, loss 0.451271, acc 0.832031, f1 0.755747\n",
      "2017-11-19T01:25:17.447562: step 75, loss 0.398608, acc 0.861328, f1 0.825268\n",
      "2017-11-19T01:25:20.559693: step 80, loss 0.519241, acc 0.804688, f1 0.7176\n",
      "2017-11-19T01:25:23.664936: step 85, loss 0.405958, acc 0.832031, f1 0.757672\n",
      "2017-11-19T01:25:26.832396: step 90, loss 0.391977, acc 0.833984, f1 0.764012\n",
      "2017-11-19T01:25:29.967168: step 95, loss 0.371594, acc 0.845703, f1 0.791195\n",
      "2017-11-19T01:25:33.050638: step 100, loss 0.394035, acc 0.847656, f1 0.777765\n",
      "\n",
      "Evaluation:\n",
      "loss 0.402584, acc 0.845417, f1 0.818545\n",
      "\n",
      "2017-11-19T01:25:45.146592: step 105, loss 0.438951, acc 0.832031, f1 0.821082\n",
      "2017-11-19T01:25:48.240774: step 110, loss 0.402954, acc 0.820312, f1 0.761205\n",
      "2017-11-19T01:25:51.331852: step 115, loss 0.389513, acc 0.833984, f1 0.763941\n",
      "2017-11-19T01:25:54.470537: step 120, loss 0.389864, acc 0.869141, f1 0.840662\n",
      "2017-11-19T01:25:57.596485: step 125, loss 0.418112, acc 0.824219, f1 0.748574\n",
      "2017-11-19T01:26:00.695224: step 130, loss 0.359836, acc 0.861328, f1 0.831149\n",
      "2017-11-19T01:26:03.819931: step 135, loss 0.437036, acc 0.84375, f1 0.774165\n",
      "2017-11-19T01:26:06.925499: step 140, loss 0.412252, acc 0.818359, f1 0.774373\n",
      "2017-11-19T01:26:10.044285: step 145, loss 0.465313, acc 0.822266, f1 0.834444\n",
      "2017-11-19T01:26:13.132767: step 150, loss 0.342504, acc 0.863281, f1 0.816609\n",
      "\n",
      "Evaluation:\n",
      "loss 0.35978, acc 0.843805, f1 0.790278\n",
      "\n",
      "2017-11-19T01:26:25.270316: step 155, loss 0.380592, acc 0.845703, f1 0.780491\n",
      "2017-11-19T01:26:28.379608: step 160, loss 0.354183, acc 0.857422, f1 0.831893\n",
      "2017-11-19T01:26:31.484771: step 165, loss 0.497307, acc 0.826172, f1 0.747531\n",
      "2017-11-19T01:26:34.580203: step 170, loss 0.370119, acc 0.84375, f1 0.811632\n",
      "2017-11-19T01:26:37.669340: step 175, loss 0.388447, acc 0.859375, f1 0.796292\n",
      "2017-11-19T01:26:40.773160: step 180, loss 0.368326, acc 0.869141, f1 0.84947\n",
      "2017-11-19T01:26:43.885113: step 185, loss 0.439117, acc 0.837891, f1 0.763985\n",
      "2017-11-19T01:26:46.992737: step 190, loss 0.36579, acc 0.855469, f1 0.831524\n",
      "2017-11-19T01:26:50.112853: step 195, loss 0.427087, acc 0.837891, f1 0.767642\n",
      "2017-11-19T01:26:53.196686: step 200, loss 0.361229, acc 0.845703, f1 0.815293\n",
      "\n",
      "Evaluation:\n",
      "loss 0.334219, acc 0.862312, f1 0.833446\n",
      "\n",
      "2017-11-19T01:27:05.268343: step 205, loss 0.451994, acc 0.835938, f1 0.764997\n",
      "2017-11-19T01:27:08.405077: step 210, loss 0.329889, acc 0.865234, f1 0.850149\n",
      "2017-11-19T01:27:11.474295: step 215, loss 0.329765, acc 0.865234, f1 0.814158\n",
      "2017-11-19T01:27:14.623927: step 220, loss 0.3098, acc 0.880859, f1 0.863379\n",
      "2017-11-19T01:27:17.713401: step 225, loss 0.344278, acc 0.845703, f1 0.791962\n",
      "2017-11-19T01:27:20.842571: step 230, loss 0.500812, acc 0.826172, f1 0.747531\n",
      "2017-11-19T01:27:23.979706: step 235, loss 0.309776, acc 0.873047, f1 0.851165\n",
      "2017-11-19T01:27:34.330514: step 240, loss 0.353106, acc 0.853516, f1 0.853071\n",
      "2017-11-19T01:27:37.471144: step 245, loss 0.33566, acc 0.855469, f1 0.80308\n",
      "2017-11-19T01:27:40.617935: step 250, loss 0.304459, acc 0.888672, f1 0.872525\n",
      "\n",
      "Evaluation:\n",
      "loss 0.318333, acc 0.865177, f1 0.835655\n",
      "\n",
      "2017-11-19T01:27:53.052332: step 255, loss 0.323202, acc 0.861328, f1 0.849\n",
      "2017-11-19T01:27:56.250573: step 260, loss 0.474432, acc 0.84375, f1 0.775994\n",
      "2017-11-19T01:27:59.365126: step 265, loss 0.280259, acc 0.880859, f1 0.857755\n",
      "2017-11-19T01:28:02.514261: step 270, loss 0.299916, acc 0.880859, f1 0.876815\n",
      "2017-11-19T01:28:05.641150: step 275, loss 0.471283, acc 0.833984, f1 0.760415\n",
      "2017-11-19T01:28:08.785045: step 280, loss 0.312134, acc 0.867188, f1 0.835325\n",
      "2017-11-19T01:28:11.905063: step 285, loss 0.315908, acc 0.884766, f1 0.877266\n",
      "2017-11-19T01:28:15.011978: step 290, loss 0.32707, acc 0.857422, f1 0.829096\n",
      "2017-11-19T01:28:18.128247: step 295, loss 0.370956, acc 0.849609, f1 0.844255\n",
      "2017-11-19T01:28:21.241643: step 300, loss 0.316203, acc 0.865234, f1 0.823845\n",
      "\n",
      "Evaluation:\n",
      "loss 0.307731, acc 0.877028, f1 0.861232\n",
      "\n",
      "2017-11-19T01:28:33.589587: step 305, loss 0.27371, acc 0.904297, f1 0.888554\n",
      "2017-11-19T01:28:36.728076: step 310, loss 0.311411, acc 0.863281, f1 0.845081\n",
      "2017-11-19T01:28:39.844221: step 315, loss 0.42285, acc 0.833984, f1 0.758491\n",
      "2017-11-19T01:28:42.981334: step 320, loss 0.323879, acc 0.880859, f1 0.882079\n",
      "2017-11-19T01:28:46.136488: step 325, loss 0.293303, acc 0.882812, f1 0.865415\n",
      "2017-11-19T01:28:49.315328: step 330, loss 0.422352, acc 0.810547, f1 0.828001\n",
      "2017-11-19T01:28:52.455146: step 335, loss 0.268412, acc 0.900391, f1 0.882654\n",
      "2017-11-19T01:28:55.556153: step 340, loss 0.306759, acc 0.869141, f1 0.863282\n",
      "2017-11-19T01:28:58.700871: step 345, loss 0.302395, acc 0.875, f1 0.838811\n",
      "2017-11-19T01:29:01.818285: step 350, loss 0.296828, acc 0.875, f1 0.853984\n",
      "\n",
      "Evaluation:\n",
      "loss 0.304668, acc 0.881146, f1 0.875717\n",
      "\n",
      "2017-11-19T01:29:14.056750: step 355, loss 0.312914, acc 0.886719, f1 0.878054\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "num_epochs = 1\n",
    "\n",
    "num_checkpoints = 5\n",
    "print_train_every = 5\n",
    "evaluate_every = 50\n",
    "checkpoint_every = 10000000\n",
    "allow_soft_placement=True\n",
    "log_device_placement=False\n",
    "\n",
    "with graph.as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=allow_soft_placement,\n",
    "      log_device_placement=log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        \n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdadeltaOptimizer(1.0)\n",
    "        grads_and_vars = optimizer.compute_gradients(loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=num_checkpoints)\n",
    "\n",
    "#         # Write vocabulary\n",
    "#         vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              input_x: x_batch,\n",
    "              input_y: y_batch,\n",
    "            }\n",
    "            _, step, summaries, cur_loss, cur_accuracy, y_pred = sess.run(\n",
    "                [train_op, global_step, train_summary_op, loss, accuracy, predictions],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "#             print(y_pred)\n",
    "#             print(y_batch)\n",
    "            if step % print_train_every == 0:\n",
    "                f1 = f1_score(y_batch, y_pred, average = 'weighted')\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}, f1 {:g}\".format(time_str, step, cur_loss, cur_accuracy,\n",
    "                                                                     f1))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              input_x: x_batch,\n",
    "              input_y: y_batch,\n",
    "            }\n",
    "            step, summaries, cur_loss, cur_accuracy, y_pred = sess.run(\n",
    "                [global_step, dev_summary_op, loss, accuracy, predictions],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            f1 = f1_score(y_batch, y_pred, average = 'weighted')\n",
    "            print(\"Test\")\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}, f1 {:g}\".format(time_str, step, cur_loss, cur_accuracy,\n",
    "                                                                    f1))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "        \n",
    "        def dev_step_batch(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              input_x: x_batch,\n",
    "              input_y: y_batch,\n",
    "            }\n",
    "            step, summaries, cur_loss, cur_accuracy, y_pred = sess.run(\n",
    "                [global_step, dev_summary_op, loss, accuracy, predictions],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            f1 = f1_score(y_batch, y_pred, average = 'weighted')\n",
    "#             print(\"{}: step {}, loss {:g}, acc {:g}, f1 {:g}\".format(time_str, step, cur_loss, cur_accuracy,\n",
    "#                                                                     f1))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "            return cur_loss, cur_accuracy, f1\n",
    "        \n",
    "        \n",
    "        sess.run(embedding_init, feed_dict={embedding_placeholder: final_embeddings})\n",
    "        # Generate batches\n",
    "        batches = batch_iter(\n",
    "            list(zip(x_train_distance, y_train)), batch_size, num_epochs)\n",
    "        \n",
    "        batches_test = list(batch_iter(\n",
    "            list(zip(x_test_distance, y_test)), batch_size, 1))\n",
    "        \n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            \n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                total_loss=0\n",
    "                total_f1=0\n",
    "                total_accuracy=0\n",
    "                len_of_batch = int(len(batches_test))\n",
    "                for batch_test in batches_test:\n",
    "                    x_batch_test, y_batch_test = zip(*batch_test)\n",
    "                    cur_loss, cur_accuracy, cur_f1 = dev_step_batch(x_batch_test, y_batch_test, writer=dev_summary_writer)\n",
    "                    total_loss+=cur_loss\n",
    "                    total_accuracy+=cur_accuracy\n",
    "                    total_f1+=cur_f1\n",
    "                print(\"loss {:g}, acc {:g}, f1 {:g}\".format(total_loss/len_of_batch, total_accuracy/len_of_batch, total_f1/len_of_batch))\n",
    "                print(\"\")\n",
    "            if current_step % checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "        final_embeddings = embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.056935  ,  0.038956  ,  0.39726499, ..., -0.108269  ,\n",
       "         0.242364  ,  0.238135  ],\n",
       "       [-0.178036  ,  0.165739  ,  0.29103801, ..., -0.206843  ,\n",
       "         0.133855  ,  0.193499  ],\n",
       "       [-0.27868199, -0.081914  ,  0.222532  , ..., -0.30557701,\n",
       "        -0.094965  ,  0.31291401],\n",
       "       ..., \n",
       "       [-0.86243898, -0.106481  ,  0.77118099, ..., -0.72408199,\n",
       "        -0.133205  ,  1.04446006],\n",
       "       [-0.122979  ,  0.053025  ,  1.02682495, ...,  0.41790599,\n",
       "        -0.55734599,  0.42464599],\n",
       "       [-0.95815903,  0.023787  ,  1.03262699, ...,  0.24498899,\n",
       "        -0.36573601,  0.328704  ]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save embeddings\n",
    "np.save('final_embeddings_52', final_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings from previous work\n",
    "final_embeddings = np.load('./final_embeddings_52.npy')\n",
    "word_dict = {}\n",
    "with open('./data/embed_tweets_en_590M_52D_data/vocabulary_dict_52.pickle', 'rb') as myfile:\n",
    "    word_dict = pickle.load(myfile)\n",
    "# with open('./data/embed_tweets_en_200M_200D/vocabulary.pickle', 'rb') as myfile:\n",
    "#     word_dict = pickle.load(myfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20632\n",
      "53\n",
      "[\"there's\", 'a', 'lot', 'of', 'stupid', '$', 'h', '!', 't', 'out', 'there', ',', 'but', 'polling', 'trump', 'v', 'kanye', 'west', 'may', 'take', 'the', 'cake', '.', 'all', 'i', 'can', 'think', 'to', 'say', 'is', ':', '#', '$', '%', '#', '$', '%', '$', '#', '%', '#', '$', '%', '#', '$', '%', '#', '$', '#', '$', '#', '$', '%']\n"
     ]
    }
   ],
   "source": [
    "#Load label data\n",
    "x_train_sentence, y_train, x_test_sentence, y_test = load_data_and_labels('./data/supervised_data/en_full.tsv.txt', './data/supervised_data/en_test.tsv')\n",
    "print(len(x_test_sentence))\n",
    "x_train, x_test = process_tweet(x_train_sentence, x_test_sentence, final_embeddings.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 53)\n",
      "(?, 53, 52, 1)\n",
      "CNN filter (4, 52, 1, 200)\n",
      "CNN filter (3, 1, 200, 200)\n",
      "(?, 22, 1, 200)\n",
      "h_pool_flat (?, 200)\n",
      "scores (?, 3)\n",
      "predictions (?,)\n",
      "losses (?,)\n"
     ]
    }
   ],
   "source": [
    "# put Word2Vec on 590 million English Tweets using 52 dimensions.\n",
    "vocabulary_size = final_embeddings.shape[0]\n",
    "embedding_size = final_embeddings.shape[1]  # Dimension of the embedding vector.\n",
    "graph = tf.Graph()\n",
    "\n",
    "sequence_length=x_train.shape[1]\n",
    "num_classes=3\n",
    "\n",
    "# filter_sizes: The number of words we want our convolutional filters to cover. \n",
    "# We will have num_filters for each size specified here. \n",
    "# For example, [3, 4, 5] means that we will have filters that slide over 3, 4 and 5 words respectively, for a total of 3 * num_filters filters.\n",
    "first_filter_sizes = [4]\n",
    "first_pool_window_sizes = [4]\n",
    "first_pool_strides = [2]\n",
    "\n",
    "\n",
    "second_filter_window_sizes = [3]\n",
    "num_filters = 200\n",
    "\n",
    "# No L2 norm\n",
    "l2_reg_lambda=0.0\n",
    "\n",
    "with graph.as_default():\n",
    "    input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "    input_y = tf.placeholder(tf.int64, [None], name=\"input_y\")\n",
    "    with tf.device('/cpu:0'):\n",
    "        embeddings = tf.Variable(tf.constant(0.0, shape=[vocabulary_size, embedding_size]),\n",
    "                        trainable=False, name=\"embedding\")\n",
    "\n",
    "        embedding_placeholder = tf.placeholder(tf.float32, [vocabulary_size, embedding_size], name='word_embedding_placeholder')\n",
    "        embedding_init = embeddings.assign(embedding_placeholder)  # assign exist word embeddings\n",
    "\n",
    "        embedded_chars = tf.nn.embedding_lookup(embeddings, input_x)\n",
    "    embedded_chars_expanded = tf.expand_dims(embedded_chars, -1)\n",
    "    print(input_x.shape)\n",
    "    print(embedded_chars_expanded.shape)\n",
    "    # Keeping track of l2 regularization loss (optional)\n",
    "    l2_loss = tf.constant(0.0)\n",
    "    \n",
    "     # Add variable initializer.\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    \n",
    "    # Create first cnn : a convolution + maxpool layer for each filter size    \n",
    "    # 1st Convolution Layer\n",
    "    for i, first_filter_size in enumerate(first_filter_sizes):\n",
    "        with tf.name_scope(\"conv-maxpool-1\"):\n",
    "            # Convolution Layer\n",
    "            filter_shape = [first_filter_size, embedding_size, 1, num_filters]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "            print(\"CNN filter\", W.shape)\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "            conv = tf.nn.conv2d(\n",
    "                embedded_chars_expanded,\n",
    "                W,\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding=\"VALID\",\n",
    "                name=\"conv\")\n",
    "            # Apply nonlinearity\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "            # Maxpooling over the outputs and transform for next layer, so the \"channel\" of convolution \n",
    "            # will become \"input_width\" for next layer\n",
    "#             pooled = tf.transpose(tf.nn.max_pool(\n",
    "#                 h,\n",
    "#                 ksize=[1, first_pool_window_sizes[i], 1, 1],\n",
    "#                 strides=[1, first_pool_strides[i], 1, 1],\n",
    "#                 padding='VALID',\n",
    "#                 name=\"pool\"), perm=[0, 1, 3, 2])\n",
    "            pooled = tf.nn.max_pool(\n",
    "                h,\n",
    "                ksize=[1, first_pool_window_sizes[i], 1, 1],\n",
    "                strides=[1, first_pool_strides[i], 1, 1],\n",
    "                padding='VALID',\n",
    "                name=\"pool\")\n",
    "#     print(\"conv1\", conv.shape)\n",
    "#     print(\"h1\", h.shape)\n",
    "#     print(\"pooled1\", pooled_1.shape)\n",
    "    \n",
    "    # 2nd Convolutional Layer\n",
    "#     for i, second_filter_size in enumerate(second_filter_window_sizes):\n",
    "#         with tf.name_scope(\"conv-maxpool-2\"):\n",
    "#             # Convolution Layer\n",
    "#             filter_shape = [second_filter_size, num_filters, 1, num_filters]\n",
    "#             W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "#             print(\"CNN filter\", W.shape)\n",
    "#             b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "#             conv = tf.nn.conv2d(\n",
    "#                 pooled,\n",
    "#                 W,\n",
    "#                 strides=[1, 1, 1, 1],\n",
    "#                 padding=\"VALID\",\n",
    "#                 name=\"conv\")\n",
    "#             # Apply nonlinearity\n",
    "#             h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "#             # Maxpooling over the outputs and transform for next layer, so the \"channel\" of convolution \n",
    "#             # will become \"input_width\" for next layer\n",
    "#             pooled = tf.nn.max_pool(\n",
    "#                 h,\n",
    "#                 ksize=[1, h.shape[1], 1, 1],\n",
    "#                 strides=[1, 1, 1, 1],\n",
    "#                 padding='VALID',\n",
    "#                 name=\"pool\")\n",
    "    for i, second_filter_size in enumerate(second_filter_window_sizes):\n",
    "        with tf.name_scope(\"conv-maxpool-2\"):\n",
    "            # Convolution Layer\n",
    "            filter_shape = [second_filter_size, 1, num_filters, num_filters]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "            print(\"CNN filter\", W.shape)\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "            conv = tf.nn.conv2d(\n",
    "                pooled,\n",
    "                W,\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding=\"VALID\",\n",
    "                name=\"conv\")\n",
    "            # Apply nonlinearity\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "            print(h.shape)\n",
    "            # Maxpooling over the outputs and transform for next layer, so the \"channel\" of convolution \n",
    "            # will become \"input_width\" for next layer\n",
    "            pooled = tf.nn.max_pool(\n",
    "                h,\n",
    "                ksize=[1, h.shape[1], 1, 1],\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding='VALID',\n",
    "                name=\"pool\")\n",
    " \n",
    "\n",
    "    h_pool_flat = tf.reshape(pooled, [-1, num_filters])  # flatten pooling layers\n",
    "    print(\"h_pool_flat\", h_pool_flat.shape)\n",
    "    \n",
    "    # Add dropout\n",
    "#     with tf.name_scope(\"dropout\"):\n",
    "#         self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "    # Final (unnormalized) scores and predictions\n",
    "    \n",
    "    # Fully connected hidden layer\n",
    "    with tf.name_scope(\"hidden\"):\n",
    "        with tf.variable_scope(\"hidden\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters, num_filters],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            out = tf.nn.relu(tf.nn.xw_plus_b(h_pool_flat, W, b))\n",
    "        \n",
    "    \n",
    "    with tf.name_scope(\"output\"):\n",
    "        with tf.variable_scope(\"output\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            scores = tf.nn.xw_plus_b(out, W, b, name=\"scores\")\n",
    "            print(\"scores\", scores.shape)\n",
    "            predictions = tf.argmax(scores, 1, name=\"predictions\")\n",
    "            print(\"predictions\", predictions.shape)\n",
    "\n",
    "\n",
    "    # Calculate mean cross-entropy loss\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=scores, labels=input_y)\n",
    "        print(\"losses\", losses.shape)\n",
    "        loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "    # Accuracy\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        correct_predictions = tf.equal(predictions, input_y)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.6184128163515918&quot;).pbtxt = 'node {\\n  name: &quot;input_x&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: -1\\n        }\\n        dim {\\n          size: 53\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;input_y&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: -1\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Const&quot;\\n  op: &quot;Const&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 6140853\\n          }\\n          dim {\\n            size: 52\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;embedding&quot;\\n  op: &quot;VariableV2&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 6140853\\n        }\\n        dim {\\n          size: 52\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;embedding/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;embedding&quot;\\n  input: &quot;Const&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;embedding/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;embedding&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;word_embedding_placeholder&quot;\\n  op: &quot;Placeholder&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 6140853\\n        }\\n        dim {\\n          size: 52\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;embedding&quot;\\n  input: &quot;word_embedding_placeholder&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;embedding_lookup&quot;\\n  op: &quot;Gather&quot;\\n  input: &quot;embedding/read&quot;\\n  input: &quot;input_x&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;Tindices&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tparams&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_indices&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ExpandDims/dim&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: -1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ExpandDims&quot;\\n  op: &quot;ExpandDims&quot;\\n  input: &quot;embedding_lookup&quot;\\n  input: &quot;ExpandDims/dim&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tdim&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Const_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;init&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^embedding/Assign&quot;\\n  device: &quot;/device:CPU:0&quot;\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/truncated_normal/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\004\\\\000\\\\000\\\\0004\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\310\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/truncated_normal/mean&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/truncated_normal/stddev&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.10000000149011612\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/truncated_normal/TruncatedNormal&quot;\\n  op: &quot;TruncatedNormal&quot;\\n  input: &quot;conv-maxpool-1/truncated_normal/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/truncated_normal/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;conv-maxpool-1/truncated_normal/TruncatedNormal&quot;\\n  input: &quot;conv-maxpool-1/truncated_normal/stddev&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/truncated_normal&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;conv-maxpool-1/truncated_normal/mul&quot;\\n  input: &quot;conv-maxpool-1/truncated_normal/mean&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/W&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 4\\n        }\\n        dim {\\n          size: 52\\n        }\\n        dim {\\n          size: 1\\n        }\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/W/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;conv-maxpool-1/W&quot;\\n  input: &quot;conv-maxpool-1/truncated_normal&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-1/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/W/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;conv-maxpool-1/W&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-1/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 200\\n          }\\n        }\\n        float_val: 0.10000000149011612\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/b&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/b/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;conv-maxpool-1/b&quot;\\n  input: &quot;conv-maxpool-1/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-1/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/b/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;conv-maxpool-1/b&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-1/b&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/conv&quot;\\n  op: &quot;Conv2D&quot;\\n  input: &quot;ExpandDims&quot;\\n  input: &quot;conv-maxpool-1/W/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;VALID&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;conv-maxpool-1/conv&quot;\\n  input: &quot;conv-maxpool-1/b/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;conv-maxpool-1/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/pool&quot;\\n  op: &quot;MaxPool&quot;\\n  input: &quot;conv-maxpool-1/relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;ksize&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 4\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;VALID&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 2\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/truncated_normal/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\003\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\310\\\\000\\\\000\\\\000\\\\310\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/truncated_normal/mean&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/truncated_normal/stddev&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.10000000149011612\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/truncated_normal/TruncatedNormal&quot;\\n  op: &quot;TruncatedNormal&quot;\\n  input: &quot;conv-maxpool-2/truncated_normal/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/truncated_normal/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;conv-maxpool-2/truncated_normal/TruncatedNormal&quot;\\n  input: &quot;conv-maxpool-2/truncated_normal/stddev&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/truncated_normal&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;conv-maxpool-2/truncated_normal/mul&quot;\\n  input: &quot;conv-maxpool-2/truncated_normal/mean&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/W&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 3\\n        }\\n        dim {\\n          size: 1\\n        }\\n        dim {\\n          size: 200\\n        }\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/W/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;conv-maxpool-2/W&quot;\\n  input: &quot;conv-maxpool-2/truncated_normal&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-2/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/W/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;conv-maxpool-2/W&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-2/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 200\\n          }\\n        }\\n        float_val: 0.10000000149011612\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/b&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/b/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;conv-maxpool-2/b&quot;\\n  input: &quot;conv-maxpool-2/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-2/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/b/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;conv-maxpool-2/b&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-2/b&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/conv&quot;\\n  op: &quot;Conv2D&quot;\\n  input: &quot;conv-maxpool-1/pool&quot;\\n  input: &quot;conv-maxpool-2/W/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;VALID&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;conv-maxpool-2/conv&quot;\\n  input: &quot;conv-maxpool-2/b/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;conv-maxpool-2/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/pool&quot;\\n  op: &quot;MaxPool&quot;\\n  input: &quot;conv-maxpool-2/relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;ksize&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 22\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;VALID&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\310\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;conv-maxpool-2/pool&quot;\\n  input: &quot;Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\310\\\\000\\\\000\\\\000\\\\310\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.12247448414564133\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.12247448414564133\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform/max&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform/mul&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 200\\n        }\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden/W&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden/W&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 200\\n          }\\n        }\\n        float_val: 0.10000000149011612\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/b&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/b/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden/hidden/b&quot;\\n  input: &quot;hidden/hidden/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/hidden/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/b/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden/hidden/b&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/hidden/b&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/L2Loss&quot;\\n  op: &quot;L2Loss&quot;\\n  input: &quot;hidden/W/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Const_1&quot;\\n  input: &quot;hidden/hidden/L2Loss&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/L2Loss_1&quot;\\n  op: &quot;L2Loss&quot;\\n  input: &quot;hidden/hidden/b/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/add_1&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden/hidden/add&quot;\\n  input: &quot;hidden/hidden/L2Loss_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/xw_plus_b/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Reshape&quot;\\n  input: &quot;hidden/W/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/xw_plus_b&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;hidden/hidden/xw_plus_b/MatMul&quot;\\n  input: &quot;hidden/hidden/b/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;hidden/hidden/xw_plus_b&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\310\\\\000\\\\000\\\\000\\\\003\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.17192047834396362\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.17192047834396362\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;output/W/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;output/W/Initializer/random_uniform/max&quot;\\n  input: &quot;output/W/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;output/W/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;output/W/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;output/W/Initializer/random_uniform/mul&quot;\\n  input: &quot;output/W/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 200\\n        }\\n        dim {\\n          size: 3\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;output/W&quot;\\n  input: &quot;output/W/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;output/W&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        float_val: 0.10000000149011612\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/b&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 3\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/b/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;output/output/b&quot;\\n  input: &quot;output/output/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/output/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/b/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;output/output/b&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/output/b&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/L2Loss&quot;\\n  op: &quot;L2Loss&quot;\\n  input: &quot;output/W/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden/hidden/add_1&quot;\\n  input: &quot;output/output/L2Loss&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/L2Loss_1&quot;\\n  op: &quot;L2Loss&quot;\\n  input: &quot;output/output/b/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/add_1&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;output/output/add&quot;\\n  input: &quot;output/output/L2Loss_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/scores/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;hidden/hidden/Relu&quot;\\n  input: &quot;output/W/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/scores&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;output/output/scores/MatMul&quot;\\n  input: &quot;output/output/b/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/predictions/dimension&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/predictions&quot;\\n  op: &quot;ArgMax&quot;\\n  input: &quot;output/output/scores&quot;\\n  input: &quot;output/output/predictions/dimension&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;output_type&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;input_y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  op: &quot;SparseSoftmaxCrossEntropyWithLogits&quot;\\n  input: &quot;output/output/scores&quot;\\n  input: &quot;input_y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tlabels&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/Mean&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  input: &quot;loss/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/mul/x&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;loss/mul/x&quot;\\n  input: &quot;output/output/add_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;loss/Mean&quot;\\n  input: &quot;loss/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;accuracy/Equal&quot;\\n  op: &quot;Equal&quot;\\n  input: &quot;output/output/predictions&quot;\\n  input: &quot;input_y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;accuracy/Cast&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;accuracy/Equal&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_BOOL\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;accuracy/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;accuracy/accuracy&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;accuracy/Cast&quot;\\n  input: &quot;accuracy/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nversions {\\n  producer: 24\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.6184128163515918&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_graph(graph.as_graph_def())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-1/W:0/grad/hist is illegal; using conv-maxpool-1/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-1/W:0/grad/sparsity is illegal; using conv-maxpool-1/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-1/b:0/grad/hist is illegal; using conv-maxpool-1/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-1/b:0/grad/sparsity is illegal; using conv-maxpool-1/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/W:0/grad/hist is illegal; using conv-maxpool-2/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/W:0/grad/sparsity is illegal; using conv-maxpool-2/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/b:0/grad/hist is illegal; using conv-maxpool-2/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/b:0/grad/sparsity is illegal; using conv-maxpool-2/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name hidden/W:0/grad/hist is illegal; using hidden/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name hidden/W:0/grad/sparsity is illegal; using hidden/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name hidden/hidden/b:0/grad/hist is illegal; using hidden/hidden/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name hidden/hidden/b:0/grad/sparsity is illegal; using hidden/hidden/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/W:0/grad/hist is illegal; using output/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/W:0/grad/sparsity is illegal; using output/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/output/b:0/grad/hist is illegal; using output/output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/output/b:0/grad/sparsity is illegal; using output/output/b_0/grad/sparsity instead.\n",
      "Writing to /home/phejimlin/Documents/Machine-learning/Milestone2/runs/1511026442\n",
      "\n",
      "Current epoch:  0\n",
      "Current epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phejimlin/anaconda3/envs/tensorflow_3.5/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-19T01:34:08.325129: step 5, loss 1.0872, acc 0.416992, f1 0.275571\n",
      "2017-11-19T01:34:08.620653: step 10, loss 1.01904, acc 0.446289, f1 0.406248\n",
      "2017-11-19T01:34:08.921894: step 15, loss 1.04824, acc 0.410156, f1 0.283891\n",
      "Current epoch:  1\n",
      "2017-11-19T01:34:09.414834: step 20, loss 1.05712, acc 0.432617, f1 0.264631\n",
      "2017-11-19T01:34:09.716680: step 25, loss 1.04282, acc 0.443359, f1 0.401082\n",
      "2017-11-19T01:34:10.021661: step 30, loss 0.999571, acc 0.466797, f1 0.412625\n",
      "2017-11-19T01:34:10.322073: step 35, loss 0.974774, acc 0.491211, f1 0.44286\n",
      "Current epoch:  2\n",
      "2017-11-19T01:34:10.599683: step 40, loss 1.08746, acc 0.415039, f1 0.25506\n",
      "2017-11-19T01:34:10.906662: step 45, loss 1.0668, acc 0.420898, f1 0.253899\n",
      "2017-11-19T01:34:11.237394: step 50, loss 0.972183, acc 0.478516, f1 0.412457\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:34:11.918859: step 50, loss 1.01784, acc 0.427443, f1 0.358912\n",
      "\n",
      "Current epoch:  3\n",
      "2017-11-19T01:34:12.258805: step 55, loss 0.995432, acc 0.445312, f1 0.340421\n",
      "2017-11-19T01:34:12.559148: step 60, loss 0.994544, acc 0.454102, f1 0.350795\n",
      "2017-11-19T01:34:12.854314: step 65, loss 1.03023, acc 0.450195, f1 0.313969\n",
      "2017-11-19T01:34:13.163024: step 70, loss 1.03664, acc 0.427734, f1 0.28608\n",
      "Current epoch:  4\n",
      "2017-11-19T01:34:13.438817: step 75, loss 0.96271, acc 0.500977, f1 0.461667\n",
      "2017-11-19T01:34:13.722521: step 80, loss 0.957309, acc 0.500977, f1 0.452046\n",
      "2017-11-19T01:34:14.041693: step 85, loss 1.07408, acc 0.470703, f1 0.385252\n",
      "2017-11-19T01:34:14.307358: step 90, loss 0.994398, acc 0.47327, f1 0.348616\n",
      "Current epoch:  5\n",
      "2017-11-19T01:34:14.632421: step 95, loss 0.975137, acc 0.524414, f1 0.505917\n",
      "2017-11-19T01:34:14.915306: step 100, loss 0.957915, acc 0.509766, f1 0.469852\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:34:15.168981: step 100, loss 0.948987, acc 0.53068, f1 0.481472\n",
      "\n",
      "2017-11-19T01:34:15.492354: step 105, loss 1.03803, acc 0.412109, f1 0.266984\n",
      "Current epoch:  6\n",
      "2017-11-19T01:34:15.824780: step 110, loss 0.970892, acc 0.484375, f1 0.368283\n",
      "2017-11-19T01:34:16.136821: step 115, loss 0.968908, acc 0.518555, f1 0.51521\n",
      "2017-11-19T01:34:16.462357: step 120, loss 0.979052, acc 0.467773, f1 0.384332\n",
      "2017-11-19T01:34:16.783672: step 125, loss 0.956114, acc 0.473633, f1 0.357732\n",
      "Current epoch:  7\n",
      "2017-11-19T01:34:17.051441: step 130, loss 0.93862, acc 0.558594, f1 0.521179\n",
      "2017-11-19T01:34:17.356705: step 135, loss 0.97221, acc 0.512695, f1 0.455971\n",
      "2017-11-19T01:34:17.657035: step 140, loss 0.926279, acc 0.506836, f1 0.451467\n",
      "Current epoch:  8\n",
      "2017-11-19T01:34:17.953211: step 145, loss 1.06565, acc 0.44043, f1 0.295875\n",
      "2017-11-19T01:34:18.244864: step 150, loss 0.938063, acc 0.513672, f1 0.450928\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:34:18.516775: step 150, loss 0.974008, acc 0.488077, f1 0.46414\n",
      "\n",
      "2017-11-19T01:34:18.858047: step 155, loss 0.93645, acc 0.535156, f1 0.489007\n",
      "2017-11-19T01:34:19.174114: step 160, loss 0.944132, acc 0.538086, f1 0.494622\n",
      "Current epoch:  9\n",
      "2017-11-19T01:34:19.471707: step 165, loss 0.919573, acc 0.529297, f1 0.507432\n",
      "2017-11-19T01:34:19.778154: step 170, loss 1.01241, acc 0.436523, f1 0.328206\n",
      "2017-11-19T01:34:20.105878: step 175, loss 0.905502, acc 0.546875, f1 0.513044\n",
      "2017-11-19T01:34:20.397324: step 180, loss 0.896582, acc 0.581761, f1 0.556957\n",
      "Current epoch:  10\n",
      "2017-11-19T01:34:20.706466: step 185, loss 1.01394, acc 0.455078, f1 0.316549\n",
      "2017-11-19T01:34:21.027294: step 190, loss 0.901596, acc 0.550781, f1 0.511917\n",
      "2017-11-19T01:34:21.299232: step 195, loss 0.926975, acc 0.547852, f1 0.527054\n",
      "Current epoch:  11\n",
      "2017-11-19T01:34:21.646718: step 200, loss 1.11046, acc 0.495117, f1 0.390523\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:34:21.900868: step 200, loss 0.937926, acc 0.529808, f1 0.483317\n",
      "\n",
      "2017-11-19T01:34:22.208326: step 205, loss 0.897322, acc 0.553711, f1 0.512458\n",
      "2017-11-19T01:34:22.544126: step 210, loss 0.901383, acc 0.548828, f1 0.538983\n",
      "2017-11-19T01:34:22.860293: step 215, loss 0.903396, acc 0.538086, f1 0.468095\n",
      "Current epoch:  12\n",
      "2017-11-19T01:34:23.139414: step 220, loss 0.890583, acc 0.570312, f1 0.559781\n",
      "2017-11-19T01:34:23.456476: step 225, loss 0.959911, acc 0.475586, f1 0.36798\n",
      "2017-11-19T01:34:23.778528: step 230, loss 0.921475, acc 0.585938, f1 0.587208\n",
      "Current epoch:  13\n",
      "2017-11-19T01:34:24.073067: step 235, loss 0.940518, acc 0.53125, f1 0.476169\n",
      "2017-11-19T01:34:24.401737: step 240, loss 0.912489, acc 0.5625, f1 0.552701\n",
      "2017-11-19T01:34:24.681270: step 245, loss 0.921463, acc 0.53418, f1 0.492704\n",
      "2017-11-19T01:34:24.981987: step 250, loss 0.912191, acc 0.514648, f1 0.422733\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:34:25.222894: step 250, loss 0.918219, acc 0.533346, f1 0.447167\n",
      "\n",
      "Current epoch:  14\n",
      "2017-11-19T01:34:25.522704: step 255, loss 0.915383, acc 0.545898, f1 0.517283\n",
      "2017-11-19T01:34:25.805098: step 260, loss 0.907496, acc 0.543945, f1 0.498785\n",
      "2017-11-19T01:34:26.235573: step 265, loss 0.925565, acc 0.560547, f1 0.533298\n",
      "2017-11-19T01:34:26.550434: step 270, loss 0.899583, acc 0.573899, f1 0.541317\n",
      "Current epoch:  15\n",
      "2017-11-19T01:34:26.841765: step 275, loss 0.903861, acc 0.532227, f1 0.473636\n",
      "2017-11-19T01:34:27.143423: step 280, loss 0.950176, acc 0.501953, f1 0.428496\n",
      "2017-11-19T01:34:27.495829: step 285, loss 0.897727, acc 0.588867, f1 0.54633\n",
      "Current epoch:  16\n",
      "2017-11-19T01:34:27.808534: step 290, loss 0.919156, acc 0.526367, f1 0.476838\n",
      "2017-11-19T01:34:28.127219: step 295, loss 0.877488, acc 0.55957, f1 0.532517\n",
      "2017-11-19T01:34:28.446380: step 300, loss 0.83865, acc 0.613281, f1 0.592099\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:34:28.685699: step 300, loss 0.915855, acc 0.538871, f1 0.52448\n",
      "\n",
      "2017-11-19T01:34:29.005594: step 305, loss 0.91817, acc 0.538086, f1 0.479417\n",
      "Current epoch:  17\n",
      "2017-11-19T01:34:29.278016: step 310, loss 0.929584, acc 0.491211, f1 0.424782\n",
      "2017-11-19T01:34:29.564252: step 315, loss 0.870546, acc 0.577148, f1 0.55451\n",
      "2017-11-19T01:34:29.848348: step 320, loss 0.875194, acc 0.580078, f1 0.544144\n",
      "Current epoch:  18\n",
      "2017-11-19T01:34:30.122758: step 325, loss 1.03071, acc 0.504883, f1 0.412932\n",
      "2017-11-19T01:34:30.390807: step 330, loss 0.898447, acc 0.547852, f1 0.501978\n",
      "2017-11-19T01:34:30.680698: step 335, loss 0.896672, acc 0.540039, f1 0.487002\n",
      "2017-11-19T01:34:30.971637: step 340, loss 0.858957, acc 0.618164, f1 0.614327\n",
      "Current epoch:  19\n",
      "2017-11-19T01:34:31.245405: step 345, loss 0.891817, acc 0.542969, f1 0.499327\n",
      "2017-11-19T01:34:31.521850: step 350, loss 0.885438, acc 0.579102, f1 0.576477\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:34:31.759712: step 350, loss 0.956056, acc 0.535139, f1 0.493383\n",
      "\n",
      "2017-11-19T01:34:32.064683: step 355, loss 0.970108, acc 0.501953, f1 0.416331\n",
      "2017-11-19T01:34:32.340419: step 360, loss 0.852795, acc 0.603774, f1 0.577207\n",
      "Current epoch:  20\n",
      "2017-11-19T01:34:32.624513: step 365, loss 0.873804, acc 0.582031, f1 0.542106\n",
      "2017-11-19T01:34:32.924298: step 370, loss 0.861592, acc 0.604492, f1 0.599906\n",
      "2017-11-19T01:34:33.204392: step 375, loss 0.968607, acc 0.470703, f1 0.362287\n",
      "Current epoch:  21\n",
      "2017-11-19T01:34:33.482704: step 380, loss 0.868506, acc 0.580078, f1 0.565618\n",
      "2017-11-19T01:34:33.772558: step 385, loss 0.83339, acc 0.598633, f1 0.576194\n",
      "2017-11-19T01:34:34.071255: step 390, loss 0.90462, acc 0.558594, f1 0.551191\n",
      "2017-11-19T01:34:34.377286: step 395, loss 0.974178, acc 0.484375, f1 0.388559\n",
      "Current epoch:  22\n",
      "2017-11-19T01:34:34.706448: step 400, loss 0.873444, acc 0.566406, f1 0.546549\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:34:34.948393: step 400, loss 0.96449, acc 0.485508, f1 0.452044\n",
      "\n",
      "2017-11-19T01:34:35.288823: step 405, loss 0.861603, acc 0.595703, f1 0.580849\n",
      "2017-11-19T01:34:35.594610: step 410, loss 0.870799, acc 0.584961, f1 0.573705\n",
      "Current epoch:  23\n",
      "2017-11-19T01:34:35.876913: step 415, loss 0.917153, acc 0.511719, f1 0.443229\n",
      "2017-11-19T01:34:36.227132: step 420, loss 0.848156, acc 0.571289, f1 0.545682\n",
      "2017-11-19T01:34:36.500589: step 425, loss 0.890504, acc 0.548828, f1 0.501833\n",
      "2017-11-19T01:34:36.809821: step 430, loss 0.837654, acc 0.624023, f1 0.618837\n",
      "Current epoch:  24\n",
      "2017-11-19T01:34:37.120461: step 435, loss 0.8829, acc 0.56543, f1 0.523681\n",
      "2017-11-19T01:34:37.437103: step 440, loss 0.888531, acc 0.549805, f1 0.492788\n",
      "2017-11-19T01:34:37.731837: step 445, loss 0.867189, acc 0.579102, f1 0.571957\n",
      "2017-11-19T01:34:38.000383: step 450, loss 0.891537, acc 0.572327, f1 0.521777\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-19T01:34:38.231304: step 450, loss 0.935649, acc 0.532861, f1 0.53737\n",
      "\n",
      "Current epoch:  25\n",
      "2017-11-19T01:34:38.536918: step 455, loss 0.842692, acc 0.628906, f1 0.627046\n",
      "2017-11-19T01:34:38.829818: step 460, loss 0.849764, acc 0.601562, f1 0.576258\n",
      "2017-11-19T01:34:39.131532: step 465, loss 0.925045, acc 0.532227, f1 0.505834\n",
      "Current epoch:  26\n",
      "2017-11-19T01:34:39.410210: step 470, loss 0.818555, acc 0.62207, f1 0.605461\n",
      "2017-11-19T01:34:39.709668: step 475, loss 0.909168, acc 0.567383, f1 0.519754\n",
      "2017-11-19T01:34:40.027448: step 480, loss 0.84074, acc 0.56543, f1 0.538754\n",
      "2017-11-19T01:34:40.332590: step 485, loss 0.831996, acc 0.601562, f1 0.589298\n",
      "Current epoch:  27\n",
      "2017-11-19T01:34:40.640314: step 490, loss 0.93132, acc 0.530273, f1 0.456798\n",
      "2017-11-19T01:34:40.999655: step 495, loss 0.820706, acc 0.604492, f1 0.590872\n",
      "2017-11-19T01:34:41.320017: step 500, loss 0.858681, acc 0.587891, f1 0.578859\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:34:41.575994: step 500, loss 0.892872, acc 0.579633, f1 0.535039\n",
      "\n",
      "Current epoch:  28\n",
      "2017-11-19T01:34:41.889389: step 505, loss 0.81689, acc 0.616211, f1 0.587382\n",
      "2017-11-19T01:34:42.198577: step 510, loss 0.835295, acc 0.611328, f1 0.586252\n",
      "2017-11-19T01:34:42.487292: step 515, loss 0.868758, acc 0.585938, f1 0.564815\n",
      "2017-11-19T01:34:42.796544: step 520, loss 0.808504, acc 0.616211, f1 0.602551\n",
      "Current epoch:  29\n",
      "2017-11-19T01:34:43.073786: step 525, loss 0.901474, acc 0.524414, f1 0.463363\n",
      "2017-11-19T01:34:43.371473: step 530, loss 0.872914, acc 0.553711, f1 0.507668\n",
      "2017-11-19T01:34:43.666323: step 535, loss 0.84333, acc 0.594727, f1 0.580573\n",
      "2017-11-19T01:34:43.942794: step 540, loss 0.859639, acc 0.583333, f1 0.537937\n",
      "Current epoch:  30\n",
      "2017-11-19T01:34:44.274454: step 545, loss 0.828836, acc 0.59668, f1 0.57519\n",
      "2017-11-19T01:34:44.582252: step 550, loss 0.885333, acc 0.588867, f1 0.54726\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:34:44.842974: step 550, loss 0.920349, acc 0.544445, f1 0.551937\n",
      "\n",
      "2017-11-19T01:34:45.160492: step 555, loss 0.806423, acc 0.641602, f1 0.642161\n",
      "Current epoch:  31\n",
      "2017-11-19T01:34:45.467716: step 560, loss 0.885947, acc 0.539062, f1 0.469905\n",
      "2017-11-19T01:34:45.771140: step 565, loss 0.808338, acc 0.618164, f1 0.582699\n",
      "2017-11-19T01:34:46.087949: step 570, loss 0.881987, acc 0.553711, f1 0.521167\n",
      "2017-11-19T01:34:46.390466: step 575, loss 0.817405, acc 0.620117, f1 0.600273\n",
      "Current epoch:  32\n",
      "2017-11-19T01:34:46.698188: step 580, loss 0.826639, acc 0.56543, f1 0.527192\n",
      "2017-11-19T01:34:46.990915: step 585, loss 0.881128, acc 0.584961, f1 0.543414\n",
      "2017-11-19T01:34:47.318124: step 590, loss 0.807076, acc 0.614258, f1 0.604331\n",
      "Current epoch:  33\n",
      "2017-11-19T01:34:47.629386: step 595, loss 0.807251, acc 0.607422, f1 0.583409\n",
      "2017-11-19T01:34:47.929460: step 600, loss 0.864367, acc 0.529297, f1 0.490774\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:34:48.165310: step 600, loss 0.989709, acc 0.466072, f1 0.415521\n",
      "\n",
      "2017-11-19T01:34:48.452090: step 605, loss 0.831503, acc 0.607422, f1 0.587859\n",
      "2017-11-19T01:34:48.737743: step 610, loss 0.84146, acc 0.601562, f1 0.582316\n",
      "Current epoch:  34\n",
      "2017-11-19T01:34:49.036923: step 615, loss 0.827663, acc 0.594727, f1 0.561922\n",
      "2017-11-19T01:34:49.313236: step 620, loss 0.887191, acc 0.567383, f1 0.552693\n",
      "2017-11-19T01:34:49.632210: step 625, loss 0.815043, acc 0.592773, f1 0.550877\n",
      "2017-11-19T01:34:49.920267: step 630, loss 0.844319, acc 0.619497, f1 0.604317\n",
      "Current epoch:  35\n",
      "2017-11-19T01:34:50.220154: step 635, loss 0.840978, acc 0.605469, f1 0.599509\n",
      "2017-11-19T01:34:50.554033: step 640, loss 0.830982, acc 0.59082, f1 0.554859\n",
      "2017-11-19T01:34:50.863286: step 645, loss 0.792262, acc 0.639648, f1 0.61889\n",
      "Current epoch:  36\n",
      "2017-11-19T01:34:51.144305: step 650, loss 0.785135, acc 0.65625, f1 0.655166\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:34:51.382616: step 650, loss 0.947967, acc 0.520405, f1 0.496302\n",
      "\n",
      "2017-11-19T01:34:51.705974: step 655, loss 0.882936, acc 0.552734, f1 0.500903\n",
      "2017-11-19T01:34:51.987466: step 660, loss 0.785439, acc 0.639648, f1 0.625718\n",
      "2017-11-19T01:34:52.282745: step 665, loss 0.90779, acc 0.542969, f1 0.455329\n",
      "Current epoch:  37\n",
      "2017-11-19T01:34:52.581590: step 670, loss 0.758465, acc 0.654297, f1 0.642606\n",
      "2017-11-19T01:34:52.860301: step 675, loss 0.808516, acc 0.632812, f1 0.603323\n",
      "2017-11-19T01:34:53.166175: step 680, loss 0.969451, acc 0.479492, f1 0.392406\n",
      "Current epoch:  38\n",
      "2017-11-19T01:34:53.471749: step 685, loss 0.77778, acc 0.632812, f1 0.615373\n",
      "2017-11-19T01:34:53.800764: step 690, loss 0.775617, acc 0.661133, f1 0.663422\n",
      "2017-11-19T01:34:54.078704: step 695, loss 0.853819, acc 0.597656, f1 0.547144\n",
      "2017-11-19T01:34:54.370880: step 700, loss 0.805114, acc 0.584961, f1 0.5616\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:34:54.598897: step 700, loss 0.9549, acc 0.500921, f1 0.472402\n",
      "\n",
      "Current epoch:  39\n",
      "2017-11-19T01:34:54.921394: step 705, loss 0.797096, acc 0.625977, f1 0.614902\n",
      "2017-11-19T01:34:55.233104: step 710, loss 0.828767, acc 0.581055, f1 0.539793\n",
      "2017-11-19T01:34:55.548225: step 715, loss 0.823039, acc 0.619141, f1 0.580779\n",
      "2017-11-19T01:34:55.824483: step 720, loss 0.803892, acc 0.616352, f1 0.609864\n",
      "Current epoch:  40\n",
      "2017-11-19T01:34:56.118379: step 725, loss 0.811214, acc 0.611328, f1 0.589233\n",
      "2017-11-19T01:34:56.418021: step 730, loss 0.80104, acc 0.602539, f1 0.573009\n",
      "2017-11-19T01:34:56.702191: step 735, loss 0.769227, acc 0.657227, f1 0.642131\n",
      "Current epoch:  41\n",
      "2017-11-19T01:34:56.981021: step 740, loss 0.806579, acc 0.636719, f1 0.640859\n",
      "2017-11-19T01:34:57.255988: step 745, loss 0.85176, acc 0.59668, f1 0.556707\n",
      "2017-11-19T01:34:57.544280: step 750, loss 0.857098, acc 0.563477, f1 0.524202\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:34:57.774114: step 750, loss 1.01252, acc 0.477801, f1 0.443441\n",
      "\n",
      "2017-11-19T01:34:58.060517: step 755, loss 0.773361, acc 0.649414, f1 0.645141\n",
      "Current epoch:  42\n",
      "2017-11-19T01:34:58.359432: step 760, loss 0.745784, acc 0.670898, f1 0.66196\n",
      "2017-11-19T01:34:58.651888: step 765, loss 0.927224, acc 0.52832, f1 0.457373\n",
      "2017-11-19T01:34:58.962242: step 770, loss 0.776935, acc 0.650391, f1 0.650004\n",
      "Current epoch:  43\n",
      "2017-11-19T01:34:59.265107: step 775, loss 0.741742, acc 0.676758, f1 0.668381\n",
      "2017-11-19T01:34:59.556599: step 780, loss 0.824167, acc 0.592773, f1 0.562989\n",
      "2017-11-19T01:34:59.852010: step 785, loss 0.784734, acc 0.630859, f1 0.618877\n",
      "2017-11-19T01:35:00.156966: step 790, loss 0.77651, acc 0.621094, f1 0.60082\n",
      "Current epoch:  44\n",
      "2017-11-19T01:35:00.461791: step 795, loss 0.835535, acc 0.645508, f1 0.604921\n",
      "2017-11-19T01:35:00.768219: step 800, loss 0.773597, acc 0.642578, f1 0.646627\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:35:01.031659: step 800, loss 0.905675, acc 0.583075, f1 0.545211\n",
      "\n",
      "2017-11-19T01:35:01.305494: step 805, loss 0.772829, acc 0.630859, f1 0.603469\n",
      "2017-11-19T01:35:01.578440: step 810, loss 0.814021, acc 0.600629, f1 0.5757\n",
      "Current epoch:  45\n",
      "2017-11-19T01:35:01.896860: step 815, loss 0.729035, acc 0.669922, f1 0.654429\n",
      "2017-11-19T01:35:02.193521: step 820, loss 0.853223, acc 0.570312, f1 0.54405\n",
      "2017-11-19T01:35:02.459188: step 825, loss 0.748894, acc 0.666016, f1 0.658208\n",
      "Current epoch:  46\n",
      "2017-11-19T01:35:02.753842: step 830, loss 0.770761, acc 0.624023, f1 0.588217\n",
      "2017-11-19T01:35:03.038953: step 835, loss 0.771401, acc 0.65332, f1 0.640142\n",
      "2017-11-19T01:35:03.322868: step 840, loss 0.818336, acc 0.605469, f1 0.585245\n",
      "2017-11-19T01:35:03.600329: step 845, loss 0.75022, acc 0.679688, f1 0.655576\n",
      "Current epoch:  47\n",
      "2017-11-19T01:35:03.868741: step 850, loss 0.896091, acc 0.560547, f1 0.508942\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:35:04.137544: step 850, loss 0.895105, acc 0.584335, f1 0.526259\n",
      "\n",
      "2017-11-19T01:35:04.440871: step 855, loss 0.81038, acc 0.602539, f1 0.584678\n",
      "2017-11-19T01:35:04.748298: step 860, loss 0.757887, acc 0.646484, f1 0.622302\n",
      "Current epoch:  48\n",
      "2017-11-19T01:35:05.055874: step 865, loss 0.816142, acc 0.574219, f1 0.53967\n",
      "2017-11-19T01:35:05.353230: step 870, loss 0.742195, acc 0.68457, f1 0.682029\n",
      "2017-11-19T01:35:05.643728: step 875, loss 0.789306, acc 0.616211, f1 0.595173\n",
      "2017-11-19T01:35:05.928122: step 880, loss 0.76201, acc 0.643555, f1 0.634454\n",
      "Current epoch:  49\n",
      "2017-11-19T01:35:06.226621: step 885, loss 0.732423, acc 0.675781, f1 0.645724\n",
      "2017-11-19T01:35:06.527334: step 890, loss 0.743621, acc 0.685547, f1 0.685003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-19T01:35:06.804668: step 895, loss 0.809379, acc 0.584961, f1 0.546412\n",
      "2017-11-19T01:35:07.083574: step 900, loss 0.757914, acc 0.649371, f1 0.620473\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:35:07.319983: step 900, loss 0.977037, acc 0.502423, f1 0.483578\n",
      "\n",
      "Current epoch:  50\n",
      "2017-11-19T01:35:07.657310: step 905, loss 0.739031, acc 0.677734, f1 0.668053\n",
      "2017-11-19T01:35:07.925771: step 910, loss 0.87332, acc 0.544922, f1 0.50479\n",
      "2017-11-19T01:35:08.225261: step 915, loss 0.823661, acc 0.636719, f1 0.591359\n",
      "Current epoch:  51\n",
      "2017-11-19T01:35:08.522632: step 920, loss 0.747442, acc 0.665039, f1 0.649768\n",
      "2017-11-19T01:35:08.844260: step 925, loss 0.721876, acc 0.683594, f1 0.680462\n",
      "2017-11-19T01:35:09.143141: step 930, loss 0.78185, acc 0.605469, f1 0.571444\n",
      "2017-11-19T01:35:09.460586: step 935, loss 0.781376, acc 0.658203, f1 0.61915\n",
      "Current epoch:  52\n",
      "2017-11-19T01:35:09.732593: step 940, loss 0.734579, acc 0.695312, f1 0.697701\n",
      "2017-11-19T01:35:10.027345: step 945, loss 0.719136, acc 0.666992, f1 0.649764\n",
      "2017-11-19T01:35:10.296687: step 950, loss 0.846495, acc 0.547852, f1 0.49612\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:35:10.525802: step 950, loss 0.916243, acc 0.53131, f1 0.519966\n",
      "\n",
      "Current epoch:  53\n",
      "2017-11-19T01:35:10.813878: step 955, loss 0.728804, acc 0.701172, f1 0.701334\n",
      "2017-11-19T01:35:11.117025: step 960, loss 0.773312, acc 0.595703, f1 0.575332\n",
      "2017-11-19T01:35:11.431206: step 965, loss 0.829821, acc 0.65625, f1 0.608839\n",
      "2017-11-19T01:35:11.741879: step 970, loss 0.72521, acc 0.6875, f1 0.688504\n",
      "Current epoch:  54\n",
      "2017-11-19T01:35:12.032883: step 975, loss 0.69806, acc 0.678711, f1 0.661535\n",
      "2017-11-19T01:35:12.315722: step 980, loss 0.806855, acc 0.583984, f1 0.538948\n",
      "2017-11-19T01:35:12.632369: step 985, loss 0.722154, acc 0.69043, f1 0.68556\n",
      "2017-11-19T01:35:12.918390: step 990, loss 0.717076, acc 0.70283, f1 0.694894\n",
      "Current epoch:  55\n",
      "2017-11-19T01:35:13.245249: step 995, loss 0.785137, acc 0.612305, f1 0.578105\n",
      "2017-11-19T01:35:13.533639: step 1000, loss 0.715871, acc 0.697266, f1 0.694605\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:35:13.781971: step 1000, loss 0.881452, acc 0.584189, f1 0.560465\n",
      "\n",
      "2017-11-19T01:35:14.058680: step 1005, loss 0.728772, acc 0.673828, f1 0.647198\n",
      "Current epoch:  56\n",
      "2017-11-19T01:35:14.332141: step 1010, loss 0.725973, acc 0.675781, f1 0.665901\n",
      "2017-11-19T01:35:14.621808: step 1015, loss 0.807256, acc 0.615234, f1 0.572944\n",
      "2017-11-19T01:35:14.909212: step 1020, loss 0.708463, acc 0.680664, f1 0.678293\n",
      "2017-11-19T01:35:15.193975: step 1025, loss 0.803147, acc 0.645508, f1 0.605787\n",
      "Current epoch:  57\n",
      "2017-11-19T01:35:15.486099: step 1030, loss 0.822023, acc 0.587891, f1 0.541028\n",
      "2017-11-19T01:35:15.761230: step 1035, loss 0.690599, acc 0.688477, f1 0.678698\n",
      "2017-11-19T01:35:16.074584: step 1040, loss 0.744854, acc 0.643555, f1 0.618968\n",
      "Current epoch:  58\n",
      "2017-11-19T01:35:16.365600: step 1045, loss 0.667406, acc 0.732422, f1 0.731392\n",
      "2017-11-19T01:35:16.659388: step 1050, loss 0.765825, acc 0.610352, f1 0.572056\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:35:16.893608: step 1050, loss 1.03226, acc 0.475039, f1 0.430861\n",
      "\n",
      "2017-11-19T01:35:17.240701: step 1055, loss 0.685618, acc 0.733398, f1 0.734248\n",
      "2017-11-19T01:35:17.546319: step 1060, loss 0.755491, acc 0.65918, f1 0.618275\n",
      "Current epoch:  59\n",
      "2017-11-19T01:35:17.878468: step 1065, loss 0.701202, acc 0.681641, f1 0.669179\n",
      "2017-11-19T01:35:18.172676: step 1070, loss 0.740284, acc 0.675781, f1 0.66475\n",
      "2017-11-19T01:35:18.492387: step 1075, loss 0.701234, acc 0.707031, f1 0.694958\n",
      "2017-11-19T01:35:18.773922: step 1080, loss 0.827647, acc 0.558176, f1 0.502469\n",
      "Current epoch:  60\n",
      "2017-11-19T01:35:19.078936: step 1085, loss 0.718889, acc 0.675781, f1 0.660624\n",
      "2017-11-19T01:35:19.381698: step 1090, loss 0.727927, acc 0.68457, f1 0.687076\n",
      "2017-11-19T01:35:19.682185: step 1095, loss 0.722624, acc 0.675781, f1 0.636243\n",
      "Current epoch:  61\n",
      "2017-11-19T01:35:19.994053: step 1100, loss 0.737248, acc 0.663086, f1 0.645786\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:35:20.247928: step 1100, loss 0.907573, acc 0.582251, f1 0.525851\n",
      "\n",
      "2017-11-19T01:35:20.587053: step 1105, loss 0.68874, acc 0.710938, f1 0.705865\n",
      "2017-11-19T01:35:20.899319: step 1110, loss 0.753134, acc 0.657227, f1 0.620567\n",
      "2017-11-19T01:35:21.192612: step 1115, loss 0.792648, acc 0.609375, f1 0.572598\n",
      "Current epoch:  62\n",
      "2017-11-19T01:35:21.485383: step 1120, loss 0.759303, acc 0.662109, f1 0.617902\n",
      "2017-11-19T01:35:21.781311: step 1125, loss 0.6871, acc 0.707031, f1 0.69548\n",
      "2017-11-19T01:35:22.069503: step 1130, loss 0.713865, acc 0.660156, f1 0.623462\n",
      "Current epoch:  63\n",
      "2017-11-19T01:35:22.376199: step 1135, loss 0.65562, acc 0.735352, f1 0.739248\n",
      "2017-11-19T01:35:22.666732: step 1140, loss 0.787623, acc 0.594727, f1 0.560924\n",
      "2017-11-19T01:35:22.950121: step 1145, loss 0.707871, acc 0.678711, f1 0.647351\n",
      "2017-11-19T01:35:23.239659: step 1150, loss 0.721601, acc 0.663086, f1 0.650963\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:35:23.489434: step 1150, loss 0.859684, acc 0.592817, f1 0.568535\n",
      "\n",
      "Current epoch:  64\n",
      "2017-11-19T01:35:23.792937: step 1155, loss 0.678368, acc 0.702148, f1 0.680564\n",
      "2017-11-19T01:35:24.058793: step 1160, loss 0.682054, acc 0.736328, f1 0.735517\n",
      "2017-11-19T01:35:24.350063: step 1165, loss 0.771571, acc 0.641602, f1 0.615968\n",
      "2017-11-19T01:35:24.628539: step 1170, loss 0.656106, acc 0.740566, f1 0.735095\n",
      "Current epoch:  65\n",
      "2017-11-19T01:35:24.915906: step 1175, loss 0.731236, acc 0.660156, f1 0.642783\n",
      "2017-11-19T01:35:25.226628: step 1180, loss 0.734874, acc 0.648438, f1 0.627173\n",
      "2017-11-19T01:35:25.543854: step 1185, loss 0.636669, acc 0.732422, f1 0.722716\n",
      "Current epoch:  66\n",
      "2017-11-19T01:35:25.869363: step 1190, loss 0.758226, acc 0.605469, f1 0.564062\n",
      "2017-11-19T01:35:26.192184: step 1195, loss 0.624247, acc 0.771484, f1 0.772215\n",
      "2017-11-19T01:35:26.491251: step 1200, loss 0.782706, acc 0.635742, f1 0.594241\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:35:26.741480: step 1200, loss 0.999784, acc 0.531068, f1 0.497433\n",
      "\n",
      "2017-11-19T01:35:27.163998: step 1205, loss 0.664295, acc 0.696289, f1 0.690252\n",
      "Current epoch:  67\n",
      "2017-11-19T01:35:27.435665: step 1210, loss 0.747682, acc 0.654297, f1 0.64804\n",
      "2017-11-19T01:35:27.745198: step 1215, loss 0.644742, acc 0.71582, f1 0.700909\n",
      "2017-11-19T01:35:28.048690: step 1220, loss 0.801104, acc 0.600586, f1 0.553684\n",
      "Current epoch:  68\n",
      "2017-11-19T01:35:28.344091: step 1225, loss 0.672937, acc 0.681641, f1 0.663815\n",
      "2017-11-19T01:35:28.675540: step 1230, loss 0.687036, acc 0.65625, f1 0.627568\n",
      "2017-11-19T01:35:28.979007: step 1235, loss 0.66039, acc 0.746094, f1 0.748599\n",
      "2017-11-19T01:35:29.262494: step 1240, loss 0.672107, acc 0.706055, f1 0.681511\n",
      "Current epoch:  69\n",
      "2017-11-19T01:35:29.559749: step 1245, loss 0.641903, acc 0.743164, f1 0.741912\n",
      "2017-11-19T01:35:29.868272: step 1250, loss 0.743985, acc 0.604492, f1 0.57253\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:35:30.142500: step 1250, loss 1.27704, acc 0.420609, f1 0.314795\n",
      "\n",
      "2017-11-19T01:35:30.462599: step 1255, loss 0.740738, acc 0.696289, f1 0.650499\n",
      "2017-11-19T01:35:30.740067: step 1260, loss 0.624049, acc 0.767296, f1 0.764497\n",
      "Current epoch:  70\n",
      "2017-11-19T01:35:31.038474: step 1265, loss 0.729101, acc 0.645508, f1 0.62005\n",
      "2017-11-19T01:35:31.322884: step 1270, loss 0.683094, acc 0.671875, f1 0.646388\n",
      "2017-11-19T01:35:31.636780: step 1275, loss 0.687536, acc 0.692383, f1 0.684574\n",
      "Current epoch:  71\n",
      "2017-11-19T01:35:31.925515: step 1280, loss 0.63603, acc 0.733398, f1 0.705291\n",
      "2017-11-19T01:35:32.242915: step 1285, loss 0.726698, acc 0.643555, f1 0.617793\n",
      "2017-11-19T01:35:32.554378: step 1290, loss 0.657582, acc 0.722656, f1 0.715328\n",
      "2017-11-19T01:35:32.859423: step 1295, loss 0.646351, acc 0.710938, f1 0.686756\n",
      "Current epoch:  72\n",
      "2017-11-19T01:35:33.163118: step 1300, loss 0.786899, acc 0.612305, f1 0.56574\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:35:33.390960: step 1300, loss 0.906132, acc 0.569116, f1 0.538625\n",
      "\n",
      "2017-11-19T01:35:33.712266: step 1305, loss 0.678419, acc 0.700195, f1 0.691192\n",
      "2017-11-19T01:35:34.053217: step 1310, loss 0.66516, acc 0.724609, f1 0.696484\n",
      "Current epoch:  73\n",
      "2017-11-19T01:35:34.363780: step 1315, loss 0.659115, acc 0.709961, f1 0.695589\n",
      "2017-11-19T01:35:34.648582: step 1320, loss 0.659214, acc 0.728516, f1 0.723439\n",
      "2017-11-19T01:35:34.954251: step 1325, loss 0.660862, acc 0.701172, f1 0.663666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-19T01:35:35.263472: step 1330, loss 0.587175, acc 0.776367, f1 0.776648\n",
      "Current epoch:  74\n",
      "2017-11-19T01:35:35.581673: step 1335, loss 0.667921, acc 0.676758, f1 0.657582\n",
      "2017-11-19T01:35:35.867864: step 1340, loss 0.628524, acc 0.734375, f1 0.729418\n",
      "2017-11-19T01:35:36.171344: step 1345, loss 0.62556, acc 0.751953, f1 0.749964\n",
      "2017-11-19T01:35:36.430555: step 1350, loss 0.706811, acc 0.70283, f1 0.658762\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:35:36.665322: step 1350, loss 0.940279, acc 0.538338, f1 0.546844\n",
      "\n",
      "Current epoch:  75\n",
      "2017-11-19T01:35:36.961209: step 1355, loss 0.786698, acc 0.597656, f1 0.561729\n",
      "2017-11-19T01:35:37.295971: step 1360, loss 0.602371, acc 0.771484, f1 0.770523\n",
      "2017-11-19T01:35:37.627467: step 1365, loss 0.658027, acc 0.689453, f1 0.679193\n",
      "Current epoch:  76\n",
      "2017-11-19T01:35:37.933050: step 1370, loss 0.654777, acc 0.695312, f1 0.683078\n",
      "2017-11-19T01:35:38.247836: step 1375, loss 0.617053, acc 0.75, f1 0.750375\n",
      "2017-11-19T01:35:38.544740: step 1380, loss 0.714938, acc 0.704102, f1 0.657459\n",
      "2017-11-19T01:35:38.824062: step 1385, loss 0.682408, acc 0.665039, f1 0.638867\n",
      "Current epoch:  77\n",
      "2017-11-19T01:35:39.110890: step 1390, loss 0.606636, acc 0.75293, f1 0.750349\n",
      "2017-11-19T01:35:39.399817: step 1395, loss 0.687683, acc 0.691406, f1 0.668895\n",
      "2017-11-19T01:35:39.716112: step 1400, loss 0.614271, acc 0.731445, f1 0.725855\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:35:39.943412: step 1400, loss 0.985363, acc 0.513426, f1 0.49286\n",
      "\n",
      "Current epoch:  78\n",
      "2017-11-19T01:35:40.238735: step 1405, loss 0.704057, acc 0.660156, f1 0.634963\n",
      "2017-11-19T01:35:40.534298: step 1410, loss 0.599075, acc 0.764648, f1 0.770114\n",
      "2017-11-19T01:35:40.827333: step 1415, loss 0.6464, acc 0.676758, f1 0.65958\n",
      "2017-11-19T01:35:41.093049: step 1420, loss 0.607272, acc 0.730469, f1 0.719889\n",
      "Current epoch:  79\n",
      "2017-11-19T01:35:41.360905: step 1425, loss 0.625698, acc 0.712891, f1 0.704254\n",
      "2017-11-19T01:35:41.639622: step 1430, loss 0.686673, acc 0.665039, f1 0.631056\n",
      "2017-11-19T01:35:41.959345: step 1435, loss 0.544559, acc 0.795898, f1 0.794461\n",
      "2017-11-19T01:35:42.215430: step 1440, loss 0.785647, acc 0.636792, f1 0.588482\n",
      "Current epoch:  80\n",
      "2017-11-19T01:35:42.531662: step 1445, loss 0.585631, acc 0.743164, f1 0.729854\n",
      "2017-11-19T01:35:42.832070: step 1450, loss 0.631187, acc 0.739258, f1 0.728478\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:35:43.061159: step 1450, loss 0.904679, acc 0.588939, f1 0.562859\n",
      "\n",
      "2017-11-19T01:35:43.378528: step 1455, loss 0.64766, acc 0.71582, f1 0.694553\n",
      "Current epoch:  81\n",
      "2017-11-19T01:35:43.688515: step 1460, loss 0.559225, acc 0.775391, f1 0.772048\n",
      "2017-11-19T01:35:44.008271: step 1465, loss 0.789096, acc 0.620117, f1 0.563202\n",
      "2017-11-19T01:35:44.291443: step 1470, loss 0.566505, acc 0.762695, f1 0.750175\n",
      "2017-11-19T01:35:44.573493: step 1475, loss 0.644075, acc 0.707031, f1 0.695545\n",
      "Current epoch:  82\n",
      "2017-11-19T01:35:44.906188: step 1480, loss 0.546596, acc 0.787109, f1 0.785442\n",
      "2017-11-19T01:35:45.205288: step 1485, loss 0.711631, acc 0.657227, f1 0.629704\n",
      "2017-11-19T01:35:45.515348: step 1490, loss 0.576996, acc 0.748047, f1 0.738481\n",
      "Current epoch:  83\n",
      "2017-11-19T01:35:45.824258: step 1495, loss 0.573686, acc 0.783203, f1 0.78052\n",
      "2017-11-19T01:35:46.130768: step 1500, loss 0.585222, acc 0.723633, f1 0.711035\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:35:46.383463: step 1500, loss 1.05662, acc 0.485799, f1 0.451342\n",
      "\n",
      "2017-11-19T01:35:46.692883: step 1505, loss 0.650868, acc 0.69043, f1 0.678542\n",
      "2017-11-19T01:35:46.982059: step 1510, loss 0.568493, acc 0.764648, f1 0.768259\n",
      "Current epoch:  84\n",
      "2017-11-19T01:35:47.289799: step 1515, loss 0.569944, acc 0.764648, f1 0.737408\n",
      "2017-11-19T01:35:47.558232: step 1520, loss 0.847821, acc 0.59375, f1 0.520575\n",
      "2017-11-19T01:35:47.854638: step 1525, loss 0.533568, acc 0.790039, f1 0.782625\n",
      "2017-11-19T01:35:48.141731: step 1530, loss 0.717567, acc 0.663522, f1 0.623842\n",
      "Current epoch:  85\n",
      "2017-11-19T01:35:48.446337: step 1535, loss 0.577725, acc 0.763672, f1 0.756914\n",
      "2017-11-19T01:35:48.734537: step 1540, loss 0.620554, acc 0.742188, f1 0.719235\n",
      "2017-11-19T01:35:49.023662: step 1545, loss 0.565036, acc 0.772461, f1 0.769987\n",
      "Current epoch:  86\n",
      "2017-11-19T01:35:49.313017: step 1550, loss 0.641594, acc 0.683594, f1 0.660874\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:35:49.561562: step 1550, loss 0.982437, acc 0.567904, f1 0.515214\n",
      "\n",
      "2017-11-19T01:35:49.842725: step 1555, loss 0.569157, acc 0.763672, f1 0.74226\n",
      "2017-11-19T01:35:50.122257: step 1560, loss 0.530994, acc 0.795898, f1 0.794718\n",
      "2017-11-19T01:35:50.449995: step 1565, loss 0.590595, acc 0.732422, f1 0.724118\n",
      "Current epoch:  87\n",
      "2017-11-19T01:35:50.713658: step 1570, loss 0.497597, acc 0.816406, f1 0.815051\n",
      "2017-11-19T01:35:51.000352: step 1575, loss 0.756584, acc 0.700195, f1 0.648718\n",
      "2017-11-19T01:35:51.300982: step 1580, loss 0.589818, acc 0.736328, f1 0.718302\n",
      "Current epoch:  88\n",
      "2017-11-19T01:35:51.567475: step 1585, loss 0.515528, acc 0.820312, f1 0.822927\n",
      "2017-11-19T01:35:51.844520: step 1590, loss 0.527364, acc 0.760742, f1 0.736895\n",
      "2017-11-19T01:35:52.166429: step 1595, loss 0.656354, acc 0.667969, f1 0.635822\n",
      "2017-11-19T01:35:52.494599: step 1600, loss 0.565985, acc 0.72168, f1 0.711841\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:35:52.753868: step 1600, loss 1.09878, acc 0.478092, f1 0.437681\n",
      "\n",
      "Current epoch:  89\n",
      "2017-11-19T01:35:53.096881: step 1605, loss 0.570068, acc 0.753906, f1 0.741686\n",
      "2017-11-19T01:35:53.394786: step 1610, loss 0.621702, acc 0.731445, f1 0.69481\n",
      "2017-11-19T01:35:53.677529: step 1615, loss 0.560328, acc 0.760742, f1 0.7524\n",
      "2017-11-19T01:35:53.969211: step 1620, loss 0.514477, acc 0.83805, f1 0.837838\n",
      "Current epoch:  90\n",
      "2017-11-19T01:35:54.273362: step 1625, loss 0.515878, acc 0.818359, f1 0.818725\n",
      "2017-11-19T01:35:54.580010: step 1630, loss 0.529367, acc 0.764648, f1 0.758835\n",
      "2017-11-19T01:35:54.871506: step 1635, loss 0.690655, acc 0.6875, f1 0.635869\n",
      "Current epoch:  91\n",
      "2017-11-19T01:35:55.144757: step 1640, loss 0.556629, acc 0.779297, f1 0.780437\n",
      "2017-11-19T01:35:55.457722: step 1645, loss 0.569693, acc 0.753906, f1 0.727851\n",
      "2017-11-19T01:35:55.777927: step 1650, loss 0.637703, acc 0.679688, f1 0.6598\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:35:56.020100: step 1650, loss 1.19207, acc 0.454973, f1 0.396913\n",
      "\n",
      "2017-11-19T01:35:56.354929: step 1655, loss 0.510036, acc 0.80957, f1 0.805372\n",
      "Current epoch:  92\n",
      "2017-11-19T01:35:56.640015: step 1660, loss 0.537198, acc 0.758789, f1 0.739945\n",
      "2017-11-19T01:35:56.964014: step 1665, loss 0.533957, acc 0.768555, f1 0.76187\n",
      "2017-11-19T01:35:57.273522: step 1670, loss 0.516566, acc 0.796875, f1 0.794288\n",
      "Current epoch:  93\n",
      "2017-11-19T01:35:57.582950: step 1675, loss 0.696644, acc 0.707031, f1 0.661832\n",
      "2017-11-19T01:35:57.854084: step 1680, loss 0.465334, acc 0.856445, f1 0.858562\n",
      "2017-11-19T01:35:58.171499: step 1685, loss 0.6477, acc 0.667969, f1 0.648377\n",
      "2017-11-19T01:35:58.446561: step 1690, loss 0.483311, acc 0.832031, f1 0.829829\n",
      "Current epoch:  94\n",
      "2017-11-19T01:35:58.734518: step 1695, loss 0.516013, acc 0.775391, f1 0.766212\n",
      "2017-11-19T01:35:59.041941: step 1700, loss 0.566539, acc 0.746094, f1 0.726427\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:35:59.292378: step 1700, loss 0.913719, acc 0.583075, f1 0.566279\n",
      "\n",
      "2017-11-19T01:35:59.653648: step 1705, loss 0.546927, acc 0.737305, f1 0.725663\n",
      "2017-11-19T01:35:59.957596: step 1710, loss 0.482517, acc 0.809748, f1 0.792159\n",
      "Current epoch:  95\n",
      "2017-11-19T01:36:00.261172: step 1715, loss 0.474524, acc 0.84668, f1 0.851512\n",
      "2017-11-19T01:36:00.598220: step 1720, loss 0.708144, acc 0.615234, f1 0.580376\n",
      "2017-11-19T01:36:00.894713: step 1725, loss 0.510163, acc 0.795898, f1 0.779381\n",
      "Current epoch:  96\n",
      "2017-11-19T01:36:01.176536: step 1730, loss 0.536445, acc 0.780273, f1 0.774969\n",
      "2017-11-19T01:36:01.461939: step 1735, loss 0.574219, acc 0.735352, f1 0.722081\n",
      "2017-11-19T01:36:01.762237: step 1740, loss 0.552428, acc 0.735352, f1 0.718728\n",
      "2017-11-19T01:36:02.062422: step 1745, loss 0.478444, acc 0.834961, f1 0.832924\n",
      "Current epoch:  97\n",
      "2017-11-19T01:36:02.365393: step 1750, loss 0.531022, acc 0.775391, f1 0.747506\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:36:02.602194: step 1750, loss 1.05536, acc 0.505622, f1 0.497606\n",
      "\n",
      "2017-11-19T01:36:02.881285: step 1755, loss 0.511443, acc 0.78125, f1 0.773791\n",
      "2017-11-19T01:36:03.171638: step 1760, loss 0.610596, acc 0.710938, f1 0.688714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch:  98\n",
      "2017-11-19T01:36:03.459124: step 1765, loss 0.498652, acc 0.817383, f1 0.796816\n",
      "2017-11-19T01:36:03.753969: step 1770, loss 0.476747, acc 0.833008, f1 0.835228\n",
      "2017-11-19T01:36:04.057348: step 1775, loss 0.762104, acc 0.617188, f1 0.580093\n",
      "2017-11-19T01:36:04.350507: step 1780, loss 0.546726, acc 0.789062, f1 0.755164\n",
      "Current epoch:  99\n",
      "2017-11-19T01:36:04.642998: step 1785, loss 0.468544, acc 0.849609, f1 0.850628\n",
      "2017-11-19T01:36:04.921431: step 1790, loss 0.438679, acc 0.825195, f1 0.820464\n",
      "2017-11-19T01:36:05.195764: step 1795, loss 0.735941, acc 0.640625, f1 0.601642\n",
      "2017-11-19T01:36:05.452227: step 1800, loss 0.503047, acc 0.81761, f1 0.820378\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:36:05.689250: step 1800, loss 1.04654, acc 0.578034, f1 0.537892\n",
      "\n",
      "Current epoch:  100\n",
      "2017-11-19T01:36:06.006089: step 1805, loss 0.483398, acc 0.77832, f1 0.765822\n",
      "2017-11-19T01:36:06.299713: step 1810, loss 0.5389, acc 0.771484, f1 0.760337\n",
      "2017-11-19T01:36:06.584764: step 1815, loss 0.454965, acc 0.820312, f1 0.81904\n",
      "Current epoch:  101\n",
      "2017-11-19T01:36:06.859944: step 1820, loss 0.624531, acc 0.673828, f1 0.652215\n",
      "2017-11-19T01:36:07.157497: step 1825, loss 0.512188, acc 0.803711, f1 0.816046\n",
      "2017-11-19T01:36:07.476851: step 1830, loss 0.394681, acc 0.883789, f1 0.882949\n",
      "2017-11-19T01:36:07.781332: step 1835, loss 0.6287, acc 0.672852, f1 0.64741\n",
      "Current epoch:  102\n",
      "2017-11-19T01:36:08.082672: step 1840, loss 0.421486, acc 0.849609, f1 0.848237\n",
      "2017-11-19T01:36:08.388486: step 1845, loss 0.678999, acc 0.678711, f1 0.661212\n",
      "2017-11-19T01:36:08.701452: step 1850, loss 0.563491, acc 0.732422, f1 0.714921\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:36:08.948412: step 1850, loss 1.04363, acc 0.572363, f1 0.525413\n",
      "\n",
      "Current epoch:  103\n",
      "2017-11-19T01:36:09.259026: step 1855, loss 0.457477, acc 0.828125, f1 0.820068\n",
      "2017-11-19T01:36:09.562859: step 1860, loss 0.43237, acc 0.833984, f1 0.831532\n",
      "2017-11-19T01:36:09.866238: step 1865, loss 0.586125, acc 0.704102, f1 0.694043\n",
      "2017-11-19T01:36:10.193915: step 1870, loss 0.492347, acc 0.798828, f1 0.789852\n",
      "Current epoch:  104\n",
      "2017-11-19T01:36:10.513860: step 1875, loss 0.44607, acc 0.847656, f1 0.85143\n",
      "2017-11-19T01:36:10.838461: step 1880, loss 0.590885, acc 0.699219, f1 0.685258\n",
      "2017-11-19T01:36:11.132781: step 1885, loss 0.404491, acc 0.861328, f1 0.859548\n",
      "2017-11-19T01:36:11.414535: step 1890, loss 0.580148, acc 0.757862, f1 0.759763\n",
      "Current epoch:  105\n",
      "2017-11-19T01:36:11.730325: step 1895, loss 0.473632, acc 0.806641, f1 0.804462\n",
      "2017-11-19T01:36:12.034481: step 1900, loss 0.508556, acc 0.758789, f1 0.754718\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:36:12.277268: step 1900, loss 1.15425, acc 0.477365, f1 0.443862\n",
      "\n",
      "2017-11-19T01:36:12.574737: step 1905, loss 0.406687, acc 0.875977, f1 0.876034\n",
      "Current epoch:  106\n",
      "2017-11-19T01:36:12.867289: step 1910, loss 0.712035, acc 0.762695, f1 0.700923\n",
      "2017-11-19T01:36:13.195189: step 1915, loss 0.581095, acc 0.727539, f1 0.699707\n",
      "2017-11-19T01:36:13.476716: step 1920, loss 0.397567, acc 0.868164, f1 0.867456\n",
      "2017-11-19T01:36:13.798468: step 1925, loss 0.54191, acc 0.739258, f1 0.73589\n",
      "Current epoch:  107\n",
      "2017-11-19T01:36:14.078178: step 1930, loss 0.40165, acc 0.875977, f1 0.875975\n",
      "2017-11-19T01:36:14.375217: step 1935, loss 0.41131, acc 0.831055, f1 0.820637\n",
      "2017-11-19T01:36:14.698418: step 1940, loss 0.46988, acc 0.808594, f1 0.801931\n",
      "Current epoch:  108\n",
      "2017-11-19T01:36:15.007825: step 1945, loss 0.472286, acc 0.806641, f1 0.792821\n",
      "2017-11-19T01:36:15.284979: step 1950, loss 0.371251, acc 0.886719, f1 0.886816\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:36:15.512528: step 1950, loss 1.04384, acc 0.532328, f1 0.52654\n",
      "\n",
      "2017-11-19T01:36:15.826221: step 1955, loss 0.649215, acc 0.675781, f1 0.637075\n",
      "2017-11-19T01:36:16.157883: step 1960, loss 0.410159, acc 0.84375, f1 0.8425\n",
      "Current epoch:  109\n",
      "2017-11-19T01:36:16.440578: step 1965, loss 0.410617, acc 0.854492, f1 0.853661\n",
      "2017-11-19T01:36:16.738945: step 1970, loss 0.49262, acc 0.769531, f1 0.761519\n",
      "2017-11-19T01:36:17.037151: step 1975, loss 0.44364, acc 0.819336, f1 0.816798\n",
      "2017-11-19T01:36:17.354307: step 1980, loss 0.487747, acc 0.779874, f1 0.777647\n",
      "Current epoch:  110\n",
      "2017-11-19T01:36:17.666822: step 1985, loss 0.454237, acc 0.820312, f1 0.813815\n",
      "2017-11-19T01:36:17.974167: step 1990, loss 0.507434, acc 0.787109, f1 0.75162\n",
      "2017-11-19T01:36:18.284512: step 1995, loss 0.492492, acc 0.776367, f1 0.767349\n",
      "Current epoch:  111\n",
      "2017-11-19T01:36:18.561533: step 2000, loss 0.393691, acc 0.84375, f1 0.841795\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:36:18.795700: step 2000, loss 1.0041, acc 0.562718, f1 0.55296\n",
      "\n",
      "2017-11-19T01:36:19.121175: step 2005, loss 0.422343, acc 0.817383, f1 0.815133\n",
      "2017-11-19T01:36:19.460146: step 2010, loss 0.394327, acc 0.84668, f1 0.843559\n",
      "2017-11-19T01:36:19.770285: step 2015, loss 0.454645, acc 0.78125, f1 0.773538\n",
      "Current epoch:  112\n",
      "2017-11-19T01:36:20.098702: step 2020, loss 0.481774, acc 0.805664, f1 0.809239\n",
      "2017-11-19T01:36:20.418466: step 2025, loss 0.354061, acc 0.875977, f1 0.873479\n",
      "2017-11-19T01:36:20.695860: step 2030, loss 0.376317, acc 0.87207, f1 0.870725\n",
      "Current epoch:  113\n",
      "2017-11-19T01:36:21.002547: step 2035, loss 0.55577, acc 0.745117, f1 0.725848\n",
      "2017-11-19T01:36:21.304654: step 2040, loss 0.404851, acc 0.84668, f1 0.830823\n",
      "2017-11-19T01:36:21.585793: step 2045, loss 0.428596, acc 0.831055, f1 0.829166\n",
      "2017-11-19T01:36:21.889303: step 2050, loss 0.424125, acc 0.816406, f1 0.814499\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:36:22.127474: step 2050, loss 1.23238, acc 0.476202, f1 0.441602\n",
      "\n",
      "Current epoch:  114\n",
      "2017-11-19T01:36:22.419933: step 2055, loss 0.541829, acc 0.728516, f1 0.723039\n",
      "2017-11-19T01:36:22.717190: step 2060, loss 0.332132, acc 0.902344, f1 0.902397\n",
      "2017-11-19T01:36:23.032508: step 2065, loss 0.374643, acc 0.868164, f1 0.867444\n",
      "2017-11-19T01:36:23.344697: step 2070, loss 0.52094, acc 0.731132, f1 0.722812\n",
      "Current epoch:  115\n",
      "2017-11-19T01:36:23.649144: step 2075, loss 0.360145, acc 0.893555, f1 0.893673\n",
      "2017-11-19T01:36:23.977694: step 2080, loss 0.399757, acc 0.836914, f1 0.823497\n",
      "2017-11-19T01:36:24.286611: step 2085, loss 0.396985, acc 0.866211, f1 0.867365\n",
      "Current epoch:  116\n",
      "2017-11-19T01:36:24.620774: step 2090, loss 0.533743, acc 0.72168, f1 0.709065\n",
      "2017-11-19T01:36:24.935916: step 2095, loss 0.397533, acc 0.838867, f1 0.837261\n",
      "2017-11-19T01:36:25.262387: step 2100, loss 0.4546, acc 0.821289, f1 0.818511\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:36:25.487698: step 2100, loss 1.04082, acc 0.576047, f1 0.551144\n",
      "\n",
      "2017-11-19T01:36:25.800642: step 2105, loss 0.372935, acc 0.852539, f1 0.851157\n",
      "Current epoch:  117\n",
      "2017-11-19T01:36:26.077897: step 2110, loss 0.531148, acc 0.745117, f1 0.727976\n",
      "2017-11-19T01:36:26.367627: step 2115, loss 0.329044, acc 0.890625, f1 0.889003\n",
      "2017-11-19T01:36:26.652885: step 2120, loss 0.342031, acc 0.889648, f1 0.889226\n",
      "Current epoch:  118\n",
      "2017-11-19T01:36:26.947655: step 2125, loss 0.336618, acc 0.878906, f1 0.877587\n",
      "2017-11-19T01:36:27.236686: step 2130, loss 0.533334, acc 0.748047, f1 0.725076\n",
      "2017-11-19T01:36:27.543907: step 2135, loss 0.370199, acc 0.853516, f1 0.850383\n",
      "2017-11-19T01:36:27.844635: step 2140, loss 0.430653, acc 0.811523, f1 0.801087\n",
      "Current epoch:  119\n",
      "2017-11-19T01:36:28.127677: step 2145, loss 0.421079, acc 0.826172, f1 0.822638\n",
      "2017-11-19T01:36:28.413865: step 2150, loss 0.388799, acc 0.84082, f1 0.823266\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:36:28.651953: step 2150, loss 1.11264, acc 0.519436, f1 0.527428\n",
      "\n",
      "2017-11-19T01:36:29.001759: step 2155, loss 0.324245, acc 0.902344, f1 0.902764\n",
      "2017-11-19T01:36:29.413289: step 2160, loss 0.46938, acc 0.759434, f1 0.753375\n",
      "Current epoch:  120\n",
      "2017-11-19T01:36:29.731050: step 2165, loss 0.333087, acc 0.901367, f1 0.90146\n",
      "2017-11-19T01:36:29.999395: step 2170, loss 0.436034, acc 0.814453, f1 0.807298\n",
      "2017-11-19T01:36:30.302016: step 2175, loss 0.442504, acc 0.826172, f1 0.825293\n",
      "Current epoch:  121\n",
      "2017-11-19T01:36:30.595508: step 2180, loss 0.378326, acc 0.845703, f1 0.83151\n",
      "2017-11-19T01:36:30.891735: step 2185, loss 0.372517, acc 0.866211, f1 0.864465\n",
      "2017-11-19T01:36:31.170559: step 2190, loss 0.593172, acc 0.696289, f1 0.682914\n",
      "2017-11-19T01:36:31.449680: step 2195, loss 0.310912, acc 0.911133, f1 0.910706\n",
      "Current epoch:  122\n",
      "2017-11-19T01:36:31.733013: step 2200, loss 0.325046, acc 0.878906, f1 0.877627\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-19T01:36:31.957604: step 2200, loss 1.23234, acc 0.495686, f1 0.475437\n",
      "\n",
      "2017-11-19T01:36:32.258307: step 2205, loss 0.464677, acc 0.779297, f1 0.77439\n",
      "2017-11-19T01:36:32.552929: step 2210, loss 0.343119, acc 0.879883, f1 0.880471\n",
      "Current epoch:  123\n",
      "2017-11-19T01:36:32.884199: step 2215, loss 0.470476, acc 0.824219, f1 0.782158\n",
      "2017-11-19T01:36:33.194939: step 2220, loss 0.405985, acc 0.833008, f1 0.829954\n",
      "2017-11-19T01:36:33.475442: step 2225, loss 0.352856, acc 0.855469, f1 0.854383\n",
      "2017-11-19T01:36:33.800870: step 2230, loss 0.359514, acc 0.868164, f1 0.869176\n",
      "Current epoch:  124\n",
      "2017-11-19T01:36:34.088506: step 2235, loss 0.308606, acc 0.888672, f1 0.888682\n",
      "2017-11-19T01:36:34.394440: step 2240, loss 0.530755, acc 0.738281, f1 0.731729\n",
      "2017-11-19T01:36:34.686013: step 2245, loss 0.263311, acc 0.941406, f1 0.941399\n",
      "2017-11-19T01:36:34.996898: step 2250, loss 0.282858, acc 0.918239, f1 0.91771\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:36:35.237407: step 2250, loss 1.1042, acc 0.537951, f1 0.542527\n",
      "\n",
      "Current epoch:  125\n",
      "2017-11-19T01:36:35.551614: step 2255, loss 0.26205, acc 0.919922, f1 0.919945\n",
      "2017-11-19T01:36:35.859624: step 2260, loss 0.609844, acc 0.706055, f1 0.665983\n",
      "2017-11-19T01:36:36.137993: step 2265, loss 0.377509, acc 0.858398, f1 0.856321\n",
      "Current epoch:  126\n",
      "2017-11-19T01:36:36.412216: step 2270, loss 0.278797, acc 0.912109, f1 0.908571\n",
      "2017-11-19T01:36:36.724189: step 2275, loss 0.3234, acc 0.896484, f1 0.899375\n",
      "2017-11-19T01:36:37.050780: step 2280, loss 0.491605, acc 0.818359, f1 0.771235\n",
      "2017-11-19T01:36:37.338382: step 2285, loss 0.269797, acc 0.916016, f1 0.91597\n",
      "Current epoch:  127\n",
      "2017-11-19T01:36:37.616292: step 2290, loss 0.457613, acc 0.791992, f1 0.785754\n",
      "2017-11-19T01:36:37.904246: step 2295, loss 0.309448, acc 0.898438, f1 0.898818\n",
      "2017-11-19T01:36:38.214575: step 2300, loss 0.301793, acc 0.901367, f1 0.90084\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:36:38.453048: step 2300, loss 1.13995, acc 0.567274, f1 0.547077\n",
      "\n",
      "Current epoch:  128\n",
      "2017-11-19T01:36:38.773329: step 2305, loss 0.325886, acc 0.866211, f1 0.864116\n",
      "2017-11-19T01:36:39.082203: step 2310, loss 0.305791, acc 0.897461, f1 0.899101\n",
      "2017-11-19T01:36:39.391844: step 2315, loss 0.374288, acc 0.842773, f1 0.820881\n",
      "2017-11-19T01:36:39.675463: step 2320, loss 0.439903, acc 0.787109, f1 0.776869\n",
      "Current epoch:  129\n",
      "2017-11-19T01:36:39.965257: step 2325, loss 0.275394, acc 0.922852, f1 0.922365\n",
      "2017-11-19T01:36:40.249820: step 2330, loss 0.336035, acc 0.853516, f1 0.852964\n",
      "2017-11-19T01:36:40.543052: step 2335, loss 0.34877, acc 0.856445, f1 0.853365\n",
      "2017-11-19T01:36:40.845826: step 2340, loss 0.289112, acc 0.893082, f1 0.893026\n",
      "Current epoch:  130\n",
      "2017-11-19T01:36:41.123898: step 2345, loss 0.227996, acc 0.946289, f1 0.94619\n",
      "2017-11-19T01:36:41.434716: step 2350, loss 0.435241, acc 0.788086, f1 0.780493\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:36:41.672444: step 2350, loss 1.54789, acc 0.446394, f1 0.387296\n",
      "\n",
      "2017-11-19T01:36:42.020671: step 2355, loss 0.278462, acc 0.916016, f1 0.915853\n",
      "Current epoch:  131\n",
      "2017-11-19T01:36:42.313820: step 2360, loss 0.283983, acc 0.888672, f1 0.88765\n",
      "2017-11-19T01:36:42.598809: step 2365, loss 0.345786, acc 0.861328, f1 0.860286\n",
      "2017-11-19T01:36:42.893642: step 2370, loss 0.237462, acc 0.936523, f1 0.936339\n",
      "2017-11-19T01:36:43.163765: step 2375, loss 0.245007, acc 0.927734, f1 0.927648\n",
      "Current epoch:  132\n",
      "2017-11-19T01:36:43.431390: step 2380, loss 0.618231, acc 0.691406, f1 0.648722\n",
      "2017-11-19T01:36:43.728719: step 2385, loss 0.251155, acc 0.927734, f1 0.927525\n",
      "2017-11-19T01:36:44.036561: step 2390, loss 0.233821, acc 0.946289, f1 0.946215\n",
      "Current epoch:  133\n",
      "2017-11-19T01:36:44.314756: step 2395, loss 0.202474, acc 0.945312, f1 0.945276\n",
      "2017-11-19T01:36:44.596146: step 2400, loss 0.383496, acc 0.805664, f1 0.80565\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:36:44.829763: step 2400, loss 1.82797, acc 0.426764, f1 0.349062\n",
      "\n",
      "2017-11-19T01:36:45.102439: step 2405, loss 0.314526, acc 0.892578, f1 0.891142\n",
      "2017-11-19T01:36:45.370546: step 2410, loss 0.233192, acc 0.939453, f1 0.939547\n",
      "Current epoch:  134\n",
      "2017-11-19T01:36:45.688594: step 2415, loss 0.216232, acc 0.94043, f1 0.940379\n",
      "2017-11-19T01:36:46.000273: step 2420, loss 0.364316, acc 0.844727, f1 0.840045\n",
      "2017-11-19T01:36:46.307034: step 2425, loss 0.264005, acc 0.907227, f1 0.905545\n",
      "2017-11-19T01:36:46.572519: step 2430, loss 0.184942, acc 0.965409, f1 0.965362\n",
      "Current epoch:  135\n",
      "2017-11-19T01:36:46.892929: step 2435, loss 0.263176, acc 0.904297, f1 0.897473\n",
      "2017-11-19T01:36:47.196974: step 2440, loss 0.551296, acc 0.733398, f1 0.703488\n",
      "2017-11-19T01:36:47.496714: step 2445, loss 0.37262, acc 0.823242, f1 0.82177\n",
      "Current epoch:  136\n",
      "2017-11-19T01:36:47.778623: step 2450, loss 0.39176, acc 0.834961, f1 0.815048\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:36:48.010688: step 2450, loss 1.26437, acc 0.502617, f1 0.510788\n",
      "\n",
      "2017-11-19T01:36:48.328553: step 2455, loss 0.209734, acc 0.950195, f1 0.950222\n",
      "2017-11-19T01:36:48.611036: step 2460, loss 0.210109, acc 0.945312, f1 0.945254\n",
      "2017-11-19T01:36:48.927781: step 2465, loss 0.215599, acc 0.935547, f1 0.93566\n",
      "Current epoch:  137\n",
      "2017-11-19T01:36:49.235023: step 2470, loss 0.203205, acc 0.945312, f1 0.945293\n",
      "2017-11-19T01:36:49.552849: step 2475, loss 0.556845, acc 0.732422, f1 0.705968\n",
      "2017-11-19T01:36:49.854799: step 2480, loss 0.28629, acc 0.895508, f1 0.894082\n",
      "Current epoch:  138\n",
      "2017-11-19T01:36:50.143744: step 2485, loss 0.196244, acc 0.954102, f1 0.953844\n",
      "2017-11-19T01:36:50.429291: step 2490, loss 0.175791, acc 0.963867, f1 0.963876\n",
      "2017-11-19T01:36:50.700325: step 2495, loss 0.178525, acc 0.953125, f1 0.953185\n",
      "2017-11-19T01:36:50.969453: step 2500, loss 0.190458, acc 0.948242, f1 0.947895\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:36:51.215612: step 2500, loss 1.2181, acc 0.557677, f1 0.549998\n",
      "\n",
      "Current epoch:  139\n",
      "2017-11-19T01:36:51.528069: step 2505, loss 0.215843, acc 0.93457, f1 0.934669\n",
      "2017-11-19T01:36:51.836463: step 2510, loss 0.605696, acc 0.708008, f1 0.666986\n",
      "2017-11-19T01:36:52.164088: step 2515, loss 0.214944, acc 0.941406, f1 0.941416\n",
      "2017-11-19T01:36:52.484218: step 2520, loss 0.185579, acc 0.955975, f1 0.955927\n",
      "Current epoch:  140\n",
      "2017-11-19T01:36:52.806189: step 2525, loss 0.178995, acc 0.958984, f1 0.958928\n",
      "2017-11-19T01:36:53.129194: step 2530, loss 0.175966, acc 0.952148, f1 0.952167\n",
      "2017-11-19T01:36:53.437078: step 2535, loss 0.375101, acc 0.844727, f1 0.857549\n",
      "Current epoch:  141\n",
      "2017-11-19T01:36:53.717473: step 2540, loss 0.171569, acc 0.955078, f1 0.955153\n",
      "2017-11-19T01:36:53.994665: step 2545, loss 0.170353, acc 0.972656, f1 0.972649\n",
      "2017-11-19T01:36:54.292559: step 2550, loss 0.155467, acc 0.970703, f1 0.970578\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:36:54.549945: step 2550, loss 1.26131, acc 0.5364, f1 0.536735\n",
      "\n",
      "2017-11-19T01:36:54.884634: step 2555, loss 0.157281, acc 0.966797, f1 0.966804\n",
      "Current epoch:  142\n",
      "2017-11-19T01:36:55.204743: step 2560, loss 0.191284, acc 0.939453, f1 0.939688\n",
      "2017-11-19T01:36:55.516491: step 2565, loss 0.452312, acc 0.786133, f1 0.774357\n",
      "2017-11-19T01:36:55.832718: step 2570, loss 0.175464, acc 0.96582, f1 0.965848\n",
      "Current epoch:  143\n",
      "2017-11-19T01:36:56.124463: step 2575, loss 0.149863, acc 0.975586, f1 0.975595\n",
      "2017-11-19T01:36:56.432613: step 2580, loss 0.154214, acc 0.963867, f1 0.963917\n",
      "2017-11-19T01:36:56.731435: step 2585, loss 0.157654, acc 0.970703, f1 0.9707\n",
      "2017-11-19T01:36:57.023230: step 2590, loss 0.164378, acc 0.959961, f1 0.959999\n",
      "Current epoch:  144\n",
      "2017-11-19T01:36:57.324929: step 2595, loss 0.160393, acc 0.962891, f1 0.962887\n",
      "2017-11-19T01:36:57.601366: step 2600, loss 0.172017, acc 0.958008, f1 0.957724\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:36:57.834232: step 2600, loss 1.40899, acc 0.505186, f1 0.514176\n",
      "\n",
      "2017-11-19T01:36:58.144875: step 2605, loss 0.423103, acc 0.818359, f1 0.837769\n",
      "2017-11-19T01:36:58.432777: step 2610, loss 0.136683, acc 0.976415, f1 0.976399\n",
      "Current epoch:  145\n",
      "2017-11-19T01:36:58.730845: step 2615, loss 0.150261, acc 0.970703, f1 0.97069\n",
      "2017-11-19T01:36:59.064166: step 2620, loss 0.161924, acc 0.957031, f1 0.957133\n",
      "2017-11-19T01:36:59.368696: step 2625, loss 0.529406, acc 0.733398, f1 0.71744\n",
      "Current epoch:  146\n",
      "2017-11-19T01:36:59.645279: step 2630, loss 0.148821, acc 0.974609, f1 0.974652\n",
      "2017-11-19T01:36:59.930931: step 2635, loss 0.135484, acc 0.974609, f1 0.974578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-19T01:37:00.214529: step 2640, loss 0.144102, acc 0.970703, f1 0.97069\n",
      "2017-11-19T01:37:00.491377: step 2645, loss 0.142054, acc 0.972656, f1 0.972686\n",
      "Current epoch:  147\n",
      "2017-11-19T01:37:00.789558: step 2650, loss 0.124512, acc 0.97168, f1 0.971681\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:37:01.044267: step 2650, loss 1.30822, acc 0.54241, f1 0.541377\n",
      "\n",
      "2017-11-19T01:37:01.379249: step 2655, loss 0.142642, acc 0.970703, f1 0.970617\n",
      "2017-11-19T01:37:01.668228: step 2660, loss 0.14157, acc 0.972656, f1 0.972613\n",
      "Current epoch:  148\n",
      "2017-11-19T01:37:01.956435: step 2665, loss 0.171911, acc 0.955078, f1 0.955131\n",
      "2017-11-19T01:37:02.286114: step 2670, loss 0.429851, acc 0.808594, f1 0.799296\n",
      "2017-11-19T01:37:02.614484: step 2675, loss 0.17276, acc 0.964844, f1 0.964869\n",
      "2017-11-19T01:37:02.886940: step 2680, loss 0.14075, acc 0.974609, f1 0.97463\n",
      "Current epoch:  149\n",
      "2017-11-19T01:37:03.179340: step 2685, loss 0.129859, acc 0.978516, f1 0.97851\n",
      "2017-11-19T01:37:03.451165: step 2690, loss 0.194712, acc 0.929688, f1 0.925722\n",
      "2017-11-19T01:37:03.753029: step 2695, loss 0.204605, acc 0.931641, f1 0.931721\n",
      "2017-11-19T01:37:04.006744: step 2700, loss 0.126404, acc 0.981132, f1 0.981138\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:37:04.238868: step 2700, loss 1.3185, acc 0.539017, f1 0.53938\n",
      "\n",
      "Current epoch:  150\n",
      "2017-11-19T01:37:04.531892: step 2705, loss 0.138487, acc 0.970703, f1 0.970697\n",
      "2017-11-19T01:37:04.821508: step 2710, loss 0.117092, acc 0.982422, f1 0.982438\n",
      "2017-11-19T01:37:05.107356: step 2715, loss 0.130381, acc 0.983398, f1 0.983401\n",
      "Current epoch:  151\n",
      "2017-11-19T01:37:05.389889: step 2720, loss 0.115859, acc 0.983398, f1 0.983377\n",
      "2017-11-19T01:37:05.677940: step 2725, loss 0.133672, acc 0.97168, f1 0.971651\n",
      "2017-11-19T01:37:05.977623: step 2730, loss 0.260051, acc 0.882812, f1 0.882126\n",
      "2017-11-19T01:37:06.289260: step 2735, loss 0.31681, acc 0.847656, f1 0.840947\n",
      "Current epoch:  152\n",
      "2017-11-19T01:37:06.572294: step 2740, loss 0.123884, acc 0.978516, f1 0.978519\n",
      "2017-11-19T01:37:06.897436: step 2745, loss 0.120921, acc 0.983398, f1 0.983407\n",
      "2017-11-19T01:37:07.214104: step 2750, loss 0.127248, acc 0.977539, f1 0.977537\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:37:07.457946: step 2750, loss 1.35132, acc 0.537757, f1 0.538258\n",
      "\n",
      "Current epoch:  153\n",
      "2017-11-19T01:37:07.835536: step 2755, loss 0.107745, acc 0.990234, f1 0.990236\n",
      "2017-11-19T01:37:08.140606: step 2760, loss 0.11843, acc 0.981445, f1 0.981456\n",
      "2017-11-19T01:37:08.440067: step 2765, loss 0.120736, acc 0.976562, f1 0.976561\n",
      "2017-11-19T01:37:08.737635: step 2770, loss 0.108513, acc 0.987305, f1 0.987311\n",
      "Current epoch:  154\n",
      "2017-11-19T01:37:09.053453: step 2775, loss 0.101178, acc 0.987305, f1 0.987325\n",
      "2017-11-19T01:37:09.344760: step 2780, loss 0.121769, acc 0.977539, f1 0.977527\n",
      "2017-11-19T01:37:09.629917: step 2785, loss 0.326177, acc 0.875, f1 0.848153\n",
      "2017-11-19T01:37:09.917340: step 2790, loss 0.193768, acc 0.930818, f1 0.930747\n",
      "Current epoch:  155\n",
      "2017-11-19T01:37:10.196428: step 2795, loss 0.157899, acc 0.956055, f1 0.955931\n",
      "2017-11-19T01:37:10.508597: step 2800, loss 0.182994, acc 0.927734, f1 0.927877\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:37:10.754341: step 2800, loss 1.59937, acc 0.493408, f1 0.480268\n",
      "\n",
      "2017-11-19T01:37:11.039854: step 2805, loss 0.123588, acc 0.972656, f1 0.97263\n",
      "Current epoch:  156\n",
      "2017-11-19T01:37:11.318617: step 2810, loss 0.120329, acc 0.978516, f1 0.978529\n",
      "2017-11-19T01:37:11.611943: step 2815, loss 0.090667, acc 0.990234, f1 0.990235\n",
      "2017-11-19T01:37:11.933953: step 2820, loss 0.0992056, acc 0.984375, f1 0.98437\n",
      "2017-11-19T01:37:12.230838: step 2825, loss 0.122232, acc 0.976562, f1 0.976578\n",
      "Current epoch:  157\n",
      "2017-11-19T01:37:12.524921: step 2830, loss 0.805153, acc 0.65332, f1 0.603039\n",
      "2017-11-19T01:37:12.824326: step 2835, loss 0.186517, acc 0.958984, f1 0.959094\n",
      "2017-11-19T01:37:13.135419: step 2840, loss 0.11064, acc 0.985352, f1 0.98535\n",
      "Current epoch:  158\n",
      "2017-11-19T01:37:13.432517: step 2845, loss 0.111902, acc 0.982422, f1 0.982378\n",
      "2017-11-19T01:37:13.754689: step 2850, loss 0.10888, acc 0.984375, f1 0.98437\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:37:13.980572: step 2850, loss 1.40276, acc 0.530486, f1 0.53162\n",
      "\n",
      "2017-11-19T01:37:14.300666: step 2855, loss 0.100131, acc 0.983398, f1 0.983397\n",
      "2017-11-19T01:37:14.597244: step 2860, loss 0.110914, acc 0.981445, f1 0.981456\n",
      "Current epoch:  159\n",
      "2017-11-19T01:37:14.889425: step 2865, loss 0.100772, acc 0.978516, f1 0.978501\n",
      "2017-11-19T01:37:15.208620: step 2870, loss 0.0922947, acc 0.984375, f1 0.98438\n",
      "2017-11-19T01:37:15.519993: step 2875, loss 0.0912324, acc 0.985352, f1 0.985349\n",
      "2017-11-19T01:37:15.830620: step 2880, loss 0.0817031, acc 0.988994, f1 0.989007\n",
      "Current epoch:  160\n",
      "2017-11-19T01:37:16.153259: step 2885, loss 0.0930558, acc 0.986328, f1 0.986338\n",
      "2017-11-19T01:37:16.476674: step 2890, loss 0.0877749, acc 0.987305, f1 0.987299\n",
      "2017-11-19T01:37:16.765932: step 2895, loss 0.0940624, acc 0.982422, f1 0.982416\n",
      "Current epoch:  161\n",
      "2017-11-19T01:37:17.064216: step 2900, loss 0.632791, acc 0.730469, f1 0.692335\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:37:17.326374: step 2900, loss 2.41691, acc 0.419639, f1 0.343763\n",
      "\n",
      "2017-11-19T01:37:17.687864: step 2905, loss 0.11375, acc 0.985352, f1 0.98536\n",
      "2017-11-19T01:37:17.994821: step 2910, loss 0.0928738, acc 0.990234, f1 0.990242\n",
      "2017-11-19T01:37:18.304531: step 2915, loss 0.0906579, acc 0.988281, f1 0.988284\n",
      "Current epoch:  162\n",
      "2017-11-19T01:37:18.616759: step 2920, loss 0.0884825, acc 0.987305, f1 0.987303\n",
      "2017-11-19T01:37:18.922455: step 2925, loss 0.0885867, acc 0.985352, f1 0.985353\n",
      "2017-11-19T01:37:19.204863: step 2930, loss 0.0914837, acc 0.987305, f1 0.987308\n",
      "Current epoch:  163\n",
      "2017-11-19T01:37:19.490409: step 2935, loss 0.0850045, acc 0.989258, f1 0.989259\n",
      "2017-11-19T01:37:19.758626: step 2940, loss 0.0829777, acc 0.985352, f1 0.985349\n",
      "2017-11-19T01:37:20.083394: step 2945, loss 0.0922335, acc 0.990234, f1 0.990243\n",
      "2017-11-19T01:37:20.386891: step 2950, loss 0.830051, acc 0.814453, f1 0.748632\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:37:20.628110: step 2950, loss 1.65235, acc 0.489434, f1 0.483681\n",
      "\n",
      "Current epoch:  164\n",
      "2017-11-19T01:37:20.982167: step 2955, loss 0.0879375, acc 0.989258, f1 0.989259\n",
      "2017-11-19T01:37:21.300869: step 2960, loss 0.0924052, acc 0.984375, f1 0.984368\n",
      "2017-11-19T01:37:21.598686: step 2965, loss 0.0884633, acc 0.984375, f1 0.984393\n",
      "2017-11-19T01:37:21.867233: step 2970, loss 0.0953855, acc 0.985849, f1 0.985847\n",
      "Current epoch:  165\n",
      "2017-11-19T01:37:22.159546: step 2975, loss 0.0794921, acc 0.992188, f1 0.992189\n",
      "2017-11-19T01:37:22.477023: step 2980, loss 0.0848043, acc 0.985352, f1 0.985354\n",
      "2017-11-19T01:37:22.750345: step 2985, loss 0.105257, acc 0.973633, f1 0.973656\n",
      "Current epoch:  166\n",
      "2017-11-19T01:37:23.049832: step 2990, loss 0.756976, acc 0.685547, f1 0.647056\n",
      "2017-11-19T01:37:23.356475: step 2995, loss 0.0912225, acc 0.991211, f1 0.99121\n",
      "2017-11-19T01:37:23.672626: step 3000, loss 0.0900192, acc 0.987305, f1 0.987302\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:37:23.915631: step 3000, loss 1.47642, acc 0.528742, f1 0.529511\n",
      "\n",
      "2017-11-19T01:37:24.200521: step 3005, loss 0.0865826, acc 0.987305, f1 0.987302\n",
      "Current epoch:  167\n",
      "2017-11-19T01:37:24.474137: step 3010, loss 0.0795214, acc 0.991211, f1 0.991215\n",
      "2017-11-19T01:37:24.775325: step 3015, loss 0.0802108, acc 0.989258, f1 0.989256\n",
      "2017-11-19T01:37:25.076878: step 3020, loss 0.0864947, acc 0.984375, f1 0.984336\n",
      "Current epoch:  168\n",
      "2017-11-19T01:37:25.345899: step 3025, loss 0.0726864, acc 0.991211, f1 0.991212\n",
      "2017-11-19T01:37:25.627558: step 3030, loss 0.0663114, acc 0.995117, f1 0.995119\n",
      "2017-11-19T01:37:25.919384: step 3035, loss 0.0840537, acc 0.989258, f1 0.989248\n",
      "2017-11-19T01:37:26.211458: step 3040, loss 0.0761119, acc 0.988281, f1 0.988282\n",
      "Current epoch:  169\n",
      "2017-11-19T01:37:26.507409: step 3045, loss 0.0780365, acc 0.991211, f1 0.991209\n",
      "2017-11-19T01:37:26.799340: step 3050, loss 0.078509, acc 0.990234, f1 0.990241\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:37:27.119582: step 3050, loss 1.54113, acc 0.537611, f1 0.536357\n",
      "\n",
      "2017-11-19T01:37:27.453846: step 3055, loss 0.0814056, acc 0.983398, f1 0.983419\n",
      "2017-11-19T01:37:27.748141: step 3060, loss 0.62052, acc 0.707547, f1 0.698686\n",
      "Current epoch:  170\n",
      "2017-11-19T01:37:28.063037: step 3065, loss 0.108837, acc 0.981445, f1 0.981349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-19T01:37:28.365029: step 3070, loss 0.0763283, acc 0.992188, f1 0.99219\n",
      "2017-11-19T01:37:28.674905: step 3075, loss 0.0719598, acc 0.993164, f1 0.993163\n",
      "Current epoch:  171\n",
      "2017-11-19T01:37:29.009062: step 3080, loss 0.0723223, acc 0.990234, f1 0.990238\n",
      "2017-11-19T01:37:29.302156: step 3085, loss 0.0674724, acc 0.990234, f1 0.990233\n",
      "2017-11-19T01:37:29.588380: step 3090, loss 0.0730574, acc 0.990234, f1 0.990237\n",
      "2017-11-19T01:37:29.927985: step 3095, loss 0.0736719, acc 0.987305, f1 0.987311\n",
      "Current epoch:  172\n",
      "2017-11-19T01:37:30.222983: step 3100, loss 0.0697884, acc 0.989258, f1 0.989265\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:37:30.453456: step 3100, loss 1.56666, acc 0.536981, f1 0.536481\n",
      "\n",
      "2017-11-19T01:37:30.788945: step 3105, loss 0.071767, acc 0.988281, f1 0.988281\n",
      "2017-11-19T01:37:31.091418: step 3110, loss 0.0784675, acc 0.983398, f1 0.983408\n",
      "Current epoch:  173\n",
      "2017-11-19T01:37:31.491853: step 3115, loss 0.0809828, acc 0.992188, f1 0.99218\n",
      "2017-11-19T01:37:31.752072: step 3120, loss 0.82261, acc 0.717773, f1 0.679149\n",
      "2017-11-19T01:37:32.073587: step 3125, loss 0.0825997, acc 0.991211, f1 0.991212\n",
      "2017-11-19T01:37:32.385459: step 3130, loss 0.0695147, acc 0.994141, f1 0.99414\n",
      "Current epoch:  174\n",
      "2017-11-19T01:37:32.645292: step 3135, loss 0.0605718, acc 0.99707, f1 0.99707\n",
      "2017-11-19T01:37:32.918540: step 3140, loss 0.0606093, acc 0.995117, f1 0.995117\n",
      "2017-11-19T01:37:33.200973: step 3145, loss 0.0687231, acc 0.991211, f1 0.991215\n",
      "2017-11-19T01:37:33.469468: step 3150, loss 0.0651446, acc 0.990566, f1 0.990566\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:37:33.699388: step 3150, loss 1.58656, acc 0.533491, f1 0.533129\n",
      "\n",
      "Current epoch:  175\n",
      "2017-11-19T01:37:34.011492: step 3155, loss 0.0624467, acc 0.996094, f1 0.99609\n",
      "2017-11-19T01:37:34.303554: step 3160, loss 0.0751309, acc 0.986328, f1 0.986328\n",
      "2017-11-19T01:37:34.582870: step 3165, loss 0.0664549, acc 0.990234, f1 0.990239\n",
      "Current epoch:  176\n",
      "2017-11-19T01:37:34.885104: step 3170, loss 0.0625212, acc 0.992188, f1 0.992187\n",
      "2017-11-19T01:37:35.185133: step 3175, loss 0.0558446, acc 0.990234, f1 0.990231\n",
      "2017-11-19T01:37:35.492508: step 3180, loss 0.0633958, acc 0.994141, f1 0.994143\n",
      "2017-11-19T01:37:35.782015: step 3185, loss 0.0984898, acc 0.980469, f1 0.980425\n",
      "Current epoch:  177\n",
      "2017-11-19T01:37:36.059649: step 3190, loss 0.354137, acc 0.829102, f1 0.827587\n",
      "2017-11-19T01:37:36.356513: step 3195, loss 0.080383, acc 0.992188, f1 0.992189\n",
      "2017-11-19T01:37:36.649833: step 3200, loss 0.0693577, acc 0.993164, f1 0.993163\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:37:36.885767: step 3200, loss 1.5804, acc 0.52879, f1 0.529305\n",
      "\n",
      "Current epoch:  178\n",
      "2017-11-19T01:37:37.166641: step 3205, loss 0.0597078, acc 0.993164, f1 0.993165\n",
      "2017-11-19T01:37:37.451491: step 3210, loss 0.0595201, acc 0.993164, f1 0.993164\n",
      "2017-11-19T01:37:37.731966: step 3215, loss 0.0653675, acc 0.988281, f1 0.988284\n",
      "2017-11-19T01:37:38.046457: step 3220, loss 0.0622985, acc 0.991211, f1 0.991202\n",
      "Current epoch:  179\n",
      "2017-11-19T01:37:38.346777: step 3225, loss 0.061412, acc 0.991211, f1 0.991211\n",
      "2017-11-19T01:37:38.664435: step 3230, loss 0.0516836, acc 0.99707, f1 0.99707\n",
      "2017-11-19T01:37:38.997371: step 3235, loss 0.0579553, acc 0.995117, f1 0.995109\n",
      "2017-11-19T01:37:39.301133: step 3240, loss 0.0631402, acc 0.992138, f1 0.992147\n",
      "Current epoch:  180\n",
      "2017-11-19T01:37:39.587233: step 3245, loss 0.0574386, acc 0.992188, f1 0.992192\n",
      "2017-11-19T01:37:39.889855: step 3250, loss 0.0619259, acc 0.989258, f1 0.989252\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:37:40.121872: step 3250, loss 1.71464, acc 0.513329, f1 0.517959\n",
      "\n",
      "2017-11-19T01:37:40.412079: step 3255, loss 0.0811679, acc 0.987305, f1 0.987342\n",
      "Current epoch:  181\n",
      "2017-11-19T01:37:40.710666: step 3260, loss 0.646821, acc 0.716797, f1 0.6929\n",
      "2017-11-19T01:37:41.009430: step 3265, loss 0.0709958, acc 0.993164, f1 0.993163\n",
      "2017-11-19T01:37:41.290076: step 3270, loss 0.0699139, acc 0.993164, f1 0.993151\n",
      "2017-11-19T01:37:41.560209: step 3275, loss 0.0617107, acc 0.994141, f1 0.994142\n",
      "Current epoch:  182\n",
      "2017-11-19T01:37:41.831331: step 3280, loss 0.0633856, acc 0.990234, f1 0.990225\n",
      "2017-11-19T01:37:42.119244: step 3285, loss 0.0545983, acc 0.991211, f1 0.991214\n",
      "2017-11-19T01:37:42.404158: step 3290, loss 0.0588047, acc 0.993164, f1 0.993167\n",
      "Current epoch:  183\n",
      "2017-11-19T01:37:42.690254: step 3295, loss 0.0447713, acc 0.998047, f1 0.998053\n",
      "2017-11-19T01:37:43.022657: step 3300, loss 0.0565075, acc 0.993164, f1 0.993164\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:37:43.284739: step 3300, loss 1.65582, acc 0.529566, f1 0.530124\n",
      "\n",
      "2017-11-19T01:37:43.638029: step 3305, loss 0.0578062, acc 0.995117, f1 0.995117\n",
      "2017-11-19T01:37:43.950973: step 3310, loss 0.0554871, acc 0.992188, f1 0.992191\n",
      "Current epoch:  184\n",
      "2017-11-19T01:37:44.225488: step 3315, loss 0.0553763, acc 0.992188, f1 0.992184\n",
      "2017-11-19T01:37:44.519059: step 3320, loss 0.0556902, acc 0.992188, f1 0.992189\n",
      "2017-11-19T01:37:44.791393: step 3325, loss 0.0497226, acc 0.99707, f1 0.997071\n",
      "2017-11-19T01:37:45.064159: step 3330, loss 0.0733081, acc 0.996855, f1 0.996855\n",
      "Current epoch:  185\n",
      "2017-11-19T01:37:45.362279: step 3335, loss 0.667308, acc 0.6875, f1 0.669968\n",
      "2017-11-19T01:37:45.643879: step 3340, loss 0.0678934, acc 0.989258, f1 0.989258\n",
      "2017-11-19T01:37:45.922771: step 3345, loss 0.0604137, acc 0.993164, f1 0.993163\n",
      "Current epoch:  186\n",
      "2017-11-19T01:37:46.194763: step 3350, loss 0.0523912, acc 0.995117, f1 0.995109\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:37:46.505679: step 3350, loss 1.66667, acc 0.523022, f1 0.524407\n",
      "\n",
      "2017-11-19T01:37:46.847738: step 3355, loss 0.054211, acc 0.995117, f1 0.995117\n",
      "2017-11-19T01:37:47.175648: step 3360, loss 0.0636713, acc 0.987305, f1 0.987307\n",
      "2017-11-19T01:37:47.474951: step 3365, loss 0.0502885, acc 0.994141, f1 0.994141\n",
      "Current epoch:  187\n",
      "2017-11-19T01:37:47.763133: step 3370, loss 0.0499703, acc 0.994141, f1 0.994136\n",
      "2017-11-19T01:37:48.101261: step 3375, loss 0.0468793, acc 0.998047, f1 0.998047\n",
      "2017-11-19T01:37:48.413830: step 3380, loss 0.0468956, acc 0.995117, f1 0.995119\n",
      "Current epoch:  188\n",
      "2017-11-19T01:37:48.713329: step 3385, loss 0.0484773, acc 0.995117, f1 0.995118\n",
      "2017-11-19T01:37:49.012803: step 3390, loss 0.0529431, acc 0.994141, f1 0.994141\n",
      "2017-11-19T01:37:49.332958: step 3395, loss 0.0533322, acc 0.991211, f1 0.991213\n",
      "2017-11-19T01:37:49.628654: step 3400, loss 0.263452, acc 0.895508, f1 0.897518\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:37:49.854356: step 3400, loss 2.60425, acc 0.553412, f1 0.490323\n",
      "\n",
      "Current epoch:  189\n",
      "2017-11-19T01:37:50.152327: step 3405, loss 0.238088, acc 0.905273, f1 0.903879\n",
      "2017-11-19T01:37:50.480562: step 3410, loss 0.0664289, acc 0.991211, f1 0.99121\n",
      "2017-11-19T01:37:50.792463: step 3415, loss 0.0499799, acc 0.999023, f1 0.999023\n",
      "2017-11-19T01:37:51.082907: step 3420, loss 0.0551551, acc 0.990566, f1 0.990565\n",
      "Current epoch:  190\n",
      "2017-11-19T01:37:51.386824: step 3425, loss 0.0510026, acc 0.994141, f1 0.994132\n",
      "2017-11-19T01:37:51.665915: step 3430, loss 0.045505, acc 0.995117, f1 0.995117\n",
      "2017-11-19T01:37:51.964038: step 3435, loss 0.0513392, acc 0.994141, f1 0.99414\n",
      "Current epoch:  191\n",
      "2017-11-19T01:37:52.231486: step 3440, loss 0.0477644, acc 0.993164, f1 0.993169\n",
      "2017-11-19T01:37:52.540460: step 3445, loss 0.0428804, acc 0.994141, f1 0.994141\n",
      "2017-11-19T01:37:52.852835: step 3450, loss 0.0498561, acc 0.993164, f1 0.993163\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:37:53.143385: step 3450, loss 1.72102, acc 0.527869, f1 0.529007\n",
      "\n",
      "2017-11-19T01:37:53.500127: step 3455, loss 0.0469064, acc 0.992188, f1 0.992191\n",
      "Current epoch:  192\n",
      "2017-11-19T01:37:53.768624: step 3460, loss 0.0484377, acc 0.994141, f1 0.99414\n",
      "2017-11-19T01:37:54.068462: step 3465, loss 0.0462962, acc 0.991211, f1 0.99121\n",
      "2017-11-19T01:37:54.403959: step 3470, loss 0.0427397, acc 0.992188, f1 0.992185\n",
      "Current epoch:  193\n",
      "2017-11-19T01:37:54.687593: step 3475, loss 0.048072, acc 0.996094, f1 0.996094\n",
      "2017-11-19T01:37:54.988615: step 3480, loss 0.368262, acc 0.814453, f1 0.811991\n",
      "2017-11-19T01:37:55.299835: step 3485, loss 0.0647177, acc 0.991211, f1 0.991217\n",
      "2017-11-19T01:37:55.601602: step 3490, loss 0.058234, acc 0.993164, f1 0.993169\n",
      "Current epoch:  194\n",
      "2017-11-19T01:37:55.907978: step 3495, loss 0.0504295, acc 0.991211, f1 0.991199\n",
      "2017-11-19T01:37:56.229165: step 3500, loss 0.0454311, acc 0.991211, f1 0.991207\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-19T01:37:56.459367: step 3500, loss 1.71478, acc 0.527966, f1 0.528149\n",
      "\n",
      "2017-11-19T01:37:56.772758: step 3505, loss 0.0487029, acc 0.993164, f1 0.993164\n",
      "2017-11-19T01:37:57.058752: step 3510, loss 0.0438689, acc 0.998428, f1 0.998428\n",
      "Current epoch:  195\n",
      "2017-11-19T01:37:57.372699: step 3515, loss 0.0408479, acc 0.99707, f1 0.997071\n",
      "2017-11-19T01:37:57.697628: step 3520, loss 0.0440775, acc 0.99707, f1 0.997071\n",
      "2017-11-19T01:37:57.995780: step 3525, loss 0.0412815, acc 0.992188, f1 0.992187\n",
      "Current epoch:  196\n",
      "2017-11-19T01:37:58.290905: step 3530, loss 0.0417428, acc 0.995117, f1 0.995118\n",
      "2017-11-19T01:37:58.594788: step 3535, loss 0.0425555, acc 0.996094, f1 0.996093\n",
      "2017-11-19T01:37:58.899757: step 3540, loss 0.038881, acc 0.996094, f1 0.996094\n",
      "2017-11-19T01:37:59.241095: step 3545, loss 0.0447995, acc 0.996094, f1 0.996094\n",
      "Current epoch:  197\n",
      "2017-11-19T01:37:59.543884: step 3550, loss 0.0434915, acc 0.995117, f1 0.995116\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:37:59.797870: step 3550, loss 1.78712, acc 0.526221, f1 0.52771\n",
      "\n",
      "2017-11-19T01:38:00.104682: step 3555, loss 0.043249, acc 0.995117, f1 0.995106\n",
      "2017-11-19T01:38:00.400626: step 3560, loss 0.0555861, acc 0.994141, f1 0.994166\n",
      "Current epoch:  198\n",
      "2017-11-19T01:38:00.732717: step 3565, loss 0.0899318, acc 0.981445, f1 0.981463\n",
      "2017-11-19T01:38:01.009335: step 3570, loss 0.0564251, acc 0.991211, f1 0.991207\n",
      "2017-11-19T01:38:01.301022: step 3575, loss 0.0504354, acc 0.994141, f1 0.994143\n",
      "2017-11-19T01:38:01.588526: step 3580, loss 0.0428019, acc 0.993164, f1 0.993161\n",
      "Current epoch:  199\n",
      "2017-11-19T01:38:01.858621: step 3585, loss 0.0342089, acc 0.998047, f1 0.998047\n",
      "2017-11-19T01:38:02.180177: step 3590, loss 0.0391544, acc 0.995117, f1 0.995114\n",
      "2017-11-19T01:38:02.499451: step 3595, loss 0.0441176, acc 0.994141, f1 0.994136\n",
      "2017-11-19T01:38:02.787161: step 3600, loss 0.0415657, acc 0.993711, f1 0.993711\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:38:03.016864: step 3600, loss 1.80612, acc 0.527384, f1 0.528598\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "num_epochs = 200\n",
    "\n",
    "num_checkpoints = 5\n",
    "print_train_every = 5\n",
    "evaluate_every = 50\n",
    "checkpoint_every = 10000000\n",
    "allow_soft_placement=True\n",
    "log_device_placement=False\n",
    "\n",
    "with graph.as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=allow_soft_placement,\n",
    "      log_device_placement=log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        \n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdadeltaOptimizer(1.0)\n",
    "        grads_and_vars = optimizer.compute_gradients(loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=num_checkpoints)\n",
    "\n",
    "#         # Write vocabulary\n",
    "#         vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              input_x: x_batch,\n",
    "              input_y: y_batch,\n",
    "            }\n",
    "            _, step, summaries, cur_loss, cur_accuracy, y_pred = sess.run(\n",
    "                [train_op, global_step, train_summary_op, loss, accuracy, predictions],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "#             print(y_pred)\n",
    "#             print(y_batch)\n",
    "            if step % print_train_every == 0:\n",
    "                f1 = f1_score(y_batch, y_pred, average = 'weighted')\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}, f1 {:g}\".format(time_str, step, cur_loss, cur_accuracy,\n",
    "                                                                     f1))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              input_x: x_batch,\n",
    "              input_y: y_batch,\n",
    "            }\n",
    "            step, summaries, cur_loss, cur_accuracy, y_pred = sess.run(\n",
    "                [global_step, dev_summary_op, loss, accuracy, predictions],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            f1 = f1_score(y_batch, y_pred, average = 'weighted')\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}, f1 {:g}\".format(time_str, step, cur_loss, cur_accuracy,\n",
    "                                                                    f1))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "                \n",
    "        def dev_step_batch(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              input_x: x_batch,\n",
    "              input_y: y_batch,\n",
    "            }\n",
    "            step, summaries, cur_loss, cur_accuracy, y_pred = sess.run(\n",
    "                [global_step, dev_summary_op, loss, accuracy, predictions],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            f1 = f1_score(y_batch, y_pred, average = 'weighted')\n",
    "#             print(\"{}: step {}, loss {:g}, acc {:g}, f1 {:g}\".format(time_str, step, cur_loss, cur_accuracy,\n",
    "#                                                                     f1))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "            return cur_loss, cur_accuracy, f1\n",
    "\n",
    "        \n",
    "        sess.run(embedding_init, feed_dict={embedding_placeholder: final_embeddings})\n",
    "        # Generate batches\n",
    "        batches = batch_iter(\n",
    "            list(zip(x_train, y_train)), batch_size, num_epochs)\n",
    "        \n",
    "        batches_test = list(batch_iter(\n",
    "            list(zip(x_test, y_test)), batch_size, 1))\n",
    "        \n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            \n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_test, y_test, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_3.5]",
   "language": "python",
   "name": "conda-env-tensorflow_3.5-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
