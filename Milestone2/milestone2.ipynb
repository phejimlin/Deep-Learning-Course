{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "\n",
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from tensorflow.contrib import learn\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from IPython.display import clear_output, Image, display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### Do not modify here ###### \n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = \"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = graph_def\n",
    "    #strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))\n",
    "###### Do not modify  here ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Train/test file separated, use in Supervised Phase\n",
    "    \"\"\"\n",
    "def load_data_and_labels(train_data_file, test_data_file):\n",
    "    \"\"\"\n",
    "    Loads MR polarity data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "    # Load data from files\n",
    "    train_data = pd.read_csv(train_data_file, sep='\\t', quoting=csv.QUOTE_NONE, header=None, names=['id', 'language', 'label', 'text'])\n",
    "    test_data = pd.read_csv(test_data_file, sep='\\t', quoting=csv.QUOTE_NONE, header=None, names=['id', 'language', 'label', 'text'])\n",
    "\n",
    "    x_train = train_data['text'].tolist()\n",
    "    y_train = train_data['label'].tolist()\n",
    "\n",
    "    x_test = test_data['text'].tolist()\n",
    "    y_test = test_data['label'].tolist()\n",
    "    \n",
    "    x_train = [s.strip() for s in x_train]\n",
    "    x_test = [s.strip() for s in x_test]\n",
    "    \n",
    "    label_encoding = {'neutral':0, 'positive':1, 'negative':2}\n",
    "    \n",
    "    y_train_encoding = [label_encoding[label] for label in y_train]    \n",
    "    y_test_encoding = [label_encoding[label] for label in y_test]\n",
    "\n",
    "    return [x_train, y_train_encoding, x_test, y_test_encoding]\n",
    "\n",
    "\"\"\"Load file without using pandas\n",
    "\"\"\"\n",
    "def load_without_pandas(paths, numbers={}):\n",
    "    label_encoding = {'neutral':0, 'positive':1, 'negative':2}\n",
    "    i=0\n",
    "    X = []\n",
    "    y = []\n",
    "    for path in paths:\n",
    "        try:\n",
    "            n = numbers[path]\n",
    "        except KeyError:\n",
    "            n = 0\n",
    "        i=0\n",
    "        with open(path, 'r', encoding='utf8') as f:\n",
    "            for line in f:\n",
    "                if n and i >= n:\n",
    "                    break\n",
    "                i += 1\n",
    "                splits = line.split('\\t')\n",
    "                y.append(label_encoding[splits[2]])\n",
    "                X.append(splits[3].rstrip())\n",
    "    y = np.array(y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "    return [X_train, y_train, X_test, y_test]\n",
    "\n",
    "\"\"\"\n",
    "    One single data, use in Distance-supervised Phase\n",
    "    \"\"\"\n",
    "def transform_data_and_labels(data):\n",
    "    \"\"\"\n",
    "    Loads MR polarity data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    x = np.array(data['text'].tolist())\n",
    "    y = data['label'].tolist()\n",
    "    \n",
    "    # encoding label\n",
    "    label_encoding = {'neutral':0, 'positive':1, 'negative':2}\n",
    "    y = [label_encoding[label] for label in y]    \n",
    "    \n",
    "    \n",
    "    y = np.array(y)\n",
    "    \n",
    "    # Randomly shuffle data\n",
    "    np.random.seed(10)\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "    x_shuffled = x[shuffle_indices]\n",
    "    y_shuffled = y[shuffle_indices]\n",
    "\n",
    "    # Split train/test set\n",
    "    # maybe we can use cross-validation to improve\n",
    "    dev_sample_index = -1 * int(0.1 * float(len(y)))\n",
    "    x_train, x_test = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "    y_train, y_test = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "    print(\"Train/Test split: {:d}/{:d}\".format(len(y_train), len(y_test)))\n",
    "    \n",
    "    return [x_train, y_train, x_test, y_test]\n",
    "\n",
    "\"\"\"Tokenize train and test tweets, find the maximum length of tweetm,\n",
    "    and find all tokens seen in the word dict\"\"\"\n",
    "def tokenize_tweet(train_tweets, test_tweets, word_dict):\n",
    "    all_tokens = {}\n",
    "    dropped = 0\n",
    "    # max_document_length = max([len(x.split(\" \")) for x in x_train_sentence])\n",
    "    ppl_re = re.compile(r'@\\S*')\n",
    "    url_re = re.compile(r'http\\S+')\n",
    "    esc_re = re.compile(r'\\\\u')\n",
    "    tknzr = TweetTokenizer()\n",
    "    # tknzr = TweetTokenizer(reduce_len=True)\n",
    "    \n",
    "    tokenized_tweets_all = []\n",
    "    max_document_length = 0\n",
    "    dropped = 0\n",
    "    for tweets in [train_tweets, test_tweets]:\n",
    "        tweets = [url_re.sub('URLTOK', ppl_re.sub('USRTOK', tweet.lower())) for tweet in tweets]\n",
    "        tokenized_tweets = []\n",
    "        for tweet in tweets:\n",
    "            if len(esc_re.findall(tweet)) > 6:\n",
    "                dropped += 1\n",
    "                continue\n",
    "            tokenized_tweet = tknzr.tokenize(tweet)\n",
    "            if len(tokenized_tweet) > 65:\n",
    "                dropped += 1\n",
    "                continue\n",
    "            for token in tokenized_tweet:\n",
    "                if token in word_dict:\n",
    "                    all_tokens[token] = True\n",
    "            tokenized_tweets. append(tokenized_tweet)     \n",
    "        tokenized_tweets_all.append(tokenized_tweets)\n",
    "        max_document_length = max(max_document_length, max([len(tweet) for tweet in tokenized_tweets]))\n",
    "    print(max_document_length)\n",
    "    print(\"dropped \", dropped)\n",
    "    return tokenized_tweets_all[0], tokenized_tweets_all[1], all_tokens, max_document_length\n",
    "\n",
    "\"\"\"\n",
    "    This function assumes that the last word in the word embedding is a zero vector, and will use it as UNKNOWN WORDS.\n",
    "    Padding will be num_voc, and unknown words will be num_voc-1.\n",
    "    The input 'num_voc' equals to the shape[0] of the word embedding.\n",
    "    Also returns all seen tokens for reducing word embedding.\n",
    "\"\"\"\n",
    "def process_tweet(train_tweets, test_tweets, word_dict, max_document_length):\n",
    "    x = []\n",
    "    num_voc = len(word_dict)\n",
    "    for tokenized_tweets in [train_tweets, test_tweets]:\n",
    "        x_curr = []\n",
    "        for tokenized_tweet in tokenized_tweets:\n",
    "#             if len(tokenized_tweet) == max_document_length:\n",
    "#                 print(tokenized_tweet)\n",
    "            \"\"\"Not sure if original paper does this, but since index 0 means USRTOK, padding should be a number\n",
    "            higher than total word count, so tf.nn.embedding_lookup will return a tensor of 0 insted of USRTOK.\"\"\"\n",
    "        #     temp = np.zeros(max_document_length, dtype=np.int).tolist()\n",
    "            temp = (np.ones(max_document_length, dtype=np.int)*(num_voc)).tolist()\n",
    "\n",
    "            for index, word in enumerate(tokenized_tweet):\n",
    "                if word in word_dict:\n",
    "#                     temp[index] = word_dict[word][0]\n",
    "                    temp[index] = word_dict[word]\n",
    "                else:\n",
    "                    temp[index] = num_voc-1\n",
    "            x_curr.append(temp)\n",
    "        x_curr = np.array(x_curr)\n",
    "        x.append(x_curr)\n",
    "    \n",
    "    return x[0], x[1]\n",
    "\n",
    "\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Current epoch: \", epoch)\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            \n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            if batch_num == num_batches_per_epoch-1:\n",
    "                yield shuffled_data[start_index:end_index], True\n",
    "            else:\n",
    "                yield shuffled_data[start_index:end_index], False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Two Layers CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SentimentTwoLayerCNNModel(object):\n",
    "    \"\"\"\n",
    "    Two layer CNN for sentiment classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "      self, sequence_length, num_classes, vocabulary_size,\n",
    "      embedding_size, first_layer_filter_sizes, second_layer_filter_sizes, num_filters,\n",
    "        first_pool_window_sizes, first_pool_strides, l2_reg_lambda=0.0):\n",
    "        \n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.int64, [None], name=\"input_y\")\n",
    "\n",
    "        # Load word embeddings\n",
    "        self.embeddings_words = tf.Variable(tf.constant(0.0, shape=[vocabulary_size, embedding_size]),\n",
    "                         name=\"embedding_words\")\n",
    "        embedding_padding = tf.Variable(tf.constant(0.0, shape=[1, embedding_size]),\n",
    "                         trainable = False, name=\"embedding_padding\")  # for unknown word its word embedding and set untrainable\n",
    "        self.embedding_placeholder = tf.placeholder(tf.float32, [vocabulary_size, embedding_size], name='word_embedding_placeholder')\n",
    "        self.embedding_init = self.embeddings_words.assign(self.embedding_placeholder)  # assign exist word embeddings\n",
    "        embeddings = tf.concat([self.embeddings_words, embedding_padding], 0, name = 'embedding')\n",
    "        embedded_chars = tf.nn.embedding_lookup(embeddings, self.input_x)\n",
    "        embedded_chars_expanded = tf.expand_dims(embedded_chars, -1)  # expand 1 dimension for channel [?,?,?,1]\n",
    "        \n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "        \n",
    "        # Create first cnn : a convolution + maxpool layer for each filter size    \n",
    "        # 1st Convolution Layer\n",
    "        for i, filter_size in enumerate(first_layer_filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-1\"):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                print(\"First CNN filter\", W.shape)\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    embedded_chars_expanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                print(\"output of first cnn\", h.shape)\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, first_pool_window_sizes[i], 1, 1],\n",
    "                    strides=[1, first_pool_strides[i], 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "\n",
    "        for i, filter_size in enumerate(second_layer_filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-2\"):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, 1, num_filters, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                print(\"Second CNN filter\", W.shape)\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    pooled,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                print(\"output of second cnn\", h.shape)\n",
    "                # Maxpooling over the outputs and transform for next layer, so the \"channel\" of convolution \n",
    "                # will become \"input_width\" for next layer\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, h.shape[1], 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "\n",
    "\n",
    "        h_pool_flat = tf.reshape(pooled, [-1, num_filters])  # flatten pooling layers\n",
    "        print(\"h_pool_flat\", h_pool_flat.shape)\n",
    "\n",
    "        # Add dropout\n",
    "    #     with tf.name_scope(\"dropout\"):\n",
    "    #         self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Final (unnormalized) scores and predictions\n",
    "\n",
    "        # Fully connected hidden layer\n",
    "        with tf.name_scope(\"hidden\"):\n",
    "            with tf.variable_scope(\"hidden\"):\n",
    "                W = tf.get_variable(\n",
    "                    \"W\",\n",
    "                    shape=[num_filters, num_filters],\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                l2_loss += tf.nn.l2_loss(W)\n",
    "                l2_loss += tf.nn.l2_loss(b)\n",
    "                out = tf.nn.relu(tf.nn.xw_plus_b(h_pool_flat, W, b))\n",
    "\n",
    "\n",
    "        with tf.name_scope(\"output\"):\n",
    "            with tf.variable_scope(\"output\"):\n",
    "                W = tf.get_variable(\n",
    "                    \"W\",\n",
    "                    shape=[num_filters, num_classes],\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "                l2_loss += tf.nn.l2_loss(W)\n",
    "                l2_loss += tf.nn.l2_loss(b)\n",
    "                scores = tf.nn.xw_plus_b(out, W, b, name=\"scores\")\n",
    "                print(\"scores\", scores.shape)\n",
    "                self.predictions = tf.argmax(scores, 1, name=\"predictions\")\n",
    "                print(\"predictions\", self.predictions.shape)\n",
    "\n",
    "\n",
    "        # Calculate mean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=scores, labels=self.input_y)\n",
    "            print(\"losses\", losses.shape)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, self.input_y)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pre-train word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# original_embeddings = np.load('./data/embed_tweets_en_590M_52D_data/en_word2vec_52_paper.npy')\n",
    "original_word_dict = {}\n",
    "with open('./data/embed_tweets_en_590M_52D_data/vocabulary_dict_52_paper.pickle', 'rb') as myfile:\n",
    "    original_word_dict = pickle.load(myfile)\n",
    "original_embeddings = np.load('./data/embed_tweets_en_590M_52D_data/en_word2vec_52_paper.npy')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6139582\n",
      "(9770612, 52)\n"
     ]
    }
   ],
   "source": [
    "print(len(original_word_dict.items()))\n",
    "print(original_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load distant-supervision phase and supervised phase dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "dropped  14\n",
      "1066546\n",
      "118504\n"
     ]
    }
   ],
   "source": [
    "files = ['./data/distant_data/sad_processed',\n",
    "        './data/distant_data/smile_processed']\n",
    "nums = {'./data/distant_data/smile_processed': 660000}\n",
    "x_train_distance, y_train_distance, x_test_distance, y_test_distance = load_without_pandas(files, nums)\n",
    "# Tokenize, get all seen words, get max length\n",
    "x_train_distance, x_test_distance, all_words_distance, max_length = tokenize_tweet(x_train_distance, x_test_distance, original_word_dict)\n",
    "# Transform tokens into indices in word embedding\n",
    "x_train_distance, x_test_distance = process_tweet(x_train_distance, x_test_distance,\n",
    "                                                  original_word_dict, max_length)\n",
    "print(len(x_train_distance))\n",
    "print(len(x_test_distance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20632\n",
      "53\n",
      "dropped  0\n"
     ]
    }
   ],
   "source": [
    "#Load label data\n",
    "x_train_sentence, y_train, x_test_sentence, y_test = load_data_and_labels('./data/supervised_data/en_full.tsv.txt', './data/supervised_data/en_test.tsv')\n",
    "print(len(x_test_sentence))\n",
    "x_train_token, x_test_token, all_words, max_length = tokenize_tweet(x_train_sentence, x_test_sentence, original_word_dict)\n",
    "del x_test_sentence,x_train_sentence\n",
    "x_train, x_test = process_tweet(x_train_token, x_test_token, original_word_dict, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.203248 -0.254872 -0.248311 ...,  0.014432 -0.674741 -0.042006]\n",
      " [-0.448944 -0.00898   0.26073  ..., -0.55629  -0.12844  -0.123721]\n",
      " [-0.538877  0.244948  0.381833 ..., -0.527609 -0.354412 -0.041238]\n",
      " ..., \n",
      " [-0.191639  0.128814  0.526031 ..., -0.284907  0.533271  0.094139]\n",
      " [ 0.061211  0.26357   0.02929  ..., -0.3555   -0.009947  0.019959]\n",
      " [ 0.        0.        0.       ...,  0.        0.        0.      ]]\n"
     ]
    }
   ],
   "source": [
    "# # Reduce word embedding size, remove unused words\n",
    "all_words.update(all_words_distance) # conbine words in distant phase and supervised phase\n",
    "reduced_final_embeddings = np.zeros((len(all_words)+1,original_embeddings.shape[1]))\n",
    "word_dict_ = {}\n",
    "for i, w in enumerate(all_words.keys()):\n",
    "    word_dict_[w] = i\n",
    "    reduced_final_embeddings[i] = original_embeddings[original_word_dict[w]]\n",
    "print(reduced_final_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Replace embeddings and save memory\n",
    "final_embeddings = reduced_final_embeddings\n",
    "word_dict = word_dict_\n",
    "del reduced_final_embeddings\n",
    "del word_dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46691\n",
      "(46692, 52)\n"
     ]
    }
   ],
   "source": [
    "print(len(word_dict.items()))\n",
    "print(final_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distant Supervision phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First CNN filter (4, 52, 1, 200)\n",
      "output of first cnn (?, 62, 1, 200)\n",
      "Second CNN filter (3, 1, 200, 200)\n",
      "output of second cnn (?, 28, 1, 200)\n",
      "h_pool_flat (?, 200)\n",
      "scores (?, 3)\n",
      "predictions (?,)\n",
      "losses (?,)\n",
      "Writing to /home/phejimlin/Documents/Machine-learning/Milestone2/runs/1511681274\n",
      "\n",
      "Current epoch:  0\n",
      "Current epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phejimlin/anaconda3/envs/tensorflow_3.5/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T15:27:58.129789: step 5, loss 1.79984, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:27:58.453427: step 10, loss 1.75882, acc 0.436523, f1 0.344448\n",
      "2017-11-26T15:27:58.777459: step 15, loss 1.68732, acc 0.574219, f1 0.42858\n",
      "2017-11-26T15:27:59.101434: step 20, loss 1.66178, acc 0.542969, f1 0.504937\n",
      "2017-11-26T15:27:59.426767: step 25, loss 1.69396, acc 0.525391, f1 0.361921\n",
      "2017-11-26T15:27:59.745288: step 30, loss 1.64602, acc 0.442383, f1 0.326612\n",
      "2017-11-26T15:28:00.050185: step 35, loss 1.62419, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:28:00.355315: step 40, loss 1.57387, acc 0.542969, f1 0.483719\n",
      "2017-11-26T15:28:00.679105: step 45, loss 1.55699, acc 0.546875, f1 0.392537\n",
      "2017-11-26T15:28:00.999855: step 50, loss 1.53988, acc 0.545898, f1 0.395418\n",
      "2017-11-26T15:28:01.300092: step 55, loss 1.51519, acc 0.567383, f1 0.419436\n",
      "2017-11-26T15:28:01.611457: step 60, loss 1.49884, acc 0.580078, f1 0.510386\n",
      "2017-11-26T15:28:01.901549: step 65, loss 1.49189, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:28:02.204965: step 70, loss 1.47541, acc 0.519531, f1 0.521825\n",
      "2017-11-26T15:28:02.487507: step 75, loss 1.4536, acc 0.556641, f1 0.399496\n",
      "2017-11-26T15:28:02.821415: step 80, loss 1.43353, acc 0.561523, f1 0.405252\n",
      "2017-11-26T15:28:03.154163: step 85, loss 1.44756, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:28:03.480989: step 90, loss 1.44142, acc 0.446289, f1 0.282611\n",
      "2017-11-26T15:28:03.805829: step 95, loss 1.40425, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:28:04.138020: step 100, loss 1.38369, acc 0.513672, f1 0.512346\n",
      "2017-11-26T15:28:04.474474: step 105, loss 1.37737, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:28:04.786720: step 110, loss 1.3516, acc 0.557617, f1 0.413494\n",
      "2017-11-26T15:28:05.096666: step 115, loss 1.34122, acc 0.547852, f1 0.408916\n",
      "2017-11-26T15:28:05.412436: step 120, loss 1.32617, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:28:05.717295: step 125, loss 1.32218, acc 0.491211, f1 0.492838\n",
      "2017-11-26T15:28:06.054501: step 130, loss 1.34488, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:28:06.360424: step 135, loss 1.30917, acc 0.454102, f1 0.294883\n",
      "2017-11-26T15:28:06.682628: step 140, loss 1.28085, acc 0.518555, f1 0.461031\n",
      "2017-11-26T15:28:06.985553: step 145, loss 1.26806, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:28:07.319099: step 150, loss 1.2539, acc 0.548828, f1 0.399194\n",
      "2017-11-26T15:28:07.646225: step 155, loss 1.23548, acc 0.585938, f1 0.436755\n",
      "2017-11-26T15:28:07.972995: step 160, loss 1.2607, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:28:08.282951: step 165, loss 1.23158, acc 0.473633, f1 0.454947\n",
      "2017-11-26T15:28:08.591863: step 170, loss 1.20354, acc 0.577148, f1 0.422408\n",
      "2017-11-26T15:28:08.883559: step 175, loss 1.19949, acc 0.549805, f1 0.394593\n",
      "2017-11-26T15:28:09.186679: step 180, loss 1.18609, acc 0.567383, f1 0.429328\n",
      "2017-11-26T15:28:09.508527: step 185, loss 1.16954, acc 0.588867, f1 0.436493\n",
      "2017-11-26T15:28:09.836467: step 190, loss 1.16554, acc 0.563477, f1 0.413039\n",
      "2017-11-26T15:28:10.155255: step 195, loss 1.15618, acc 0.563477, f1 0.429445\n",
      "2017-11-26T15:28:10.479189: step 200, loss 1.18654, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:28:10.813965: step 205, loss 1.13575, acc 0.575195, f1 0.428071\n",
      "2017-11-26T15:28:11.131986: step 210, loss 1.14826, acc 0.462891, f1 0.428291\n",
      "2017-11-26T15:28:11.439538: step 215, loss 1.11837, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:28:11.770037: step 220, loss 1.11645, acc 0.547852, f1 0.391967\n",
      "2017-11-26T15:28:12.108340: step 225, loss 1.1043, acc 0.535156, f1 0.45263\n",
      "2017-11-26T15:28:12.435237: step 230, loss 1.09133, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:28:12.759558: step 235, loss 1.09177, acc 0.574219, f1 0.418909\n",
      "2017-11-26T15:28:13.076053: step 240, loss 1.07713, acc 0.538086, f1 0.416095\n",
      "2017-11-26T15:28:13.415038: step 245, loss 1.11419, acc 0.438477, f1 0.267313\n",
      "2017-11-26T15:28:13.745624: step 250, loss 1.07058, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:28:14.079338: step 255, loss 1.04414, acc 0.580078, f1 0.427351\n",
      "2017-11-26T15:28:14.392256: step 260, loss 1.0751, acc 0.530273, f1 0.367503\n",
      "2017-11-26T15:28:14.720510: step 265, loss 1.03928, acc 0.53418, f1 0.470935\n",
      "2017-11-26T15:28:15.056942: step 270, loss 1.04252, acc 0.53125, f1 0.368622\n",
      "2017-11-26T15:28:15.372127: step 275, loss 1.03116, acc 0.499023, f1 0.497721\n",
      "2017-11-26T15:28:15.669369: step 280, loss 1.01603, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:28:15.956082: step 285, loss 1.01186, acc 0.530273, f1 0.482688\n",
      "2017-11-26T15:28:16.294155: step 290, loss 1.00206, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:28:16.629059: step 295, loss 1.00333, acc 0.5, f1 0.50061\n",
      "2017-11-26T15:28:16.959769: step 300, loss 0.997475, acc 0.533203, f1 0.370865\n",
      "2017-11-26T15:28:17.289757: step 305, loss 0.974352, acc 0.578125, f1 0.423577\n",
      "2017-11-26T15:28:17.618415: step 310, loss 0.979044, acc 0.529297, f1 0.521938\n",
      "2017-11-26T15:28:17.946185: step 315, loss 0.96953, acc 0.550781, f1 0.393336\n",
      "2017-11-26T15:28:18.273412: step 320, loss 0.960357, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:28:18.606620: step 325, loss 0.957617, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:28:18.930952: step 330, loss 0.953443, acc 0.548828, f1 0.405054\n",
      "2017-11-26T15:28:19.277691: step 335, loss 0.945014, acc 0.55957, f1 0.402595\n",
      "2017-11-26T15:28:19.611392: step 340, loss 0.94489, acc 0.524414, f1 0.442524\n",
      "2017-11-26T15:28:19.922814: step 345, loss 0.938938, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:28:20.253165: step 350, loss 0.923759, acc 0.582031, f1 0.42826\n",
      "2017-11-26T15:28:20.568629: step 355, loss 0.951315, acc 0.53125, f1 0.368622\n",
      "2017-11-26T15:28:20.877267: step 360, loss 0.917254, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:28:21.195147: step 365, loss 0.920888, acc 0.521484, f1 0.491813\n",
      "2017-11-26T15:28:21.515006: step 370, loss 0.915868, acc 0.522461, f1 0.519588\n",
      "2017-11-26T15:28:21.803116: step 375, loss 0.905528, acc 0.560547, f1 0.41523\n",
      "2017-11-26T15:28:22.125376: step 380, loss 0.901492, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:28:22.442180: step 385, loss 0.90258, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:28:22.754189: step 390, loss 0.895429, acc 0.540039, f1 0.439489\n",
      "2017-11-26T15:28:23.086209: step 395, loss 0.883674, acc 0.580078, f1 0.425916\n",
      "2017-11-26T15:28:23.410575: step 400, loss 0.880198, acc 0.582031, f1 0.42826\n",
      "2017-11-26T15:28:23.733402: step 405, loss 0.876637, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:28:24.050023: step 410, loss 0.87806, acc 0.538086, f1 0.412669\n",
      "2017-11-26T15:28:24.353535: step 415, loss 0.877483, acc 0.533203, f1 0.384964\n",
      "2017-11-26T15:28:24.663321: step 420, loss 0.876921, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:28:24.976951: step 425, loss 0.892643, acc 0.429688, f1 0.258282\n",
      "2017-11-26T15:28:25.292881: step 430, loss 0.865022, acc 0.548828, f1 0.400778\n",
      "2017-11-26T15:28:25.619212: step 435, loss 0.852048, acc 0.579102, f1 0.424746\n",
      "2017-11-26T15:28:25.926505: step 440, loss 0.849757, acc 0.583984, f1 0.430607\n",
      "2017-11-26T15:28:26.256263: step 445, loss 0.855767, acc 0.536133, f1 0.386063\n",
      "2017-11-26T15:28:26.576627: step 450, loss 0.863438, acc 0.466797, f1 0.459156\n",
      "2017-11-26T15:28:26.877069: step 455, loss 0.843589, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:28:27.194859: step 460, loss 0.844227, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:28:27.517611: step 465, loss 0.835871, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:28:27.827696: step 470, loss 0.855101, acc 0.536133, f1 0.374236\n",
      "2017-11-26T15:28:28.143971: step 475, loss 0.838028, acc 0.541016, f1 0.385719\n",
      "2017-11-26T15:28:28.482145: step 480, loss 0.837777, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:28:28.805834: step 485, loss 0.821461, acc 0.578125, f1 0.423577\n",
      "2017-11-26T15:28:29.134968: step 490, loss 0.828223, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:28:29.463887: step 495, loss 0.827473, acc 0.53418, f1 0.399836\n",
      "2017-11-26T15:28:29.801861: step 500, loss 0.832152, acc 0.473633, f1 0.458554\n",
      "2017-11-26T15:28:30.130680: step 505, loss 0.839513, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:28:30.466944: step 510, loss 0.810983, acc 0.587891, f1 0.435314\n",
      "2017-11-26T15:28:30.792579: step 515, loss 0.8093, acc 0.584961, f1 0.434254\n",
      "2017-11-26T15:28:31.124904: step 520, loss 0.818697, acc 0.533203, f1 0.370865\n",
      "2017-11-26T15:28:31.449603: step 525, loss 0.810834, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:28:31.776874: step 530, loss 0.811407, acc 0.537109, f1 0.376045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T15:28:32.092966: step 535, loss 0.80677, acc 0.5625, f1 0.433916\n",
      "2017-11-26T15:28:32.402120: step 540, loss 0.809048, acc 0.527344, f1 0.513013\n",
      "2017-11-26T15:28:32.728260: step 545, loss 0.803593, acc 0.560547, f1 0.466718\n",
      "2017-11-26T15:28:33.059763: step 550, loss 0.802991, acc 0.550781, f1 0.454034\n",
      "2017-11-26T15:28:33.372134: step 555, loss 0.807443, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:28:33.671146: step 560, loss 0.798617, acc 0.564453, f1 0.441768\n",
      "2017-11-26T15:28:33.990447: step 565, loss 0.792619, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:28:34.304528: step 570, loss 0.791351, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:28:34.623681: step 575, loss 0.79344, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:28:34.952944: step 580, loss 0.791107, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:28:35.285086: step 585, loss 0.799426, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:28:35.590235: step 590, loss 0.790093, acc 0.543945, f1 0.495733\n",
      "2017-11-26T15:28:35.901739: step 595, loss 0.786358, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:28:36.229887: step 600, loss 0.782148, acc 0.555664, f1 0.401802\n",
      "2017-11-26T15:28:36.538988: step 605, loss 0.779085, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:28:36.851453: step 610, loss 0.787022, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:28:37.157121: step 615, loss 0.783357, acc 0.540039, f1 0.381167\n",
      "2017-11-26T15:28:37.482415: step 620, loss 0.779652, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:28:37.798651: step 625, loss 0.781281, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:28:38.116417: step 630, loss 0.777264, acc 0.563477, f1 0.470771\n",
      "2017-11-26T15:28:38.439623: step 635, loss 0.803834, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:28:38.750520: step 640, loss 0.772536, acc 0.551758, f1 0.39412\n",
      "2017-11-26T15:28:39.053194: step 645, loss 0.771387, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:28:39.368980: step 650, loss 0.764652, acc 0.579102, f1 0.424746\n",
      "2017-11-26T15:28:39.676573: step 655, loss 0.767885, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:28:39.984417: step 660, loss 0.771458, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:28:40.319134: step 665, loss 0.769596, acc 0.552734, f1 0.456318\n",
      "2017-11-26T15:28:40.646527: step 670, loss 0.770014, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:28:40.957703: step 675, loss 0.769527, acc 0.52832, f1 0.468903\n",
      "2017-11-26T15:28:41.292736: step 680, loss 0.762537, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:28:41.619620: step 685, loss 0.766757, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:28:41.954580: step 690, loss 0.773476, acc 0.524414, f1 0.360808\n",
      "2017-11-26T15:28:42.253082: step 695, loss 0.766359, acc 0.53125, f1 0.496746\n",
      "2017-11-26T15:28:42.556147: step 700, loss 0.7663, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:28:42.868516: step 705, loss 0.763295, acc 0.53418, f1 0.371988\n",
      "2017-11-26T15:28:43.170748: step 710, loss 0.763869, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:28:43.479148: step 715, loss 0.75446, acc 0.582031, f1 0.4293\n",
      "2017-11-26T15:28:43.803917: step 720, loss 0.750996, acc 0.579102, f1 0.424746\n",
      "2017-11-26T15:28:44.114619: step 725, loss 0.756556, acc 0.549805, f1 0.390787\n",
      "2017-11-26T15:28:44.428035: step 730, loss 0.748144, acc 0.583984, f1 0.430607\n",
      "2017-11-26T15:28:44.763294: step 735, loss 0.749163, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:28:45.096302: step 740, loss 0.768804, acc 0.52832, f1 0.365267\n",
      "2017-11-26T15:28:45.431093: step 745, loss 0.757959, acc 0.526367, f1 0.499822\n",
      "2017-11-26T15:28:45.761119: step 750, loss 0.751197, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:28:46.099944: step 755, loss 0.747552, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:28:46.456800: step 760, loss 0.749532, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:28:46.789139: step 765, loss 0.74801, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:28:47.111973: step 770, loss 0.746404, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:28:47.434576: step 775, loss 0.751364, acc 0.556641, f1 0.510737\n",
      "2017-11-26T15:28:47.761080: step 780, loss 0.747326, acc 0.569336, f1 0.422946\n",
      "2017-11-26T15:28:48.075743: step 785, loss 0.744306, acc 0.597656, f1 0.519583\n",
      "2017-11-26T15:28:48.390625: step 790, loss 0.744997, acc 0.580078, f1 0.438661\n",
      "2017-11-26T15:28:48.681859: step 795, loss 0.74439, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:28:49.005142: step 800, loss 0.744822, acc 0.566406, f1 0.427254\n",
      "2017-11-26T15:28:49.314290: step 805, loss 0.741603, acc 0.573242, f1 0.417744\n",
      "2017-11-26T15:28:49.647053: step 810, loss 0.746102, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:28:49.976130: step 815, loss 0.744616, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:28:50.271988: step 820, loss 0.738585, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:28:50.594469: step 825, loss 0.740209, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:28:50.907943: step 830, loss 0.747252, acc 0.53125, f1 0.368622\n",
      "2017-11-26T15:28:51.248085: step 835, loss 0.740535, acc 0.560547, f1 0.404442\n",
      "2017-11-26T15:28:51.569588: step 840, loss 0.74324, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:28:51.898947: step 845, loss 0.733992, acc 0.584961, f1 0.431783\n",
      "2017-11-26T15:28:52.236532: step 850, loss 0.754455, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:28:52.563910: step 855, loss 0.734435, acc 0.581055, f1 0.428128\n",
      "2017-11-26T15:28:52.899242: step 860, loss 0.747928, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:28:53.224118: step 865, loss 0.741547, acc 0.538086, f1 0.376489\n",
      "2017-11-26T15:28:53.555474: step 870, loss 0.739463, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:28:53.879371: step 875, loss 0.737538, acc 0.556641, f1 0.401573\n",
      "2017-11-26T15:28:54.186304: step 880, loss 0.748812, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:28:54.488479: step 885, loss 0.748683, acc 0.446289, f1 0.426179\n",
      "2017-11-26T15:28:54.811021: step 890, loss 0.736211, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:28:55.143157: step 895, loss 0.735829, acc 0.577148, f1 0.473758\n",
      "2017-11-26T15:28:55.478934: step 900, loss 0.733579, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:28:55.816531: step 905, loss 0.728964, acc 0.577148, f1 0.422408\n",
      "2017-11-26T15:28:56.134491: step 910, loss 0.733758, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:28:56.458021: step 915, loss 0.736735, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:28:56.790297: step 920, loss 0.734356, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:28:57.139062: step 925, loss 0.735932, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:28:57.457193: step 930, loss 0.732694, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:28:57.769403: step 935, loss 0.731046, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:28:58.085853: step 940, loss 0.737745, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:28:58.389926: step 945, loss 0.735804, acc 0.509766, f1 0.49527\n",
      "2017-11-26T15:28:58.698303: step 950, loss 0.731005, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:28:59.014414: step 955, loss 0.742619, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:28:59.333445: step 960, loss 0.73143, acc 0.568359, f1 0.447481\n",
      "2017-11-26T15:28:59.648114: step 965, loss 0.729081, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:28:59.960083: step 970, loss 0.736412, acc 0.5, f1 0.48891\n",
      "2017-11-26T15:29:00.271840: step 975, loss 0.731884, acc 0.533203, f1 0.400283\n",
      "2017-11-26T15:29:00.591444: step 980, loss 0.731411, acc 0.538086, f1 0.376489\n",
      "2017-11-26T15:29:00.908904: step 985, loss 0.737174, acc 0.46875, f1 0.457463\n",
      "2017-11-26T15:29:01.211511: step 990, loss 0.727245, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:29:01.520150: step 995, loss 0.726827, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:29:01.840112: step 1000, loss 0.723859, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:29:02.175164: step 1005, loss 0.732849, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:29:02.507415: step 1010, loss 0.73113, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:29:02.832832: step 1015, loss 0.726461, acc 0.556641, f1 0.399845\n",
      "2017-11-26T15:29:03.168414: step 1020, loss 0.72803, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:29:03.482012: step 1025, loss 0.727186, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:29:03.808862: step 1030, loss 0.722575, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:29:04.147905: step 1035, loss 0.7259, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:29:04.488042: step 1040, loss 0.725116, acc 0.556641, f1 0.398099\n",
      "\n",
      "Evaluation:\n",
      "loss 0.727211, acc 0.545569, f1 0.473567\n",
      "\n",
      "Current epoch:  1\n",
      "2017-11-26T15:29:07.350481: step 1045, loss 0.725035, acc 0.56543, f1 0.426096\n",
      "2017-11-26T15:29:07.661082: step 1050, loss 0.734836, acc 0.546875, f1 0.386679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T15:29:07.975922: step 1055, loss 0.721804, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:29:08.267717: step 1060, loss 0.721071, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:29:08.585241: step 1065, loss 0.72898, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:29:08.886077: step 1070, loss 0.725048, acc 0.543945, f1 0.399349\n",
      "2017-11-26T15:29:09.207880: step 1075, loss 0.723226, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:29:09.509376: step 1080, loss 0.730776, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:29:09.809036: step 1085, loss 0.723735, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:29:10.102684: step 1090, loss 0.719732, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:29:10.443179: step 1095, loss 0.729016, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:29:10.769342: step 1100, loss 0.718936, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:29:11.090485: step 1105, loss 0.72063, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:29:11.413771: step 1110, loss 0.717869, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:29:11.736132: step 1115, loss 0.727411, acc 0.532227, f1 0.369743\n",
      "2017-11-26T15:29:12.057358: step 1120, loss 0.71531, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:29:12.375507: step 1125, loss 0.711784, acc 0.585938, f1 0.432959\n",
      "2017-11-26T15:29:12.695557: step 1130, loss 0.715962, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:29:13.012282: step 1135, loss 0.720926, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:29:13.335774: step 1140, loss 0.721589, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:29:13.665667: step 1145, loss 0.713172, acc 0.577148, f1 0.422408\n",
      "2017-11-26T15:29:13.998488: step 1150, loss 0.717099, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:29:14.319883: step 1155, loss 0.713845, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:29:14.645026: step 1160, loss 0.719521, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:29:14.971816: step 1165, loss 0.71871, acc 0.548828, f1 0.407214\n",
      "2017-11-26T15:29:15.304997: step 1170, loss 0.725177, acc 0.527344, f1 0.36415\n",
      "2017-11-26T15:29:15.631891: step 1175, loss 0.720533, acc 0.541992, f1 0.415677\n",
      "2017-11-26T15:29:15.969971: step 1180, loss 0.716954, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:29:16.292253: step 1185, loss 0.722403, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:29:16.633443: step 1190, loss 0.716897, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:29:16.965296: step 1195, loss 0.714706, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:29:17.299343: step 1200, loss 0.714418, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:29:17.626302: step 1205, loss 0.717728, acc 0.558594, f1 0.472813\n",
      "2017-11-26T15:29:17.951593: step 1210, loss 0.718812, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:29:18.286416: step 1215, loss 0.718008, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:29:18.612840: step 1220, loss 0.717974, acc 0.554688, f1 0.459383\n",
      "2017-11-26T15:29:18.933847: step 1225, loss 0.711318, acc 0.577148, f1 0.422408\n",
      "2017-11-26T15:29:19.270844: step 1230, loss 0.714584, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:29:19.600626: step 1235, loss 0.717829, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:29:19.928056: step 1240, loss 0.711401, acc 0.581055, f1 0.430184\n",
      "2017-11-26T15:29:20.242558: step 1245, loss 0.709714, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:29:20.544872: step 1250, loss 0.716142, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:29:20.845472: step 1255, loss 0.718375, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:29:21.185398: step 1260, loss 0.707924, acc 0.591797, f1 0.440035\n",
      "2017-11-26T15:29:21.520666: step 1265, loss 0.711059, acc 0.577148, f1 0.422408\n",
      "2017-11-26T15:29:21.842367: step 1270, loss 0.715987, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:29:22.176988: step 1275, loss 0.716618, acc 0.552734, f1 0.409845\n",
      "2017-11-26T15:29:22.510234: step 1280, loss 0.715948, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:29:22.829631: step 1285, loss 0.714767, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:29:23.138030: step 1290, loss 0.712318, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:29:23.453281: step 1295, loss 0.713196, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:29:23.772593: step 1300, loss 0.7149, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:29:24.090835: step 1305, loss 0.715392, acc 0.537109, f1 0.489703\n",
      "2017-11-26T15:29:24.384227: step 1310, loss 0.711711, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:29:24.703743: step 1315, loss 0.711898, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:29:25.031880: step 1320, loss 0.717899, acc 0.527344, f1 0.36415\n",
      "2017-11-26T15:29:25.346284: step 1325, loss 0.718132, acc 0.535156, f1 0.373111\n",
      "2017-11-26T15:29:25.673619: step 1330, loss 0.717276, acc 0.50293, f1 0.502939\n",
      "2017-11-26T15:29:25.989844: step 1335, loss 0.710221, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:29:26.316482: step 1340, loss 0.733357, acc 0.527344, f1 0.36415\n",
      "2017-11-26T15:29:26.637571: step 1345, loss 0.710411, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:29:26.970725: step 1350, loss 0.711058, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:29:27.299978: step 1355, loss 0.711105, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:29:27.632925: step 1360, loss 0.711898, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:29:27.943330: step 1365, loss 0.713743, acc 0.538086, f1 0.390277\n",
      "2017-11-26T15:29:28.251890: step 1370, loss 0.704904, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:29:28.543523: step 1375, loss 0.713125, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:29:28.851177: step 1380, loss 0.702001, acc 0.588867, f1 0.436493\n",
      "2017-11-26T15:29:29.155878: step 1385, loss 0.714273, acc 0.527344, f1 0.519602\n",
      "2017-11-26T15:29:29.466965: step 1390, loss 0.711365, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:29:29.763132: step 1395, loss 0.714604, acc 0.516602, f1 0.503801\n",
      "2017-11-26T15:29:30.080189: step 1400, loss 0.710308, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:29:30.402191: step 1405, loss 0.710736, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:29:30.710583: step 1410, loss 0.70947, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:29:31.010286: step 1415, loss 0.705782, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:29:31.355301: step 1420, loss 0.708254, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:29:31.700501: step 1425, loss 0.708446, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:29:32.027117: step 1430, loss 0.709987, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:29:32.341035: step 1435, loss 0.711016, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:29:32.649815: step 1440, loss 0.709767, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:29:32.983118: step 1445, loss 0.711581, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:29:33.316262: step 1450, loss 0.704964, acc 0.575195, f1 0.420074\n",
      "2017-11-26T15:29:33.633154: step 1455, loss 0.707989, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:29:33.954337: step 1460, loss 0.713157, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:29:34.273529: step 1465, loss 0.711957, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:29:34.597730: step 1470, loss 0.709021, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:29:34.922052: step 1475, loss 0.707106, acc 0.573242, f1 0.418456\n",
      "2017-11-26T15:29:35.242984: step 1480, loss 0.706303, acc 0.578125, f1 0.425652\n",
      "2017-11-26T15:29:35.566397: step 1485, loss 0.710018, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:29:35.891252: step 1490, loss 0.704973, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:29:36.213639: step 1495, loss 0.708195, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:29:36.542420: step 1500, loss 0.709682, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:29:36.871272: step 1505, loss 0.700882, acc 0.581055, f1 0.427088\n",
      "2017-11-26T15:29:37.208011: step 1510, loss 0.707491, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:29:37.542839: step 1515, loss 0.701793, acc 0.573242, f1 0.417744\n",
      "2017-11-26T15:29:37.894262: step 1520, loss 0.707837, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:29:38.208432: step 1525, loss 0.710952, acc 0.536133, f1 0.374236\n",
      "2017-11-26T15:29:38.545923: step 1530, loss 0.703471, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:29:38.881288: step 1535, loss 0.705342, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:29:39.186612: step 1540, loss 0.705527, acc 0.570312, f1 0.415302\n",
      "2017-11-26T15:29:39.465112: step 1545, loss 0.706899, acc 0.567383, f1 0.412527\n",
      "2017-11-26T15:29:39.769837: step 1550, loss 0.704202, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:29:40.094927: step 1555, loss 0.718646, acc 0.538086, f1 0.376489\n",
      "2017-11-26T15:29:40.414098: step 1560, loss 0.707314, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:29:40.739437: step 1565, loss 0.701845, acc 0.582031, f1 0.42826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T15:29:41.067687: step 1570, loss 0.708478, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:29:41.383024: step 1575, loss 0.702492, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:29:41.711831: step 1580, loss 0.703105, acc 0.573242, f1 0.417744\n",
      "2017-11-26T15:29:42.038625: step 1585, loss 0.700936, acc 0.573242, f1 0.417744\n",
      "2017-11-26T15:29:42.372309: step 1590, loss 0.70507, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:29:42.691980: step 1595, loss 0.706985, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:29:43.036232: step 1600, loss 0.707543, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:29:43.376786: step 1605, loss 0.70135, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:29:43.721195: step 1610, loss 0.699805, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:29:44.046392: step 1615, loss 0.705725, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:29:44.383234: step 1620, loss 0.713733, acc 0.535156, f1 0.373111\n",
      "2017-11-26T15:29:44.713395: step 1625, loss 0.70894, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:29:45.015536: step 1630, loss 0.706003, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:29:45.344323: step 1635, loss 0.706801, acc 0.538086, f1 0.376489\n",
      "2017-11-26T15:29:45.679161: step 1640, loss 0.699359, acc 0.573242, f1 0.417744\n",
      "2017-11-26T15:29:46.014460: step 1645, loss 0.711389, acc 0.53418, f1 0.371988\n",
      "2017-11-26T15:29:46.339312: step 1650, loss 0.707144, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:29:46.674890: step 1655, loss 0.705236, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:29:47.006475: step 1660, loss 0.697414, acc 0.576172, f1 0.421241\n",
      "2017-11-26T15:29:47.320188: step 1665, loss 0.700539, acc 0.573242, f1 0.417744\n",
      "2017-11-26T15:29:47.641409: step 1670, loss 0.703591, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:29:47.971834: step 1675, loss 0.704538, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:29:48.310770: step 1680, loss 0.709473, acc 0.484375, f1 0.483905\n",
      "2017-11-26T15:29:48.628285: step 1685, loss 0.699594, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:29:48.954768: step 1690, loss 0.707266, acc 0.536133, f1 0.399612\n",
      "2017-11-26T15:29:49.262956: step 1695, loss 0.702089, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:29:49.568526: step 1700, loss 0.702986, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:29:49.884563: step 1705, loss 0.701489, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:29:50.190652: step 1710, loss 0.699537, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:29:50.508825: step 1715, loss 0.703629, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:29:50.828172: step 1720, loss 0.701344, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:29:51.131674: step 1725, loss 0.712024, acc 0.452148, f1 0.425504\n",
      "2017-11-26T15:29:51.434398: step 1730, loss 0.703319, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:29:51.760882: step 1735, loss 0.70834, acc 0.519531, f1 0.355258\n",
      "2017-11-26T15:29:52.093586: step 1740, loss 0.700898, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:29:52.416615: step 1745, loss 0.701251, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:29:52.738759: step 1750, loss 0.703884, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:29:53.066366: step 1755, loss 0.701483, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:29:53.387331: step 1760, loss 0.699019, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:29:53.698418: step 1765, loss 0.717983, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:29:53.999044: step 1770, loss 0.701278, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:29:54.317036: step 1775, loss 0.705914, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:29:54.644392: step 1780, loss 0.701521, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:29:54.937546: step 1785, loss 0.70292, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:29:55.248942: step 1790, loss 0.700672, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:29:55.544113: step 1795, loss 0.703161, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:29:55.842685: step 1800, loss 0.701025, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:29:56.141520: step 1805, loss 0.701577, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:29:56.426783: step 1810, loss 0.700724, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:29:56.726224: step 1815, loss 0.701855, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:29:57.024909: step 1820, loss 0.698371, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:29:57.346176: step 1825, loss 0.711902, acc 0.514648, f1 0.349735\n",
      "2017-11-26T15:29:57.659018: step 1830, loss 0.706441, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:29:57.960426: step 1835, loss 0.704501, acc 0.530273, f1 0.367503\n",
      "2017-11-26T15:29:58.274158: step 1840, loss 0.697943, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:29:58.586413: step 1845, loss 0.698228, acc 0.573242, f1 0.417744\n",
      "2017-11-26T15:29:58.899826: step 1850, loss 0.69834, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:29:59.209506: step 1855, loss 0.703589, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:29:59.506582: step 1860, loss 0.698821, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:29:59.822462: step 1865, loss 0.701114, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:30:00.108557: step 1870, loss 0.698692, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:30:00.390603: step 1875, loss 0.701364, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:30:00.686812: step 1880, loss 0.699048, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:30:00.978664: step 1885, loss 0.702273, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:30:01.273367: step 1890, loss 0.698688, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:30:01.573127: step 1895, loss 0.699852, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:30:01.865156: step 1900, loss 0.70128, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:30:02.196679: step 1905, loss 0.700431, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:30:02.526110: step 1910, loss 0.703527, acc 0.541992, f1 0.458594\n",
      "2017-11-26T15:30:02.851247: step 1915, loss 0.698393, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:30:03.186328: step 1920, loss 0.696728, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:30:03.484877: step 1925, loss 0.701316, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:30:03.799376: step 1930, loss 0.697134, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:30:04.120001: step 1935, loss 0.707013, acc 0.518555, f1 0.354151\n",
      "2017-11-26T15:30:04.421526: step 1940, loss 0.699807, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:30:04.734949: step 1945, loss 0.697477, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:30:05.055799: step 1950, loss 0.703173, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:30:05.371478: step 1955, loss 0.700444, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:30:05.704038: step 1960, loss 0.696591, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:30:06.042068: step 1965, loss 0.707969, acc 0.532227, f1 0.369743\n",
      "2017-11-26T15:30:06.370283: step 1970, loss 0.700974, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:30:06.674675: step 1975, loss 0.703883, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:30:06.998488: step 1980, loss 0.703016, acc 0.538086, f1 0.376489\n",
      "2017-11-26T15:30:07.317309: step 1985, loss 0.701403, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:30:07.650919: step 1990, loss 0.698201, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:30:07.986626: step 1995, loss 0.698739, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:30:08.322224: step 2000, loss 0.701277, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:30:08.644243: step 2005, loss 0.689645, acc 0.591797, f1 0.440035\n",
      "2017-11-26T15:30:08.941754: step 2010, loss 0.704337, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:30:09.241137: step 2015, loss 0.692249, acc 0.582031, f1 0.42826\n",
      "2017-11-26T15:30:09.552680: step 2020, loss 0.697827, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:30:09.890774: step 2025, loss 0.697472, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:30:10.221676: step 2030, loss 0.708058, acc 0.530273, f1 0.367503\n",
      "2017-11-26T15:30:10.544547: step 2035, loss 0.699508, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:30:10.872654: step 2040, loss 0.701657, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:30:11.166187: step 2045, loss 0.692756, acc 0.578125, f1 0.423577\n",
      "2017-11-26T15:30:11.474056: step 2050, loss 0.703576, acc 0.544922, f1 0.505139\n",
      "2017-11-26T15:30:11.777798: step 2055, loss 0.699387, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:30:12.075228: step 2060, loss 0.700333, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:30:12.362243: step 2065, loss 0.701119, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:30:12.689791: step 2070, loss 0.696452, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:30:13.001449: step 2075, loss 0.701478, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:30:13.300886: step 2080, loss 0.691767, acc 0.598633, f1 0.448335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation:\n",
      "loss 0.70414, acc 0.511138, f1 0.508756\n",
      "\n",
      "Current epoch:  2\n",
      "2017-11-26T15:30:15.669777: step 2085, loss 0.705078, acc 0.480469, f1 0.477497\n",
      "2017-11-26T15:30:16.006348: step 2090, loss 0.702983, acc 0.538086, f1 0.376489\n",
      "2017-11-26T15:30:16.320365: step 2095, loss 0.699575, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:30:16.615763: step 2100, loss 0.694854, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:30:16.922956: step 2105, loss 0.69485, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:30:17.250300: step 2110, loss 0.70243, acc 0.530273, f1 0.367503\n",
      "2017-11-26T15:30:17.573674: step 2115, loss 0.700387, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:30:17.900365: step 2120, loss 0.698204, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:30:18.206790: step 2125, loss 0.69141, acc 0.580078, f1 0.425916\n",
      "2017-11-26T15:30:18.510126: step 2130, loss 0.698741, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:30:18.841004: step 2135, loss 0.702951, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:30:19.155337: step 2140, loss 0.691783, acc 0.580078, f1 0.425916\n",
      "2017-11-26T15:30:19.450272: step 2145, loss 0.696001, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:30:19.760798: step 2150, loss 0.698447, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:30:20.038970: step 2155, loss 0.700334, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:30:20.317193: step 2160, loss 0.69419, acc 0.573242, f1 0.417744\n",
      "2017-11-26T15:30:20.623731: step 2165, loss 0.698279, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:30:20.951980: step 2170, loss 0.702106, acc 0.536133, f1 0.401645\n",
      "2017-11-26T15:30:21.287187: step 2175, loss 0.697749, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:30:21.613425: step 2180, loss 0.695408, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:30:21.920956: step 2185, loss 0.698067, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:30:22.266310: step 2190, loss 0.699781, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:30:22.586195: step 2195, loss 0.700591, acc 0.545898, f1 0.386233\n",
      "2017-11-26T15:30:22.898295: step 2200, loss 0.703311, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:30:23.198640: step 2205, loss 0.697735, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:30:23.516869: step 2210, loss 0.69612, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:30:23.844701: step 2215, loss 0.694083, acc 0.582031, f1 0.42826\n",
      "2017-11-26T15:30:24.173253: step 2220, loss 0.701811, acc 0.543945, f1 0.458255\n",
      "2017-11-26T15:30:24.471011: step 2225, loss 0.690301, acc 0.581055, f1 0.427088\n",
      "2017-11-26T15:30:24.770695: step 2230, loss 0.694594, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:30:25.064399: step 2235, loss 0.697015, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:30:25.368269: step 2240, loss 0.694971, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:30:25.671861: step 2245, loss 0.694832, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:30:26.006201: step 2250, loss 0.700229, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:30:26.317036: step 2255, loss 0.6993, acc 0.560547, f1 0.418939\n",
      "2017-11-26T15:30:26.627063: step 2260, loss 0.691971, acc 0.574219, f1 0.418909\n",
      "2017-11-26T15:30:26.924414: step 2265, loss 0.699143, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:30:27.241793: step 2270, loss 0.69819, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:30:27.547620: step 2275, loss 0.697759, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:30:27.851747: step 2280, loss 0.700503, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:30:28.158772: step 2285, loss 0.695021, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:30:28.485432: step 2290, loss 0.703837, acc 0.481445, f1 0.480656\n",
      "2017-11-26T15:30:28.816460: step 2295, loss 0.698709, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:30:29.150454: step 2300, loss 0.697434, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:30:29.464011: step 2305, loss 0.697114, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:30:29.755019: step 2310, loss 0.701775, acc 0.538086, f1 0.376489\n",
      "2017-11-26T15:30:30.075771: step 2315, loss 0.694403, acc 0.577148, f1 0.422408\n",
      "2017-11-26T15:30:30.371462: step 2320, loss 0.687772, acc 0.588867, f1 0.436493\n",
      "2017-11-26T15:30:30.683528: step 2325, loss 0.697952, acc 0.568359, f1 0.412983\n",
      "2017-11-26T15:30:30.985464: step 2330, loss 0.698671, acc 0.566406, f1 0.410668\n",
      "2017-11-26T15:30:31.293294: step 2335, loss 0.693147, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:30:31.601572: step 2340, loss 0.69762, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:30:31.899274: step 2345, loss 0.70351, acc 0.487305, f1 0.468579\n",
      "2017-11-26T15:30:32.213730: step 2350, loss 0.700947, acc 0.547852, f1 0.526399\n",
      "2017-11-26T15:30:32.537565: step 2355, loss 0.703473, acc 0.536133, f1 0.374236\n",
      "2017-11-26T15:30:32.847836: step 2360, loss 0.692664, acc 0.574219, f1 0.418909\n",
      "2017-11-26T15:30:33.147558: step 2365, loss 0.698814, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:30:33.468668: step 2370, loss 0.693613, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:30:33.801507: step 2375, loss 0.691474, acc 0.577148, f1 0.422408\n",
      "2017-11-26T15:30:34.107206: step 2380, loss 0.694059, acc 0.574219, f1 0.418909\n",
      "2017-11-26T15:30:34.429422: step 2385, loss 0.693432, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:30:34.773260: step 2390, loss 0.700014, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:30:35.095660: step 2395, loss 0.695065, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:30:35.419853: step 2400, loss 0.698103, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:30:35.720509: step 2405, loss 0.697774, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:30:36.056388: step 2410, loss 0.691415, acc 0.575195, f1 0.420074\n",
      "2017-11-26T15:30:36.396735: step 2415, loss 0.694268, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:30:36.708200: step 2420, loss 0.69516, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:30:37.018460: step 2425, loss 0.698907, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:30:37.331264: step 2430, loss 0.694054, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:30:37.636957: step 2435, loss 0.702698, acc 0.535156, f1 0.373111\n",
      "2017-11-26T15:30:37.934373: step 2440, loss 0.706749, acc 0.523438, f1 0.359696\n",
      "2017-11-26T15:30:38.259221: step 2445, loss 0.694219, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:30:38.586956: step 2450, loss 0.695723, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:30:38.909209: step 2455, loss 0.698675, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:30:39.243599: step 2460, loss 0.699271, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:30:39.572765: step 2465, loss 0.697835, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:30:39.870456: step 2470, loss 0.700956, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:30:40.170932: step 2475, loss 0.691813, acc 0.574219, f1 0.418909\n",
      "2017-11-26T15:30:40.474843: step 2480, loss 0.698305, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:30:40.803381: step 2485, loss 0.699421, acc 0.537109, f1 0.375362\n",
      "2017-11-26T15:30:41.125049: step 2490, loss 0.693006, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:30:41.415933: step 2495, loss 0.702253, acc 0.533203, f1 0.370865\n",
      "2017-11-26T15:30:41.717540: step 2500, loss 0.697747, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:30:42.027899: step 2505, loss 0.699308, acc 0.56543, f1 0.463494\n",
      "2017-11-26T15:30:42.326533: step 2510, loss 0.691446, acc 0.580078, f1 0.425916\n",
      "2017-11-26T15:30:42.649904: step 2515, loss 0.692312, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:30:42.988207: step 2520, loss 0.695973, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:30:43.321190: step 2525, loss 0.694367, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:30:43.655110: step 2530, loss 0.697512, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:30:43.974103: step 2535, loss 0.697621, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:30:44.298687: step 2540, loss 0.700718, acc 0.538086, f1 0.522398\n",
      "2017-11-26T15:30:44.598302: step 2545, loss 0.697472, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:30:44.912603: step 2550, loss 0.696747, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:30:45.242766: step 2555, loss 0.694076, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:30:45.577493: step 2560, loss 0.697307, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:30:45.904976: step 2565, loss 0.696594, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:30:46.225300: step 2570, loss 0.700279, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:30:46.549967: step 2575, loss 0.697703, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:30:46.878860: step 2580, loss 0.69725, acc 0.567383, f1 0.411825\n",
      "2017-11-26T15:30:47.217056: step 2585, loss 0.693004, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:30:47.537017: step 2590, loss 0.69982, acc 0.548828, f1 0.388955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T15:30:47.861036: step 2595, loss 0.694422, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:30:48.164856: step 2600, loss 0.700133, acc 0.535156, f1 0.373111\n",
      "2017-11-26T15:30:48.469291: step 2605, loss 0.689098, acc 0.581055, f1 0.427088\n",
      "2017-11-26T15:30:48.784454: step 2610, loss 0.687813, acc 0.583008, f1 0.429433\n",
      "2017-11-26T15:30:49.075469: step 2615, loss 0.698781, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:30:49.389259: step 2620, loss 0.697271, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:30:49.672422: step 2625, loss 0.696922, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:30:49.991617: step 2630, loss 0.699015, acc 0.536133, f1 0.374236\n",
      "2017-11-26T15:30:50.301965: step 2635, loss 0.70268, acc 0.459961, f1 0.289822\n",
      "2017-11-26T15:30:50.611225: step 2640, loss 0.696041, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:30:50.941752: step 2645, loss 0.696611, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:30:51.274418: step 2650, loss 0.701357, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:30:51.588560: step 2655, loss 0.696047, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:30:51.892974: step 2660, loss 0.697244, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:30:52.212568: step 2665, loss 0.706248, acc 0.538086, f1 0.376489\n",
      "2017-11-26T15:30:52.531441: step 2670, loss 0.688763, acc 0.583984, f1 0.430607\n",
      "2017-11-26T15:30:52.862940: step 2675, loss 0.69614, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:30:53.207736: step 2680, loss 0.697329, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:30:53.544700: step 2685, loss 0.691922, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:30:53.866369: step 2690, loss 0.697403, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:30:54.172201: step 2695, loss 0.693369, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:30:54.480142: step 2700, loss 0.695313, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:30:54.790036: step 2705, loss 0.704617, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:30:55.118333: step 2710, loss 0.697215, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:30:55.439285: step 2715, loss 0.700166, acc 0.538086, f1 0.500417\n",
      "2017-11-26T15:30:55.758279: step 2720, loss 0.699297, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:30:56.069450: step 2725, loss 0.696605, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:30:56.383538: step 2730, loss 0.697521, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:30:56.722074: step 2735, loss 0.696109, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:30:57.036800: step 2740, loss 0.697448, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:30:57.354550: step 2745, loss 0.696016, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:30:57.693446: step 2750, loss 0.695699, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:30:58.011544: step 2755, loss 0.694035, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:30:58.308812: step 2760, loss 0.698008, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:30:58.603867: step 2765, loss 0.69611, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:30:58.901446: step 2770, loss 0.693633, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:30:59.231134: step 2775, loss 0.694776, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:30:59.537476: step 2780, loss 0.7079, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:30:59.866460: step 2785, loss 0.696091, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:31:00.188508: step 2790, loss 0.693897, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:31:00.507788: step 2795, loss 0.695444, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:31:00.806242: step 2800, loss 0.697611, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:31:01.130708: step 2805, loss 0.694626, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:31:01.422145: step 2810, loss 0.693341, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:31:01.730312: step 2815, loss 0.689498, acc 0.577148, f1 0.422408\n",
      "2017-11-26T15:31:02.025952: step 2820, loss 0.699066, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:31:02.333711: step 2825, loss 0.696591, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:31:02.666575: step 2830, loss 0.690799, acc 0.573242, f1 0.417744\n",
      "2017-11-26T15:31:02.997302: step 2835, loss 0.693591, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:31:03.309976: step 2840, loss 0.695732, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:31:03.602029: step 2845, loss 0.696156, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:31:03.898139: step 2850, loss 0.699132, acc 0.530273, f1 0.367503\n",
      "2017-11-26T15:31:04.208097: step 2855, loss 0.691538, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:31:04.517937: step 2860, loss 0.689155, acc 0.580078, f1 0.425916\n",
      "2017-11-26T15:31:04.821080: step 2865, loss 0.69569, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:31:05.111775: step 2870, loss 0.6929, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:31:05.425033: step 2875, loss 0.699099, acc 0.53418, f1 0.371988\n",
      "2017-11-26T15:31:05.723497: step 2880, loss 0.693375, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:31:06.026097: step 2885, loss 0.693012, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:31:06.356442: step 2890, loss 0.695381, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:31:06.682878: step 2895, loss 0.69454, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:31:06.995619: step 2900, loss 0.693085, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:31:07.320539: step 2905, loss 0.695511, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:31:07.649193: step 2910, loss 0.694244, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:31:07.985794: step 2915, loss 0.688196, acc 0.581055, f1 0.427088\n",
      "2017-11-26T15:31:08.317833: step 2920, loss 0.695088, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:31:08.638403: step 2925, loss 0.688784, acc 0.578125, f1 0.423577\n",
      "2017-11-26T15:31:08.953473: step 2930, loss 0.691827, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:31:09.285950: step 2935, loss 0.692797, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:31:09.598337: step 2940, loss 0.694564, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:31:09.929519: step 2945, loss 0.6917, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:31:10.270508: step 2950, loss 0.695546, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:31:10.582643: step 2955, loss 0.694271, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:31:10.897103: step 2960, loss 0.698326, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:31:11.224035: step 2965, loss 0.697269, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:31:11.549399: step 2970, loss 0.691928, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:31:11.878590: step 2975, loss 0.69486, acc 0.580078, f1 0.425916\n",
      "2017-11-26T15:31:12.205286: step 2980, loss 0.688968, acc 0.573242, f1 0.417744\n",
      "2017-11-26T15:31:12.505428: step 2985, loss 0.696719, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:31:12.829219: step 2990, loss 0.695655, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:31:13.150618: step 2995, loss 0.694146, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:31:13.472011: step 3000, loss 0.69443, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:31:13.783210: step 3005, loss 0.698652, acc 0.533203, f1 0.370865\n",
      "2017-11-26T15:31:14.091649: step 3010, loss 0.685086, acc 0.588867, f1 0.436493\n",
      "2017-11-26T15:31:14.385055: step 3015, loss 0.694506, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:31:14.678032: step 3020, loss 0.685435, acc 0.586914, f1 0.434136\n",
      "2017-11-26T15:31:14.992726: step 3025, loss 0.694564, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:31:15.321193: step 3030, loss 0.693598, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:31:15.641712: step 3035, loss 0.702043, acc 0.527344, f1 0.36415\n",
      "2017-11-26T15:31:15.942247: step 3040, loss 0.695754, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:31:16.264542: step 3045, loss 0.695705, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:31:16.582139: step 3050, loss 0.695142, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:31:16.881257: step 3055, loss 0.700617, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:31:17.179098: step 3060, loss 0.693556, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:31:17.496897: step 3065, loss 0.702266, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:31:17.817857: step 3070, loss 0.694798, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:31:18.150414: step 3075, loss 0.698473, acc 0.53125, f1 0.368622\n",
      "2017-11-26T15:31:18.485666: step 3080, loss 0.697169, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:31:18.810379: step 3085, loss 0.694529, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:31:19.125361: step 3090, loss 0.692383, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:31:19.435514: step 3095, loss 0.6958, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:31:19.737863: step 3100, loss 0.697705, acc 0.537109, f1 0.375362\n",
      "2017-11-26T15:31:20.031301: step 3105, loss 0.696751, acc 0.542969, f1 0.38214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T15:31:20.320488: step 3110, loss 0.692036, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:31:20.618525: step 3115, loss 0.701773, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:31:20.902190: step 3120, loss 0.692357, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:31:21.199197: step 3125, loss 0.690895, acc 0.567383, f1 0.410778\n",
      "\n",
      "Evaluation:\n",
      "loss 0.695691, acc 0.55893, f1 0.400903\n",
      "\n",
      "Current epoch:  3\n",
      "2017-11-26T15:31:23.663724: step 3130, loss 0.694363, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:31:23.988862: step 3135, loss 0.687418, acc 0.598633, f1 0.448335\n",
      "2017-11-26T15:31:24.322692: step 3140, loss 0.695273, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:31:24.649251: step 3145, loss 0.691122, acc 0.584961, f1 0.431783\n",
      "2017-11-26T15:31:24.940804: step 3150, loss 0.692444, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:31:25.261409: step 3155, loss 0.69687, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:31:25.588245: step 3160, loss 0.692523, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:31:25.904639: step 3165, loss 0.705111, acc 0.535156, f1 0.373111\n",
      "2017-11-26T15:31:26.216368: step 3170, loss 0.692446, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:31:26.546034: step 3175, loss 0.69265, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:31:26.839281: step 3180, loss 0.687871, acc 0.577148, f1 0.422408\n",
      "2017-11-26T15:31:27.157805: step 3185, loss 0.694663, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:31:27.475307: step 3190, loss 0.691248, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:31:27.786363: step 3195, loss 0.690239, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:31:28.096437: step 3200, loss 0.693265, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:31:28.408392: step 3205, loss 0.692028, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:31:28.743093: step 3210, loss 0.693799, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:31:29.047572: step 3215, loss 0.692787, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:31:29.375374: step 3220, loss 0.691818, acc 0.576172, f1 0.421241\n",
      "2017-11-26T15:31:29.709248: step 3225, loss 0.694987, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:31:30.042697: step 3230, loss 0.694809, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:31:30.369543: step 3235, loss 0.704847, acc 0.537109, f1 0.375362\n",
      "2017-11-26T15:31:30.690392: step 3240, loss 0.697384, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:31:31.016914: step 3245, loss 0.700408, acc 0.532227, f1 0.369743\n",
      "2017-11-26T15:31:31.350368: step 3250, loss 0.688687, acc 0.578125, f1 0.423577\n",
      "2017-11-26T15:31:31.662301: step 3255, loss 0.694989, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:31:31.983156: step 3260, loss 0.690973, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:31:32.326244: step 3265, loss 0.687298, acc 0.584961, f1 0.431783\n",
      "2017-11-26T15:31:32.644537: step 3270, loss 0.693869, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:31:32.974799: step 3275, loss 0.698975, acc 0.522461, f1 0.358584\n",
      "2017-11-26T15:31:33.283207: step 3280, loss 0.688617, acc 0.576172, f1 0.421241\n",
      "2017-11-26T15:31:33.607305: step 3285, loss 0.699249, acc 0.523438, f1 0.359696\n",
      "2017-11-26T15:31:33.934886: step 3290, loss 0.693403, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:31:34.263527: step 3295, loss 0.68543, acc 0.585938, f1 0.432959\n",
      "2017-11-26T15:31:34.585451: step 3300, loss 0.690291, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:31:34.890616: step 3305, loss 0.690572, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:31:35.178595: step 3310, loss 0.691137, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:31:35.480832: step 3315, loss 0.693127, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:31:35.793375: step 3320, loss 0.689807, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:31:36.111565: step 3325, loss 0.691112, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:31:36.440201: step 3330, loss 0.701652, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:31:36.763675: step 3335, loss 0.692965, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:31:37.097010: step 3340, loss 0.692868, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:31:37.400244: step 3345, loss 0.696487, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:31:37.707044: step 3350, loss 0.689643, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:31:38.003697: step 3355, loss 0.694605, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:31:38.328440: step 3360, loss 0.690783, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:31:38.636590: step 3365, loss 0.686671, acc 0.581055, f1 0.427088\n",
      "2017-11-26T15:31:38.960982: step 3370, loss 0.691417, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:31:39.278783: step 3375, loss 0.700676, acc 0.52832, f1 0.365267\n",
      "2017-11-26T15:31:39.591037: step 3380, loss 0.690966, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:31:39.881414: step 3385, loss 0.702044, acc 0.53418, f1 0.371988\n",
      "2017-11-26T15:31:40.165495: step 3390, loss 0.693435, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:31:40.461060: step 3395, loss 0.693335, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:31:40.782464: step 3400, loss 0.696951, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:31:41.107815: step 3405, loss 0.698054, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:31:41.416702: step 3410, loss 0.697053, acc 0.529297, f1 0.366384\n",
      "2017-11-26T15:31:41.741202: step 3415, loss 0.693915, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:31:42.061097: step 3420, loss 0.687151, acc 0.583008, f1 0.429433\n",
      "2017-11-26T15:31:42.375911: step 3425, loss 0.686421, acc 0.578125, f1 0.423577\n",
      "2017-11-26T15:31:42.678901: step 3430, loss 0.695586, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:31:43.017117: step 3435, loss 0.688831, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:31:43.351622: step 3440, loss 0.690729, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:31:43.673576: step 3445, loss 0.690009, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:31:44.009774: step 3450, loss 0.687786, acc 0.574219, f1 0.418909\n",
      "2017-11-26T15:31:44.330546: step 3455, loss 0.696229, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:31:44.651817: step 3460, loss 0.695162, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:31:44.971998: step 3465, loss 0.693708, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:31:45.297214: step 3470, loss 0.691511, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:31:45.615026: step 3475, loss 0.695139, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:31:45.938949: step 3480, loss 0.694728, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:31:46.273196: step 3485, loss 0.692971, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:31:46.602197: step 3490, loss 0.692498, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:31:46.934118: step 3495, loss 0.693756, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:31:47.260274: step 3500, loss 0.693784, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:31:47.557111: step 3505, loss 0.694256, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:31:47.872208: step 3510, loss 0.694301, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:31:48.186414: step 3515, loss 0.693671, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:31:48.511676: step 3520, loss 0.689998, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:31:48.801626: step 3525, loss 0.691004, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:31:49.124665: step 3530, loss 0.68906, acc 0.577148, f1 0.422408\n",
      "2017-11-26T15:31:49.454797: step 3535, loss 0.690744, acc 0.575195, f1 0.420074\n",
      "2017-11-26T15:31:49.774391: step 3540, loss 0.69139, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:31:50.087391: step 3545, loss 0.69348, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:31:50.401037: step 3550, loss 0.695647, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:31:50.699678: step 3555, loss 0.690041, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:31:51.011524: step 3560, loss 0.687989, acc 0.576172, f1 0.421241\n",
      "2017-11-26T15:31:51.314190: step 3565, loss 0.694497, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:31:51.604057: step 3570, loss 0.694173, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:31:51.895122: step 3575, loss 0.694721, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:31:52.196619: step 3580, loss 0.691131, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:31:52.503876: step 3585, loss 0.68442, acc 0.583984, f1 0.430607\n",
      "2017-11-26T15:31:52.816597: step 3590, loss 0.690736, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:31:53.109902: step 3595, loss 0.694444, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:31:53.385126: step 3600, loss 0.694725, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:31:53.668778: step 3605, loss 0.694311, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:31:53.958768: step 3610, loss 0.693322, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:31:54.247812: step 3615, loss 0.692784, acc 0.5625, f1 0.405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T15:31:54.549094: step 3620, loss 0.695594, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:31:54.843844: step 3625, loss 0.692595, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:31:55.140584: step 3630, loss 0.692921, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:31:55.439350: step 3635, loss 0.687972, acc 0.578125, f1 0.423577\n",
      "2017-11-26T15:31:55.725967: step 3640, loss 0.693391, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:31:56.027288: step 3645, loss 0.691458, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:31:56.318918: step 3650, loss 0.692668, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:31:56.653726: step 3655, loss 0.690302, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:31:56.964827: step 3660, loss 0.699424, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:31:57.271940: step 3665, loss 0.700093, acc 0.537109, f1 0.375362\n",
      "2017-11-26T15:31:57.590082: step 3670, loss 0.691761, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:31:57.896923: step 3675, loss 0.694181, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:31:58.213243: step 3680, loss 0.693735, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:31:58.508276: step 3685, loss 0.692805, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:31:58.823709: step 3690, loss 0.689705, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:31:59.134854: step 3695, loss 0.697823, acc 0.533203, f1 0.370865\n",
      "2017-11-26T15:31:59.445244: step 3700, loss 0.687111, acc 0.577148, f1 0.422408\n",
      "2017-11-26T15:31:59.747414: step 3705, loss 0.697202, acc 0.52832, f1 0.365267\n",
      "2017-11-26T15:32:00.054618: step 3710, loss 0.689718, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:32:00.347611: step 3715, loss 0.696197, acc 0.536133, f1 0.374236\n",
      "2017-11-26T15:32:00.640536: step 3720, loss 0.695916, acc 0.537109, f1 0.375362\n",
      "2017-11-26T15:32:00.927321: step 3725, loss 0.69928, acc 0.519531, f1 0.355258\n",
      "2017-11-26T15:32:01.235640: step 3730, loss 0.691636, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:32:01.564869: step 3735, loss 0.695616, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:32:01.892936: step 3740, loss 0.696021, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:32:02.213776: step 3745, loss 0.695581, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:32:02.537041: step 3750, loss 0.690474, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:32:02.836637: step 3755, loss 0.689735, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:32:03.137916: step 3760, loss 0.689966, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:32:03.434818: step 3765, loss 0.687304, acc 0.574219, f1 0.418909\n",
      "2017-11-26T15:32:03.746069: step 3770, loss 0.694162, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:32:04.064523: step 3775, loss 0.698736, acc 0.530273, f1 0.367503\n",
      "2017-11-26T15:32:04.372643: step 3780, loss 0.69181, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:32:04.685860: step 3785, loss 0.687181, acc 0.574219, f1 0.418909\n",
      "2017-11-26T15:32:04.985632: step 3790, loss 0.695684, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:32:05.275773: step 3795, loss 0.689663, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:32:05.562627: step 3800, loss 0.686293, acc 0.583984, f1 0.430607\n",
      "2017-11-26T15:32:05.867117: step 3805, loss 0.692874, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:32:06.195999: step 3810, loss 0.689931, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:32:06.511566: step 3815, loss 0.691816, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:32:06.850517: step 3820, loss 0.694301, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:32:07.166270: step 3825, loss 0.695589, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:32:07.483419: step 3830, loss 0.691286, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:32:07.818722: step 3835, loss 0.695378, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:32:08.129733: step 3840, loss 0.696293, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:32:08.434512: step 3845, loss 0.691853, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:32:08.745346: step 3850, loss 0.691735, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:32:09.050994: step 3855, loss 0.696539, acc 0.556641, f1 0.4297\n",
      "2017-11-26T15:32:09.373009: step 3860, loss 0.696381, acc 0.537109, f1 0.375362\n",
      "2017-11-26T15:32:09.701123: step 3865, loss 0.69823, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:32:10.025700: step 3870, loss 0.701918, acc 0.517578, f1 0.353046\n",
      "2017-11-26T15:32:10.358183: step 3875, loss 0.696552, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:32:10.673977: step 3880, loss 0.689002, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:32:10.973046: step 3885, loss 0.692799, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:32:11.283401: step 3890, loss 0.695185, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:32:11.591937: step 3895, loss 0.693326, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:32:11.910201: step 3900, loss 0.696955, acc 0.526367, f1 0.363035\n",
      "2017-11-26T15:32:12.219297: step 3905, loss 0.687935, acc 0.578125, f1 0.423577\n",
      "2017-11-26T15:32:12.529189: step 3910, loss 0.696231, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:32:12.858684: step 3915, loss 0.701742, acc 0.521484, f1 0.357475\n",
      "2017-11-26T15:32:13.176148: step 3920, loss 0.695707, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:32:13.509175: step 3925, loss 0.684055, acc 0.585938, f1 0.432959\n",
      "2017-11-26T15:32:13.832008: step 3930, loss 0.689719, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:32:14.150030: step 3935, loss 0.689094, acc 0.573242, f1 0.417744\n",
      "2017-11-26T15:32:14.460562: step 3940, loss 0.692381, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:32:14.769154: step 3945, loss 0.691463, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:32:15.097707: step 3950, loss 0.692426, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:32:15.428430: step 3955, loss 0.694309, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:32:15.749429: step 3960, loss 0.692427, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:32:16.049143: step 3965, loss 0.700023, acc 0.526367, f1 0.363035\n",
      "2017-11-26T15:32:16.347983: step 3970, loss 0.691057, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:32:16.650686: step 3975, loss 0.692599, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:32:16.960719: step 3980, loss 0.694607, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:32:17.270251: step 3985, loss 0.691763, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:32:17.597071: step 3990, loss 0.689151, acc 0.574219, f1 0.418909\n",
      "2017-11-26T15:32:17.916170: step 3995, loss 0.699928, acc 0.523438, f1 0.359696\n",
      "2017-11-26T15:32:18.247539: step 4000, loss 0.691854, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:32:18.578296: step 4005, loss 0.685568, acc 0.581055, f1 0.427088\n",
      "2017-11-26T15:32:18.914534: step 4010, loss 0.691184, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:32:19.217020: step 4015, loss 0.697291, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:32:19.512057: step 4020, loss 0.691331, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:32:19.807534: step 4025, loss 0.693838, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:32:20.106209: step 4030, loss 0.699305, acc 0.536133, f1 0.374236\n",
      "2017-11-26T15:32:20.402547: step 4035, loss 0.692365, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:32:20.695869: step 4040, loss 0.693251, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:32:20.997906: step 4045, loss 0.700671, acc 0.524414, f1 0.360808\n",
      "2017-11-26T15:32:21.320117: step 4050, loss 0.688183, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:32:21.650833: step 4055, loss 0.690598, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:32:21.971455: step 4060, loss 0.692242, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:32:22.270977: step 4065, loss 0.69107, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:32:22.594975: step 4070, loss 0.692434, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:32:22.919281: step 4075, loss 0.684362, acc 0.582031, f1 0.42826\n",
      "2017-11-26T15:32:23.229485: step 4080, loss 0.685822, acc 0.579102, f1 0.424746\n",
      "2017-11-26T15:32:23.522185: step 4085, loss 0.690521, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:32:23.830142: step 4090, loss 0.686638, acc 0.575195, f1 0.420074\n",
      "2017-11-26T15:32:24.138012: step 4095, loss 0.689031, acc 0.573242, f1 0.417744\n",
      "2017-11-26T15:32:24.448463: step 4100, loss 0.688359, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:32:24.773906: step 4105, loss 0.695509, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:32:25.109209: step 4110, loss 0.690089, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:32:25.429959: step 4115, loss 0.69768, acc 0.52832, f1 0.365267\n",
      "2017-11-26T15:32:25.759601: step 4120, loss 0.692778, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:32:26.058226: step 4125, loss 0.698423, acc 0.536133, f1 0.374236\n",
      "2017-11-26T15:32:26.382227: step 4130, loss 0.691317, acc 0.558594, f1 0.400396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T15:32:26.707408: step 4135, loss 0.690109, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:32:27.049206: step 4140, loss 0.689983, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:32:27.359988: step 4145, loss 0.691294, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:32:27.690895: step 4150, loss 0.695181, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:32:28.017242: step 4155, loss 0.694658, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:32:28.334613: step 4160, loss 0.693633, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:32:28.652253: step 4165, loss 0.685445, acc 0.579102, f1 0.424746\n",
      "\n",
      "Evaluation:\n",
      "loss 0.692, acc 0.55893, f1 0.400903\n",
      "\n",
      "Current epoch:  4\n",
      "2017-11-26T15:32:31.090764: step 4170, loss 0.688393, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:32:31.412644: step 4175, loss 0.690093, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:32:31.715222: step 4180, loss 0.692011, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:32:32.034140: step 4185, loss 0.692377, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:32:32.361729: step 4190, loss 0.689233, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:32:32.695630: step 4195, loss 0.695422, acc 0.535156, f1 0.373111\n",
      "2017-11-26T15:32:33.006815: step 4200, loss 0.690947, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:32:33.304961: step 4205, loss 0.701277, acc 0.517578, f1 0.353046\n",
      "2017-11-26T15:32:33.631598: step 4210, loss 0.691503, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:32:33.935004: step 4215, loss 0.685577, acc 0.578125, f1 0.423577\n",
      "2017-11-26T15:32:34.259530: step 4220, loss 0.688739, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:32:34.579795: step 4225, loss 0.688935, acc 0.573242, f1 0.417744\n",
      "2017-11-26T15:32:34.907690: step 4230, loss 0.690474, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:32:35.226895: step 4235, loss 0.695466, acc 0.535156, f1 0.373111\n",
      "2017-11-26T15:32:35.552145: step 4240, loss 0.686952, acc 0.592773, f1 0.441218\n",
      "2017-11-26T15:32:35.874417: step 4245, loss 0.690304, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:32:36.190851: step 4250, loss 0.691477, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:32:36.515681: step 4255, loss 0.694669, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:32:36.844890: step 4260, loss 0.685985, acc 0.591797, f1 0.440035\n",
      "2017-11-26T15:32:37.164006: step 4265, loss 0.690022, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:32:37.484483: step 4270, loss 0.691386, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:32:37.800501: step 4275, loss 0.695779, acc 0.53125, f1 0.368622\n",
      "2017-11-26T15:32:38.295276: step 4280, loss 0.687382, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:32:38.601690: step 4285, loss 0.695581, acc 0.538086, f1 0.376489\n",
      "2017-11-26T15:32:38.903780: step 4290, loss 0.693962, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:32:39.237989: step 4295, loss 0.6886, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:32:39.558105: step 4300, loss 0.695921, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:32:39.869591: step 4305, loss 0.69364, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:32:40.182694: step 4310, loss 0.692421, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:32:40.510178: step 4315, loss 0.694895, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:32:40.816308: step 4320, loss 0.695844, acc 0.536133, f1 0.374236\n",
      "2017-11-26T15:32:41.131110: step 4325, loss 0.689834, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:32:41.453684: step 4330, loss 0.694387, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:32:41.774314: step 4335, loss 0.691619, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:32:42.107376: step 4340, loss 0.692249, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:32:42.448465: step 4345, loss 0.690933, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:32:42.785375: step 4350, loss 0.696005, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:32:43.096309: step 4355, loss 0.691787, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:32:43.377464: step 4360, loss 0.685991, acc 0.578125, f1 0.423577\n",
      "2017-11-26T15:32:43.678814: step 4365, loss 0.694774, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:32:44.013729: step 4370, loss 0.692067, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:32:44.349415: step 4375, loss 0.692434, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:32:44.675366: step 4380, loss 0.692625, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:32:44.990397: step 4385, loss 0.690766, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:32:45.293329: step 4390, loss 0.689086, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:32:45.623960: step 4395, loss 0.69462, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:32:45.969523: step 4400, loss 0.692802, acc 0.573242, f1 0.417744\n",
      "2017-11-26T15:32:46.286046: step 4405, loss 0.692317, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:32:46.608387: step 4410, loss 0.688914, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:32:46.937220: step 4415, loss 0.694131, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:32:47.253773: step 4420, loss 0.693394, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:32:47.576693: step 4425, loss 0.695822, acc 0.52832, f1 0.365267\n",
      "2017-11-26T15:32:47.885320: step 4430, loss 0.692013, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:32:48.188831: step 4435, loss 0.690734, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:32:48.496318: step 4440, loss 0.694574, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:32:48.812708: step 4445, loss 0.695813, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:32:49.133947: step 4450, loss 0.684323, acc 0.581055, f1 0.427088\n",
      "2017-11-26T15:32:49.449411: step 4455, loss 0.687438, acc 0.580078, f1 0.425916\n",
      "2017-11-26T15:32:49.767861: step 4460, loss 0.699351, acc 0.532227, f1 0.369743\n",
      "2017-11-26T15:32:50.083693: step 4465, loss 0.692966, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:32:50.397460: step 4470, loss 0.695612, acc 0.554688, f1 0.398936\n",
      "2017-11-26T15:32:50.724359: step 4475, loss 0.692848, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:32:51.038668: step 4480, loss 0.695868, acc 0.532227, f1 0.369743\n",
      "2017-11-26T15:32:51.367643: step 4485, loss 0.6884, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:32:51.687593: step 4490, loss 0.691599, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:32:51.999243: step 4495, loss 0.686878, acc 0.574219, f1 0.418909\n",
      "2017-11-26T15:32:52.326561: step 4500, loss 0.688889, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:32:52.651169: step 4505, loss 0.686826, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:32:52.982615: step 4510, loss 0.689142, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:32:53.293001: step 4515, loss 0.693872, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:32:53.597010: step 4520, loss 0.693942, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:32:53.896219: step 4525, loss 0.689491, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:32:54.198136: step 4530, loss 0.688903, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:32:54.505606: step 4535, loss 0.697888, acc 0.510742, f1 0.345337\n",
      "2017-11-26T15:32:54.810283: step 4540, loss 0.694681, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:32:55.106534: step 4545, loss 0.688411, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:32:55.403301: step 4550, loss 0.694431, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:32:55.709073: step 4555, loss 0.691846, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:32:56.010286: step 4560, loss 0.690444, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:32:56.307677: step 4565, loss 0.68881, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:32:56.611291: step 4570, loss 0.690923, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:32:56.910359: step 4575, loss 0.692871, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:32:57.213659: step 4580, loss 0.697413, acc 0.527344, f1 0.36415\n",
      "2017-11-26T15:32:57.528960: step 4585, loss 0.687927, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:32:57.834222: step 4590, loss 0.690032, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:32:58.140560: step 4595, loss 0.687879, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:32:58.463956: step 4600, loss 0.691915, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:32:58.789290: step 4605, loss 0.689419, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:32:59.114129: step 4610, loss 0.689074, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:32:59.449054: step 4615, loss 0.69519, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:32:59.765103: step 4620, loss 0.683529, acc 0.597656, f1 0.447146\n",
      "2017-11-26T15:33:00.089216: step 4625, loss 0.691691, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:33:00.426408: step 4630, loss 0.692645, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:33:00.766349: step 4635, loss 0.693975, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:33:01.101551: step 4640, loss 0.688805, acc 0.573242, f1 0.417744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T15:33:01.419359: step 4645, loss 0.686555, acc 0.574219, f1 0.418909\n",
      "2017-11-26T15:33:01.715800: step 4650, loss 0.691159, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:33:02.005396: step 4655, loss 0.687385, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:33:02.319114: step 4660, loss 0.689381, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:33:02.648028: step 4665, loss 0.69083, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:33:02.978724: step 4670, loss 0.692793, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:33:03.300194: step 4675, loss 0.688869, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:33:03.604977: step 4680, loss 0.696968, acc 0.53418, f1 0.371988\n",
      "2017-11-26T15:33:03.902847: step 4685, loss 0.696434, acc 0.537109, f1 0.375362\n",
      "2017-11-26T15:33:04.174937: step 4690, loss 0.69312, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:33:04.494544: step 4695, loss 0.698683, acc 0.532227, f1 0.369743\n",
      "2017-11-26T15:33:04.820012: step 4700, loss 0.690941, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:33:05.145200: step 4705, loss 0.688065, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:33:05.478107: step 4710, loss 0.691059, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:33:05.793191: step 4715, loss 0.694257, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:33:06.128817: step 4720, loss 0.691351, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:33:06.454083: step 4725, loss 0.694417, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:33:06.765441: step 4730, loss 0.702868, acc 0.510742, f1 0.345337\n",
      "2017-11-26T15:33:07.085455: step 4735, loss 0.690871, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:33:07.372480: step 4740, loss 0.693671, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:33:07.665268: step 4745, loss 0.694435, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:33:07.960909: step 4750, loss 0.694179, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:33:08.268813: step 4755, loss 0.692536, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:33:08.568108: step 4760, loss 0.693448, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:33:08.890232: step 4765, loss 0.687002, acc 0.573242, f1 0.417744\n",
      "2017-11-26T15:33:09.193613: step 4770, loss 0.687077, acc 0.574219, f1 0.418909\n",
      "2017-11-26T15:33:09.496287: step 4775, loss 0.692917, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:33:09.810125: step 4780, loss 0.690407, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:33:10.105937: step 4785, loss 0.691215, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:33:10.426915: step 4790, loss 0.691793, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:33:10.755787: step 4795, loss 0.694873, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:33:11.084381: step 4800, loss 0.690176, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:33:11.392463: step 4805, loss 0.689141, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:33:11.717151: step 4810, loss 0.695284, acc 0.532227, f1 0.369743\n",
      "2017-11-26T15:33:12.021672: step 4815, loss 0.695597, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:33:12.348163: step 4820, loss 0.6882, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:33:12.674068: step 4825, loss 0.68796, acc 0.581055, f1 0.427088\n",
      "2017-11-26T15:33:12.994653: step 4830, loss 0.697545, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:33:13.319535: step 4835, loss 0.683767, acc 0.586914, f1 0.434136\n",
      "2017-11-26T15:33:13.649860: step 4840, loss 0.688452, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:33:13.969181: step 4845, loss 0.688964, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:33:14.289027: step 4850, loss 0.689702, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:33:14.607316: step 4855, loss 0.683798, acc 0.59082, f1 0.438854\n",
      "2017-11-26T15:33:14.930421: step 4860, loss 0.680843, acc 0.59375, f1 0.442402\n",
      "2017-11-26T15:33:15.237102: step 4865, loss 0.690203, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:33:15.563301: step 4870, loss 0.691164, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:33:15.881306: step 4875, loss 0.690543, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:33:16.215189: step 4880, loss 0.690341, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:33:16.528107: step 4885, loss 0.692372, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:33:16.849650: step 4890, loss 0.69086, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:33:17.188889: step 4895, loss 0.691861, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:33:17.511707: step 4900, loss 0.688745, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:33:17.858254: step 4905, loss 0.690669, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:33:18.182464: step 4910, loss 0.68837, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:33:18.508623: step 4915, loss 0.69408, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:33:18.824040: step 4920, loss 0.69628, acc 0.537109, f1 0.375362\n",
      "2017-11-26T15:33:19.110488: step 4925, loss 0.691531, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:33:19.441917: step 4930, loss 0.695897, acc 0.537109, f1 0.375362\n",
      "2017-11-26T15:33:19.751703: step 4935, loss 0.690303, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:33:20.058156: step 4940, loss 0.691317, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:33:20.364398: step 4945, loss 0.691423, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:33:20.668878: step 4950, loss 0.69678, acc 0.532227, f1 0.369743\n",
      "2017-11-26T15:33:20.970129: step 4955, loss 0.685716, acc 0.581055, f1 0.427088\n",
      "2017-11-26T15:33:21.293627: step 4960, loss 0.692974, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:33:21.625904: step 4965, loss 0.690414, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:33:21.949983: step 4970, loss 0.691814, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:33:22.277222: step 4975, loss 0.691447, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:33:22.597515: step 4980, loss 0.692694, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:33:22.921097: step 4985, loss 0.693724, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:33:23.236831: step 4990, loss 0.696393, acc 0.524414, f1 0.360808\n",
      "2017-11-26T15:33:23.535370: step 4995, loss 0.691152, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:33:23.839276: step 5000, loss 0.682503, acc 0.585938, f1 0.432959\n",
      "2017-11-26T15:33:24.143787: step 5005, loss 0.690017, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:33:24.452797: step 5010, loss 0.691981, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:33:24.762279: step 5015, loss 0.682658, acc 0.594727, f1 0.443587\n",
      "2017-11-26T15:33:25.055121: step 5020, loss 0.689548, acc 0.574219, f1 0.418909\n",
      "2017-11-26T15:33:25.408684: step 5025, loss 0.693306, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:33:25.725292: step 5030, loss 0.688862, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:33:26.046505: step 5035, loss 0.689374, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:33:26.364565: step 5040, loss 0.69555, acc 0.537109, f1 0.375362\n",
      "2017-11-26T15:33:26.690184: step 5045, loss 0.693699, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:33:26.994690: step 5050, loss 0.690266, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:33:27.319395: step 5055, loss 0.685639, acc 0.578125, f1 0.423577\n",
      "2017-11-26T15:33:27.628648: step 5060, loss 0.691225, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:33:27.936301: step 5065, loss 0.687989, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:33:28.241961: step 5070, loss 0.693021, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:33:28.569462: step 5075, loss 0.698113, acc 0.520508, f1 0.356366\n",
      "2017-11-26T15:33:28.891799: step 5080, loss 0.697406, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:33:29.226895: step 5085, loss 0.690864, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:33:29.532402: step 5090, loss 0.685721, acc 0.577148, f1 0.422408\n",
      "2017-11-26T15:33:29.830336: step 5095, loss 0.691073, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:33:30.134402: step 5100, loss 0.684697, acc 0.580078, f1 0.425916\n",
      "2017-11-26T15:33:30.460420: step 5105, loss 0.7008, acc 0.518555, f1 0.354151\n",
      "2017-11-26T15:33:30.782268: step 5110, loss 0.691788, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:33:31.115858: step 5115, loss 0.689616, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:33:31.451011: step 5120, loss 0.695916, acc 0.536133, f1 0.374236\n",
      "2017-11-26T15:33:31.774717: step 5125, loss 0.688204, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:33:32.089583: step 5130, loss 0.687597, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:33:32.394729: step 5135, loss 0.69009, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:33:32.715829: step 5140, loss 0.689203, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:33:33.023674: step 5145, loss 0.686153, acc 0.573242, f1 0.417744\n",
      "2017-11-26T15:33:33.319935: step 5150, loss 0.692553, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:33:33.622437: step 5155, loss 0.697357, acc 0.53418, f1 0.371988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T15:33:33.945530: step 5160, loss 0.693416, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:33:34.253981: step 5165, loss 0.692004, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:33:34.552034: step 5170, loss 0.688895, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:33:34.855141: step 5175, loss 0.688557, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:33:35.191611: step 5180, loss 0.688413, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:33:35.529288: step 5185, loss 0.690171, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:33:35.846980: step 5190, loss 0.698803, acc 0.527344, f1 0.36415\n",
      "2017-11-26T15:33:36.158189: step 5195, loss 0.692776, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:33:36.486197: step 5200, loss 0.695394, acc 0.538086, f1 0.376489\n",
      "2017-11-26T15:33:36.816244: step 5205, loss 0.687686, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:33:37.119196: step 5210, loss 0.693917, acc 0.542705, f1 0.381834\n",
      "\n",
      "Evaluation:\n",
      "loss 0.690767, acc 0.55893, f1 0.400903\n",
      "\n",
      "Current epoch:  5\n",
      "2017-11-26T15:33:39.821556: step 5215, loss 0.693496, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:33:40.126851: step 5220, loss 0.690812, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:33:40.591699: step 5225, loss 0.694455, acc 0.537109, f1 0.375362\n",
      "2017-11-26T15:33:40.899810: step 5230, loss 0.695489, acc 0.527344, f1 0.36415\n",
      "2017-11-26T15:33:41.195826: step 5235, loss 0.683718, acc 0.582031, f1 0.42826\n",
      "2017-11-26T15:33:41.508729: step 5240, loss 0.69011, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:33:41.827861: step 5245, loss 0.691204, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:33:42.131602: step 5250, loss 0.698723, acc 0.535156, f1 0.373111\n",
      "2017-11-26T15:33:42.443478: step 5255, loss 0.689546, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:33:42.747944: step 5260, loss 0.689339, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:33:43.061956: step 5265, loss 0.692144, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:33:43.363743: step 5270, loss 0.686778, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:33:43.676317: step 5275, loss 0.689133, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:33:43.987787: step 5280, loss 0.693343, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:33:44.293596: step 5285, loss 0.681748, acc 0.588867, f1 0.436493\n",
      "2017-11-26T15:33:44.607197: step 5290, loss 0.687679, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:33:44.907473: step 5295, loss 0.690839, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:33:45.199028: step 5300, loss 0.68186, acc 0.592773, f1 0.441218\n",
      "2017-11-26T15:33:45.509224: step 5305, loss 0.6836, acc 0.597656, f1 0.447146\n",
      "2017-11-26T15:33:45.843609: step 5310, loss 0.693272, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:33:46.177289: step 5315, loss 0.691613, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:33:46.503438: step 5320, loss 0.695223, acc 0.53418, f1 0.371988\n",
      "2017-11-26T15:33:46.812292: step 5325, loss 0.694659, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:33:47.138864: step 5330, loss 0.687352, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:33:47.458589: step 5335, loss 0.695053, acc 0.557617, f1 0.399946\n",
      "2017-11-26T15:33:47.763515: step 5340, loss 0.6881, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:33:48.099549: step 5345, loss 0.691357, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:33:48.406464: step 5350, loss 0.686802, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:33:48.714594: step 5355, loss 0.692651, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:33:49.026185: step 5360, loss 0.688615, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:33:49.322251: step 5365, loss 0.695634, acc 0.533203, f1 0.370865\n",
      "2017-11-26T15:33:49.612224: step 5370, loss 0.691018, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:33:49.911695: step 5375, loss 0.696386, acc 0.535156, f1 0.373111\n",
      "2017-11-26T15:33:50.215057: step 5380, loss 0.695647, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:33:50.525576: step 5385, loss 0.690175, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:33:50.816062: step 5390, loss 0.687157, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:33:51.105063: step 5395, loss 0.687297, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:33:51.409948: step 5400, loss 0.694756, acc 0.538086, f1 0.376489\n",
      "2017-11-26T15:33:51.704656: step 5405, loss 0.693156, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:33:52.012615: step 5410, loss 0.685879, acc 0.584961, f1 0.431783\n",
      "2017-11-26T15:33:52.320260: step 5415, loss 0.691621, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:33:52.629837: step 5420, loss 0.691362, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:33:52.930846: step 5425, loss 0.691001, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:33:53.251814: step 5430, loss 0.693321, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:33:53.558102: step 5435, loss 0.693542, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:33:53.852499: step 5440, loss 0.693473, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:33:54.145404: step 5445, loss 0.687229, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:33:54.436517: step 5450, loss 0.682425, acc 0.584961, f1 0.431783\n",
      "2017-11-26T15:33:54.730680: step 5455, loss 0.689565, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:33:55.027233: step 5460, loss 0.690143, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:33:55.365142: step 5465, loss 0.69293, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:33:55.692722: step 5470, loss 0.686858, acc 0.579102, f1 0.424746\n",
      "2017-11-26T15:33:56.030701: step 5475, loss 0.695213, acc 0.538086, f1 0.376489\n",
      "2017-11-26T15:33:56.353929: step 5480, loss 0.691545, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:33:56.672276: step 5485, loss 0.693475, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:33:56.985988: step 5490, loss 0.689009, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:33:57.288320: step 5495, loss 0.687778, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:33:57.604251: step 5500, loss 0.68896, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:33:57.922231: step 5505, loss 0.695173, acc 0.53125, f1 0.368622\n",
      "2017-11-26T15:33:58.237714: step 5510, loss 0.689027, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:33:58.533715: step 5515, loss 0.691748, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:33:58.848313: step 5520, loss 0.695189, acc 0.543945, f1 0.385381\n",
      "2017-11-26T15:33:59.171278: step 5525, loss 0.694012, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:33:59.474788: step 5530, loss 0.690628, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:33:59.773656: step 5535, loss 0.693187, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:34:00.072328: step 5540, loss 0.692358, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:34:00.396582: step 5545, loss 0.694439, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:34:00.701637: step 5550, loss 0.695209, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:34:01.018867: step 5555, loss 0.686346, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:34:01.327808: step 5560, loss 0.698463, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:34:01.633099: step 5565, loss 0.692565, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:34:01.921261: step 5570, loss 0.68996, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:34:02.253748: step 5575, loss 0.695789, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:34:02.575665: step 5580, loss 0.694185, acc 0.538086, f1 0.376489\n",
      "2017-11-26T15:34:02.893292: step 5585, loss 0.692944, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:34:03.221236: step 5590, loss 0.686935, acc 0.578125, f1 0.423577\n",
      "2017-11-26T15:34:03.545266: step 5595, loss 0.687612, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:34:03.859713: step 5600, loss 0.695465, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:34:04.175437: step 5605, loss 0.692259, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:34:04.504261: step 5610, loss 0.689147, acc 0.577148, f1 0.422408\n",
      "2017-11-26T15:34:04.820471: step 5615, loss 0.691126, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:34:05.147223: step 5620, loss 0.690812, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:34:05.449151: step 5625, loss 0.694125, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:34:05.753382: step 5630, loss 0.691031, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:34:06.056564: step 5635, loss 0.692694, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:34:06.374251: step 5640, loss 0.688243, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:34:06.683566: step 5645, loss 0.69325, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:34:07.001228: step 5650, loss 0.689301, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:34:07.311086: step 5655, loss 0.693835, acc 0.538086, f1 0.376489\n",
      "2017-11-26T15:34:07.641374: step 5660, loss 0.686346, acc 0.574219, f1 0.418909\n",
      "2017-11-26T15:34:07.957001: step 5665, loss 0.690621, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:34:08.274466: step 5670, loss 0.691871, acc 0.550781, f1 0.391235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T15:34:08.587382: step 5675, loss 0.694085, acc 0.538086, f1 0.376489\n",
      "2017-11-26T15:34:08.897386: step 5680, loss 0.69115, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:34:09.188332: step 5685, loss 0.684099, acc 0.581055, f1 0.427088\n",
      "2017-11-26T15:34:09.482024: step 5690, loss 0.687826, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:34:09.803680: step 5695, loss 0.693079, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:34:10.113267: step 5700, loss 0.684591, acc 0.579102, f1 0.424746\n",
      "2017-11-26T15:34:10.423271: step 5705, loss 0.697906, acc 0.533203, f1 0.370865\n",
      "2017-11-26T15:34:10.711471: step 5710, loss 0.69207, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:34:11.040204: step 5715, loss 0.687994, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:34:11.345603: step 5720, loss 0.689236, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:34:11.641051: step 5725, loss 0.687722, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:34:11.930124: step 5730, loss 0.693356, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:34:12.264024: step 5735, loss 0.68764, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:34:12.583957: step 5740, loss 0.694383, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:34:12.913220: step 5745, loss 0.69576, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:34:13.219518: step 5750, loss 0.6881, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:34:13.539471: step 5755, loss 0.68846, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:34:13.860508: step 5760, loss 0.687566, acc 0.576172, f1 0.421241\n",
      "2017-11-26T15:34:14.173489: step 5765, loss 0.693064, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:34:14.470585: step 5770, loss 0.692868, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:34:14.768839: step 5775, loss 0.689958, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:34:15.080307: step 5780, loss 0.693836, acc 0.538086, f1 0.376489\n",
      "2017-11-26T15:34:15.384521: step 5785, loss 0.697203, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:34:15.663231: step 5790, loss 0.689385, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:34:15.991340: step 5795, loss 0.697778, acc 0.530273, f1 0.367503\n",
      "2017-11-26T15:34:16.308644: step 5800, loss 0.69367, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:34:16.617657: step 5805, loss 0.690264, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:34:16.936341: step 5810, loss 0.693625, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:34:17.247309: step 5815, loss 0.681267, acc 0.588867, f1 0.436493\n",
      "2017-11-26T15:34:17.575196: step 5820, loss 0.689673, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:34:17.890026: step 5825, loss 0.690388, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:34:18.194553: step 5830, loss 0.691681, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:34:18.488631: step 5835, loss 0.690135, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:34:18.796152: step 5840, loss 0.692567, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:34:19.101802: step 5845, loss 0.687756, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:34:19.433821: step 5850, loss 0.690874, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:34:19.754814: step 5855, loss 0.688435, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:34:20.049951: step 5860, loss 0.688071, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:34:20.383323: step 5865, loss 0.690845, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:34:20.720888: step 5870, loss 0.69235, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:34:21.044727: step 5875, loss 0.690377, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:34:21.373292: step 5880, loss 0.691982, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:34:21.706114: step 5885, loss 0.687237, acc 0.576172, f1 0.421241\n",
      "2017-11-26T15:34:22.017096: step 5890, loss 0.688316, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:34:22.326302: step 5895, loss 0.694349, acc 0.535156, f1 0.373111\n",
      "2017-11-26T15:34:22.635127: step 5900, loss 0.689197, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:34:22.960088: step 5905, loss 0.690856, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:34:23.286077: step 5910, loss 0.689467, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:34:23.607629: step 5915, loss 0.691207, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:34:23.922753: step 5920, loss 0.689212, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:34:24.238191: step 5925, loss 0.685142, acc 0.576172, f1 0.421241\n",
      "2017-11-26T15:34:24.537317: step 5930, loss 0.689594, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:34:24.833599: step 5935, loss 0.693667, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:34:25.132994: step 5940, loss 0.701724, acc 0.513672, f1 0.348634\n",
      "2017-11-26T15:34:25.431298: step 5945, loss 0.685857, acc 0.580078, f1 0.425916\n",
      "2017-11-26T15:34:25.739549: step 5950, loss 0.692415, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:34:26.036296: step 5955, loss 0.689415, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:34:26.349618: step 5960, loss 0.690459, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:34:26.689160: step 5965, loss 0.695795, acc 0.535156, f1 0.373111\n",
      "2017-11-26T15:34:26.996294: step 5970, loss 0.694169, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:34:27.308321: step 5975, loss 0.68557, acc 0.574219, f1 0.418909\n",
      "2017-11-26T15:34:27.605410: step 5980, loss 0.7012, acc 0.53418, f1 0.371988\n",
      "2017-11-26T15:34:27.897259: step 5985, loss 0.688182, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:34:28.196454: step 5990, loss 0.687098, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:34:28.496156: step 5995, loss 0.689154, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:34:28.801425: step 6000, loss 0.690189, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:34:29.113932: step 6005, loss 0.688181, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:34:29.441380: step 6010, loss 0.691654, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:34:29.773479: step 6015, loss 0.686231, acc 0.574219, f1 0.418909\n",
      "2017-11-26T15:34:30.098570: step 6020, loss 0.690855, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:34:30.416073: step 6025, loss 0.692021, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:34:30.752041: step 6030, loss 0.695763, acc 0.535156, f1 0.373111\n",
      "2017-11-26T15:34:31.078146: step 6035, loss 0.690404, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:34:31.403701: step 6040, loss 0.688445, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:34:31.709603: step 6045, loss 0.685921, acc 0.584961, f1 0.431783\n",
      "2017-11-26T15:34:32.012663: step 6050, loss 0.685447, acc 0.578125, f1 0.423577\n",
      "2017-11-26T15:34:32.312470: step 6055, loss 0.694011, acc 0.537109, f1 0.375362\n",
      "2017-11-26T15:34:32.646847: step 6060, loss 0.688058, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:34:32.971823: step 6065, loss 0.690075, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:34:33.298645: step 6070, loss 0.693746, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:34:33.622397: step 6075, loss 0.688137, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:34:33.923489: step 6080, loss 0.685943, acc 0.573242, f1 0.417744\n",
      "2017-11-26T15:34:34.210354: step 6085, loss 0.691264, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:34:34.499712: step 6090, loss 0.700158, acc 0.536133, f1 0.374236\n",
      "2017-11-26T15:34:34.805513: step 6095, loss 0.692178, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:34:35.105699: step 6100, loss 0.690312, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:34:35.429336: step 6105, loss 0.691863, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:34:35.758333: step 6110, loss 0.68554, acc 0.574219, f1 0.418909\n",
      "2017-11-26T15:34:36.063954: step 6115, loss 0.688016, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:34:36.384268: step 6120, loss 0.69444, acc 0.537109, f1 0.375362\n",
      "2017-11-26T15:34:36.714587: step 6125, loss 0.693895, acc 0.537109, f1 0.375362\n",
      "2017-11-26T15:34:37.037837: step 6130, loss 0.68913, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:34:37.363414: step 6135, loss 0.690449, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:34:37.671469: step 6140, loss 0.692121, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:34:37.981786: step 6145, loss 0.69039, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:34:38.310369: step 6150, loss 0.693476, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:34:38.628103: step 6155, loss 0.689762, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:34:38.931388: step 6160, loss 0.692468, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:34:39.246370: step 6165, loss 0.696196, acc 0.518555, f1 0.354151\n",
      "2017-11-26T15:34:39.582504: step 6170, loss 0.694073, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:34:39.909240: step 6175, loss 0.693276, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:34:40.242994: step 6180, loss 0.691818, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:34:40.569634: step 6185, loss 0.690878, acc 0.558594, f1 0.400396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T15:34:40.894506: step 6190, loss 0.68552, acc 0.577148, f1 0.422408\n",
      "2017-11-26T15:34:41.227144: step 6195, loss 0.690023, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:34:41.551563: step 6200, loss 0.687801, acc 0.573242, f1 0.417744\n",
      "2017-11-26T15:34:41.883508: step 6205, loss 0.6893, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:34:42.199816: step 6210, loss 0.689143, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:34:42.504529: step 6215, loss 0.680495, acc 0.588867, f1 0.436493\n",
      "2017-11-26T15:34:42.824377: step 6220, loss 0.693055, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:34:43.153102: step 6225, loss 0.693765, acc 0.536133, f1 0.374236\n",
      "2017-11-26T15:34:43.489195: step 6230, loss 0.689406, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:34:43.805382: step 6235, loss 0.683132, acc 0.582031, f1 0.42826\n",
      "2017-11-26T15:34:44.106377: step 6240, loss 0.688543, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:34:44.424299: step 6245, loss 0.686669, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:34:44.749538: step 6250, loss 0.689812, acc 0.558594, f1 0.400396\n",
      "\n",
      "Evaluation:\n",
      "loss 0.690343, acc 0.55893, f1 0.400903\n",
      "\n",
      "Current epoch:  6\n",
      "2017-11-26T15:34:47.662548: step 6255, loss 0.693864, acc 0.538086, f1 0.376489\n",
      "2017-11-26T15:34:47.984843: step 6260, loss 0.689386, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:34:48.311546: step 6265, loss 0.6947, acc 0.535156, f1 0.373111\n",
      "2017-11-26T15:34:48.638277: step 6270, loss 0.686493, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:34:48.972468: step 6275, loss 0.692121, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:34:49.294714: step 6280, loss 0.688632, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:34:49.628451: step 6285, loss 0.690932, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:34:49.957524: step 6290, loss 0.696122, acc 0.522461, f1 0.358584\n",
      "2017-11-26T15:34:50.279216: step 6295, loss 0.686159, acc 0.575195, f1 0.420074\n",
      "2017-11-26T15:34:50.603258: step 6300, loss 0.689519, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:34:50.924352: step 6305, loss 0.689165, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:34:51.228040: step 6310, loss 0.689488, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:34:51.543511: step 6315, loss 0.683966, acc 0.583984, f1 0.430607\n",
      "2017-11-26T15:34:51.869392: step 6320, loss 0.690224, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:34:52.191650: step 6325, loss 0.693284, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:34:52.520124: step 6330, loss 0.695346, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:34:52.831944: step 6335, loss 0.688522, acc 0.575195, f1 0.420074\n",
      "2017-11-26T15:34:53.163088: step 6340, loss 0.693019, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:34:53.488999: step 6345, loss 0.682305, acc 0.591797, f1 0.440035\n",
      "2017-11-26T15:34:53.801718: step 6350, loss 0.691742, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:34:54.124210: step 6355, loss 0.690527, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:34:54.450743: step 6360, loss 0.692837, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:34:54.762578: step 6365, loss 0.689754, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:34:55.074942: step 6370, loss 0.688366, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:34:55.391675: step 6375, loss 0.683116, acc 0.589844, f1 0.437673\n",
      "2017-11-26T15:34:55.724405: step 6380, loss 0.696138, acc 0.52832, f1 0.365267\n",
      "2017-11-26T15:34:56.040339: step 6385, loss 0.696277, acc 0.535156, f1 0.373111\n",
      "2017-11-26T15:34:56.334535: step 6390, loss 0.69182, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:34:56.642241: step 6395, loss 0.689651, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:34:56.963092: step 6400, loss 0.680954, acc 0.591797, f1 0.440035\n",
      "2017-11-26T15:34:57.288815: step 6405, loss 0.689698, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:34:57.621937: step 6410, loss 0.688685, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:34:57.941402: step 6415, loss 0.697286, acc 0.52832, f1 0.365267\n",
      "2017-11-26T15:34:58.270939: step 6420, loss 0.693735, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:34:58.594171: step 6425, loss 0.688845, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:34:58.925202: step 6430, loss 0.690944, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:34:59.253390: step 6435, loss 0.686647, acc 0.578125, f1 0.423577\n",
      "2017-11-26T15:34:59.582926: step 6440, loss 0.691149, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:34:59.901511: step 6445, loss 0.693516, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:35:00.210674: step 6450, loss 0.697751, acc 0.525391, f1 0.361921\n",
      "2017-11-26T15:35:00.525402: step 6455, loss 0.689841, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:35:00.839366: step 6460, loss 0.687927, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:35:01.161895: step 6465, loss 0.695214, acc 0.529297, f1 0.366384\n",
      "2017-11-26T15:35:01.471710: step 6470, loss 0.695734, acc 0.533203, f1 0.370865\n",
      "2017-11-26T15:35:01.806364: step 6475, loss 0.695697, acc 0.536133, f1 0.374236\n",
      "2017-11-26T15:35:02.129931: step 6480, loss 0.691064, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:35:02.460157: step 6485, loss 0.692122, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:35:02.791751: step 6490, loss 0.689728, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:35:03.128124: step 6495, loss 0.693373, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:35:03.445375: step 6500, loss 0.690737, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:35:03.739710: step 6505, loss 0.685858, acc 0.580078, f1 0.425916\n",
      "2017-11-26T15:35:04.060114: step 6510, loss 0.689588, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:35:04.349867: step 6515, loss 0.689837, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:35:04.651893: step 6520, loss 0.692236, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:35:04.955044: step 6525, loss 0.682472, acc 0.583984, f1 0.430607\n",
      "2017-11-26T15:35:05.267825: step 6530, loss 0.689253, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:35:05.583722: step 6535, loss 0.694231, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:35:05.928840: step 6540, loss 0.688064, acc 0.583984, f1 0.430607\n",
      "2017-11-26T15:35:06.250924: step 6545, loss 0.686494, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:35:06.572706: step 6550, loss 0.695408, acc 0.53418, f1 0.371988\n",
      "2017-11-26T15:35:06.906581: step 6555, loss 0.687842, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:35:07.234942: step 6560, loss 0.680256, acc 0.592773, f1 0.441218\n",
      "2017-11-26T15:35:07.565267: step 6565, loss 0.688086, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:35:07.892242: step 6570, loss 0.697266, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:35:08.226307: step 6575, loss 0.695039, acc 0.536133, f1 0.374236\n",
      "2017-11-26T15:35:08.536944: step 6580, loss 0.691194, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:35:08.856254: step 6585, loss 0.691385, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:35:09.192103: step 6590, loss 0.688439, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:35:09.516234: step 6595, loss 0.693578, acc 0.537109, f1 0.375362\n",
      "2017-11-26T15:35:09.830629: step 6600, loss 0.695365, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:35:10.129088: step 6605, loss 0.689579, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:35:10.418410: step 6610, loss 0.694823, acc 0.533203, f1 0.370865\n",
      "2017-11-26T15:35:10.729453: step 6615, loss 0.693209, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:35:11.024187: step 6620, loss 0.688036, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:35:11.309516: step 6625, loss 0.693277, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:35:11.604408: step 6630, loss 0.68863, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:35:11.898009: step 6635, loss 0.692683, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:35:12.194754: step 6640, loss 0.693603, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:35:12.504686: step 6645, loss 0.690249, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:35:12.841018: step 6650, loss 0.685893, acc 0.579102, f1 0.424746\n",
      "2017-11-26T15:35:13.164327: step 6655, loss 0.688999, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:35:13.444821: step 6660, loss 0.693007, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:35:13.758627: step 6665, loss 0.692937, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:35:14.079390: step 6670, loss 0.689218, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:35:14.414365: step 6675, loss 0.687617, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:35:14.720477: step 6680, loss 0.686948, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:35:15.030791: step 6685, loss 0.696431, acc 0.53125, f1 0.368622\n",
      "2017-11-26T15:35:15.326367: step 6690, loss 0.683049, acc 0.586914, f1 0.434136\n",
      "2017-11-26T15:35:15.614433: step 6695, loss 0.691381, acc 0.550781, f1 0.391235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T15:35:15.919417: step 6700, loss 0.688258, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:35:16.221902: step 6705, loss 0.690841, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:35:16.535903: step 6710, loss 0.689397, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:35:16.852347: step 6715, loss 0.694771, acc 0.529297, f1 0.366384\n",
      "2017-11-26T15:35:17.170083: step 6720, loss 0.693635, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:35:17.484057: step 6725, loss 0.682646, acc 0.584961, f1 0.431783\n",
      "2017-11-26T15:35:17.791519: step 6730, loss 0.686386, acc 0.581055, f1 0.427088\n",
      "2017-11-26T15:35:18.105879: step 6735, loss 0.690625, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:35:18.409419: step 6740, loss 0.692795, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:35:18.693747: step 6745, loss 0.684079, acc 0.583008, f1 0.429433\n",
      "2017-11-26T15:35:19.020131: step 6750, loss 0.690585, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:35:19.358763: step 6755, loss 0.689535, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:35:19.685719: step 6760, loss 0.690134, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:35:20.028651: step 6765, loss 0.693172, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:35:20.353000: step 6770, loss 0.68555, acc 0.575195, f1 0.420074\n",
      "2017-11-26T15:35:20.665530: step 6775, loss 0.688785, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:35:20.982741: step 6780, loss 0.692291, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:35:21.304282: step 6785, loss 0.692162, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:35:21.597054: step 6790, loss 0.693295, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:35:21.892407: step 6795, loss 0.693895, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:35:22.208974: step 6800, loss 0.687174, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:35:22.529968: step 6805, loss 0.689217, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:35:22.859602: step 6810, loss 0.683119, acc 0.582031, f1 0.42826\n",
      "2017-11-26T15:35:23.163046: step 6815, loss 0.695814, acc 0.532227, f1 0.369743\n",
      "2017-11-26T15:35:23.490467: step 6820, loss 0.695047, acc 0.529297, f1 0.366384\n",
      "2017-11-26T15:35:23.821019: step 6825, loss 0.691075, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:35:24.138354: step 6830, loss 0.691992, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:35:24.451604: step 6835, loss 0.686232, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:35:24.755515: step 6840, loss 0.681313, acc 0.587891, f1 0.435314\n",
      "2017-11-26T15:35:25.077173: step 6845, loss 0.69232, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:35:25.384947: step 6850, loss 0.689529, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:35:25.682639: step 6855, loss 0.690066, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:35:25.969864: step 6860, loss 0.694376, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:35:26.293664: step 6865, loss 0.688933, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:35:26.625872: step 6870, loss 0.691733, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:35:26.950410: step 6875, loss 0.68805, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:35:27.268880: step 6880, loss 0.692086, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:35:27.601522: step 6885, loss 0.69223, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:35:27.936971: step 6890, loss 0.69213, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:35:28.259857: step 6895, loss 0.690667, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:35:28.592495: step 6900, loss 0.68458, acc 0.575195, f1 0.420074\n",
      "2017-11-26T15:35:28.920660: step 6905, loss 0.685249, acc 0.574219, f1 0.418909\n",
      "2017-11-26T15:35:29.249101: step 6910, loss 0.682514, acc 0.583984, f1 0.430607\n",
      "2017-11-26T15:35:29.577245: step 6915, loss 0.685384, acc 0.573242, f1 0.417744\n",
      "2017-11-26T15:35:29.905960: step 6920, loss 0.685678, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:35:30.237156: step 6925, loss 0.686459, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:35:30.565268: step 6930, loss 0.694554, acc 0.530273, f1 0.367503\n",
      "2017-11-26T15:35:30.891576: step 6935, loss 0.693597, acc 0.537109, f1 0.375362\n",
      "2017-11-26T15:35:31.225218: step 6940, loss 0.692613, acc 0.573242, f1 0.417744\n",
      "2017-11-26T15:35:31.555358: step 6945, loss 0.693073, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:35:31.867881: step 6950, loss 0.689404, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:35:32.174814: step 6955, loss 0.684859, acc 0.575195, f1 0.420074\n",
      "2017-11-26T15:35:32.492380: step 6960, loss 0.691356, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:35:32.814718: step 6965, loss 0.688288, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:35:33.119076: step 6970, loss 0.699483, acc 0.530273, f1 0.367503\n",
      "2017-11-26T15:35:33.428245: step 6975, loss 0.684346, acc 0.588867, f1 0.436493\n",
      "2017-11-26T15:35:33.760928: step 6980, loss 0.690781, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:35:34.068495: step 6985, loss 0.692654, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:35:34.403619: step 6990, loss 0.690583, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:35:34.733151: step 6995, loss 0.695212, acc 0.53418, f1 0.371988\n",
      "2017-11-26T15:35:35.051168: step 7000, loss 0.690494, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:35:35.373597: step 7005, loss 0.688735, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:35:35.695014: step 7010, loss 0.696129, acc 0.53125, f1 0.368622\n",
      "2017-11-26T15:35:36.009892: step 7015, loss 0.690084, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:35:36.337116: step 7020, loss 0.687744, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:35:36.667204: step 7025, loss 0.690415, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:35:37.001439: step 7030, loss 0.688677, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:35:37.321921: step 7035, loss 0.691181, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:35:37.620350: step 7040, loss 0.691626, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:35:37.933233: step 7045, loss 0.692243, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:35:38.260007: step 7050, loss 0.693915, acc 0.536133, f1 0.374236\n",
      "2017-11-26T15:35:38.561482: step 7055, loss 0.687861, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:35:38.873768: step 7060, loss 0.691862, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:35:39.201128: step 7065, loss 0.683285, acc 0.583984, f1 0.430607\n",
      "2017-11-26T15:35:39.504845: step 7070, loss 0.691089, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:35:39.809968: step 7075, loss 0.686692, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:35:40.107867: step 7080, loss 0.69766, acc 0.52832, f1 0.365267\n",
      "2017-11-26T15:35:40.435978: step 7085, loss 0.689013, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:35:40.733572: step 7090, loss 0.686523, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:35:41.053155: step 7095, loss 0.690469, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:35:41.357868: step 7100, loss 0.692876, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:35:41.661988: step 7105, loss 0.688597, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:35:41.958056: step 7110, loss 0.691264, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:35:42.274833: step 7115, loss 0.694935, acc 0.535156, f1 0.373111\n",
      "2017-11-26T15:35:42.560257: step 7120, loss 0.694247, acc 0.53125, f1 0.368622\n",
      "2017-11-26T15:35:42.847428: step 7125, loss 0.679933, acc 0.591797, f1 0.440035\n",
      "2017-11-26T15:35:43.162613: step 7130, loss 0.697334, acc 0.526367, f1 0.363035\n",
      "2017-11-26T15:35:43.479309: step 7135, loss 0.692998, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:35:43.793067: step 7140, loss 0.690113, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:35:44.123318: step 7145, loss 0.686082, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:35:44.453913: step 7150, loss 0.687274, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:35:44.771978: step 7155, loss 0.690089, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:35:45.091353: step 7160, loss 0.685988, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:35:45.421090: step 7165, loss 0.691654, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:35:45.731724: step 7170, loss 0.690506, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:35:46.063140: step 7175, loss 0.690985, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:35:46.382971: step 7180, loss 0.691047, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:35:46.698081: step 7185, loss 0.692973, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:35:46.992488: step 7190, loss 0.692137, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:35:47.279084: step 7195, loss 0.696386, acc 0.525391, f1 0.361921\n",
      "2017-11-26T15:35:47.577793: step 7200, loss 0.689665, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:35:47.892971: step 7205, loss 0.692052, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:35:48.217091: step 7210, loss 0.685282, acc 0.575195, f1 0.420074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T15:35:48.542071: step 7215, loss 0.68714, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:35:49.022389: step 7220, loss 0.699061, acc 0.535156, f1 0.373111\n",
      "2017-11-26T15:35:49.341749: step 7225, loss 0.684704, acc 0.576172, f1 0.421241\n",
      "2017-11-26T15:35:49.672138: step 7230, loss 0.688958, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:35:50.008254: step 7235, loss 0.694542, acc 0.537109, f1 0.375362\n",
      "2017-11-26T15:35:50.293315: step 7240, loss 0.686261, acc 0.581055, f1 0.427088\n",
      "2017-11-26T15:35:50.609576: step 7245, loss 0.688495, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:35:50.913588: step 7250, loss 0.696108, acc 0.524414, f1 0.360808\n",
      "2017-11-26T15:35:51.226684: step 7255, loss 0.690985, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:35:51.551877: step 7260, loss 0.69485, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:35:51.858603: step 7265, loss 0.68745, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:35:52.182748: step 7270, loss 0.694044, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:35:52.501615: step 7275, loss 0.690884, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:35:52.841779: step 7280, loss 0.693513, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:35:53.158323: step 7285, loss 0.69049, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:35:53.481616: step 7290, loss 0.68671, acc 0.572266, f1 0.416581\n",
      "\n",
      "Evaluation:\n",
      "loss 0.691276, acc 0.55893, f1 0.400903\n",
      "\n",
      "Current epoch:  7\n",
      "2017-11-26T15:35:56.043215: step 7295, loss 0.691026, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:35:56.362626: step 7300, loss 0.688267, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:35:56.697219: step 7305, loss 0.689381, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:35:57.030529: step 7310, loss 0.684465, acc 0.586914, f1 0.434136\n",
      "2017-11-26T15:35:57.363133: step 7315, loss 0.691963, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:35:57.689914: step 7320, loss 0.682854, acc 0.583984, f1 0.430607\n",
      "2017-11-26T15:35:58.007223: step 7325, loss 0.691936, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:35:58.329413: step 7330, loss 0.691408, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:35:58.651036: step 7335, loss 0.691125, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:35:58.969943: step 7340, loss 0.691555, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:35:59.289305: step 7345, loss 0.692471, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:35:59.594427: step 7350, loss 0.691029, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:35:59.887686: step 7355, loss 0.694702, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:36:00.189178: step 7360, loss 0.691636, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:36:00.478667: step 7365, loss 0.689639, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:36:00.766765: step 7370, loss 0.685573, acc 0.573242, f1 0.417744\n",
      "2017-11-26T15:36:01.049361: step 7375, loss 0.684357, acc 0.582031, f1 0.42826\n",
      "2017-11-26T15:36:01.350499: step 7380, loss 0.691247, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:36:01.648496: step 7385, loss 0.693507, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:36:01.988212: step 7390, loss 0.691524, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:36:02.310478: step 7395, loss 0.689628, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:36:02.649564: step 7400, loss 0.688218, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:36:02.984249: step 7405, loss 0.688866, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:36:03.314899: step 7410, loss 0.684287, acc 0.581055, f1 0.427088\n",
      "2017-11-26T15:36:03.633551: step 7415, loss 0.687167, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:36:03.965363: step 7420, loss 0.690994, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:36:04.294072: step 7425, loss 0.689428, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:36:04.619705: step 7430, loss 0.692005, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:36:04.942412: step 7435, loss 0.692399, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:36:05.239816: step 7440, loss 0.689132, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:36:05.559787: step 7445, loss 0.686847, acc 0.578125, f1 0.423577\n",
      "2017-11-26T15:36:05.869968: step 7450, loss 0.689803, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:36:06.180054: step 7455, loss 0.690453, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:36:06.476562: step 7460, loss 0.691836, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:36:06.774573: step 7465, loss 0.700128, acc 0.526367, f1 0.363035\n",
      "2017-11-26T15:36:07.085840: step 7470, loss 0.686436, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:36:07.381562: step 7475, loss 0.686884, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:36:07.682681: step 7480, loss 0.686531, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:36:07.966222: step 7485, loss 0.691883, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:36:08.272383: step 7490, loss 0.686089, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:36:08.592940: step 7495, loss 0.691144, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:36:08.929635: step 7500, loss 0.685138, acc 0.575195, f1 0.420074\n",
      "2017-11-26T15:36:09.265191: step 7505, loss 0.694741, acc 0.533203, f1 0.370865\n",
      "2017-11-26T15:36:09.562452: step 7510, loss 0.691637, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:36:09.865739: step 7515, loss 0.692541, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:36:10.166757: step 7520, loss 0.686769, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:36:10.471814: step 7525, loss 0.690026, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:36:10.809492: step 7530, loss 0.686796, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:36:11.134719: step 7535, loss 0.688883, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:36:11.449507: step 7540, loss 0.683464, acc 0.579102, f1 0.424746\n",
      "2017-11-26T15:36:11.753098: step 7545, loss 0.698259, acc 0.523438, f1 0.359696\n",
      "2017-11-26T15:36:12.089710: step 7550, loss 0.688672, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:36:12.417701: step 7555, loss 0.692423, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:36:12.734791: step 7560, loss 0.694072, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:36:13.060660: step 7565, loss 0.689278, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:36:13.386825: step 7570, loss 0.689282, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:36:13.698498: step 7575, loss 0.688657, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:36:14.003270: step 7580, loss 0.691687, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:36:14.320328: step 7585, loss 0.691269, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:36:14.640124: step 7590, loss 0.695568, acc 0.53125, f1 0.368622\n",
      "2017-11-26T15:36:14.969713: step 7595, loss 0.686309, acc 0.575195, f1 0.420074\n",
      "2017-11-26T15:36:15.291544: step 7600, loss 0.69741, acc 0.525391, f1 0.361921\n",
      "2017-11-26T15:36:15.612722: step 7605, loss 0.68289, acc 0.586914, f1 0.434136\n",
      "2017-11-26T15:36:15.935918: step 7610, loss 0.693116, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:36:16.261476: step 7615, loss 0.690243, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:36:16.563701: step 7620, loss 0.691655, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:36:16.898294: step 7625, loss 0.68764, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:36:17.227558: step 7630, loss 0.687567, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:36:17.563834: step 7635, loss 0.692464, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:36:17.876024: step 7640, loss 0.692569, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:36:18.161731: step 7645, loss 0.69608, acc 0.533203, f1 0.370865\n",
      "2017-11-26T15:36:18.465285: step 7650, loss 0.686532, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:36:18.786049: step 7655, loss 0.691117, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:36:19.108283: step 7660, loss 0.687858, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:36:19.429289: step 7665, loss 0.690336, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:36:19.757982: step 7670, loss 0.687847, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:36:20.076957: step 7675, loss 0.697716, acc 0.522461, f1 0.358584\n",
      "2017-11-26T15:36:20.391425: step 7680, loss 0.689898, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:36:20.706019: step 7685, loss 0.692849, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:36:21.012964: step 7690, loss 0.684633, acc 0.578125, f1 0.423577\n",
      "2017-11-26T15:36:21.336861: step 7695, loss 0.691322, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:36:21.661916: step 7700, loss 0.692539, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:36:21.962313: step 7705, loss 0.687518, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:36:22.269826: step 7710, loss 0.682079, acc 0.583984, f1 0.430607\n",
      "2017-11-26T15:36:22.568127: step 7715, loss 0.691219, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:36:22.900615: step 7720, loss 0.686391, acc 0.566406, f1 0.40962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T15:36:23.225040: step 7725, loss 0.690679, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:36:23.545342: step 7730, loss 0.692053, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:36:23.873815: step 7735, loss 0.688909, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:36:24.196490: step 7740, loss 0.68687, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:36:24.507252: step 7745, loss 0.682174, acc 0.583008, f1 0.429433\n",
      "2017-11-26T15:36:24.837397: step 7750, loss 0.69266, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:36:25.169822: step 7755, loss 0.691915, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:36:25.500005: step 7760, loss 0.693129, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:36:25.823863: step 7765, loss 0.692104, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:36:26.154529: step 7770, loss 0.68892, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:36:26.466724: step 7775, loss 0.682859, acc 0.580078, f1 0.425916\n",
      "2017-11-26T15:36:26.773924: step 7780, loss 0.688609, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:36:27.100191: step 7785, loss 0.689879, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:36:27.398984: step 7790, loss 0.690639, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:36:27.703243: step 7795, loss 0.69356, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:36:28.049095: step 7800, loss 0.697081, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:36:28.374411: step 7805, loss 0.687676, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:36:28.704856: step 7810, loss 0.693039, acc 0.538086, f1 0.376489\n",
      "2017-11-26T15:36:29.018290: step 7815, loss 0.69097, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:36:29.324225: step 7820, loss 0.683401, acc 0.579102, f1 0.424746\n",
      "2017-11-26T15:36:29.646936: step 7825, loss 0.689753, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:36:29.957901: step 7830, loss 0.684403, acc 0.581055, f1 0.427088\n",
      "2017-11-26T15:36:30.293867: step 7835, loss 0.691337, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:36:30.615360: step 7840, loss 0.687899, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:36:30.952687: step 7845, loss 0.692683, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:36:31.286837: step 7850, loss 0.69033, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:36:31.606626: step 7855, loss 0.6917, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:36:31.904254: step 7860, loss 0.694455, acc 0.537109, f1 0.375362\n",
      "2017-11-26T15:36:32.225297: step 7865, loss 0.688727, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:36:32.546379: step 7870, loss 0.696777, acc 0.537109, f1 0.375362\n",
      "2017-11-26T15:36:32.859322: step 7875, loss 0.688693, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:36:33.167906: step 7880, loss 0.688999, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:36:33.482157: step 7885, loss 0.688429, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:36:33.793193: step 7890, loss 0.685126, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:36:34.111475: step 7895, loss 0.693166, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:36:34.444410: step 7900, loss 0.692125, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:36:34.766517: step 7905, loss 0.687428, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:36:35.096063: step 7910, loss 0.689301, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:36:35.434642: step 7915, loss 0.688627, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:36:35.778346: step 7920, loss 0.689309, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:36:36.095274: step 7925, loss 0.692117, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:36:36.388532: step 7930, loss 0.692294, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:36:36.714761: step 7935, loss 0.692674, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:36:37.018160: step 7940, loss 0.683339, acc 0.584961, f1 0.431783\n",
      "2017-11-26T15:36:37.326438: step 7945, loss 0.691057, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:36:37.641226: step 7950, loss 0.684803, acc 0.580078, f1 0.425916\n",
      "2017-11-26T15:36:37.932515: step 7955, loss 0.692349, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:36:38.219437: step 7960, loss 0.69292, acc 0.579102, f1 0.424746\n",
      "2017-11-26T15:36:38.518286: step 7965, loss 0.691631, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:36:38.824571: step 7970, loss 0.69322, acc 0.538086, f1 0.376489\n",
      "2017-11-26T15:36:39.158422: step 7975, loss 0.683776, acc 0.584961, f1 0.431783\n",
      "2017-11-26T15:36:39.467730: step 7980, loss 0.691963, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:36:39.781184: step 7985, loss 0.687307, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:36:40.089172: step 7990, loss 0.684012, acc 0.583008, f1 0.429433\n",
      "2017-11-26T15:36:40.392992: step 7995, loss 0.692879, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:36:40.728422: step 8000, loss 0.688958, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:36:41.055510: step 8005, loss 0.69151, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:36:41.391548: step 8010, loss 0.687977, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:36:41.714208: step 8015, loss 0.689015, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:36:42.038861: step 8020, loss 0.692192, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:36:42.346491: step 8025, loss 0.687632, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:36:42.666532: step 8030, loss 0.69043, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:36:42.976254: step 8035, loss 0.698463, acc 0.518555, f1 0.354151\n",
      "2017-11-26T15:36:43.269581: step 8040, loss 0.690409, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:36:43.588635: step 8045, loss 0.695741, acc 0.530273, f1 0.367503\n",
      "2017-11-26T15:36:43.881386: step 8050, loss 0.68574, acc 0.573242, f1 0.417744\n",
      "2017-11-26T15:36:44.206800: step 8055, loss 0.689845, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:36:44.526519: step 8060, loss 0.690683, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:36:44.844296: step 8065, loss 0.685664, acc 0.583008, f1 0.429433\n",
      "2017-11-26T15:36:45.174855: step 8070, loss 0.694023, acc 0.535156, f1 0.373111\n",
      "2017-11-26T15:36:45.505612: step 8075, loss 0.691025, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:36:45.827228: step 8080, loss 0.689225, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:36:46.143208: step 8085, loss 0.690849, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:36:46.472864: step 8090, loss 0.68351, acc 0.582031, f1 0.42826\n",
      "2017-11-26T15:36:46.795132: step 8095, loss 0.686816, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:36:47.124065: step 8100, loss 0.685176, acc 0.585938, f1 0.432959\n",
      "2017-11-26T15:36:47.461955: step 8105, loss 0.692793, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:36:47.781107: step 8110, loss 0.689423, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:36:48.106475: step 8115, loss 0.683646, acc 0.579102, f1 0.424746\n",
      "2017-11-26T15:36:48.433976: step 8120, loss 0.692742, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:36:48.738940: step 8125, loss 0.694038, acc 0.533203, f1 0.370865\n",
      "2017-11-26T15:36:49.069540: step 8130, loss 0.689174, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:36:49.402143: step 8135, loss 0.68719, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:36:49.724472: step 8140, loss 0.685287, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:36:50.051850: step 8145, loss 0.687615, acc 0.577148, f1 0.422408\n",
      "2017-11-26T15:36:50.390269: step 8150, loss 0.697297, acc 0.516602, f1 0.351941\n",
      "2017-11-26T15:36:50.714428: step 8155, loss 0.692177, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:36:51.041163: step 8160, loss 0.697047, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:36:51.511970: step 8165, loss 0.693409, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:36:51.834618: step 8170, loss 0.684125, acc 0.59082, f1 0.438854\n",
      "2017-11-26T15:36:52.157000: step 8175, loss 0.689453, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:36:52.484907: step 8180, loss 0.693172, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:36:52.801093: step 8185, loss 0.684887, acc 0.574219, f1 0.418909\n",
      "2017-11-26T15:36:53.099735: step 8190, loss 0.687827, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:36:53.413953: step 8195, loss 0.68355, acc 0.579102, f1 0.424746\n",
      "2017-11-26T15:36:53.709125: step 8200, loss 0.690486, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:36:54.018597: step 8205, loss 0.689893, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:36:54.340289: step 8210, loss 0.695276, acc 0.533203, f1 0.370865\n",
      "2017-11-26T15:36:54.654434: step 8215, loss 0.689616, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:36:54.989531: step 8220, loss 0.680513, acc 0.587891, f1 0.435314\n",
      "2017-11-26T15:36:55.322934: step 8225, loss 0.68496, acc 0.575195, f1 0.420074\n",
      "2017-11-26T15:36:55.633722: step 8230, loss 0.687448, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:36:55.947527: step 8235, loss 0.679493, acc 0.599609, f1 0.449524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T15:36:56.268154: step 8240, loss 0.684277, acc 0.582031, f1 0.42826\n",
      "2017-11-26T15:36:56.605993: step 8245, loss 0.695369, acc 0.525391, f1 0.361921\n",
      "2017-11-26T15:36:56.931383: step 8250, loss 0.694989, acc 0.53125, f1 0.368622\n",
      "2017-11-26T15:36:57.254743: step 8255, loss 0.69006, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:36:57.592493: step 8260, loss 0.693545, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:36:57.922451: step 8265, loss 0.686303, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:36:58.244693: step 8270, loss 0.691277, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:36:58.575310: step 8275, loss 0.687581, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:36:58.892458: step 8280, loss 0.689588, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:36:59.180038: step 8285, loss 0.688385, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:36:59.473054: step 8290, loss 0.691703, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:36:59.788182: step 8295, loss 0.691214, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:37:00.081204: step 8300, loss 0.687645, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:37:00.372609: step 8305, loss 0.692916, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:37:00.692548: step 8310, loss 0.688654, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:37:00.997636: step 8315, loss 0.692672, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:37:01.309324: step 8320, loss 0.68979, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:37:01.643224: step 8325, loss 0.683739, acc 0.579102, f1 0.424746\n",
      "2017-11-26T15:37:01.961793: step 8330, loss 0.684853, acc 0.583008, f1 0.429433\n",
      "2017-11-26T15:37:02.298929: step 8335, loss 0.689427, acc 0.555664, f1 0.396953\n",
      "\n",
      "Evaluation:\n",
      "loss 0.69072, acc 0.55893, f1 0.400903\n",
      "\n",
      "Current epoch:  8\n",
      "2017-11-26T15:37:04.916880: step 8340, loss 0.687059, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:37:05.203511: step 8345, loss 0.692703, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:37:05.503449: step 8350, loss 0.69508, acc 0.535156, f1 0.373111\n",
      "2017-11-26T15:37:05.824975: step 8355, loss 0.68999, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:37:06.154459: step 8360, loss 0.684837, acc 0.575195, f1 0.420074\n",
      "2017-11-26T15:37:06.482084: step 8365, loss 0.687878, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:37:06.797400: step 8370, loss 0.692053, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:37:07.121080: step 8375, loss 0.691775, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:37:07.451471: step 8380, loss 0.694628, acc 0.53125, f1 0.368622\n",
      "2017-11-26T15:37:07.781159: step 8385, loss 0.686637, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:37:08.098452: step 8390, loss 0.692726, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:37:08.417909: step 8395, loss 0.688001, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:37:08.751109: step 8400, loss 0.689192, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:37:09.066544: step 8405, loss 0.699009, acc 0.525391, f1 0.361921\n",
      "2017-11-26T15:37:09.382225: step 8410, loss 0.693101, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:37:09.705215: step 8415, loss 0.690611, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:37:10.008719: step 8420, loss 0.694207, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:37:10.303877: step 8425, loss 0.690744, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:37:10.623432: step 8430, loss 0.69138, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:37:10.935281: step 8435, loss 0.686627, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:37:11.256405: step 8440, loss 0.685392, acc 0.581055, f1 0.427088\n",
      "2017-11-26T15:37:11.573665: step 8445, loss 0.6921, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:37:11.881654: step 8450, loss 0.687884, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:37:12.205013: step 8455, loss 0.680019, acc 0.586914, f1 0.434136\n",
      "2017-11-26T15:37:12.530336: step 8460, loss 0.687079, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:37:12.860294: step 8465, loss 0.689068, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:37:13.181576: step 8470, loss 0.688926, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:37:13.512287: step 8475, loss 0.686331, acc 0.575195, f1 0.420074\n",
      "2017-11-26T15:37:13.813264: step 8480, loss 0.692511, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:37:14.135961: step 8485, loss 0.68885, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:37:14.444654: step 8490, loss 0.686527, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:37:14.750365: step 8495, loss 0.688078, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:37:15.064686: step 8500, loss 0.690207, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:37:15.373576: step 8505, loss 0.693788, acc 0.532227, f1 0.369743\n",
      "2017-11-26T15:37:15.671337: step 8510, loss 0.692165, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:37:16.004462: step 8515, loss 0.688718, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:37:16.326947: step 8520, loss 0.693269, acc 0.53418, f1 0.371988\n",
      "2017-11-26T15:37:16.651151: step 8525, loss 0.686476, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:37:16.966373: step 8530, loss 0.678188, acc 0.59668, f1 0.445959\n",
      "2017-11-26T15:37:17.291433: step 8535, loss 0.688645, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:37:17.609931: step 8540, loss 0.691425, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:37:17.931548: step 8545, loss 0.690114, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:37:18.258381: step 8550, loss 0.690758, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:37:18.576693: step 8555, loss 0.690065, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:37:18.882929: step 8560, loss 0.688744, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:37:19.197331: step 8565, loss 0.691068, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:37:19.522652: step 8570, loss 0.684706, acc 0.573242, f1 0.417744\n",
      "2017-11-26T15:37:19.842649: step 8575, loss 0.69508, acc 0.533203, f1 0.370865\n",
      "2017-11-26T15:37:20.161217: step 8580, loss 0.68764, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:37:20.476551: step 8585, loss 0.69089, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:37:20.796580: step 8590, loss 0.685268, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:37:21.111343: step 8595, loss 0.690516, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:37:21.407797: step 8600, loss 0.690311, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:37:21.724919: step 8605, loss 0.691216, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:37:22.048126: step 8610, loss 0.68542, acc 0.577148, f1 0.422408\n",
      "2017-11-26T15:37:22.358109: step 8615, loss 0.685739, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:37:22.674765: step 8620, loss 0.688275, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:37:22.983953: step 8625, loss 0.681458, acc 0.584961, f1 0.431783\n",
      "2017-11-26T15:37:23.303518: step 8630, loss 0.689648, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:37:23.602596: step 8635, loss 0.692392, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:37:23.934828: step 8640, loss 0.686922, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:37:24.254618: step 8645, loss 0.687131, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:37:24.571021: step 8650, loss 0.680395, acc 0.591797, f1 0.440035\n",
      "2017-11-26T15:37:24.900413: step 8655, loss 0.695211, acc 0.527344, f1 0.36415\n",
      "2017-11-26T15:37:25.207552: step 8660, loss 0.692835, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:37:25.518966: step 8665, loss 0.691083, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:37:25.824135: step 8670, loss 0.690359, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:37:26.130575: step 8675, loss 0.688477, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:37:26.430162: step 8680, loss 0.697512, acc 0.532227, f1 0.369743\n",
      "2017-11-26T15:37:26.726246: step 8685, loss 0.690017, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:37:27.040115: step 8690, loss 0.68881, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:37:27.371768: step 8695, loss 0.689129, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:37:27.692473: step 8700, loss 0.68469, acc 0.573242, f1 0.417744\n",
      "2017-11-26T15:37:28.026734: step 8705, loss 0.692634, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:37:28.346118: step 8710, loss 0.691406, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:37:28.697292: step 8715, loss 0.684835, acc 0.577148, f1 0.422408\n",
      "2017-11-26T15:37:29.031389: step 8720, loss 0.69381, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:37:29.361483: step 8725, loss 0.683171, acc 0.579102, f1 0.424746\n",
      "2017-11-26T15:37:29.696114: step 8730, loss 0.693169, acc 0.538086, f1 0.376489\n",
      "2017-11-26T15:37:30.002429: step 8735, loss 0.691877, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:37:30.342079: step 8740, loss 0.684247, acc 0.578125, f1 0.423577\n",
      "2017-11-26T15:37:30.672598: step 8745, loss 0.692199, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:37:30.995110: step 8750, loss 0.684126, acc 0.581055, f1 0.427088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T15:37:31.307253: step 8755, loss 0.691177, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:37:31.603526: step 8760, loss 0.684873, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:37:31.911470: step 8765, loss 0.685583, acc 0.573242, f1 0.417744\n",
      "2017-11-26T15:37:32.214929: step 8770, loss 0.693535, acc 0.538086, f1 0.376489\n",
      "2017-11-26T15:37:32.546073: step 8775, loss 0.691548, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:37:32.868686: step 8780, loss 0.690234, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:37:33.205891: step 8785, loss 0.689036, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:37:33.540468: step 8790, loss 0.688905, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:37:33.866081: step 8795, loss 0.690913, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:37:34.195295: step 8800, loss 0.690203, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:37:34.527035: step 8805, loss 0.683469, acc 0.581055, f1 0.427088\n",
      "2017-11-26T15:37:34.842397: step 8810, loss 0.691302, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:37:35.155355: step 8815, loss 0.695038, acc 0.535156, f1 0.373111\n",
      "2017-11-26T15:37:35.473770: step 8820, loss 0.688484, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:37:35.790476: step 8825, loss 0.691161, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:37:36.123199: step 8830, loss 0.686519, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:37:36.450986: step 8835, loss 0.6919, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:37:36.771582: step 8840, loss 0.690064, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:37:37.097943: step 8845, loss 0.689514, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:37:37.374606: step 8850, loss 0.686151, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:37:37.654991: step 8855, loss 0.690972, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:37:37.947303: step 8860, loss 0.691092, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:37:38.245521: step 8865, loss 0.683257, acc 0.59375, f1 0.442402\n",
      "2017-11-26T15:37:38.546230: step 8870, loss 0.6902, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:37:38.850863: step 8875, loss 0.692887, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:37:39.159766: step 8880, loss 0.687713, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:37:39.488814: step 8885, loss 0.694061, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:37:39.797615: step 8890, loss 0.690226, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:37:40.118167: step 8895, loss 0.688574, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:37:40.431951: step 8900, loss 0.692955, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:37:40.747910: step 8905, loss 0.692217, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:37:41.047562: step 8910, loss 0.687685, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:37:41.370877: step 8915, loss 0.68371, acc 0.583008, f1 0.429433\n",
      "2017-11-26T15:37:41.664593: step 8920, loss 0.685989, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:37:41.967612: step 8925, loss 0.689512, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:37:42.285527: step 8930, loss 0.691684, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:37:42.575547: step 8935, loss 0.69033, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:37:42.861374: step 8940, loss 0.68565, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:37:43.175681: step 8945, loss 0.690158, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:37:43.480198: step 8950, loss 0.691857, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:37:43.775504: step 8955, loss 0.693848, acc 0.53125, f1 0.368622\n",
      "2017-11-26T15:37:44.098488: step 8960, loss 0.690192, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:37:44.437126: step 8965, loss 0.69113, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:37:44.762499: step 8970, loss 0.690657, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:37:45.069078: step 8975, loss 0.690371, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:37:45.373825: step 8980, loss 0.688569, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:37:45.678418: step 8985, loss 0.688159, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:37:45.974383: step 8990, loss 0.700598, acc 0.525391, f1 0.361921\n",
      "2017-11-26T15:37:46.282283: step 8995, loss 0.689855, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:37:46.584281: step 9000, loss 0.686222, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:37:46.899087: step 9005, loss 0.687222, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:37:47.219051: step 9010, loss 0.692409, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:37:47.525193: step 9015, loss 0.689017, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:37:47.849956: step 9020, loss 0.690377, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:37:48.160955: step 9025, loss 0.692546, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:37:48.469087: step 9030, loss 0.686496, acc 0.578125, f1 0.423577\n",
      "2017-11-26T15:37:48.783665: step 9035, loss 0.688004, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:37:49.103910: step 9040, loss 0.686548, acc 0.574219, f1 0.418909\n",
      "2017-11-26T15:37:49.431677: step 9045, loss 0.685646, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:37:49.742747: step 9050, loss 0.689467, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:37:50.052042: step 9055, loss 0.691532, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:37:50.341763: step 9060, loss 0.693709, acc 0.53418, f1 0.371988\n",
      "2017-11-26T15:37:50.663346: step 9065, loss 0.689457, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:37:50.988636: step 9070, loss 0.69671, acc 0.525391, f1 0.361921\n",
      "2017-11-26T15:37:51.316550: step 9075, loss 0.693296, acc 0.538086, f1 0.376489\n",
      "2017-11-26T15:37:51.619812: step 9080, loss 0.682399, acc 0.587891, f1 0.435314\n",
      "2017-11-26T15:37:51.946508: step 9085, loss 0.690245, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:37:52.269528: step 9090, loss 0.69273, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:37:52.588035: step 9095, loss 0.691751, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:37:52.893254: step 9100, loss 0.688458, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:37:53.374050: step 9105, loss 0.691955, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:37:53.704811: step 9110, loss 0.68645, acc 0.575195, f1 0.420074\n",
      "2017-11-26T15:37:54.029629: step 9115, loss 0.686472, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:37:54.357550: step 9120, loss 0.687776, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:37:54.688547: step 9125, loss 0.689768, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:37:55.022874: step 9130, loss 0.693469, acc 0.53418, f1 0.371988\n",
      "2017-11-26T15:37:55.358241: step 9135, loss 0.690371, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:37:55.662611: step 9140, loss 0.689121, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:37:55.980229: step 9145, loss 0.693885, acc 0.533203, f1 0.370865\n",
      "2017-11-26T15:37:56.312070: step 9150, loss 0.687901, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:37:56.655558: step 9155, loss 0.696847, acc 0.524414, f1 0.360808\n",
      "2017-11-26T15:37:56.979410: step 9160, loss 0.686072, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:37:57.308616: step 9165, loss 0.69554, acc 0.537109, f1 0.375362\n",
      "2017-11-26T15:37:57.603031: step 9170, loss 0.690954, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:37:57.925044: step 9175, loss 0.693328, acc 0.536133, f1 0.374236\n",
      "2017-11-26T15:37:58.254736: step 9180, loss 0.691298, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:37:58.590206: step 9185, loss 0.687138, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:37:58.914077: step 9190, loss 0.691378, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:37:59.229559: step 9195, loss 0.688286, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:37:59.542580: step 9200, loss 0.693986, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:37:59.870174: step 9205, loss 0.697983, acc 0.53125, f1 0.368622\n",
      "2017-11-26T15:38:00.199112: step 9210, loss 0.68166, acc 0.586914, f1 0.434136\n",
      "2017-11-26T15:38:00.526342: step 9215, loss 0.688229, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:38:00.838969: step 9220, loss 0.685115, acc 0.573242, f1 0.417744\n",
      "2017-11-26T15:38:01.144575: step 9225, loss 0.688407, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:38:01.461155: step 9230, loss 0.68018, acc 0.594727, f1 0.443587\n",
      "2017-11-26T15:38:01.785220: step 9235, loss 0.693304, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:38:02.125727: step 9240, loss 0.687702, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:38:02.458105: step 9245, loss 0.686427, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:38:02.793657: step 9250, loss 0.686958, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:38:03.113459: step 9255, loss 0.688016, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:38:03.410455: step 9260, loss 0.68955, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:38:03.724111: step 9265, loss 0.685992, acc 0.567383, f1 0.410778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T15:38:04.034914: step 9270, loss 0.686195, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:38:04.352577: step 9275, loss 0.693181, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:38:04.690681: step 9280, loss 0.690955, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:38:05.013678: step 9285, loss 0.686307, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:38:05.334020: step 9290, loss 0.690315, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:38:05.657921: step 9295, loss 0.683748, acc 0.578125, f1 0.423577\n",
      "2017-11-26T15:38:05.976690: step 9300, loss 0.696119, acc 0.524414, f1 0.360808\n",
      "2017-11-26T15:38:06.281762: step 9305, loss 0.685917, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:38:06.579396: step 9310, loss 0.693492, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:38:06.885539: step 9315, loss 0.687318, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:38:07.173280: step 9320, loss 0.698766, acc 0.524414, f1 0.360808\n",
      "2017-11-26T15:38:07.475231: step 9325, loss 0.687707, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:38:07.791964: step 9330, loss 0.69414, acc 0.527344, f1 0.36415\n",
      "2017-11-26T15:38:08.105643: step 9335, loss 0.689145, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:38:08.417740: step 9340, loss 0.688775, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:38:08.746725: step 9345, loss 0.683244, acc 0.579102, f1 0.424746\n",
      "2017-11-26T15:38:09.066264: step 9350, loss 0.684897, acc 0.573242, f1 0.417744\n",
      "2017-11-26T15:38:09.388349: step 9355, loss 0.690937, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:38:09.693867: step 9360, loss 0.687, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:38:10.018004: step 9365, loss 0.6878, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:38:10.340843: step 9370, loss 0.692514, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:38:10.660738: step 9375, loss 0.686947, acc 0.564453, f1 0.407308\n",
      "\n",
      "Evaluation:\n",
      "loss 0.688486, acc 0.55893, f1 0.400903\n",
      "\n",
      "Current epoch:  9\n",
      "2017-11-26T15:38:13.284252: step 9380, loss 0.688089, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:38:13.606626: step 9385, loss 0.69319, acc 0.53418, f1 0.371988\n",
      "2017-11-26T15:38:13.909071: step 9390, loss 0.689421, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:38:14.228189: step 9395, loss 0.686727, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:38:14.557458: step 9400, loss 0.688973, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:38:14.860097: step 9405, loss 0.696417, acc 0.527344, f1 0.36415\n",
      "2017-11-26T15:38:15.169408: step 9410, loss 0.686817, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:38:15.504898: step 9415, loss 0.691891, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:38:15.838501: step 9420, loss 0.690827, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:38:16.158299: step 9425, loss 0.689358, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:38:16.486163: step 9430, loss 0.698301, acc 0.529297, f1 0.366384\n",
      "2017-11-26T15:38:16.783036: step 9435, loss 0.689394, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:38:17.112045: step 9440, loss 0.693731, acc 0.537109, f1 0.375362\n",
      "2017-11-26T15:38:17.438315: step 9445, loss 0.686145, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:38:17.758981: step 9450, loss 0.687039, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:38:18.089754: step 9455, loss 0.688448, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:38:18.411220: step 9460, loss 0.690123, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:38:18.738718: step 9465, loss 0.690253, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:38:19.058961: step 9470, loss 0.69192, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:38:19.357232: step 9475, loss 0.688909, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:38:19.651534: step 9480, loss 0.690448, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:38:19.962959: step 9485, loss 0.689097, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:38:20.265629: step 9490, loss 0.683204, acc 0.579102, f1 0.424746\n",
      "2017-11-26T15:38:20.593460: step 9495, loss 0.677143, acc 0.599609, f1 0.449524\n",
      "2017-11-26T15:38:20.912280: step 9500, loss 0.690406, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:38:21.240537: step 9505, loss 0.683778, acc 0.579102, f1 0.424746\n",
      "2017-11-26T15:38:21.580426: step 9510, loss 0.688138, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:38:21.888137: step 9515, loss 0.690006, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:38:22.193167: step 9520, loss 0.695968, acc 0.52832, f1 0.365267\n",
      "2017-11-26T15:38:22.508262: step 9525, loss 0.686493, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:38:22.830604: step 9530, loss 0.695696, acc 0.525391, f1 0.361921\n",
      "2017-11-26T15:38:23.137305: step 9535, loss 0.686427, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:38:23.458350: step 9540, loss 0.68411, acc 0.573242, f1 0.417744\n",
      "2017-11-26T15:38:23.786382: step 9545, loss 0.690708, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:38:24.116759: step 9550, loss 0.693981, acc 0.537109, f1 0.375362\n",
      "2017-11-26T15:38:24.432313: step 9555, loss 0.688159, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:38:24.723126: step 9560, loss 0.694495, acc 0.53418, f1 0.371988\n",
      "2017-11-26T15:38:25.033700: step 9565, loss 0.688463, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:38:25.347060: step 9570, loss 0.694772, acc 0.53125, f1 0.368622\n",
      "2017-11-26T15:38:25.668812: step 9575, loss 0.690798, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:38:25.967868: step 9580, loss 0.685624, acc 0.576172, f1 0.421241\n",
      "2017-11-26T15:38:26.278344: step 9585, loss 0.689224, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:38:26.581678: step 9590, loss 0.691208, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:38:26.880150: step 9595, loss 0.689644, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:38:27.158551: step 9600, loss 0.686628, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:38:27.486748: step 9605, loss 0.685905, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:38:27.791190: step 9610, loss 0.686752, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:38:28.107316: step 9615, loss 0.68927, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:38:28.398719: step 9620, loss 0.691614, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:38:28.686204: step 9625, loss 0.68653, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:38:28.985566: step 9630, loss 0.692464, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:38:29.322565: step 9635, loss 0.690745, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:38:29.652970: step 9640, loss 0.69453, acc 0.537109, f1 0.375362\n",
      "2017-11-26T15:38:29.982399: step 9645, loss 0.688545, acc 0.580078, f1 0.425916\n",
      "2017-11-26T15:38:30.304871: step 9650, loss 0.687661, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:38:30.619137: step 9655, loss 0.69418, acc 0.533203, f1 0.370865\n",
      "2017-11-26T15:38:30.943377: step 9660, loss 0.692725, acc 0.536133, f1 0.374236\n",
      "2017-11-26T15:38:31.259109: step 9665, loss 0.688105, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:38:31.563963: step 9670, loss 0.688749, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:38:31.886725: step 9675, loss 0.68303, acc 0.587891, f1 0.435314\n",
      "2017-11-26T15:38:32.207007: step 9680, loss 0.683634, acc 0.575195, f1 0.420074\n",
      "2017-11-26T15:38:32.528510: step 9685, loss 0.687774, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:38:32.853284: step 9690, loss 0.688947, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:38:33.184138: step 9695, loss 0.692041, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:38:33.500659: step 9700, loss 0.690142, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:38:33.845834: step 9705, loss 0.692028, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:38:34.167175: step 9710, loss 0.687585, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:38:34.491023: step 9715, loss 0.698867, acc 0.510742, f1 0.345337\n",
      "2017-11-26T15:38:34.793090: step 9720, loss 0.688719, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:38:35.097333: step 9725, loss 0.693248, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:38:35.412948: step 9730, loss 0.689919, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:38:35.729397: step 9735, loss 0.693514, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:38:36.033198: step 9740, loss 0.689369, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:38:36.340319: step 9745, loss 0.684955, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:38:36.644763: step 9750, loss 0.688554, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:38:36.969219: step 9755, loss 0.68775, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:38:37.291894: step 9760, loss 0.69424, acc 0.537109, f1 0.375362\n",
      "2017-11-26T15:38:37.601301: step 9765, loss 0.690369, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:38:37.903581: step 9770, loss 0.686665, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:38:38.234249: step 9775, loss 0.692332, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:38:38.538913: step 9780, loss 0.6883, acc 0.560547, f1 0.402696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T15:38:38.840923: step 9785, loss 0.698296, acc 0.530273, f1 0.367503\n",
      "2017-11-26T15:38:39.161961: step 9790, loss 0.688997, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:38:39.475226: step 9795, loss 0.693462, acc 0.53125, f1 0.368622\n",
      "2017-11-26T15:38:39.772544: step 9800, loss 0.69148, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:38:40.094922: step 9805, loss 0.693579, acc 0.533203, f1 0.370865\n",
      "2017-11-26T15:38:40.422118: step 9810, loss 0.690459, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:38:40.737610: step 9815, loss 0.683821, acc 0.578125, f1 0.423577\n",
      "2017-11-26T15:38:41.054364: step 9820, loss 0.685679, acc 0.581055, f1 0.427088\n",
      "2017-11-26T15:38:41.379958: step 9825, loss 0.687726, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:38:41.697764: step 9830, loss 0.686874, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:38:42.002521: step 9835, loss 0.688096, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:38:42.318411: step 9840, loss 0.68414, acc 0.575195, f1 0.420074\n",
      "2017-11-26T15:38:42.634672: step 9845, loss 0.68652, acc 0.583008, f1 0.429433\n",
      "2017-11-26T15:38:42.945921: step 9850, loss 0.689782, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:38:43.270740: step 9855, loss 0.686617, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:38:43.586360: step 9860, loss 0.689506, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:38:43.913300: step 9865, loss 0.684606, acc 0.575195, f1 0.420074\n",
      "2017-11-26T15:38:44.208515: step 9870, loss 0.695592, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:38:44.531132: step 9875, loss 0.689887, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:38:44.860241: step 9880, loss 0.684912, acc 0.577148, f1 0.422408\n",
      "2017-11-26T15:38:45.176969: step 9885, loss 0.694287, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:38:45.477633: step 9890, loss 0.687531, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:38:45.795412: step 9895, loss 0.682886, acc 0.582031, f1 0.42826\n",
      "2017-11-26T15:38:46.113965: step 9900, loss 0.688714, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:38:46.422921: step 9905, loss 0.687417, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:38:46.745342: step 9910, loss 0.678057, acc 0.59668, f1 0.445959\n",
      "2017-11-26T15:38:47.059913: step 9915, loss 0.687946, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:38:47.354663: step 9920, loss 0.693697, acc 0.53418, f1 0.371988\n",
      "2017-11-26T15:38:47.655766: step 9925, loss 0.688984, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:38:47.949043: step 9930, loss 0.692096, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:38:48.245485: step 9935, loss 0.689206, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:38:48.550466: step 9940, loss 0.69155, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:38:48.849053: step 9945, loss 0.688833, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:38:49.139122: step 9950, loss 0.699809, acc 0.525391, f1 0.361921\n",
      "2017-11-26T15:38:49.449567: step 9955, loss 0.696532, acc 0.526367, f1 0.363035\n",
      "2017-11-26T15:38:49.761256: step 9960, loss 0.687883, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:38:50.068007: step 9965, loss 0.688708, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:38:50.385754: step 9970, loss 0.692143, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:38:50.702780: step 9975, loss 0.68681, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:38:51.016398: step 9980, loss 0.691713, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:38:51.337583: step 9985, loss 0.689429, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:38:51.668507: step 9990, loss 0.695907, acc 0.524414, f1 0.360808\n",
      "2017-11-26T15:38:51.991323: step 9995, loss 0.683835, acc 0.580078, f1 0.425916\n",
      "2017-11-26T15:38:52.324019: step 10000, loss 0.687507, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:38:52.646078: step 10005, loss 0.690248, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:38:52.972103: step 10010, loss 0.692381, acc 0.538086, f1 0.376489\n",
      "2017-11-26T15:38:53.294733: step 10015, loss 0.688565, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:38:53.625792: step 10020, loss 0.688586, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:38:53.954438: step 10025, loss 0.688658, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:38:54.248965: step 10030, loss 0.689563, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:38:54.545528: step 10035, loss 0.691988, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:38:54.839348: step 10040, loss 0.692959, acc 0.535156, f1 0.373111\n",
      "2017-11-26T15:38:55.120791: step 10045, loss 0.687139, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:38:55.584416: step 10050, loss 0.686128, acc 0.574219, f1 0.418909\n",
      "2017-11-26T15:38:55.910338: step 10055, loss 0.695951, acc 0.521484, f1 0.357475\n",
      "2017-11-26T15:38:56.239609: step 10060, loss 0.690301, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:38:56.570173: step 10065, loss 0.691439, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:38:56.888926: step 10070, loss 0.689158, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:38:57.220143: step 10075, loss 0.690651, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:38:57.544198: step 10080, loss 0.692596, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:38:57.874077: step 10085, loss 0.6864, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:38:58.188593: step 10090, loss 0.689744, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:38:58.486146: step 10095, loss 0.68707, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:38:58.778363: step 10100, loss 0.687709, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:38:59.069095: step 10105, loss 0.686973, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:38:59.370434: step 10110, loss 0.689765, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:38:59.674364: step 10115, loss 0.679828, acc 0.594727, f1 0.443587\n",
      "2017-11-26T15:38:59.969468: step 10120, loss 0.685242, acc 0.578125, f1 0.423577\n",
      "2017-11-26T15:39:00.283805: step 10125, loss 0.689234, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:39:00.613465: step 10130, loss 0.693696, acc 0.53125, f1 0.368622\n",
      "2017-11-26T15:39:00.931841: step 10135, loss 0.688646, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:39:01.270221: step 10140, loss 0.688366, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:39:01.598169: step 10145, loss 0.687062, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:39:01.926621: step 10150, loss 0.691688, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:39:02.230132: step 10155, loss 0.688003, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:39:02.540235: step 10160, loss 0.688688, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:39:02.853542: step 10165, loss 0.681395, acc 0.586914, f1 0.434136\n",
      "2017-11-26T15:39:03.173234: step 10170, loss 0.693157, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:39:03.481289: step 10175, loss 0.691036, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:39:03.795628: step 10180, loss 0.687218, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:39:04.110227: step 10185, loss 0.68669, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:39:04.430959: step 10190, loss 0.685438, acc 0.575195, f1 0.420074\n",
      "2017-11-26T15:39:04.722029: step 10195, loss 0.683603, acc 0.585938, f1 0.432959\n",
      "2017-11-26T15:39:05.042102: step 10200, loss 0.700899, acc 0.519531, f1 0.355258\n",
      "2017-11-26T15:39:05.384949: step 10205, loss 0.686212, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:39:05.716071: step 10210, loss 0.684572, acc 0.574219, f1 0.418909\n",
      "2017-11-26T15:39:06.047674: step 10215, loss 0.689541, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:39:06.378078: step 10220, loss 0.687157, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:39:06.698740: step 10225, loss 0.685951, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:39:06.995523: step 10230, loss 0.691232, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:39:07.297308: step 10235, loss 0.686988, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:39:07.630945: step 10240, loss 0.692998, acc 0.535156, f1 0.373111\n",
      "2017-11-26T15:39:07.939471: step 10245, loss 0.691625, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:39:08.256615: step 10250, loss 0.687021, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:39:08.584736: step 10255, loss 0.690788, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:39:08.917374: step 10260, loss 0.689738, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:39:09.244960: step 10265, loss 0.69189, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:39:09.576415: step 10270, loss 0.688814, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:39:09.898235: step 10275, loss 0.683568, acc 0.575195, f1 0.420074\n",
      "2017-11-26T15:39:10.211954: step 10280, loss 0.689408, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:39:10.535533: step 10285, loss 0.684812, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:39:10.855185: step 10290, loss 0.698873, acc 0.520508, f1 0.356366\n",
      "2017-11-26T15:39:11.181698: step 10295, loss 0.685504, acc 0.569336, f1 0.413096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T15:39:11.505854: step 10300, loss 0.682462, acc 0.580078, f1 0.425916\n",
      "2017-11-26T15:39:11.844163: step 10305, loss 0.692814, acc 0.536133, f1 0.374236\n",
      "2017-11-26T15:39:12.158649: step 10310, loss 0.681253, acc 0.586914, f1 0.434136\n",
      "2017-11-26T15:39:12.481457: step 10315, loss 0.689574, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:39:12.784084: step 10320, loss 0.688931, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:39:13.090024: step 10325, loss 0.688723, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:39:13.415175: step 10330, loss 0.690021, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:39:13.759705: step 10335, loss 0.691287, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:39:14.065608: step 10340, loss 0.685944, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:39:14.375819: step 10345, loss 0.687213, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:39:14.668660: step 10350, loss 0.687987, acc 0.573242, f1 0.417744\n",
      "2017-11-26T15:39:14.994775: step 10355, loss 0.691581, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:39:15.326350: step 10360, loss 0.690998, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:39:15.644083: step 10365, loss 0.691531, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:39:15.958412: step 10370, loss 0.691358, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:39:16.282819: step 10375, loss 0.689149, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:39:16.581873: step 10380, loss 0.692352, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:39:16.894524: step 10385, loss 0.693887, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:39:17.210244: step 10390, loss 0.683347, acc 0.582031, f1 0.42826\n",
      "2017-11-26T15:39:17.536947: step 10395, loss 0.691127, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:39:17.849862: step 10400, loss 0.688094, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:39:18.145373: step 10405, loss 0.685777, acc 0.577148, f1 0.422408\n",
      "2017-11-26T15:39:18.443976: step 10410, loss 0.687943, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:39:18.755266: step 10415, loss 0.686836, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:39:19.054114: step 10420, loss 0.687685, acc 0.562278, f1 0.404737\n",
      "\n",
      "Evaluation:\n",
      "loss 0.688298, acc 0.55893, f1 0.400903\n",
      "\n",
      "Current epoch:  10\n",
      "2017-11-26T15:39:21.713670: step 10425, loss 0.691859, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:39:22.013324: step 10430, loss 0.686465, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:39:22.326958: step 10435, loss 0.690422, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:39:22.651318: step 10440, loss 0.688388, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:39:22.969375: step 10445, loss 0.687029, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:39:23.275558: step 10450, loss 0.68384, acc 0.594727, f1 0.443587\n",
      "2017-11-26T15:39:23.591891: step 10455, loss 0.68584, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:39:23.910942: step 10460, loss 0.696526, acc 0.526367, f1 0.363035\n",
      "2017-11-26T15:39:24.247943: step 10465, loss 0.686564, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:39:24.569433: step 10470, loss 0.687512, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:39:24.893465: step 10475, loss 0.692296, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:39:25.206769: step 10480, loss 0.686616, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:39:25.526423: step 10485, loss 0.689468, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:39:25.849200: step 10490, loss 0.686902, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:39:26.141288: step 10495, loss 0.681074, acc 0.592773, f1 0.441218\n",
      "2017-11-26T15:39:26.453073: step 10500, loss 0.688213, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:39:26.773635: step 10505, loss 0.687854, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:39:27.087349: step 10510, loss 0.697973, acc 0.514648, f1 0.349735\n",
      "2017-11-26T15:39:27.398300: step 10515, loss 0.694187, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:39:27.707460: step 10520, loss 0.688288, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:39:28.031010: step 10525, loss 0.691609, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:39:28.342759: step 10530, loss 0.681395, acc 0.583984, f1 0.430607\n",
      "2017-11-26T15:39:28.637481: step 10535, loss 0.689372, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:39:28.935958: step 10540, loss 0.686091, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:39:29.245906: step 10545, loss 0.684596, acc 0.574219, f1 0.418909\n",
      "2017-11-26T15:39:29.533521: step 10550, loss 0.692585, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:39:29.846699: step 10555, loss 0.684661, acc 0.575195, f1 0.420074\n",
      "2017-11-26T15:39:30.159257: step 10560, loss 0.690507, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:39:30.483626: step 10565, loss 0.687681, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:39:30.799433: step 10570, loss 0.689324, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:39:31.133372: step 10575, loss 0.702911, acc 0.513672, f1 0.348634\n",
      "2017-11-26T15:39:31.455156: step 10580, loss 0.691762, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:39:31.780326: step 10585, loss 0.687083, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:39:32.095774: step 10590, loss 0.690743, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:39:32.421369: step 10595, loss 0.693823, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:39:32.743808: step 10600, loss 0.689283, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:39:33.071160: step 10605, loss 0.693538, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:39:33.402461: step 10610, loss 0.686806, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:39:33.707638: step 10615, loss 0.685073, acc 0.575195, f1 0.420074\n",
      "2017-11-26T15:39:34.013217: step 10620, loss 0.688142, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:39:34.309725: step 10625, loss 0.687328, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:39:34.637361: step 10630, loss 0.691151, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:39:34.962282: step 10635, loss 0.684236, acc 0.576172, f1 0.421241\n",
      "2017-11-26T15:39:35.288529: step 10640, loss 0.690725, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:39:35.587543: step 10645, loss 0.687118, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:39:35.894411: step 10650, loss 0.682899, acc 0.577148, f1 0.422408\n",
      "2017-11-26T15:39:36.197488: step 10655, loss 0.692415, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:39:36.521834: step 10660, loss 0.690555, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:39:36.855348: step 10665, loss 0.691208, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:39:37.142990: step 10670, loss 0.686314, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:39:37.433723: step 10675, loss 0.681483, acc 0.584961, f1 0.431783\n",
      "2017-11-26T15:39:37.778613: step 10680, loss 0.691767, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:39:38.112160: step 10685, loss 0.693654, acc 0.529297, f1 0.366384\n",
      "2017-11-26T15:39:38.448698: step 10690, loss 0.690743, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:39:38.771219: step 10695, loss 0.692631, acc 0.537109, f1 0.375362\n",
      "2017-11-26T15:39:39.113915: step 10700, loss 0.685424, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:39:39.441429: step 10705, loss 0.693563, acc 0.536133, f1 0.374236\n",
      "2017-11-26T15:39:39.762518: step 10710, loss 0.683755, acc 0.581055, f1 0.427088\n",
      "2017-11-26T15:39:40.095719: step 10715, loss 0.696038, acc 0.53125, f1 0.368622\n",
      "2017-11-26T15:39:40.420111: step 10720, loss 0.68429, acc 0.574219, f1 0.418909\n",
      "2017-11-26T15:39:40.732059: step 10725, loss 0.686283, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:39:41.067282: step 10730, loss 0.689726, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:39:41.389340: step 10735, loss 0.693911, acc 0.535156, f1 0.373111\n",
      "2017-11-26T15:39:41.719629: step 10740, loss 0.69014, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:39:42.040854: step 10745, loss 0.69225, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:39:42.351657: step 10750, loss 0.691504, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:39:42.646217: step 10755, loss 0.686789, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:39:42.962226: step 10760, loss 0.675415, acc 0.597656, f1 0.447146\n",
      "2017-11-26T15:39:43.284417: step 10765, loss 0.693778, acc 0.532227, f1 0.369743\n",
      "2017-11-26T15:39:43.596081: step 10770, loss 0.691168, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:39:43.938789: step 10775, loss 0.693933, acc 0.533203, f1 0.370865\n",
      "2017-11-26T15:39:44.267986: step 10780, loss 0.693007, acc 0.537109, f1 0.375362\n",
      "2017-11-26T15:39:44.588512: step 10785, loss 0.691258, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:39:44.917321: step 10790, loss 0.685682, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:39:45.244897: step 10795, loss 0.691984, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:39:45.569083: step 10800, loss 0.685183, acc 0.574219, f1 0.418909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T15:39:45.902666: step 10805, loss 0.688736, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:39:46.212713: step 10810, loss 0.694431, acc 0.530273, f1 0.367503\n",
      "2017-11-26T15:39:46.525714: step 10815, loss 0.690587, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:39:46.838647: step 10820, loss 0.68627, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:39:47.168535: step 10825, loss 0.690541, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:39:47.478454: step 10830, loss 0.69292, acc 0.536133, f1 0.374236\n",
      "2017-11-26T15:39:47.794047: step 10835, loss 0.68808, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:39:48.093314: step 10840, loss 0.684191, acc 0.576172, f1 0.421241\n",
      "2017-11-26T15:39:48.408500: step 10845, loss 0.690761, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:39:48.734055: step 10850, loss 0.692127, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:39:49.063255: step 10855, loss 0.696812, acc 0.526367, f1 0.363035\n",
      "2017-11-26T15:39:49.390358: step 10860, loss 0.686032, acc 0.574219, f1 0.418909\n",
      "2017-11-26T15:39:49.703643: step 10865, loss 0.687313, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:39:50.018327: step 10870, loss 0.69012, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:39:50.341117: step 10875, loss 0.693853, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:39:50.677381: step 10880, loss 0.689466, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:39:51.007953: step 10885, loss 0.690192, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:39:51.331481: step 10890, loss 0.694456, acc 0.537109, f1 0.375362\n",
      "2017-11-26T15:39:51.659056: step 10895, loss 0.684571, acc 0.575195, f1 0.420074\n",
      "2017-11-26T15:39:51.995224: step 10900, loss 0.684841, acc 0.591797, f1 0.440035\n",
      "2017-11-26T15:39:52.322363: step 10905, loss 0.685385, acc 0.577148, f1 0.422408\n",
      "2017-11-26T15:39:52.652539: step 10910, loss 0.696086, acc 0.515625, f1 0.350838\n",
      "2017-11-26T15:39:52.975976: step 10915, loss 0.688161, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:39:53.292308: step 10920, loss 0.693674, acc 0.53418, f1 0.371988\n",
      "2017-11-26T15:39:53.622152: step 10925, loss 0.68905, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:39:53.952284: step 10930, loss 0.687509, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:39:54.279619: step 10935, loss 0.691807, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:39:54.581115: step 10940, loss 0.685292, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:39:54.883858: step 10945, loss 0.694508, acc 0.526367, f1 0.363035\n",
      "2017-11-26T15:39:55.183056: step 10950, loss 0.694116, acc 0.53125, f1 0.368622\n",
      "2017-11-26T15:39:55.490813: step 10955, loss 0.686644, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:39:55.822370: step 10960, loss 0.689974, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:39:56.136566: step 10965, loss 0.691172, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:39:56.443440: step 10970, loss 0.690135, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:39:56.772347: step 10975, loss 0.689421, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:39:57.094524: step 10980, loss 0.690838, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:39:57.422799: step 10985, loss 0.689427, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:39:57.888385: step 10990, loss 0.685648, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:39:58.212802: step 10995, loss 0.687996, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:39:58.516470: step 11000, loss 0.686681, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:39:58.817159: step 11005, loss 0.688633, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:39:59.116922: step 11010, loss 0.689552, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:39:59.426920: step 11015, loss 0.687374, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:39:59.750299: step 11020, loss 0.695281, acc 0.537109, f1 0.375362\n",
      "2017-11-26T15:40:00.078888: step 11025, loss 0.686033, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:40:00.393688: step 11030, loss 0.692702, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:40:00.702314: step 11035, loss 0.690531, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:40:01.016906: step 11040, loss 0.695501, acc 0.53125, f1 0.368622\n",
      "2017-11-26T15:40:01.336008: step 11045, loss 0.686507, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:40:01.665164: step 11050, loss 0.687526, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:40:01.985244: step 11055, loss 0.692008, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:40:02.304915: step 11060, loss 0.686891, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:40:02.620353: step 11065, loss 0.685607, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:40:02.921786: step 11070, loss 0.697704, acc 0.530273, f1 0.367503\n",
      "2017-11-26T15:40:03.237151: step 11075, loss 0.685701, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:40:03.547052: step 11080, loss 0.700423, acc 0.518555, f1 0.354151\n",
      "2017-11-26T15:40:03.878296: step 11085, loss 0.685495, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:40:04.201918: step 11090, loss 0.686652, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:40:04.528700: step 11095, loss 0.687022, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:40:04.852131: step 11100, loss 0.691212, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:40:05.183607: step 11105, loss 0.688087, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:40:05.518528: step 11110, loss 0.690443, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:40:05.824267: step 11115, loss 0.690028, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:40:06.150041: step 11120, loss 0.696859, acc 0.52832, f1 0.365267\n",
      "2017-11-26T15:40:06.457872: step 11125, loss 0.686506, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:40:06.751941: step 11130, loss 0.685075, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:40:07.040598: step 11135, loss 0.691495, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:40:07.353599: step 11140, loss 0.690517, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:40:07.663361: step 11145, loss 0.692358, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:40:07.971775: step 11150, loss 0.692744, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:40:08.282535: step 11155, loss 0.69487, acc 0.537109, f1 0.375362\n",
      "2017-11-26T15:40:08.604411: step 11160, loss 0.691276, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:40:08.918278: step 11165, loss 0.690086, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:40:09.257625: step 11170, loss 0.685658, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:40:09.586862: step 11175, loss 0.684751, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:40:09.897955: step 11180, loss 0.685773, acc 0.575195, f1 0.420074\n",
      "2017-11-26T15:40:10.212882: step 11185, loss 0.681229, acc 0.586914, f1 0.434136\n",
      "2017-11-26T15:40:10.502276: step 11190, loss 0.686067, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:40:10.812691: step 11195, loss 0.691261, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:40:11.094982: step 11200, loss 0.689186, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:40:11.408273: step 11205, loss 0.695646, acc 0.536133, f1 0.374236\n",
      "2017-11-26T15:40:11.705150: step 11210, loss 0.691529, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:40:12.044019: step 11215, loss 0.688323, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:40:12.363502: step 11220, loss 0.687358, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:40:12.689601: step 11225, loss 0.69243, acc 0.538086, f1 0.376489\n",
      "2017-11-26T15:40:13.000632: step 11230, loss 0.693346, acc 0.536133, f1 0.374236\n",
      "2017-11-26T15:40:13.316290: step 11235, loss 0.69376, acc 0.53418, f1 0.371988\n",
      "2017-11-26T15:40:13.626391: step 11240, loss 0.687772, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:40:13.952530: step 11245, loss 0.682406, acc 0.582031, f1 0.42826\n",
      "2017-11-26T15:40:14.265040: step 11250, loss 0.689859, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:40:14.556397: step 11255, loss 0.692523, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:40:14.858641: step 11260, loss 0.690972, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:40:15.167728: step 11265, loss 0.683679, acc 0.589844, f1 0.437673\n",
      "2017-11-26T15:40:15.477900: step 11270, loss 0.688961, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:40:15.804326: step 11275, loss 0.694417, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:40:16.127137: step 11280, loss 0.682959, acc 0.598633, f1 0.448335\n",
      "2017-11-26T15:40:16.454681: step 11285, loss 0.690787, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:40:16.786996: step 11290, loss 0.694502, acc 0.530273, f1 0.367503\n",
      "2017-11-26T15:40:17.084333: step 11295, loss 0.689284, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:40:17.398254: step 11300, loss 0.689543, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:40:17.720876: step 11305, loss 0.684672, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:40:18.035120: step 11310, loss 0.686456, acc 0.566406, f1 0.40962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T15:40:18.361464: step 11315, loss 0.70008, acc 0.523438, f1 0.359696\n",
      "2017-11-26T15:40:18.706667: step 11320, loss 0.68983, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:40:19.038141: step 11325, loss 0.68916, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:40:19.358035: step 11330, loss 0.692888, acc 0.535156, f1 0.373111\n",
      "2017-11-26T15:40:19.692998: step 11335, loss 0.686777, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:40:20.013843: step 11340, loss 0.690269, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:40:20.346542: step 11345, loss 0.684804, acc 0.583984, f1 0.430607\n",
      "2017-11-26T15:40:20.674079: step 11350, loss 0.684939, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:40:21.007065: step 11355, loss 0.692192, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:40:21.351251: step 11360, loss 0.688215, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:40:21.668932: step 11365, loss 0.690365, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:40:21.985902: step 11370, loss 0.691366, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:40:22.296379: step 11375, loss 0.68951, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:40:22.617095: step 11380, loss 0.687548, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:40:22.945206: step 11385, loss 0.692625, acc 0.538086, f1 0.376489\n",
      "2017-11-26T15:40:23.258553: step 11390, loss 0.692193, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:40:23.578258: step 11395, loss 0.69571, acc 0.52832, f1 0.365267\n",
      "2017-11-26T15:40:23.879936: step 11400, loss 0.686161, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:40:24.179449: step 11405, loss 0.688858, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:40:24.514449: step 11410, loss 0.688286, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:40:24.829372: step 11415, loss 0.684262, acc 0.573242, f1 0.417744\n",
      "2017-11-26T15:40:25.143934: step 11420, loss 0.691387, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:40:25.453096: step 11425, loss 0.693187, acc 0.53125, f1 0.368622\n",
      "2017-11-26T15:40:25.773049: step 11430, loss 0.694219, acc 0.524414, f1 0.360808\n",
      "2017-11-26T15:40:26.080325: step 11435, loss 0.687694, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:40:26.401475: step 11440, loss 0.693118, acc 0.53418, f1 0.371988\n",
      "2017-11-26T15:40:26.710118: step 11445, loss 0.687521, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:40:27.009046: step 11450, loss 0.688579, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:40:27.322736: step 11455, loss 0.682273, acc 0.581055, f1 0.427088\n",
      "2017-11-26T15:40:27.597477: step 11460, loss 0.690366, acc 0.548828, f1 0.388955\n",
      "\n",
      "Evaluation:\n",
      "loss 0.688489, acc 0.55893, f1 0.400903\n",
      "\n",
      "Current epoch:  11\n",
      "2017-11-26T15:40:30.132033: step 11465, loss 0.68466, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:40:30.444056: step 11470, loss 0.695469, acc 0.532227, f1 0.369743\n",
      "2017-11-26T15:40:30.739889: step 11475, loss 0.687316, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:40:31.065916: step 11480, loss 0.690761, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:40:31.370652: step 11485, loss 0.688302, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:40:31.686215: step 11490, loss 0.684153, acc 0.576172, f1 0.421241\n",
      "2017-11-26T15:40:32.014555: step 11495, loss 0.688085, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:40:32.312074: step 11500, loss 0.686316, acc 0.575195, f1 0.420074\n",
      "2017-11-26T15:40:32.616667: step 11505, loss 0.688696, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:40:32.918167: step 11510, loss 0.688743, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:40:33.246541: step 11515, loss 0.679171, acc 0.59375, f1 0.442402\n",
      "2017-11-26T15:40:33.560652: step 11520, loss 0.692213, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:40:33.862549: step 11525, loss 0.68569, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:40:34.161149: step 11530, loss 0.687845, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:40:34.482432: step 11535, loss 0.692119, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:40:34.804866: step 11540, loss 0.682985, acc 0.579102, f1 0.424746\n",
      "2017-11-26T15:40:35.102507: step 11545, loss 0.686758, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:40:35.428031: step 11550, loss 0.68998, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:40:35.741499: step 11555, loss 0.690074, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:40:36.066408: step 11560, loss 0.692245, acc 0.538086, f1 0.376489\n",
      "2017-11-26T15:40:36.380527: step 11565, loss 0.688675, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:40:36.710996: step 11570, loss 0.691371, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:40:37.026967: step 11575, loss 0.689641, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:40:37.351106: step 11580, loss 0.686414, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:40:37.690407: step 11585, loss 0.684539, acc 0.574219, f1 0.418909\n",
      "2017-11-26T15:40:38.020788: step 11590, loss 0.682472, acc 0.583984, f1 0.430607\n",
      "2017-11-26T15:40:38.360416: step 11595, loss 0.689564, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:40:38.696427: step 11600, loss 0.69175, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:40:39.021636: step 11605, loss 0.685931, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:40:39.354104: step 11610, loss 0.689095, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:40:39.676190: step 11615, loss 0.694177, acc 0.535156, f1 0.373111\n",
      "2017-11-26T15:40:40.007092: step 11620, loss 0.696115, acc 0.52832, f1 0.365267\n",
      "2017-11-26T15:40:40.337300: step 11625, loss 0.690658, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:40:40.660311: step 11630, loss 0.680503, acc 0.584961, f1 0.431783\n",
      "2017-11-26T15:40:40.998273: step 11635, loss 0.684333, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:40:41.331859: step 11640, loss 0.687223, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:40:41.656590: step 11645, loss 0.687945, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:40:41.991038: step 11650, loss 0.68901, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:40:42.308948: step 11655, loss 0.68831, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:40:42.623652: step 11660, loss 0.693281, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:40:42.934727: step 11665, loss 0.685892, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:40:43.248108: step 11670, loss 0.688393, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:40:43.567144: step 11675, loss 0.688796, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:40:43.878053: step 11680, loss 0.697094, acc 0.511719, f1 0.346435\n",
      "2017-11-26T15:40:44.204725: step 11685, loss 0.689208, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:40:44.544098: step 11690, loss 0.681155, acc 0.605469, f1 0.45668\n",
      "2017-11-26T15:40:44.849372: step 11695, loss 0.689581, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:40:45.165868: step 11700, loss 0.688909, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:40:45.479280: step 11705, loss 0.689593, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:40:45.805400: step 11710, loss 0.689034, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:40:46.111612: step 11715, loss 0.681035, acc 0.583984, f1 0.430607\n",
      "2017-11-26T15:40:46.433736: step 11720, loss 0.690002, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:40:46.758206: step 11725, loss 0.689821, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:40:47.068308: step 11730, loss 0.693208, acc 0.532227, f1 0.369743\n",
      "2017-11-26T15:40:47.370847: step 11735, loss 0.69305, acc 0.536133, f1 0.374236\n",
      "2017-11-26T15:40:47.710218: step 11740, loss 0.686266, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:40:48.030396: step 11745, loss 0.687496, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:40:48.363950: step 11750, loss 0.694548, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:40:48.686831: step 11755, loss 0.688436, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:40:49.025604: step 11760, loss 0.690525, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:40:49.353213: step 11765, loss 0.692833, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:40:49.654104: step 11770, loss 0.693188, acc 0.535156, f1 0.373111\n",
      "2017-11-26T15:40:49.967343: step 11775, loss 0.689939, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:40:50.298766: step 11780, loss 0.689598, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:40:50.641136: step 11785, loss 0.683252, acc 0.587891, f1 0.435314\n",
      "2017-11-26T15:40:50.964795: step 11790, loss 0.685642, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:40:51.265616: step 11795, loss 0.684801, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:40:51.584046: step 11800, loss 0.683611, acc 0.577148, f1 0.422408\n",
      "2017-11-26T15:40:51.907390: step 11805, loss 0.68383, acc 0.576172, f1 0.421241\n",
      "2017-11-26T15:40:52.200408: step 11810, loss 0.695194, acc 0.52832, f1 0.365267\n",
      "2017-11-26T15:40:52.499712: step 11815, loss 0.694693, acc 0.543945, f1 0.383273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T15:40:52.792665: step 11820, loss 0.687461, acc 0.576172, f1 0.421241\n",
      "2017-11-26T15:40:53.123962: step 11825, loss 0.689067, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:40:53.463757: step 11830, loss 0.687937, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:40:53.756863: step 11835, loss 0.680502, acc 0.597656, f1 0.447146\n",
      "2017-11-26T15:40:54.082530: step 11840, loss 0.684727, acc 0.573242, f1 0.417744\n",
      "2017-11-26T15:40:54.402849: step 11845, loss 0.696739, acc 0.527344, f1 0.36415\n",
      "2017-11-26T15:40:54.735274: step 11850, loss 0.691439, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:40:55.066016: step 11855, loss 0.686941, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:40:55.363754: step 11860, loss 0.691617, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:40:55.643084: step 11865, loss 0.686644, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:40:55.939940: step 11870, loss 0.69012, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:40:56.248548: step 11875, loss 0.690683, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:40:56.544582: step 11880, loss 0.692271, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:40:56.862597: step 11885, loss 0.684439, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:40:57.181888: step 11890, loss 0.69169, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:40:57.501083: step 11895, loss 0.696914, acc 0.523438, f1 0.359696\n",
      "2017-11-26T15:40:57.808860: step 11900, loss 0.685433, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:40:58.118586: step 11905, loss 0.682577, acc 0.579102, f1 0.424746\n",
      "2017-11-26T15:40:58.439484: step 11910, loss 0.682575, acc 0.585938, f1 0.432959\n",
      "2017-11-26T15:40:58.760675: step 11915, loss 0.688416, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:40:59.075202: step 11920, loss 0.684059, acc 0.575195, f1 0.420074\n",
      "2017-11-26T15:40:59.381452: step 11925, loss 0.689409, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:40:59.851681: step 11930, loss 0.687754, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:41:00.176053: step 11935, loss 0.689411, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:41:00.505358: step 11940, loss 0.691149, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:41:00.808297: step 11945, loss 0.687564, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:41:01.110338: step 11950, loss 0.684352, acc 0.579102, f1 0.424746\n",
      "2017-11-26T15:41:01.427879: step 11955, loss 0.684191, acc 0.573242, f1 0.417744\n",
      "2017-11-26T15:41:01.744777: step 11960, loss 0.692064, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:41:02.060397: step 11965, loss 0.692689, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:41:02.372370: step 11970, loss 0.683862, acc 0.578125, f1 0.423577\n",
      "2017-11-26T15:41:02.715164: step 11975, loss 0.693502, acc 0.538086, f1 0.376489\n",
      "2017-11-26T15:41:03.032350: step 11980, loss 0.694263, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:41:03.365043: step 11985, loss 0.69307, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:41:03.697141: step 11990, loss 0.681732, acc 0.583008, f1 0.429433\n",
      "2017-11-26T15:41:04.009148: step 11995, loss 0.682756, acc 0.580078, f1 0.425916\n",
      "2017-11-26T15:41:04.329445: step 12000, loss 0.691652, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:41:04.646582: step 12005, loss 0.692501, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:41:04.963904: step 12010, loss 0.688091, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:41:05.284320: step 12015, loss 0.685736, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:41:05.594289: step 12020, loss 0.690336, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:41:05.927715: step 12025, loss 0.688722, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:41:06.230196: step 12030, loss 0.695189, acc 0.52832, f1 0.365267\n",
      "2017-11-26T15:41:06.545497: step 12035, loss 0.686239, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:41:06.859295: step 12040, loss 0.695217, acc 0.524414, f1 0.360808\n",
      "2017-11-26T15:41:07.165177: step 12045, loss 0.687299, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:41:07.476049: step 12050, loss 0.690101, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:41:07.783624: step 12055, loss 0.684387, acc 0.573242, f1 0.417744\n",
      "2017-11-26T15:41:08.113470: step 12060, loss 0.69196, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:41:08.419358: step 12065, loss 0.689566, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:41:08.726553: step 12070, loss 0.685555, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:41:09.049508: step 12075, loss 0.681493, acc 0.585938, f1 0.432959\n",
      "2017-11-26T15:41:09.378593: step 12080, loss 0.687621, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:41:09.710077: step 12085, loss 0.688884, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:41:10.045209: step 12090, loss 0.692226, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:41:10.366062: step 12095, loss 0.693991, acc 0.525391, f1 0.361921\n",
      "2017-11-26T15:41:10.697933: step 12100, loss 0.683207, acc 0.589844, f1 0.437673\n",
      "2017-11-26T15:41:11.027874: step 12105, loss 0.686221, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:41:11.351625: step 12110, loss 0.689682, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:41:11.686017: step 12115, loss 0.688748, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:41:12.036066: step 12120, loss 0.693501, acc 0.536133, f1 0.374236\n",
      "2017-11-26T15:41:12.361447: step 12125, loss 0.686477, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:41:12.681261: step 12130, loss 0.687333, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:41:13.006209: step 12135, loss 0.694938, acc 0.52832, f1 0.365267\n",
      "2017-11-26T15:41:13.320384: step 12140, loss 0.68986, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:41:13.641146: step 12145, loss 0.689159, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:41:13.964090: step 12150, loss 0.688849, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:41:14.289854: step 12155, loss 0.687035, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:41:14.626917: step 12160, loss 0.685567, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:41:14.944645: step 12165, loss 0.6853, acc 0.585938, f1 0.432959\n",
      "2017-11-26T15:41:15.271297: step 12170, loss 0.685296, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:41:15.583492: step 12175, loss 0.688103, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:41:15.901744: step 12180, loss 0.694442, acc 0.529297, f1 0.366384\n",
      "2017-11-26T15:41:16.232389: step 12185, loss 0.684432, acc 0.575195, f1 0.420074\n",
      "2017-11-26T15:41:16.548654: step 12190, loss 0.695478, acc 0.53418, f1 0.371988\n",
      "2017-11-26T15:41:16.889291: step 12195, loss 0.687959, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:41:17.226498: step 12200, loss 0.682862, acc 0.582031, f1 0.42826\n",
      "2017-11-26T15:41:17.545680: step 12205, loss 0.687344, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:41:17.885820: step 12210, loss 0.692622, acc 0.535156, f1 0.373111\n",
      "2017-11-26T15:41:18.206223: step 12215, loss 0.686682, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:41:18.533417: step 12220, loss 0.686747, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:41:18.837016: step 12225, loss 0.682144, acc 0.580078, f1 0.425916\n",
      "2017-11-26T15:41:19.157172: step 12230, loss 0.69212, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:41:19.479961: step 12235, loss 0.6824, acc 0.582031, f1 0.42826\n",
      "2017-11-26T15:41:19.788891: step 12240, loss 0.68812, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:41:20.125154: step 12245, loss 0.686091, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:41:20.446638: step 12250, loss 0.690227, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:41:20.794004: step 12255, loss 0.688086, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:41:21.124434: step 12260, loss 0.686759, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:41:21.449958: step 12265, loss 0.688844, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:41:21.786158: step 12270, loss 0.686713, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:41:22.125368: step 12275, loss 0.683571, acc 0.575195, f1 0.420074\n",
      "2017-11-26T15:41:22.427770: step 12280, loss 0.687882, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:41:22.722196: step 12285, loss 0.681342, acc 0.584961, f1 0.431783\n",
      "2017-11-26T15:41:23.043291: step 12290, loss 0.684425, acc 0.583984, f1 0.430607\n",
      "2017-11-26T15:41:23.364716: step 12295, loss 0.685332, acc 0.573242, f1 0.417744\n",
      "2017-11-26T15:41:23.703374: step 12300, loss 0.690915, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:41:24.005599: step 12305, loss 0.686536, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:41:24.328125: step 12310, loss 0.69733, acc 0.513672, f1 0.348634\n",
      "2017-11-26T15:41:24.664150: step 12315, loss 0.696277, acc 0.529297, f1 0.366384\n",
      "2017-11-26T15:41:24.976445: step 12320, loss 0.683418, acc 0.577148, f1 0.422408\n",
      "2017-11-26T15:41:25.297560: step 12325, loss 0.691991, acc 0.540039, f1 0.378746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T15:41:25.618503: step 12330, loss 0.686243, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:41:25.950651: step 12335, loss 0.685344, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:41:26.284667: step 12340, loss 0.679902, acc 0.587891, f1 0.435314\n",
      "2017-11-26T15:41:26.625900: step 12345, loss 0.69672, acc 0.532227, f1 0.369743\n",
      "2017-11-26T15:41:26.958119: step 12350, loss 0.688129, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:41:27.262242: step 12355, loss 0.692102, acc 0.538086, f1 0.376489\n",
      "2017-11-26T15:41:27.587524: step 12360, loss 0.683072, acc 0.581055, f1 0.427088\n",
      "2017-11-26T15:41:27.902359: step 12365, loss 0.691169, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:41:28.220219: step 12370, loss 0.690297, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:41:28.541699: step 12375, loss 0.683691, acc 0.576172, f1 0.421241\n",
      "2017-11-26T15:41:28.877036: step 12380, loss 0.685806, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:41:29.191005: step 12385, loss 0.687963, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:41:29.504225: step 12390, loss 0.690985, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:41:29.814507: step 12395, loss 0.689348, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:41:30.138510: step 12400, loss 0.686413, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:41:30.455334: step 12405, loss 0.687092, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:41:30.763688: step 12410, loss 0.690366, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:41:31.081751: step 12415, loss 0.690749, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:41:31.399141: step 12420, loss 0.68451, acc 0.575195, f1 0.420074\n",
      "2017-11-26T15:41:31.719896: step 12425, loss 0.683798, acc 0.587891, f1 0.435314\n",
      "2017-11-26T15:41:32.046104: step 12430, loss 0.686947, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:41:32.373469: step 12435, loss 0.687704, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:41:32.677098: step 12440, loss 0.689442, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:41:32.990987: step 12445, loss 0.682249, acc 0.583984, f1 0.430607\n",
      "2017-11-26T15:41:33.308000: step 12450, loss 0.698143, acc 0.523438, f1 0.359696\n",
      "2017-11-26T15:41:33.629863: step 12455, loss 0.688761, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:41:33.931384: step 12460, loss 0.687895, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:41:34.254308: step 12465, loss 0.695966, acc 0.537109, f1 0.375362\n",
      "2017-11-26T15:41:34.570402: step 12470, loss 0.686564, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:41:34.898855: step 12475, loss 0.685127, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:41:35.204246: step 12480, loss 0.687289, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:41:35.528370: step 12485, loss 0.682284, acc 0.581055, f1 0.427088\n",
      "2017-11-26T15:41:35.857254: step 12490, loss 0.693612, acc 0.537109, f1 0.375362\n",
      "2017-11-26T15:41:36.187419: step 12495, loss 0.685205, acc 0.582031, f1 0.42826\n",
      "2017-11-26T15:41:36.496020: step 12500, loss 0.685388, acc 0.568359, f1 0.411937\n",
      "\n",
      "Evaluation:\n",
      "loss 0.688358, acc 0.55893, f1 0.400903\n",
      "\n",
      "Current epoch:  12\n",
      "2017-11-26T15:41:39.165420: step 12505, loss 0.68778, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:41:39.482134: step 12510, loss 0.693008, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:41:39.794080: step 12515, loss 0.687859, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:41:40.121964: step 12520, loss 0.688413, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:41:40.456534: step 12525, loss 0.692304, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:41:40.790420: step 12530, loss 0.687947, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:41:41.131198: step 12535, loss 0.685254, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:41:41.460299: step 12540, loss 0.697607, acc 0.52832, f1 0.365267\n",
      "2017-11-26T15:41:41.784972: step 12545, loss 0.687809, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:41:42.109177: step 12550, loss 0.687416, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:41:42.406364: step 12555, loss 0.687502, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:41:42.730085: step 12560, loss 0.689616, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:41:43.054446: step 12565, loss 0.688772, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:41:43.380945: step 12570, loss 0.688888, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:41:43.688006: step 12575, loss 0.691946, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:41:44.007099: step 12580, loss 0.690411, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:41:44.330921: step 12585, loss 0.686322, acc 0.573242, f1 0.417744\n",
      "2017-11-26T15:41:44.647237: step 12590, loss 0.688746, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:41:44.957991: step 12595, loss 0.6854, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:41:45.291265: step 12600, loss 0.686051, acc 0.575195, f1 0.420074\n",
      "2017-11-26T15:41:45.626761: step 12605, loss 0.684959, acc 0.573242, f1 0.417744\n",
      "2017-11-26T15:41:45.955630: step 12610, loss 0.689028, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:41:46.284020: step 12615, loss 0.688908, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:41:46.580046: step 12620, loss 0.690628, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:41:46.902492: step 12625, loss 0.683586, acc 0.576172, f1 0.421241\n",
      "2017-11-26T15:41:47.243325: step 12630, loss 0.696544, acc 0.523438, f1 0.359696\n",
      "2017-11-26T15:41:47.557791: step 12635, loss 0.697388, acc 0.533203, f1 0.370865\n",
      "2017-11-26T15:41:47.862540: step 12640, loss 0.688992, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:41:48.180265: step 12645, loss 0.690524, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:41:48.508524: step 12650, loss 0.686172, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:41:48.831621: step 12655, loss 0.678723, acc 0.597656, f1 0.447146\n",
      "2017-11-26T15:41:49.142360: step 12660, loss 0.689677, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:41:49.465475: step 12665, loss 0.689388, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:41:49.781642: step 12670, loss 0.693266, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:41:50.102029: step 12675, loss 0.684271, acc 0.580078, f1 0.425916\n",
      "2017-11-26T15:41:50.430410: step 12680, loss 0.687189, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:41:50.758903: step 12685, loss 0.692928, acc 0.538086, f1 0.376489\n",
      "2017-11-26T15:41:51.071213: step 12690, loss 0.691279, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:41:51.381997: step 12695, loss 0.69022, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:41:51.696720: step 12700, loss 0.69253, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:41:52.019968: step 12705, loss 0.689826, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:41:52.335560: step 12710, loss 0.694887, acc 0.538086, f1 0.376489\n",
      "2017-11-26T15:41:52.645364: step 12715, loss 0.686773, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:41:52.954695: step 12720, loss 0.683869, acc 0.576172, f1 0.421241\n",
      "2017-11-26T15:41:53.304124: step 12725, loss 0.687888, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:41:53.617074: step 12730, loss 0.682598, acc 0.582031, f1 0.42826\n",
      "2017-11-26T15:41:53.925125: step 12735, loss 0.690161, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:41:54.236429: step 12740, loss 0.685644, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:41:54.550599: step 12745, loss 0.693275, acc 0.53125, f1 0.368622\n",
      "2017-11-26T15:41:54.869879: step 12750, loss 0.68826, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:41:55.188735: step 12755, loss 0.689656, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:41:55.515286: step 12760, loss 0.692109, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:41:55.842853: step 12765, loss 0.68488, acc 0.575195, f1 0.420074\n",
      "2017-11-26T15:41:56.184719: step 12770, loss 0.690363, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:41:56.511172: step 12775, loss 0.683301, acc 0.578125, f1 0.423577\n",
      "2017-11-26T15:41:56.852594: step 12780, loss 0.692679, acc 0.53418, f1 0.371988\n",
      "2017-11-26T15:41:57.196600: step 12785, loss 0.688896, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:41:57.525257: step 12790, loss 0.696089, acc 0.524414, f1 0.360808\n",
      "2017-11-26T15:41:57.861750: step 12795, loss 0.68863, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:41:58.174801: step 12800, loss 0.698349, acc 0.520508, f1 0.356366\n",
      "2017-11-26T15:41:58.489385: step 12805, loss 0.682741, acc 0.587891, f1 0.435314\n",
      "2017-11-26T15:41:58.826389: step 12810, loss 0.681965, acc 0.585938, f1 0.432959\n",
      "2017-11-26T15:41:59.155210: step 12815, loss 0.693836, acc 0.525391, f1 0.361921\n",
      "2017-11-26T15:41:59.471116: step 12820, loss 0.691416, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:41:59.791097: step 12825, loss 0.699975, acc 0.521484, f1 0.357475\n",
      "2017-11-26T15:42:00.115305: step 12830, loss 0.687768, acc 0.55957, f1 0.401545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T15:42:00.432812: step 12835, loss 0.688908, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:42:00.753039: step 12840, loss 0.692689, acc 0.537109, f1 0.375362\n",
      "2017-11-26T15:42:01.061108: step 12845, loss 0.685307, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:42:01.379894: step 12850, loss 0.689124, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:42:01.712303: step 12855, loss 0.688003, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:42:02.037771: step 12860, loss 0.687922, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:42:02.362088: step 12865, loss 0.689246, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:42:02.702194: step 12870, loss 0.686179, acc 0.583984, f1 0.430607\n",
      "2017-11-26T15:42:03.189874: step 12875, loss 0.69043, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:42:03.500976: step 12880, loss 0.691957, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:42:03.834026: step 12885, loss 0.689215, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:42:04.164607: step 12890, loss 0.691496, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:42:04.487443: step 12895, loss 0.691911, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:42:04.817961: step 12900, loss 0.684167, acc 0.581055, f1 0.427088\n",
      "2017-11-26T15:42:05.140737: step 12905, loss 0.68368, acc 0.576172, f1 0.421241\n",
      "2017-11-26T15:42:05.434681: step 12910, loss 0.683844, acc 0.580078, f1 0.425916\n",
      "2017-11-26T15:42:05.739224: step 12915, loss 0.69587, acc 0.533203, f1 0.370865\n",
      "2017-11-26T15:42:06.037912: step 12920, loss 0.682259, acc 0.582031, f1 0.42826\n",
      "2017-11-26T15:42:06.348495: step 12925, loss 0.688559, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:42:06.661385: step 12930, loss 0.694, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:42:06.972006: step 12935, loss 0.683609, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:42:07.294691: step 12940, loss 0.687123, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:42:07.599930: step 12945, loss 0.685563, acc 0.573242, f1 0.417744\n",
      "2017-11-26T15:42:07.920149: step 12950, loss 0.687512, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:42:08.241043: step 12955, loss 0.689631, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:42:08.538464: step 12960, loss 0.688501, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:42:08.863705: step 12965, loss 0.695136, acc 0.522461, f1 0.358584\n",
      "2017-11-26T15:42:09.182920: step 12970, loss 0.688742, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:42:09.509688: step 12975, loss 0.692399, acc 0.536133, f1 0.374236\n",
      "2017-11-26T15:42:09.846103: step 12980, loss 0.68346, acc 0.581055, f1 0.427088\n",
      "2017-11-26T15:42:10.181103: step 12985, loss 0.69389, acc 0.535156, f1 0.373111\n",
      "2017-11-26T15:42:10.501904: step 12990, loss 0.690551, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:42:10.827496: step 12995, loss 0.689823, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:42:11.157959: step 13000, loss 0.687102, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:42:11.477666: step 13005, loss 0.683023, acc 0.583008, f1 0.429433\n",
      "2017-11-26T15:42:11.806003: step 13010, loss 0.687134, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:42:12.126759: step 13015, loss 0.687461, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:42:12.455034: step 13020, loss 0.690515, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:42:12.754769: step 13025, loss 0.686821, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:42:13.070970: step 13030, loss 0.690133, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:42:13.413298: step 13035, loss 0.688184, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:42:13.729586: step 13040, loss 0.684928, acc 0.578125, f1 0.423577\n",
      "2017-11-26T15:42:14.050716: step 13045, loss 0.692435, acc 0.53418, f1 0.371988\n",
      "2017-11-26T15:42:14.355327: step 13050, loss 0.685095, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:42:14.679679: step 13055, loss 0.691416, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:42:15.015079: step 13060, loss 0.692918, acc 0.536133, f1 0.374236\n",
      "2017-11-26T15:42:15.337737: step 13065, loss 0.689182, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:42:15.659808: step 13070, loss 0.691615, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:42:15.975740: step 13075, loss 0.688355, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:42:16.292378: step 13080, loss 0.693883, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:42:16.613206: step 13085, loss 0.688377, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:42:16.934946: step 13090, loss 0.687494, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:42:17.273624: step 13095, loss 0.687284, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:42:17.593466: step 13100, loss 0.691705, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:42:17.926194: step 13105, loss 0.68829, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:42:18.266097: step 13110, loss 0.687302, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:42:18.595204: step 13115, loss 0.69105, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:42:18.908194: step 13120, loss 0.68578, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:42:19.229482: step 13125, loss 0.6917, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:42:19.564610: step 13130, loss 0.685749, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:42:19.884493: step 13135, loss 0.689762, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:42:20.187377: step 13140, loss 0.693118, acc 0.538086, f1 0.376489\n",
      "2017-11-26T15:42:20.507389: step 13145, loss 0.685236, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:42:20.832426: step 13150, loss 0.687995, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:42:21.163301: step 13155, loss 0.680104, acc 0.595703, f1 0.444772\n",
      "2017-11-26T15:42:21.466615: step 13160, loss 0.691614, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:42:21.793160: step 13165, loss 0.691493, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:42:22.129142: step 13170, loss 0.690562, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:42:22.453107: step 13175, loss 0.682939, acc 0.584961, f1 0.431783\n",
      "2017-11-26T15:42:22.786242: step 13180, loss 0.686274, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:42:23.112217: step 13185, loss 0.689517, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:42:23.437009: step 13190, loss 0.681279, acc 0.585938, f1 0.432959\n",
      "2017-11-26T15:42:23.742775: step 13195, loss 0.686297, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:42:24.056220: step 13200, loss 0.691809, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:42:24.382661: step 13205, loss 0.687148, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:42:24.708871: step 13210, loss 0.689564, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:42:25.025239: step 13215, loss 0.688141, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:42:25.330780: step 13220, loss 0.697217, acc 0.517578, f1 0.353046\n",
      "2017-11-26T15:42:25.648048: step 13225, loss 0.690099, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:42:25.975016: step 13230, loss 0.686939, acc 0.575195, f1 0.420074\n",
      "2017-11-26T15:42:26.302912: step 13235, loss 0.690517, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:42:26.629206: step 13240, loss 0.696879, acc 0.529297, f1 0.366384\n",
      "2017-11-26T15:42:26.964274: step 13245, loss 0.691142, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:42:27.274865: step 13250, loss 0.688622, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:42:27.576351: step 13255, loss 0.687617, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:42:27.878921: step 13260, loss 0.687324, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:42:28.202440: step 13265, loss 0.687118, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:42:28.530340: step 13270, loss 0.686049, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:42:28.854374: step 13275, loss 0.692554, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:42:29.171367: step 13280, loss 0.685245, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:42:29.477328: step 13285, loss 0.682652, acc 0.581055, f1 0.427088\n",
      "2017-11-26T15:42:29.779797: step 13290, loss 0.683515, acc 0.580078, f1 0.425916\n",
      "2017-11-26T15:42:30.078986: step 13295, loss 0.6869, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:42:30.383417: step 13300, loss 0.689476, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:42:30.696015: step 13305, loss 0.691441, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:42:31.009602: step 13310, loss 0.682314, acc 0.583008, f1 0.429433\n",
      "2017-11-26T15:42:31.322478: step 13315, loss 0.687485, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:42:31.629358: step 13320, loss 0.683364, acc 0.580078, f1 0.425916\n",
      "2017-11-26T15:42:31.959322: step 13325, loss 0.693458, acc 0.53418, f1 0.371988\n",
      "2017-11-26T15:42:32.287617: step 13330, loss 0.685564, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:42:32.626095: step 13335, loss 0.683606, acc 0.574219, f1 0.418909\n",
      "2017-11-26T15:42:32.950968: step 13340, loss 0.685972, acc 0.566406, f1 0.40962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T15:42:33.271631: step 13345, loss 0.680435, acc 0.588867, f1 0.436493\n",
      "2017-11-26T15:42:33.566302: step 13350, loss 0.690051, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:42:33.863819: step 13355, loss 0.688795, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:42:34.176097: step 13360, loss 0.685993, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:42:34.517968: step 13365, loss 0.690507, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:42:34.843591: step 13370, loss 0.688955, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:42:35.178086: step 13375, loss 0.680573, acc 0.595703, f1 0.444772\n",
      "2017-11-26T15:42:35.491099: step 13380, loss 0.689422, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:42:35.824335: step 13385, loss 0.686433, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:42:36.137134: step 13390, loss 0.689658, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:42:36.460851: step 13395, loss 0.692669, acc 0.53418, f1 0.371988\n",
      "2017-11-26T15:42:36.785206: step 13400, loss 0.685565, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:42:37.110121: step 13405, loss 0.685026, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:42:37.445274: step 13410, loss 0.688732, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:42:37.788795: step 13415, loss 0.686992, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:42:38.100135: step 13420, loss 0.683937, acc 0.575195, f1 0.420074\n",
      "2017-11-26T15:42:38.422947: step 13425, loss 0.683747, acc 0.576172, f1 0.421241\n",
      "2017-11-26T15:42:38.743223: step 13430, loss 0.687186, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:42:39.064462: step 13435, loss 0.68879, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:42:39.395318: step 13440, loss 0.688274, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:42:39.721482: step 13445, loss 0.685282, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:42:40.056332: step 13450, loss 0.690565, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:42:40.381472: step 13455, loss 0.681772, acc 0.586914, f1 0.434136\n",
      "2017-11-26T15:42:40.690171: step 13460, loss 0.6846, acc 0.574219, f1 0.418909\n",
      "2017-11-26T15:42:41.001544: step 13465, loss 0.690789, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:42:41.311034: step 13470, loss 0.690042, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:42:41.626325: step 13475, loss 0.689343, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:42:41.942996: step 13480, loss 0.683957, acc 0.577148, f1 0.422408\n",
      "2017-11-26T15:42:42.261118: step 13485, loss 0.687994, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:42:42.573263: step 13490, loss 0.685956, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:42:42.887048: step 13495, loss 0.692092, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:42:43.210074: step 13500, loss 0.692561, acc 0.538086, f1 0.376489\n",
      "2017-11-26T15:42:43.535586: step 13505, loss 0.698622, acc 0.518555, f1 0.354151\n",
      "2017-11-26T15:42:43.852690: step 13510, loss 0.690478, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:42:44.166083: step 13515, loss 0.686311, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:42:44.488336: step 13520, loss 0.686108, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:42:44.799041: step 13525, loss 0.694295, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:42:45.114901: step 13530, loss 0.683741, acc 0.581055, f1 0.427088\n",
      "2017-11-26T15:42:45.440331: step 13535, loss 0.68824, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:42:45.771691: step 13540, loss 0.685985, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:42:46.107447: step 13545, loss 0.682969, acc 0.578125, f1 0.423577\n",
      "\n",
      "Evaluation:\n",
      "loss 0.68792, acc 0.55893, f1 0.400903\n",
      "\n",
      "Current epoch:  13\n",
      "2017-11-26T15:42:48.646388: step 13550, loss 0.681554, acc 0.584961, f1 0.431783\n",
      "2017-11-26T15:42:48.970498: step 13555, loss 0.693974, acc 0.53125, f1 0.368622\n",
      "2017-11-26T15:42:49.292548: step 13560, loss 0.69246, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:42:49.602219: step 13565, loss 0.69339, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:42:49.924329: step 13570, loss 0.689741, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:42:50.242401: step 13575, loss 0.694597, acc 0.532227, f1 0.369743\n",
      "2017-11-26T15:42:50.540565: step 13580, loss 0.685719, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:42:50.869968: step 13585, loss 0.687925, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:42:51.198393: step 13590, loss 0.70019, acc 0.509766, f1 0.34424\n",
      "2017-11-26T15:42:51.513685: step 13595, loss 0.687896, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:42:51.834032: step 13600, loss 0.691608, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:42:52.153071: step 13605, loss 0.684725, acc 0.579102, f1 0.424746\n",
      "2017-11-26T15:42:52.487062: step 13610, loss 0.688002, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:42:52.824810: step 13615, loss 0.688807, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:42:53.165677: step 13620, loss 0.69223, acc 0.538086, f1 0.376489\n",
      "2017-11-26T15:42:53.490749: step 13625, loss 0.689176, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:42:53.813132: step 13630, loss 0.694947, acc 0.537109, f1 0.375362\n",
      "2017-11-26T15:42:54.134017: step 13635, loss 0.693462, acc 0.535156, f1 0.373111\n",
      "2017-11-26T15:42:54.446054: step 13640, loss 0.695474, acc 0.537109, f1 0.375362\n",
      "2017-11-26T15:42:54.748912: step 13645, loss 0.690514, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:42:55.066886: step 13650, loss 0.687535, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:42:55.379296: step 13655, loss 0.688143, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:42:55.688897: step 13660, loss 0.690844, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:42:55.992703: step 13665, loss 0.691846, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:42:56.317418: step 13670, loss 0.683701, acc 0.579102, f1 0.424746\n",
      "2017-11-26T15:42:56.641675: step 13675, loss 0.686366, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:42:56.966438: step 13680, loss 0.688083, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:42:57.305833: step 13685, loss 0.684403, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:42:57.631658: step 13690, loss 0.685904, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:42:57.953876: step 13695, loss 0.690708, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:42:58.294842: step 13700, loss 0.685447, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:42:58.623424: step 13705, loss 0.686587, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:42:58.950303: step 13710, loss 0.697779, acc 0.524414, f1 0.360808\n",
      "2017-11-26T15:42:59.277831: step 13715, loss 0.681693, acc 0.585938, f1 0.432959\n",
      "2017-11-26T15:42:59.600126: step 13720, loss 0.68878, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:42:59.939615: step 13725, loss 0.687328, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:43:00.263347: step 13730, loss 0.691363, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:43:00.581102: step 13735, loss 0.68087, acc 0.589844, f1 0.437673\n",
      "2017-11-26T15:43:00.913924: step 13740, loss 0.688919, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:43:01.251232: step 13745, loss 0.691187, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:43:01.587262: step 13750, loss 0.691237, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:43:01.924679: step 13755, loss 0.690888, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:43:02.250842: step 13760, loss 0.690757, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:43:02.539824: step 13765, loss 0.696592, acc 0.529297, f1 0.366384\n",
      "2017-11-26T15:43:02.841513: step 13770, loss 0.67791, acc 0.604492, f1 0.455485\n",
      "2017-11-26T15:43:03.172548: step 13775, loss 0.684633, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:43:03.495901: step 13780, loss 0.684916, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:43:03.814241: step 13785, loss 0.683666, acc 0.580078, f1 0.425916\n",
      "2017-11-26T15:43:04.150048: step 13790, loss 0.689659, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:43:04.481310: step 13795, loss 0.692398, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:43:04.794467: step 13800, loss 0.684786, acc 0.574219, f1 0.418909\n",
      "2017-11-26T15:43:05.115639: step 13805, loss 0.687584, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:43:05.431208: step 13810, loss 0.696084, acc 0.518555, f1 0.354151\n",
      "2017-11-26T15:43:05.905661: step 13815, loss 0.686078, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:43:06.227959: step 13820, loss 0.688489, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:43:06.543252: step 13825, loss 0.684884, acc 0.573242, f1 0.417744\n",
      "2017-11-26T15:43:06.854779: step 13830, loss 0.692442, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:43:07.158094: step 13835, loss 0.694097, acc 0.533203, f1 0.370865\n",
      "2017-11-26T15:43:07.480267: step 13840, loss 0.691696, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:43:07.786954: step 13845, loss 0.688814, acc 0.553711, f1 0.394663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T15:43:08.101831: step 13850, loss 0.68629, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:43:08.416649: step 13855, loss 0.684291, acc 0.573242, f1 0.417744\n",
      "2017-11-26T15:43:08.724213: step 13860, loss 0.694788, acc 0.533203, f1 0.370865\n",
      "2017-11-26T15:43:09.053436: step 13865, loss 0.692434, acc 0.533203, f1 0.370865\n",
      "2017-11-26T15:43:09.363546: step 13870, loss 0.698526, acc 0.516602, f1 0.351941\n",
      "2017-11-26T15:43:09.660908: step 13875, loss 0.686522, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:43:09.956563: step 13880, loss 0.682294, acc 0.580078, f1 0.425916\n",
      "2017-11-26T15:43:10.256298: step 13885, loss 0.681074, acc 0.585938, f1 0.432959\n",
      "2017-11-26T15:43:10.588086: step 13890, loss 0.691732, acc 0.538086, f1 0.376489\n",
      "2017-11-26T15:43:10.899612: step 13895, loss 0.688503, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:43:11.205251: step 13900, loss 0.689573, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:43:11.512241: step 13905, loss 0.686542, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:43:11.830813: step 13910, loss 0.685827, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:43:12.142010: step 13915, loss 0.689519, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:43:12.463715: step 13920, loss 0.685292, acc 0.578125, f1 0.423577\n",
      "2017-11-26T15:43:12.797340: step 13925, loss 0.688107, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:43:13.108141: step 13930, loss 0.684263, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:43:13.450705: step 13935, loss 0.690614, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:43:13.761249: step 13940, loss 0.690943, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:43:14.088762: step 13945, loss 0.689919, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:43:14.409304: step 13950, loss 0.685548, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:43:14.746144: step 13955, loss 0.692007, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:43:15.045016: step 13960, loss 0.691583, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:43:15.350470: step 13965, loss 0.684143, acc 0.579102, f1 0.424746\n",
      "2017-11-26T15:43:15.678254: step 13970, loss 0.691485, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:43:15.981929: step 13975, loss 0.689575, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:43:16.295943: step 13980, loss 0.693721, acc 0.53418, f1 0.371988\n",
      "2017-11-26T15:43:16.615366: step 13985, loss 0.687135, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:43:16.945606: step 13990, loss 0.69129, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:43:17.269201: step 13995, loss 0.691305, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:43:17.594183: step 14000, loss 0.691297, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:43:17.903030: step 14005, loss 0.6906, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:43:18.232454: step 14010, loss 0.69629, acc 0.53125, f1 0.368622\n",
      "2017-11-26T15:43:18.562557: step 14015, loss 0.688767, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:43:18.866794: step 14020, loss 0.691918, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:43:19.184992: step 14025, loss 0.691168, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:43:19.516692: step 14030, loss 0.689177, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:43:19.836542: step 14035, loss 0.688147, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:43:20.156536: step 14040, loss 0.686831, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:43:20.459665: step 14045, loss 0.688183, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:43:20.773565: step 14050, loss 0.686221, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:43:21.124701: step 14055, loss 0.686985, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:43:21.449005: step 14060, loss 0.686651, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:43:21.784167: step 14065, loss 0.687238, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:43:22.109450: step 14070, loss 0.686044, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:43:22.428391: step 14075, loss 0.685867, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:43:22.758011: step 14080, loss 0.692283, acc 0.536133, f1 0.374236\n",
      "2017-11-26T15:43:23.070513: step 14085, loss 0.686357, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:43:23.375388: step 14090, loss 0.689657, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:43:23.703413: step 14095, loss 0.691025, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:43:24.022609: step 14100, loss 0.689154, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:43:24.347530: step 14105, loss 0.685621, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:43:24.672559: step 14110, loss 0.686558, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:43:24.982565: step 14115, loss 0.689952, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:43:25.307509: step 14120, loss 0.692181, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:43:25.642212: step 14125, loss 0.687636, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:43:25.969353: step 14130, loss 0.686966, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:43:26.283369: step 14135, loss 0.687385, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:43:26.608956: step 14140, loss 0.689116, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:43:26.920602: step 14145, loss 0.689324, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:43:27.236671: step 14150, loss 0.682339, acc 0.582031, f1 0.42826\n",
      "2017-11-26T15:43:27.561994: step 14155, loss 0.69318, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:43:27.881329: step 14160, loss 0.693187, acc 0.537109, f1 0.375362\n",
      "2017-11-26T15:43:28.208920: step 14165, loss 0.682755, acc 0.584961, f1 0.431783\n",
      "2017-11-26T15:43:28.539552: step 14170, loss 0.684057, acc 0.579102, f1 0.424746\n",
      "2017-11-26T15:43:28.864953: step 14175, loss 0.689349, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:43:29.195504: step 14180, loss 0.691353, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:43:29.522893: step 14185, loss 0.684742, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:43:29.836795: step 14190, loss 0.69041, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:43:30.160949: step 14195, loss 0.687262, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:43:30.494958: step 14200, loss 0.685331, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:43:30.820020: step 14205, loss 0.689803, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:43:31.143624: step 14210, loss 0.689538, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:43:31.475141: step 14215, loss 0.685562, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:43:31.798952: step 14220, loss 0.683888, acc 0.575195, f1 0.420074\n",
      "2017-11-26T15:43:32.106503: step 14225, loss 0.671124, acc 0.617188, f1 0.47109\n",
      "2017-11-26T15:43:32.422000: step 14230, loss 0.687238, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:43:32.762038: step 14235, loss 0.686838, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:43:33.082289: step 14240, loss 0.689192, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:43:33.416029: step 14245, loss 0.687859, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:43:33.738119: step 14250, loss 0.687979, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:43:34.059262: step 14255, loss 0.696863, acc 0.532227, f1 0.369743\n",
      "2017-11-26T15:43:34.394915: step 14260, loss 0.690535, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:43:34.706608: step 14265, loss 0.686614, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:43:35.023136: step 14270, loss 0.686823, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:43:35.353303: step 14275, loss 0.696428, acc 0.514648, f1 0.349735\n",
      "2017-11-26T15:43:35.684037: step 14280, loss 0.69037, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:43:35.981630: step 14285, loss 0.694296, acc 0.535156, f1 0.373111\n",
      "2017-11-26T15:43:36.287317: step 14290, loss 0.691785, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:43:36.614470: step 14295, loss 0.689973, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:43:36.949953: step 14300, loss 0.690695, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:43:37.272893: step 14305, loss 0.686148, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:43:37.591052: step 14310, loss 0.68524, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:43:37.911162: step 14315, loss 0.687151, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:43:38.229984: step 14320, loss 0.683338, acc 0.573242, f1 0.417744\n",
      "2017-11-26T15:43:38.559587: step 14325, loss 0.689586, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:43:38.885388: step 14330, loss 0.68669, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:43:39.215943: step 14335, loss 0.686653, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:43:39.534886: step 14340, loss 0.687944, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:43:39.856323: step 14345, loss 0.689476, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:43:40.193189: step 14350, loss 0.688331, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:43:40.500725: step 14355, loss 0.686548, acc 0.564453, f1 0.407308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T15:43:40.821591: step 14360, loss 0.685973, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:43:41.142954: step 14365, loss 0.688046, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:43:41.441816: step 14370, loss 0.687036, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:43:41.757241: step 14375, loss 0.691037, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:43:42.076623: step 14380, loss 0.694162, acc 0.533203, f1 0.370865\n",
      "2017-11-26T15:43:42.373667: step 14385, loss 0.687667, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:43:42.695310: step 14390, loss 0.692992, acc 0.537109, f1 0.375362\n",
      "2017-11-26T15:43:43.012079: step 14395, loss 0.695502, acc 0.523438, f1 0.359696\n",
      "2017-11-26T15:43:43.314529: step 14400, loss 0.689308, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:43:43.636688: step 14405, loss 0.687353, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:43:43.948787: step 14410, loss 0.689609, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:43:44.269293: step 14415, loss 0.689131, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:43:44.594124: step 14420, loss 0.686254, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:43:44.917530: step 14425, loss 0.685645, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:43:45.236935: step 14430, loss 0.689117, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:43:45.552597: step 14435, loss 0.686024, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:43:45.882181: step 14440, loss 0.684419, acc 0.576172, f1 0.421241\n",
      "2017-11-26T15:43:46.205147: step 14445, loss 0.686131, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:43:46.545027: step 14450, loss 0.693546, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:43:46.872322: step 14455, loss 0.684627, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:43:47.188765: step 14460, loss 0.688651, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:43:47.494826: step 14465, loss 0.691151, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:43:47.797688: step 14470, loss 0.682149, acc 0.583008, f1 0.429433\n",
      "2017-11-26T15:43:48.104152: step 14475, loss 0.689799, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:43:48.419994: step 14480, loss 0.685728, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:43:48.737332: step 14485, loss 0.694351, acc 0.532227, f1 0.369743\n",
      "2017-11-26T15:43:49.047410: step 14490, loss 0.690631, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:43:49.372085: step 14495, loss 0.690665, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:43:49.702997: step 14500, loss 0.685427, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:43:50.030472: step 14505, loss 0.688687, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:43:50.360168: step 14510, loss 0.692909, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:43:50.688832: step 14515, loss 0.694825, acc 0.53125, f1 0.368622\n",
      "2017-11-26T15:43:51.010508: step 14520, loss 0.688516, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:43:51.339544: step 14525, loss 0.691125, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:43:51.648582: step 14530, loss 0.69124, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:43:51.974237: step 14535, loss 0.68179, acc 0.583008, f1 0.429433\n",
      "2017-11-26T15:43:52.303981: step 14540, loss 0.682393, acc 0.580078, f1 0.425916\n",
      "2017-11-26T15:43:52.643396: step 14545, loss 0.686994, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:43:52.973985: step 14550, loss 0.691435, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:43:53.294141: step 14555, loss 0.690857, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:43:53.627156: step 14560, loss 0.688736, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:43:53.943756: step 14565, loss 0.688911, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:43:54.268140: step 14570, loss 0.685682, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:43:54.581633: step 14575, loss 0.692164, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:43:54.890025: step 14580, loss 0.692502, acc 0.537109, f1 0.375362\n",
      "2017-11-26T15:43:55.208335: step 14585, loss 0.684454, acc 0.576172, f1 0.421241\n",
      "\n",
      "Evaluation:\n",
      "loss 0.687735, acc 0.55893, f1 0.400903\n",
      "\n",
      "Current epoch:  14\n",
      "2017-11-26T15:43:57.858400: step 14590, loss 0.6909, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:43:58.181919: step 14595, loss 0.689673, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:43:58.489382: step 14600, loss 0.68415, acc 0.574219, f1 0.418909\n",
      "2017-11-26T15:43:58.820575: step 14605, loss 0.68749, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:43:59.098566: step 14610, loss 0.680022, acc 0.603516, f1 0.454291\n",
      "2017-11-26T15:43:59.395149: step 14615, loss 0.69702, acc 0.525391, f1 0.361921\n",
      "2017-11-26T15:43:59.702003: step 14620, loss 0.686988, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:44:00.016644: step 14625, loss 0.693768, acc 0.53418, f1 0.371988\n",
      "2017-11-26T15:44:00.325001: step 14630, loss 0.692827, acc 0.535156, f1 0.373111\n",
      "2017-11-26T15:44:00.627685: step 14635, loss 0.687143, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:44:00.960779: step 14640, loss 0.682419, acc 0.582031, f1 0.42826\n",
      "2017-11-26T15:44:01.264780: step 14645, loss 0.685248, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:44:01.579003: step 14650, loss 0.690371, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:44:01.892723: step 14655, loss 0.687548, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:44:02.190099: step 14660, loss 0.688844, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:44:02.508734: step 14665, loss 0.685843, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:44:02.833505: step 14670, loss 0.695109, acc 0.529297, f1 0.366384\n",
      "2017-11-26T15:44:03.151836: step 14675, loss 0.681701, acc 0.581055, f1 0.427088\n",
      "2017-11-26T15:44:03.470480: step 14680, loss 0.688556, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:44:03.792615: step 14685, loss 0.687318, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:44:04.081303: step 14690, loss 0.690483, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:44:04.405399: step 14695, loss 0.691659, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:44:04.738038: step 14700, loss 0.693366, acc 0.535156, f1 0.373111\n",
      "2017-11-26T15:44:05.059664: step 14705, loss 0.68921, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:44:05.392796: step 14710, loss 0.691589, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:44:05.704771: step 14715, loss 0.683924, acc 0.574219, f1 0.418909\n",
      "2017-11-26T15:44:06.009442: step 14720, loss 0.691008, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:44:06.314376: step 14725, loss 0.6884, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:44:06.614062: step 14730, loss 0.686856, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:44:06.931895: step 14735, loss 0.690273, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:44:07.233104: step 14740, loss 0.69422, acc 0.52832, f1 0.365267\n",
      "2017-11-26T15:44:07.547837: step 14745, loss 0.689604, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:44:07.877855: step 14750, loss 0.694845, acc 0.527344, f1 0.36415\n",
      "2017-11-26T15:44:08.203553: step 14755, loss 0.688123, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:44:08.692163: step 14760, loss 0.695057, acc 0.522461, f1 0.358584\n",
      "2017-11-26T15:44:09.028183: step 14765, loss 0.686487, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:44:09.356195: step 14770, loss 0.685419, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:44:09.694201: step 14775, loss 0.692366, acc 0.538086, f1 0.376489\n",
      "2017-11-26T15:44:10.004352: step 14780, loss 0.688544, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:44:10.316445: step 14785, loss 0.692826, acc 0.535156, f1 0.373111\n",
      "2017-11-26T15:44:10.653199: step 14790, loss 0.684272, acc 0.575195, f1 0.420074\n",
      "2017-11-26T15:44:10.965833: step 14795, loss 0.68822, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:44:11.289277: step 14800, loss 0.685533, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:44:11.621176: step 14805, loss 0.687791, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:44:11.948567: step 14810, loss 0.68796, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:44:12.272419: step 14815, loss 0.691023, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:44:12.570596: step 14820, loss 0.69265, acc 0.535156, f1 0.373111\n",
      "2017-11-26T15:44:12.886187: step 14825, loss 0.686825, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:44:13.201582: step 14830, loss 0.687015, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:44:13.541201: step 14835, loss 0.693056, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:44:13.864235: step 14840, loss 0.682709, acc 0.59375, f1 0.442402\n",
      "2017-11-26T15:44:14.189077: step 14845, loss 0.682777, acc 0.584961, f1 0.431783\n",
      "2017-11-26T15:44:14.507823: step 14850, loss 0.688656, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:44:14.841862: step 14855, loss 0.690843, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:44:15.177881: step 14860, loss 0.684736, acc 0.571289, f1 0.415418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T15:44:15.513932: step 14865, loss 0.688298, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:44:15.849161: step 14870, loss 0.689746, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:44:16.168725: step 14875, loss 0.689186, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:44:16.477726: step 14880, loss 0.684565, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:44:16.789527: step 14885, loss 0.690619, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:44:17.102960: step 14890, loss 0.694317, acc 0.529297, f1 0.366384\n",
      "2017-11-26T15:44:17.424292: step 14895, loss 0.69149, acc 0.538086, f1 0.376489\n",
      "2017-11-26T15:44:17.746615: step 14900, loss 0.691735, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:44:18.090902: step 14905, loss 0.687391, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:44:18.419201: step 14910, loss 0.68084, acc 0.584961, f1 0.431783\n",
      "2017-11-26T15:44:18.723119: step 14915, loss 0.690941, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:44:19.030116: step 14920, loss 0.688956, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:44:19.354602: step 14925, loss 0.691638, acc 0.538086, f1 0.376489\n",
      "2017-11-26T15:44:19.654501: step 14930, loss 0.688276, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:44:19.963124: step 14935, loss 0.686157, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:44:20.268461: step 14940, loss 0.685005, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:44:20.585102: step 14945, loss 0.692512, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:44:20.903308: step 14950, loss 0.68712, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:44:21.190847: step 14955, loss 0.692631, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:44:21.493533: step 14960, loss 0.684749, acc 0.580078, f1 0.425916\n",
      "2017-11-26T15:44:21.806580: step 14965, loss 0.684861, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:44:22.138048: step 14970, loss 0.684609, acc 0.573242, f1 0.417744\n",
      "2017-11-26T15:44:22.469367: step 14975, loss 0.69268, acc 0.537109, f1 0.375362\n",
      "2017-11-26T15:44:22.804491: step 14980, loss 0.689668, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:44:23.129150: step 14985, loss 0.682443, acc 0.582031, f1 0.42826\n",
      "2017-11-26T15:44:23.462296: step 14990, loss 0.69372, acc 0.533203, f1 0.370865\n",
      "2017-11-26T15:44:23.792317: step 14995, loss 0.688694, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:44:24.134978: step 15000, loss 0.685128, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:44:24.457120: step 15005, loss 0.688752, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:44:24.776363: step 15010, loss 0.69089, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:44:25.104585: step 15015, loss 0.691813, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:44:25.428902: step 15020, loss 0.688737, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:44:25.738777: step 15025, loss 0.687922, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:44:26.046927: step 15030, loss 0.689447, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:44:26.367166: step 15035, loss 0.693334, acc 0.535156, f1 0.373111\n",
      "2017-11-26T15:44:26.677169: step 15040, loss 0.692045, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:44:26.994315: step 15045, loss 0.688116, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:44:27.324099: step 15050, loss 0.690036, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:44:27.646897: step 15055, loss 0.683523, acc 0.577148, f1 0.422408\n",
      "2017-11-26T15:44:27.976265: step 15060, loss 0.691316, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:44:28.297350: step 15065, loss 0.687984, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:44:28.615811: step 15070, loss 0.688164, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:44:28.921810: step 15075, loss 0.684579, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:44:29.235822: step 15080, loss 0.686693, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:44:29.569028: step 15085, loss 0.687962, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:44:29.878202: step 15090, loss 0.690812, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:44:30.199365: step 15095, loss 0.690545, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:44:30.517617: step 15100, loss 0.692121, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:44:30.848069: step 15105, loss 0.686923, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:44:31.177960: step 15110, loss 0.691056, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:44:31.481188: step 15115, loss 0.690701, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:44:31.793867: step 15120, loss 0.689535, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:44:32.119201: step 15125, loss 0.685242, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:44:32.434989: step 15130, loss 0.684842, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:44:32.754939: step 15135, loss 0.694731, acc 0.522461, f1 0.358584\n",
      "2017-11-26T15:44:33.032283: step 15140, loss 0.685161, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:44:33.335501: step 15145, loss 0.689036, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:44:33.629906: step 15150, loss 0.695631, acc 0.52832, f1 0.365267\n",
      "2017-11-26T15:44:33.953046: step 15155, loss 0.684428, acc 0.573242, f1 0.417744\n",
      "2017-11-26T15:44:34.276022: step 15160, loss 0.681054, acc 0.594727, f1 0.443587\n",
      "2017-11-26T15:44:34.594380: step 15165, loss 0.687748, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:44:34.923331: step 15170, loss 0.693146, acc 0.53418, f1 0.371988\n",
      "2017-11-26T15:44:35.247387: step 15175, loss 0.689216, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:44:35.555810: step 15180, loss 0.690382, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:44:35.876412: step 15185, loss 0.686336, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:44:36.205702: step 15190, loss 0.687465, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:44:36.513422: step 15195, loss 0.688597, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:44:36.844273: step 15200, loss 0.690446, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:44:37.176033: step 15205, loss 0.689362, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:44:37.508399: step 15210, loss 0.694412, acc 0.52832, f1 0.365267\n",
      "2017-11-26T15:44:37.829313: step 15215, loss 0.690131, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:44:38.157724: step 15220, loss 0.687579, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:44:38.485857: step 15225, loss 0.695814, acc 0.519531, f1 0.355258\n",
      "2017-11-26T15:44:38.792724: step 15230, loss 0.685889, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:44:39.093005: step 15235, loss 0.689602, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:44:39.413178: step 15240, loss 0.691289, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:44:39.726055: step 15245, loss 0.699177, acc 0.519531, f1 0.355258\n",
      "2017-11-26T15:44:40.056389: step 15250, loss 0.692083, acc 0.537109, f1 0.375362\n",
      "2017-11-26T15:44:40.393244: step 15255, loss 0.684013, acc 0.575195, f1 0.420074\n",
      "2017-11-26T15:44:40.729358: step 15260, loss 0.687709, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:44:41.069929: step 15265, loss 0.69354, acc 0.533203, f1 0.370865\n",
      "2017-11-26T15:44:41.385486: step 15270, loss 0.693791, acc 0.536133, f1 0.374236\n",
      "2017-11-26T15:44:41.680677: step 15275, loss 0.686156, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:44:42.005119: step 15280, loss 0.683167, acc 0.577148, f1 0.422408\n",
      "2017-11-26T15:44:42.328701: step 15285, loss 0.692244, acc 0.538086, f1 0.376489\n",
      "2017-11-26T15:44:42.654919: step 15290, loss 0.694427, acc 0.537109, f1 0.375362\n",
      "2017-11-26T15:44:42.983444: step 15295, loss 0.689263, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:44:43.298363: step 15300, loss 0.685478, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:44:43.586235: step 15305, loss 0.690289, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:44:43.901842: step 15310, loss 0.68643, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:44:44.244189: step 15315, loss 0.686014, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:44:44.572798: step 15320, loss 0.692557, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:44:44.894270: step 15325, loss 0.692698, acc 0.538086, f1 0.376489\n",
      "2017-11-26T15:44:45.212175: step 15330, loss 0.687052, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:44:45.509731: step 15335, loss 0.691728, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:44:45.818352: step 15340, loss 0.689932, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:44:46.147670: step 15345, loss 0.688174, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:44:46.489518: step 15350, loss 0.688602, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:44:46.831873: step 15355, loss 0.688274, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:44:47.155054: step 15360, loss 0.687565, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:44:47.469705: step 15365, loss 0.689425, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:44:47.807441: step 15370, loss 0.695184, acc 0.532227, f1 0.369743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T15:44:48.132014: step 15375, loss 0.688017, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:44:48.455621: step 15380, loss 0.690447, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:44:48.771008: step 15385, loss 0.683758, acc 0.575195, f1 0.420074\n",
      "2017-11-26T15:44:49.100477: step 15390, loss 0.693481, acc 0.53418, f1 0.371988\n",
      "2017-11-26T15:44:49.435240: step 15395, loss 0.683012, acc 0.582031, f1 0.42826\n",
      "2017-11-26T15:44:49.765707: step 15400, loss 0.687261, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:44:50.089776: step 15405, loss 0.691242, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:44:50.409748: step 15410, loss 0.686567, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:44:50.740189: step 15415, loss 0.685167, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:44:51.049291: step 15420, loss 0.688629, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:44:51.367678: step 15425, loss 0.69031, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:44:51.696264: step 15430, loss 0.684746, acc 0.579102, f1 0.424746\n",
      "2017-11-26T15:44:52.008340: step 15435, loss 0.685555, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:44:52.344476: step 15440, loss 0.684067, acc 0.574219, f1 0.418909\n",
      "2017-11-26T15:44:52.677971: step 15445, loss 0.69017, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:44:52.976968: step 15450, loss 0.688143, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:44:53.294015: step 15455, loss 0.684254, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:44:53.597389: step 15460, loss 0.692059, acc 0.536133, f1 0.374236\n",
      "2017-11-26T15:44:53.929176: step 15465, loss 0.689889, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:44:54.238801: step 15470, loss 0.689583, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:44:54.568981: step 15475, loss 0.691316, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:44:54.897528: step 15480, loss 0.695289, acc 0.53418, f1 0.371988\n",
      "2017-11-26T15:44:55.210876: step 15485, loss 0.688021, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:44:55.534292: step 15490, loss 0.693014, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:44:55.844587: step 15495, loss 0.690971, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:44:56.145608: step 15500, loss 0.69139, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:44:56.453251: step 15505, loss 0.692643, acc 0.535156, f1 0.373111\n",
      "2017-11-26T15:44:56.746408: step 15510, loss 0.692829, acc 0.538086, f1 0.376489\n",
      "2017-11-26T15:44:57.065453: step 15515, loss 0.686708, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:44:57.378020: step 15520, loss 0.689705, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:44:57.692578: step 15525, loss 0.68518, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:44:58.006162: step 15530, loss 0.6942, acc 0.524414, f1 0.360808\n",
      "2017-11-26T15:44:58.320184: step 15535, loss 0.688409, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:44:58.637952: step 15540, loss 0.684587, acc 0.575195, f1 0.420074\n",
      "2017-11-26T15:44:58.954760: step 15545, loss 0.689949, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:44:59.276988: step 15550, loss 0.686668, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:44:59.593990: step 15555, loss 0.684691, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:44:59.901721: step 15560, loss 0.682771, acc 0.583008, f1 0.429433\n",
      "2017-11-26T15:45:00.185800: step 15565, loss 0.686306, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:45:00.480622: step 15570, loss 0.682206, acc 0.579102, f1 0.424746\n",
      "2017-11-26T15:45:00.798070: step 15575, loss 0.687505, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:45:01.133738: step 15580, loss 0.688, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:45:01.466184: step 15585, loss 0.690058, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:45:01.792541: step 15590, loss 0.687245, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:45:02.129433: step 15595, loss 0.688942, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:45:02.443293: step 15600, loss 0.688354, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:45:02.759683: step 15605, loss 0.68594, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:45:03.098810: step 15610, loss 0.692338, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:45:03.416616: step 15615, loss 0.68848, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:45:03.731632: step 15620, loss 0.688181, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:45:04.059686: step 15625, loss 0.684947, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:45:04.355395: step 15630, loss 0.69122, acc 0.542705, f1 0.381834\n",
      "\n",
      "Evaluation:\n",
      "loss 0.687817, acc 0.55893, f1 0.400903\n",
      "\n",
      "Current epoch:  15\n",
      "2017-11-26T15:45:07.034697: step 15635, loss 0.689988, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:45:07.371334: step 15640, loss 0.68621, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:45:07.708412: step 15645, loss 0.682585, acc 0.579102, f1 0.424746\n",
      "2017-11-26T15:45:08.041303: step 15650, loss 0.69337, acc 0.538086, f1 0.376489\n",
      "2017-11-26T15:45:08.371510: step 15655, loss 0.687526, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:45:08.686261: step 15660, loss 0.689774, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:45:09.024086: step 15665, loss 0.688802, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:45:09.358803: step 15670, loss 0.693238, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:45:09.679085: step 15675, loss 0.690556, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:45:09.991431: step 15680, loss 0.688394, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:45:10.294795: step 15685, loss 0.676344, acc 0.598633, f1 0.448335\n",
      "2017-11-26T15:45:10.623565: step 15690, loss 0.687188, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:45:10.943337: step 15695, loss 0.690312, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:45:11.421980: step 15700, loss 0.689009, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:45:11.753897: step 15705, loss 0.69013, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:45:12.065833: step 15710, loss 0.683694, acc 0.579102, f1 0.424746\n",
      "2017-11-26T15:45:12.380802: step 15715, loss 0.688401, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:45:12.696257: step 15720, loss 0.69042, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:45:13.013019: step 15725, loss 0.687855, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:45:13.332421: step 15730, loss 0.68967, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:45:13.633302: step 15735, loss 0.685044, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:45:13.951576: step 15740, loss 0.685715, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:45:14.265122: step 15745, loss 0.688742, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:45:14.573550: step 15750, loss 0.68784, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:45:14.881457: step 15755, loss 0.682335, acc 0.583008, f1 0.429433\n",
      "2017-11-26T15:45:15.185128: step 15760, loss 0.690046, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:45:15.522515: step 15765, loss 0.68697, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:45:15.855740: step 15770, loss 0.686716, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:45:16.185850: step 15775, loss 0.688424, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:45:16.520508: step 15780, loss 0.682917, acc 0.580078, f1 0.425916\n",
      "2017-11-26T15:45:16.855686: step 15785, loss 0.691817, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:45:17.182459: step 15790, loss 0.688909, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:45:17.499494: step 15795, loss 0.692053, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:45:17.817819: step 15800, loss 0.691075, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:45:18.131976: step 15805, loss 0.688211, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:45:18.461596: step 15810, loss 0.684107, acc 0.584961, f1 0.431783\n",
      "2017-11-26T15:45:18.784370: step 15815, loss 0.688289, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:45:19.120331: step 15820, loss 0.686759, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:45:19.415647: step 15825, loss 0.686094, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:45:19.702138: step 15830, loss 0.68874, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:45:20.016707: step 15835, loss 0.687561, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:45:20.366192: step 15840, loss 0.684371, acc 0.573242, f1 0.417744\n",
      "2017-11-26T15:45:20.694756: step 15845, loss 0.694804, acc 0.529297, f1 0.366384\n",
      "2017-11-26T15:45:21.014876: step 15850, loss 0.695865, acc 0.523438, f1 0.359696\n",
      "2017-11-26T15:45:21.346671: step 15855, loss 0.690092, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:45:21.678404: step 15860, loss 0.685228, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:45:22.005858: step 15865, loss 0.681587, acc 0.583984, f1 0.430607\n",
      "2017-11-26T15:45:22.363406: step 15870, loss 0.676657, acc 0.605469, f1 0.45668\n",
      "2017-11-26T15:45:22.703126: step 15875, loss 0.688847, acc 0.552734, f1 0.393519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T15:45:23.017191: step 15880, loss 0.686995, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:45:23.328987: step 15885, loss 0.684249, acc 0.575195, f1 0.420074\n",
      "2017-11-26T15:45:23.641188: step 15890, loss 0.686308, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:45:23.972697: step 15895, loss 0.693474, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:45:24.297515: step 15900, loss 0.684031, acc 0.574219, f1 0.418909\n",
      "2017-11-26T15:45:24.602118: step 15905, loss 0.690516, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:45:24.909247: step 15910, loss 0.691061, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:45:25.221648: step 15915, loss 0.68665, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:45:25.540574: step 15920, loss 0.68943, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:45:25.868108: step 15925, loss 0.690897, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:45:26.198922: step 15930, loss 0.685677, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:45:26.509806: step 15935, loss 0.688371, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:45:26.842861: step 15940, loss 0.682226, acc 0.582031, f1 0.42826\n",
      "2017-11-26T15:45:27.171367: step 15945, loss 0.68507, acc 0.573242, f1 0.417744\n",
      "2017-11-26T15:45:27.472113: step 15950, loss 0.692766, acc 0.536133, f1 0.374236\n",
      "2017-11-26T15:45:27.793901: step 15955, loss 0.692526, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:45:28.106037: step 15960, loss 0.687306, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:45:28.440313: step 15965, loss 0.687083, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:45:28.762857: step 15970, loss 0.689963, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:45:29.103097: step 15975, loss 0.692506, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:45:29.422445: step 15980, loss 0.689911, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:45:29.760392: step 15985, loss 0.693798, acc 0.530273, f1 0.367503\n",
      "2017-11-26T15:45:30.067756: step 15990, loss 0.691229, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:45:30.387077: step 15995, loss 0.684891, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:45:30.706914: step 16000, loss 0.689562, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:45:31.014680: step 16005, loss 0.690058, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:45:31.314524: step 16010, loss 0.68874, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:45:31.638997: step 16015, loss 0.688876, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:45:31.944881: step 16020, loss 0.685434, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:45:32.259780: step 16025, loss 0.694359, acc 0.53125, f1 0.368622\n",
      "2017-11-26T15:45:32.573564: step 16030, loss 0.68697, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:45:32.897120: step 16035, loss 0.690396, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:45:33.211562: step 16040, loss 0.688797, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:45:33.509770: step 16045, loss 0.692198, acc 0.538086, f1 0.376489\n",
      "2017-11-26T15:45:33.820831: step 16050, loss 0.686413, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:45:34.138751: step 16055, loss 0.68044, acc 0.584961, f1 0.431783\n",
      "2017-11-26T15:45:34.465891: step 16060, loss 0.68999, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:45:34.809033: step 16065, loss 0.681723, acc 0.584961, f1 0.431783\n",
      "2017-11-26T15:45:35.129502: step 16070, loss 0.690294, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:45:35.443656: step 16075, loss 0.685251, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:45:35.754423: step 16080, loss 0.684246, acc 0.574219, f1 0.418909\n",
      "2017-11-26T15:45:36.059261: step 16085, loss 0.692357, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:45:36.381886: step 16090, loss 0.687805, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:45:36.715164: step 16095, loss 0.685151, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:45:37.022369: step 16100, loss 0.687206, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:45:37.333186: step 16105, loss 0.683779, acc 0.575195, f1 0.420074\n",
      "2017-11-26T15:45:37.672380: step 16110, loss 0.682925, acc 0.575195, f1 0.420074\n",
      "2017-11-26T15:45:37.982498: step 16115, loss 0.688081, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:45:38.277510: step 16120, loss 0.691635, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:45:38.612657: step 16125, loss 0.687834, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:45:38.908736: step 16130, loss 0.693956, acc 0.527344, f1 0.36415\n",
      "2017-11-26T15:45:39.231965: step 16135, loss 0.686782, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:45:39.543454: step 16140, loss 0.686316, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:45:39.876219: step 16145, loss 0.687226, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:45:40.217782: step 16150, loss 0.690606, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:45:40.520077: step 16155, loss 0.691324, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:45:40.828050: step 16160, loss 0.692276, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:45:41.187144: step 16165, loss 0.683668, acc 0.576172, f1 0.421241\n",
      "2017-11-26T15:45:41.497923: step 16170, loss 0.684318, acc 0.576172, f1 0.421241\n",
      "2017-11-26T15:45:41.824478: step 16175, loss 0.690299, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:45:42.162553: step 16180, loss 0.69173, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:45:42.475376: step 16185, loss 0.675834, acc 0.603516, f1 0.454291\n",
      "2017-11-26T15:45:42.784851: step 16190, loss 0.700582, acc 0.520508, f1 0.356366\n",
      "2017-11-26T15:45:43.094562: step 16195, loss 0.684921, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:45:43.406416: step 16200, loss 0.68837, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:45:43.698995: step 16205, loss 0.694153, acc 0.532227, f1 0.369743\n",
      "2017-11-26T15:45:43.988787: step 16210, loss 0.689883, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:45:44.330846: step 16215, loss 0.694345, acc 0.525391, f1 0.361921\n",
      "2017-11-26T15:45:44.668498: step 16220, loss 0.68799, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:45:44.988677: step 16225, loss 0.684529, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:45:45.323953: step 16230, loss 0.686726, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:45:45.665304: step 16235, loss 0.685712, acc 0.577148, f1 0.422408\n",
      "2017-11-26T15:45:45.981764: step 16240, loss 0.684752, acc 0.574219, f1 0.418909\n",
      "2017-11-26T15:45:46.308026: step 16245, loss 0.690014, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:45:46.622309: step 16250, loss 0.686021, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:45:46.960009: step 16255, loss 0.688733, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:45:47.279328: step 16260, loss 0.688919, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:45:47.615838: step 16265, loss 0.690519, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:45:47.938688: step 16270, loss 0.681543, acc 0.591797, f1 0.440035\n",
      "2017-11-26T15:45:48.274411: step 16275, loss 0.686525, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:45:48.608390: step 16280, loss 0.689313, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:45:48.925185: step 16285, loss 0.689879, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:45:49.241688: step 16290, loss 0.690159, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:45:49.569409: step 16295, loss 0.685595, acc 0.575195, f1 0.420074\n",
      "2017-11-26T15:45:49.890640: step 16300, loss 0.691191, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:45:50.208459: step 16305, loss 0.688434, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:45:50.503758: step 16310, loss 0.689973, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:45:50.828668: step 16315, loss 0.682734, acc 0.585938, f1 0.432959\n",
      "2017-11-26T15:45:51.163055: step 16320, loss 0.687659, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:45:51.488165: step 16325, loss 0.68967, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:45:51.828778: step 16330, loss 0.689337, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:45:52.173812: step 16335, loss 0.68782, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:45:52.481248: step 16340, loss 0.688027, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:45:52.821785: step 16345, loss 0.691043, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:45:53.123997: step 16350, loss 0.687428, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:45:53.423783: step 16355, loss 0.689434, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:45:53.746233: step 16360, loss 0.68605, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:45:54.048074: step 16365, loss 0.687278, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:45:54.375759: step 16370, loss 0.690016, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:45:54.711272: step 16375, loss 0.687091, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:45:55.017008: step 16380, loss 0.688622, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:45:55.345540: step 16385, loss 0.692441, acc 0.533203, f1 0.370865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T15:45:55.695375: step 16390, loss 0.692075, acc 0.538086, f1 0.376489\n",
      "2017-11-26T15:45:56.024966: step 16395, loss 0.687312, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:45:56.364812: step 16400, loss 0.69151, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:45:56.670202: step 16405, loss 0.68986, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:45:56.984819: step 16410, loss 0.69404, acc 0.530273, f1 0.367503\n",
      "2017-11-26T15:45:57.296993: step 16415, loss 0.684346, acc 0.573242, f1 0.417744\n",
      "2017-11-26T15:45:57.631028: step 16420, loss 0.693141, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:45:57.954148: step 16425, loss 0.685363, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:45:58.261704: step 16430, loss 0.685245, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:45:58.561801: step 16435, loss 0.697538, acc 0.519531, f1 0.355258\n",
      "2017-11-26T15:45:58.901484: step 16440, loss 0.687819, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:45:59.215599: step 16445, loss 0.690678, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:45:59.511544: step 16450, loss 0.686621, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:45:59.829749: step 16455, loss 0.689438, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:46:00.157940: step 16460, loss 0.69289, acc 0.53125, f1 0.368622\n",
      "2017-11-26T15:46:00.462290: step 16465, loss 0.687768, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:46:00.784705: step 16470, loss 0.685502, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:46:01.117273: step 16475, loss 0.688901, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:46:01.454686: step 16480, loss 0.689978, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:46:01.758888: step 16485, loss 0.688195, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:46:02.088992: step 16490, loss 0.694068, acc 0.525391, f1 0.361921\n",
      "2017-11-26T15:46:02.386160: step 16495, loss 0.691458, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:46:02.685189: step 16500, loss 0.689661, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:46:02.987063: step 16505, loss 0.692044, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:46:03.288180: step 16510, loss 0.685291, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:46:03.580090: step 16515, loss 0.687039, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:46:03.880028: step 16520, loss 0.68746, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:46:04.176876: step 16525, loss 0.688653, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:46:04.488392: step 16530, loss 0.688247, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:46:04.801370: step 16535, loss 0.685183, acc 0.575195, f1 0.420074\n",
      "2017-11-26T15:46:05.102702: step 16540, loss 0.688014, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:46:05.406710: step 16545, loss 0.686791, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:46:05.733284: step 16550, loss 0.680156, acc 0.585938, f1 0.432959\n",
      "2017-11-26T15:46:06.025616: step 16555, loss 0.68942, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:46:06.344427: step 16560, loss 0.686458, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:46:06.662621: step 16565, loss 0.684009, acc 0.576172, f1 0.421241\n",
      "2017-11-26T15:46:06.990216: step 16570, loss 0.684776, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:46:07.302517: step 16575, loss 0.689269, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:46:07.632316: step 16580, loss 0.687057, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:46:07.922773: step 16585, loss 0.686241, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:46:08.220044: step 16590, loss 0.687992, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:46:08.515460: step 16595, loss 0.689804, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:46:08.814705: step 16600, loss 0.687361, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:46:09.144831: step 16605, loss 0.68465, acc 0.575195, f1 0.420074\n",
      "2017-11-26T15:46:09.455275: step 16610, loss 0.684315, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:46:09.777089: step 16615, loss 0.691349, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:46:10.088365: step 16620, loss 0.687297, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:46:10.407597: step 16625, loss 0.689652, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:46:10.726566: step 16630, loss 0.690646, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:46:11.044431: step 16635, loss 0.690071, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:46:11.367169: step 16640, loss 0.681805, acc 0.582031, f1 0.42826\n",
      "2017-11-26T15:46:11.676099: step 16645, loss 0.688884, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:46:11.988296: step 16650, loss 0.689109, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:46:12.325053: step 16655, loss 0.687621, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:46:12.640010: step 16660, loss 0.685222, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:46:12.941651: step 16665, loss 0.688101, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:46:13.263519: step 16670, loss 0.689335, acc 0.550781, f1 0.391235\n",
      "\n",
      "Evaluation:\n",
      "loss 0.68783, acc 0.55893, f1 0.400903\n",
      "\n",
      "Current epoch:  16\n",
      "2017-11-26T15:46:16.125127: step 16675, loss 0.689296, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:46:16.453486: step 16680, loss 0.700324, acc 0.510742, f1 0.345337\n",
      "2017-11-26T15:46:16.780959: step 16685, loss 0.688958, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:46:17.092956: step 16690, loss 0.686288, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:46:17.424065: step 16695, loss 0.690783, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:46:17.752815: step 16700, loss 0.687003, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:46:18.095038: step 16705, loss 0.690234, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:46:18.426924: step 16710, loss 0.685592, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:46:18.744163: step 16715, loss 0.692067, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:46:19.079721: step 16720, loss 0.690321, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:46:19.402703: step 16725, loss 0.687552, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:46:19.734202: step 16730, loss 0.689722, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:46:20.082386: step 16735, loss 0.688396, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:46:20.397424: step 16740, loss 0.687714, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:46:20.700034: step 16745, loss 0.686507, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:46:21.016377: step 16750, loss 0.686488, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:46:21.337789: step 16755, loss 0.689392, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:46:21.661197: step 16760, loss 0.687631, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:46:21.973714: step 16765, loss 0.688488, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:46:22.292321: step 16770, loss 0.690711, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:46:22.618002: step 16775, loss 0.684672, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:46:22.946040: step 16780, loss 0.689407, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:46:23.260357: step 16785, loss 0.689575, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:46:23.586794: step 16790, loss 0.688952, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:46:23.902262: step 16795, loss 0.686371, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:46:24.210355: step 16800, loss 0.685735, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:46:24.518455: step 16805, loss 0.683957, acc 0.573242, f1 0.417744\n",
      "2017-11-26T15:46:24.846066: step 16810, loss 0.683804, acc 0.575195, f1 0.420074\n",
      "2017-11-26T15:46:25.177326: step 16815, loss 0.689912, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:46:25.510552: step 16820, loss 0.682012, acc 0.589844, f1 0.437673\n",
      "2017-11-26T15:46:25.834765: step 16825, loss 0.686289, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:46:26.154916: step 16830, loss 0.692169, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:46:26.483765: step 16835, loss 0.685091, acc 0.573242, f1 0.417744\n",
      "2017-11-26T15:46:26.809728: step 16840, loss 0.681387, acc 0.586914, f1 0.434136\n",
      "2017-11-26T15:46:27.130721: step 16845, loss 0.691452, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:46:27.434831: step 16850, loss 0.692967, acc 0.532227, f1 0.369743\n",
      "2017-11-26T15:46:27.760291: step 16855, loss 0.692198, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:46:28.093564: step 16860, loss 0.683854, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:46:28.420833: step 16865, loss 0.689123, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:46:28.735674: step 16870, loss 0.683667, acc 0.575195, f1 0.420074\n",
      "2017-11-26T15:46:29.025354: step 16875, loss 0.69043, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:46:29.337204: step 16880, loss 0.692287, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:46:29.676299: step 16885, loss 0.691482, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:46:30.013962: step 16890, loss 0.69133, acc 0.541016, f1 0.379877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T15:46:30.337728: step 16895, loss 0.688409, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:46:30.635879: step 16900, loss 0.69217, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:46:30.941655: step 16905, loss 0.687533, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:46:31.258388: step 16910, loss 0.688257, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:46:31.587054: step 16915, loss 0.68668, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:46:31.896130: step 16920, loss 0.688861, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:46:32.226192: step 16925, loss 0.686124, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:46:32.530180: step 16930, loss 0.689681, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:46:32.867070: step 16935, loss 0.68958, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:46:33.197313: step 16940, loss 0.684685, acc 0.578125, f1 0.423577\n",
      "2017-11-26T15:46:33.529474: step 16945, loss 0.68982, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:46:33.837310: step 16950, loss 0.688615, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:46:34.149583: step 16955, loss 0.690847, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:46:34.467200: step 16960, loss 0.686345, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:46:34.806419: step 16965, loss 0.687163, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:46:35.108673: step 16970, loss 0.69372, acc 0.532227, f1 0.369743\n",
      "2017-11-26T15:46:35.437629: step 16975, loss 0.687958, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:46:35.776463: step 16980, loss 0.681982, acc 0.583008, f1 0.429433\n",
      "2017-11-26T15:46:36.108737: step 16985, loss 0.685392, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:46:36.435385: step 16990, loss 0.683122, acc 0.577148, f1 0.422408\n",
      "2017-11-26T15:46:36.762328: step 16995, loss 0.684601, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:46:37.092145: step 17000, loss 0.686823, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:46:37.414000: step 17005, loss 0.690548, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:46:37.751910: step 17010, loss 0.68633, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:46:38.095354: step 17015, loss 0.690598, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:46:38.419835: step 17020, loss 0.688207, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:46:38.752683: step 17025, loss 0.687235, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:46:39.087224: step 17030, loss 0.691708, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:46:39.409572: step 17035, loss 0.690808, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:46:39.732711: step 17040, loss 0.682493, acc 0.583984, f1 0.430607\n",
      "2017-11-26T15:46:40.054274: step 17045, loss 0.689581, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:46:40.375489: step 17050, loss 0.681982, acc 0.588867, f1 0.436493\n",
      "2017-11-26T15:46:40.699359: step 17055, loss 0.684914, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:46:41.001461: step 17060, loss 0.684501, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:46:41.300052: step 17065, loss 0.686647, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:46:41.609071: step 17070, loss 0.687876, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:46:41.951848: step 17075, loss 0.691525, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:46:42.285584: step 17080, loss 0.690653, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:46:42.617008: step 17085, loss 0.686607, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:46:42.943129: step 17090, loss 0.687719, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:46:43.252642: step 17095, loss 0.689859, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:46:43.562233: step 17100, loss 0.689451, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:46:43.885819: step 17105, loss 0.686839, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:46:44.219478: step 17110, loss 0.68608, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:46:44.553627: step 17115, loss 0.692481, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:46:44.843665: step 17120, loss 0.68714, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:46:45.163693: step 17125, loss 0.690638, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:46:45.490487: step 17130, loss 0.688843, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:46:45.826787: step 17135, loss 0.690916, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:46:46.172316: step 17140, loss 0.69111, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:46:46.501448: step 17145, loss 0.688194, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:46:46.821899: step 17150, loss 0.688839, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:46:47.162039: step 17155, loss 0.685913, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:46:47.479850: step 17160, loss 0.687438, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:46:47.814190: step 17165, loss 0.686054, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:46:48.125893: step 17170, loss 0.685248, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:46:48.460975: step 17175, loss 0.692823, acc 0.533203, f1 0.370865\n",
      "2017-11-26T15:46:48.784056: step 17180, loss 0.689039, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:46:49.106368: step 17185, loss 0.685776, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:46:49.416278: step 17190, loss 0.688212, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:46:49.740476: step 17195, loss 0.69029, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:46:50.030176: step 17200, loss 0.690237, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:46:50.331166: step 17205, loss 0.687649, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:46:50.643769: step 17210, loss 0.692219, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:46:50.955399: step 17215, loss 0.688619, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:46:51.289591: step 17220, loss 0.690073, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:46:51.613079: step 17225, loss 0.690855, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:46:51.950676: step 17230, loss 0.687797, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:46:52.273621: step 17235, loss 0.681258, acc 0.583984, f1 0.430607\n",
      "2017-11-26T15:46:52.608542: step 17240, loss 0.68682, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:46:52.935606: step 17245, loss 0.68599, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:46:53.250376: step 17250, loss 0.691797, acc 0.536133, f1 0.374236\n",
      "2017-11-26T15:46:53.552914: step 17255, loss 0.6879, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:46:53.870754: step 17260, loss 0.692919, acc 0.537109, f1 0.375362\n",
      "2017-11-26T15:46:54.206535: step 17265, loss 0.695732, acc 0.535156, f1 0.373111\n",
      "2017-11-26T15:46:54.528863: step 17270, loss 0.69151, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:46:54.836417: step 17275, loss 0.690984, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:46:55.136058: step 17280, loss 0.686696, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:46:55.465987: step 17285, loss 0.684733, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:46:55.783310: step 17290, loss 0.690391, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:46:56.074272: step 17295, loss 0.693945, acc 0.530273, f1 0.367503\n",
      "2017-11-26T15:46:56.379061: step 17300, loss 0.678447, acc 0.599609, f1 0.449524\n",
      "2017-11-26T15:46:56.685284: step 17305, loss 0.689422, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:46:56.997283: step 17310, loss 0.684273, acc 0.574219, f1 0.418909\n",
      "2017-11-26T15:46:57.322823: step 17315, loss 0.691364, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:46:57.630357: step 17320, loss 0.686338, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:46:57.957389: step 17325, loss 0.685808, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:46:58.302321: step 17330, loss 0.689707, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:46:58.627693: step 17335, loss 0.682616, acc 0.582031, f1 0.42826\n",
      "2017-11-26T15:46:58.944563: step 17340, loss 0.687619, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:46:59.278052: step 17345, loss 0.686341, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:46:59.594530: step 17350, loss 0.687608, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:46:59.915756: step 17355, loss 0.6847, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:47:00.233144: step 17360, loss 0.692158, acc 0.538086, f1 0.376489\n",
      "2017-11-26T15:47:00.561287: step 17365, loss 0.688932, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:47:00.874145: step 17370, loss 0.692445, acc 0.53418, f1 0.371988\n",
      "2017-11-26T15:47:01.198900: step 17375, loss 0.689681, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:47:01.521175: step 17380, loss 0.684854, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:47:01.825262: step 17385, loss 0.686998, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:47:02.165083: step 17390, loss 0.682737, acc 0.576172, f1 0.421241\n",
      "2017-11-26T15:47:02.480725: step 17395, loss 0.687051, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:47:02.825075: step 17400, loss 0.69451, acc 0.527344, f1 0.36415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T15:47:03.149364: step 17405, loss 0.689529, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:47:03.463059: step 17410, loss 0.685035, acc 0.574219, f1 0.418909\n",
      "2017-11-26T15:47:03.773118: step 17415, loss 0.68598, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:47:04.087864: step 17420, loss 0.684547, acc 0.579102, f1 0.424746\n",
      "2017-11-26T15:47:04.408782: step 17425, loss 0.685317, acc 0.580078, f1 0.425916\n",
      "2017-11-26T15:47:04.723936: step 17430, loss 0.679939, acc 0.592773, f1 0.441218\n",
      "2017-11-26T15:47:05.027744: step 17435, loss 0.69436, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:47:05.335779: step 17440, loss 0.68689, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:47:05.637355: step 17445, loss 0.691385, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:47:05.943910: step 17450, loss 0.678451, acc 0.59668, f1 0.445959\n",
      "2017-11-26T15:47:06.261218: step 17455, loss 0.685371, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:47:06.568132: step 17460, loss 0.687391, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:47:06.878318: step 17465, loss 0.69128, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:47:07.185233: step 17470, loss 0.679458, acc 0.591797, f1 0.440035\n",
      "2017-11-26T15:47:07.494821: step 17475, loss 0.693465, acc 0.53418, f1 0.371988\n",
      "2017-11-26T15:47:07.774381: step 17480, loss 0.692568, acc 0.536133, f1 0.374236\n",
      "2017-11-26T15:47:08.089646: step 17485, loss 0.687407, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:47:08.397142: step 17490, loss 0.689126, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:47:08.703477: step 17495, loss 0.693123, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:47:09.015761: step 17500, loss 0.692908, acc 0.532227, f1 0.369743\n",
      "2017-11-26T15:47:09.332850: step 17505, loss 0.688874, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:47:09.657847: step 17510, loss 0.688606, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:47:09.963864: step 17515, loss 0.685758, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:47:10.285701: step 17520, loss 0.687961, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:47:10.608617: step 17525, loss 0.6838, acc 0.574219, f1 0.418909\n",
      "2017-11-26T15:47:10.906220: step 17530, loss 0.685996, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:47:11.230140: step 17535, loss 0.691932, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:47:11.551084: step 17540, loss 0.684551, acc 0.575195, f1 0.420074\n",
      "2017-11-26T15:47:11.886385: step 17545, loss 0.686082, acc 0.576172, f1 0.421241\n",
      "2017-11-26T15:47:12.212233: step 17550, loss 0.687259, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:47:12.516566: step 17555, loss 0.690209, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:47:12.843531: step 17560, loss 0.691631, acc 0.538086, f1 0.376489\n",
      "2017-11-26T15:47:13.167583: step 17565, loss 0.68175, acc 0.583984, f1 0.430607\n",
      "2017-11-26T15:47:13.488717: step 17570, loss 0.687471, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:47:13.800495: step 17575, loss 0.686837, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:47:14.117940: step 17580, loss 0.691121, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:47:14.437322: step 17585, loss 0.689518, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:47:14.761061: step 17590, loss 0.684654, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:47:15.064558: step 17595, loss 0.684209, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:47:15.375183: step 17600, loss 0.68135, acc 0.583984, f1 0.430607\n",
      "2017-11-26T15:47:15.669722: step 17605, loss 0.693558, acc 0.532227, f1 0.369743\n",
      "2017-11-26T15:47:15.971755: step 17610, loss 0.691196, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:47:16.292636: step 17615, loss 0.691829, acc 0.536133, f1 0.374236\n",
      "2017-11-26T15:47:16.615010: step 17620, loss 0.684655, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:47:16.932880: step 17625, loss 0.683694, acc 0.576172, f1 0.421241\n",
      "2017-11-26T15:47:17.244512: step 17630, loss 0.690079, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:47:17.571533: step 17635, loss 0.692352, acc 0.536133, f1 0.374236\n",
      "2017-11-26T15:47:17.900030: step 17640, loss 0.69051, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:47:18.216881: step 17645, loss 0.693507, acc 0.535156, f1 0.373111\n",
      "2017-11-26T15:47:18.541752: step 17650, loss 0.689851, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:47:18.880500: step 17655, loss 0.689014, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:47:19.211097: step 17660, loss 0.686738, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:47:19.538493: step 17665, loss 0.685619, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:47:19.851234: step 17670, loss 0.691735, acc 0.538086, f1 0.376489\n",
      "2017-11-26T15:47:20.169922: step 17675, loss 0.692561, acc 0.536133, f1 0.374236\n",
      "2017-11-26T15:47:20.473594: step 17680, loss 0.689993, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:47:20.780654: step 17685, loss 0.691277, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:47:21.094825: step 17690, loss 0.690644, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:47:21.419922: step 17695, loss 0.689177, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:47:21.887839: step 17700, loss 0.691145, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:47:22.210018: step 17705, loss 0.687908, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:47:22.533907: step 17710, loss 0.690425, acc 0.545898, f1 0.385543\n",
      "\n",
      "Evaluation:\n",
      "loss 0.687576, acc 0.55893, f1 0.400903\n",
      "\n",
      "Current epoch:  17\n",
      "2017-11-26T15:47:25.237263: step 17715, loss 0.691412, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:47:25.535102: step 17720, loss 0.689243, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:47:25.869193: step 17725, loss 0.689858, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:47:26.188860: step 17730, loss 0.691279, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:47:26.488940: step 17735, loss 0.688761, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:47:26.808118: step 17740, loss 0.681286, acc 0.584961, f1 0.431783\n",
      "2017-11-26T15:47:27.138668: step 17745, loss 0.689551, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:47:27.459113: step 17750, loss 0.683414, acc 0.580078, f1 0.425916\n",
      "2017-11-26T15:47:27.789843: step 17755, loss 0.690159, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:47:28.122093: step 17760, loss 0.690407, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:47:28.452845: step 17765, loss 0.685499, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:47:28.769664: step 17770, loss 0.690827, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:47:29.097525: step 17775, loss 0.688655, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:47:29.421607: step 17780, loss 0.685965, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:47:29.755840: step 17785, loss 0.68914, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:47:30.087736: step 17790, loss 0.681117, acc 0.584961, f1 0.431783\n",
      "2017-11-26T15:47:30.428020: step 17795, loss 0.684866, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:47:30.740238: step 17800, loss 0.687722, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:47:31.061442: step 17805, loss 0.686261, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:47:31.376630: step 17810, loss 0.689026, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:47:31.706850: step 17815, loss 0.69036, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:47:32.009986: step 17820, loss 0.69076, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:47:32.304465: step 17825, loss 0.687195, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:47:32.630712: step 17830, loss 0.688694, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:47:32.963384: step 17835, loss 0.690242, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:47:33.272128: step 17840, loss 0.691368, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:47:33.588139: step 17845, loss 0.688174, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:47:33.915599: step 17850, loss 0.690719, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:47:34.235751: step 17855, loss 0.688806, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:47:34.538940: step 17860, loss 0.689881, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:47:34.832706: step 17865, loss 0.685027, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:47:35.152153: step 17870, loss 0.690848, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:47:35.476693: step 17875, loss 0.689037, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:47:35.781032: step 17880, loss 0.687525, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:47:36.093423: step 17885, loss 0.689304, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:47:36.382298: step 17890, loss 0.696659, acc 0.529297, f1 0.366384\n",
      "2017-11-26T15:47:36.679923: step 17895, loss 0.686579, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:47:36.989089: step 17900, loss 0.696236, acc 0.526367, f1 0.363035\n",
      "2017-11-26T15:47:37.323786: step 17905, loss 0.682913, acc 0.577148, f1 0.422408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T15:47:37.654959: step 17910, loss 0.687918, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:47:37.965774: step 17915, loss 0.6836, acc 0.576172, f1 0.421241\n",
      "2017-11-26T15:47:38.280353: step 17920, loss 0.69073, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:47:38.585499: step 17925, loss 0.689848, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:47:38.922392: step 17930, loss 0.692959, acc 0.536133, f1 0.374236\n",
      "2017-11-26T15:47:39.235269: step 17935, loss 0.68629, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:47:39.557608: step 17940, loss 0.686262, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:47:39.862968: step 17945, loss 0.694413, acc 0.529297, f1 0.366384\n",
      "2017-11-26T15:47:40.182690: step 17950, loss 0.685413, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:47:40.504499: step 17955, loss 0.686951, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:47:40.808867: step 17960, loss 0.688336, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:47:41.117351: step 17965, loss 0.685682, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:47:41.409441: step 17970, loss 0.685996, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:47:41.720579: step 17975, loss 0.687794, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:47:42.018937: step 17980, loss 0.691681, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:47:42.352592: step 17985, loss 0.685029, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:47:42.665197: step 17990, loss 0.68329, acc 0.578125, f1 0.423577\n",
      "2017-11-26T15:47:42.980899: step 17995, loss 0.688328, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:47:43.288500: step 18000, loss 0.694553, acc 0.53418, f1 0.371988\n",
      "2017-11-26T15:47:43.605878: step 18005, loss 0.686028, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:47:43.911944: step 18010, loss 0.682645, acc 0.575195, f1 0.420074\n",
      "2017-11-26T15:47:44.214932: step 18015, loss 0.689226, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:47:44.529063: step 18020, loss 0.690259, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:47:44.849863: step 18025, loss 0.683595, acc 0.576172, f1 0.421241\n",
      "2017-11-26T15:47:45.179705: step 18030, loss 0.691169, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:47:45.507055: step 18035, loss 0.690045, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:47:45.843329: step 18040, loss 0.686871, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:47:46.173137: step 18045, loss 0.692085, acc 0.535156, f1 0.373111\n",
      "2017-11-26T15:47:46.486942: step 18050, loss 0.690192, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:47:46.785826: step 18055, loss 0.682109, acc 0.580078, f1 0.425916\n",
      "2017-11-26T15:47:47.114323: step 18060, loss 0.689871, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:47:47.437460: step 18065, loss 0.688119, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:47:47.768438: step 18070, loss 0.689612, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:47:48.100434: step 18075, loss 0.685713, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:47:48.439427: step 18080, loss 0.68081, acc 0.59668, f1 0.445959\n",
      "2017-11-26T15:47:48.763149: step 18085, loss 0.68528, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:47:49.087037: step 18090, loss 0.683848, acc 0.575195, f1 0.420074\n",
      "2017-11-26T15:47:49.428642: step 18095, loss 0.694046, acc 0.536133, f1 0.374236\n",
      "2017-11-26T15:47:49.754213: step 18100, loss 0.679332, acc 0.600586, f1 0.450714\n",
      "2017-11-26T15:47:50.059364: step 18105, loss 0.685536, acc 0.574219, f1 0.418909\n",
      "2017-11-26T15:47:50.385330: step 18110, loss 0.685878, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:47:50.716189: step 18115, loss 0.688317, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:47:51.036831: step 18120, loss 0.685498, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:47:51.346272: step 18125, loss 0.687532, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:47:51.654247: step 18130, loss 0.68931, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:47:51.980069: step 18135, loss 0.68984, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:47:52.309844: step 18140, loss 0.689301, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:47:52.635116: step 18145, loss 0.682993, acc 0.582031, f1 0.42826\n",
      "2017-11-26T15:47:52.973157: step 18150, loss 0.68585, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:47:53.308815: step 18155, loss 0.685838, acc 0.575195, f1 0.420074\n",
      "2017-11-26T15:47:53.625691: step 18160, loss 0.689505, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:47:53.947712: step 18165, loss 0.679918, acc 0.589844, f1 0.437673\n",
      "2017-11-26T15:47:54.248031: step 18170, loss 0.684651, acc 0.577148, f1 0.422408\n",
      "2017-11-26T15:47:54.553238: step 18175, loss 0.688797, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:47:54.856722: step 18180, loss 0.689045, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:47:55.159316: step 18185, loss 0.687896, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:47:55.492002: step 18190, loss 0.690566, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:47:55.811743: step 18195, loss 0.686587, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:47:56.124434: step 18200, loss 0.688585, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:47:56.452335: step 18205, loss 0.689944, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:47:56.768460: step 18210, loss 0.686874, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:47:57.046028: step 18215, loss 0.68735, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:47:57.348086: step 18220, loss 0.686273, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:47:57.660352: step 18225, loss 0.688944, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:47:57.978492: step 18230, loss 0.687121, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:47:58.269463: step 18235, loss 0.691522, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:47:58.576814: step 18240, loss 0.687087, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:47:58.907508: step 18245, loss 0.686859, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:47:59.245828: step 18250, loss 0.681825, acc 0.579102, f1 0.424746\n",
      "2017-11-26T15:47:59.581407: step 18255, loss 0.68703, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:47:59.896316: step 18260, loss 0.689926, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:48:00.223044: step 18265, loss 0.692165, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:48:00.545881: step 18270, loss 0.681131, acc 0.587891, f1 0.435314\n",
      "2017-11-26T15:48:00.857437: step 18275, loss 0.684955, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:48:01.156165: step 18280, loss 0.685799, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:48:01.471458: step 18285, loss 0.684072, acc 0.580078, f1 0.425916\n",
      "2017-11-26T15:48:01.792057: step 18290, loss 0.691234, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:48:02.127939: step 18295, loss 0.68947, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:48:02.457815: step 18300, loss 0.686439, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:48:02.784680: step 18305, loss 0.685464, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:48:03.103412: step 18310, loss 0.689552, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:48:03.426905: step 18315, loss 0.690712, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:48:03.738268: step 18320, loss 0.687662, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:48:04.037229: step 18325, loss 0.685124, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:48:04.351976: step 18330, loss 0.681711, acc 0.588867, f1 0.436493\n",
      "2017-11-26T15:48:04.668413: step 18335, loss 0.68941, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:48:04.976797: step 18340, loss 0.689892, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:48:05.290466: step 18345, loss 0.690768, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:48:05.610253: step 18350, loss 0.688061, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:48:05.940337: step 18355, loss 0.692097, acc 0.537109, f1 0.375362\n",
      "2017-11-26T15:48:06.271860: step 18360, loss 0.690157, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:48:06.577515: step 18365, loss 0.693354, acc 0.53418, f1 0.371988\n",
      "2017-11-26T15:48:06.886752: step 18370, loss 0.695573, acc 0.53125, f1 0.368622\n",
      "2017-11-26T15:48:07.196534: step 18375, loss 0.689914, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:48:07.490908: step 18380, loss 0.688342, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:48:07.785221: step 18385, loss 0.683093, acc 0.577148, f1 0.422408\n",
      "2017-11-26T15:48:08.119255: step 18390, loss 0.683862, acc 0.573242, f1 0.417744\n",
      "2017-11-26T15:48:08.435699: step 18395, loss 0.694218, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:48:08.737310: step 18400, loss 0.686485, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:48:09.043076: step 18405, loss 0.698041, acc 0.524414, f1 0.360808\n",
      "2017-11-26T15:48:09.358050: step 18410, loss 0.683792, acc 0.580078, f1 0.425916\n",
      "2017-11-26T15:48:09.658224: step 18415, loss 0.687946, acc 0.556641, f1 0.398099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T15:48:09.954573: step 18420, loss 0.692775, acc 0.538086, f1 0.376489\n",
      "2017-11-26T15:48:10.259541: step 18425, loss 0.692021, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:48:10.584923: step 18430, loss 0.693031, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:48:10.901067: step 18435, loss 0.689353, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:48:11.236919: step 18440, loss 0.683908, acc 0.573242, f1 0.417744\n",
      "2017-11-26T15:48:11.558487: step 18445, loss 0.681374, acc 0.581055, f1 0.427088\n",
      "2017-11-26T15:48:11.863456: step 18450, loss 0.691091, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:48:12.175554: step 18455, loss 0.682986, acc 0.588867, f1 0.436493\n",
      "2017-11-26T15:48:12.478341: step 18460, loss 0.688038, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:48:12.799362: step 18465, loss 0.686318, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:48:13.106183: step 18470, loss 0.683932, acc 0.575195, f1 0.420074\n",
      "2017-11-26T15:48:13.406333: step 18475, loss 0.690125, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:48:13.713075: step 18480, loss 0.687717, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:48:14.024596: step 18485, loss 0.689185, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:48:14.324712: step 18490, loss 0.681039, acc 0.585938, f1 0.432959\n",
      "2017-11-26T15:48:14.652842: step 18495, loss 0.686865, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:48:14.971592: step 18500, loss 0.695587, acc 0.533203, f1 0.370865\n",
      "2017-11-26T15:48:15.282954: step 18505, loss 0.682039, acc 0.580078, f1 0.425916\n",
      "2017-11-26T15:48:15.606271: step 18510, loss 0.687801, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:48:15.906996: step 18515, loss 0.685493, acc 0.573242, f1 0.417744\n",
      "2017-11-26T15:48:16.239789: step 18520, loss 0.690951, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:48:16.563094: step 18525, loss 0.688483, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:48:16.892251: step 18530, loss 0.691371, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:48:17.215415: step 18535, loss 0.691391, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:48:17.535380: step 18540, loss 0.68807, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:48:17.876515: step 18545, loss 0.687033, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:48:18.175343: step 18550, loss 0.68747, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:48:18.495179: step 18555, loss 0.692654, acc 0.533203, f1 0.370865\n",
      "2017-11-26T15:48:18.824672: step 18560, loss 0.683826, acc 0.580078, f1 0.425916\n",
      "2017-11-26T15:48:19.142848: step 18565, loss 0.682553, acc 0.580078, f1 0.425916\n",
      "2017-11-26T15:48:19.458506: step 18570, loss 0.689269, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:48:19.774383: step 18575, loss 0.689067, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:48:20.110292: step 18580, loss 0.688279, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:48:20.434309: step 18585, loss 0.685736, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:48:20.756041: step 18590, loss 0.688177, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:48:21.072827: step 18595, loss 0.684884, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:48:21.399910: step 18600, loss 0.688016, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:48:21.700909: step 18605, loss 0.689359, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:48:22.008856: step 18610, loss 0.681465, acc 0.585938, f1 0.432959\n",
      "2017-11-26T15:48:22.322886: step 18615, loss 0.689643, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:48:22.645428: step 18620, loss 0.6896, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:48:22.971707: step 18625, loss 0.69057, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:48:23.299078: step 18630, loss 0.693677, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:48:23.629920: step 18635, loss 0.687414, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:48:24.088244: step 18640, loss 0.683846, acc 0.576172, f1 0.421241\n",
      "2017-11-26T15:48:24.387809: step 18645, loss 0.692382, acc 0.536133, f1 0.374236\n",
      "2017-11-26T15:48:24.681857: step 18650, loss 0.690063, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:48:24.986410: step 18655, loss 0.681429, acc 0.584961, f1 0.431783\n",
      "2017-11-26T15:48:25.293048: step 18660, loss 0.686987, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:48:25.621317: step 18665, loss 0.686966, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:48:25.953943: step 18670, loss 0.689949, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:48:26.261973: step 18675, loss 0.687727, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:48:26.564452: step 18680, loss 0.690136, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:48:26.866442: step 18685, loss 0.689926, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:48:27.176958: step 18690, loss 0.690852, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:48:27.508639: step 18695, loss 0.689589, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:48:27.822515: step 18700, loss 0.693104, acc 0.527344, f1 0.36415\n",
      "2017-11-26T15:48:28.147324: step 18705, loss 0.690267, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:48:28.448536: step 18710, loss 0.681154, acc 0.582031, f1 0.42826\n",
      "2017-11-26T15:48:28.792477: step 18715, loss 0.691699, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:48:29.109045: step 18720, loss 0.690294, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:48:29.429462: step 18725, loss 0.686526, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:48:29.762883: step 18730, loss 0.688431, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:48:30.072070: step 18735, loss 0.687766, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:48:30.388945: step 18740, loss 0.689906, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:48:30.716189: step 18745, loss 0.68502, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:48:31.026946: step 18750, loss 0.685864, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:48:31.333269: step 18755, loss 0.689582, acc 0.550781, f1 0.391235\n",
      "\n",
      "Evaluation:\n",
      "loss 0.68755, acc 0.55893, f1 0.400903\n",
      "\n",
      "Current epoch:  18\n",
      "2017-11-26T15:48:34.020303: step 18760, loss 0.691917, acc 0.538086, f1 0.376489\n",
      "2017-11-26T15:48:34.338788: step 18765, loss 0.697153, acc 0.53125, f1 0.368622\n",
      "2017-11-26T15:48:34.665143: step 18770, loss 0.684768, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:48:34.999092: step 18775, loss 0.687942, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:48:35.311356: step 18780, loss 0.690979, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:48:35.620460: step 18785, loss 0.69025, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:48:35.938657: step 18790, loss 0.692379, acc 0.538086, f1 0.376489\n",
      "2017-11-26T15:48:36.250754: step 18795, loss 0.694956, acc 0.522461, f1 0.358584\n",
      "2017-11-26T15:48:36.548393: step 18800, loss 0.688853, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:48:36.826646: step 18805, loss 0.688046, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:48:37.124508: step 18810, loss 0.692992, acc 0.537109, f1 0.375362\n",
      "2017-11-26T15:48:37.438726: step 18815, loss 0.688588, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:48:37.737958: step 18820, loss 0.685482, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:48:38.044514: step 18825, loss 0.690279, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:48:38.337940: step 18830, loss 0.689372, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:48:38.634032: step 18835, loss 0.683199, acc 0.579102, f1 0.424746\n",
      "2017-11-26T15:48:38.937535: step 18840, loss 0.693214, acc 0.529297, f1 0.366384\n",
      "2017-11-26T15:48:39.244463: step 18845, loss 0.692111, acc 0.538086, f1 0.376489\n",
      "2017-11-26T15:48:39.571436: step 18850, loss 0.684711, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:48:39.890222: step 18855, loss 0.687227, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:48:40.216165: step 18860, loss 0.690881, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:48:40.528630: step 18865, loss 0.687733, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:48:40.837992: step 18870, loss 0.685068, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:48:41.127637: step 18875, loss 0.692501, acc 0.53418, f1 0.371988\n",
      "2017-11-26T15:48:41.412462: step 18880, loss 0.682735, acc 0.579102, f1 0.424746\n",
      "2017-11-26T15:48:41.720203: step 18885, loss 0.688805, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:48:42.016669: step 18890, loss 0.695697, acc 0.523438, f1 0.359696\n",
      "2017-11-26T15:48:42.309200: step 18895, loss 0.680956, acc 0.588867, f1 0.436493\n",
      "2017-11-26T15:48:42.601969: step 18900, loss 0.689902, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:48:42.906178: step 18905, loss 0.685733, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:48:43.206205: step 18910, loss 0.687488, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:48:43.499761: step 18915, loss 0.688365, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:48:43.811791: step 18920, loss 0.681615, acc 0.584961, f1 0.431783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T15:48:44.120223: step 18925, loss 0.68727, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:48:44.432284: step 18930, loss 0.687701, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:48:44.758514: step 18935, loss 0.693428, acc 0.533203, f1 0.370865\n",
      "2017-11-26T15:48:45.076623: step 18940, loss 0.686827, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:48:45.374818: step 18945, loss 0.688767, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:48:45.682205: step 18950, loss 0.686786, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:48:46.006293: step 18955, loss 0.687992, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:48:46.334687: step 18960, loss 0.688135, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:48:46.658922: step 18965, loss 0.690086, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:48:46.976419: step 18970, loss 0.68943, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:48:47.307137: step 18975, loss 0.682055, acc 0.577148, f1 0.422408\n",
      "2017-11-26T15:48:47.647628: step 18980, loss 0.69196, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:48:47.958703: step 18985, loss 0.687358, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:48:48.276000: step 18990, loss 0.690777, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:48:48.593156: step 18995, loss 0.692821, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:48:48.912225: step 19000, loss 0.692587, acc 0.538086, f1 0.376489\n",
      "2017-11-26T15:48:49.217477: step 19005, loss 0.690804, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:48:49.530277: step 19010, loss 0.684475, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:48:49.833309: step 19015, loss 0.686015, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:48:50.155092: step 19020, loss 0.690495, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:48:50.466980: step 19025, loss 0.688729, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:48:50.792777: step 19030, loss 0.686959, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:48:51.108997: step 19035, loss 0.692242, acc 0.538086, f1 0.376489\n",
      "2017-11-26T15:48:51.419436: step 19040, loss 0.687654, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:48:51.748295: step 19045, loss 0.688102, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:48:52.071619: step 19050, loss 0.685394, acc 0.577148, f1 0.422408\n",
      "2017-11-26T15:48:52.388881: step 19055, loss 0.685223, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:48:52.713645: step 19060, loss 0.682347, acc 0.582031, f1 0.42826\n",
      "2017-11-26T15:48:53.043610: step 19065, loss 0.683217, acc 0.579102, f1 0.424746\n",
      "2017-11-26T15:48:53.360837: step 19070, loss 0.683228, acc 0.574219, f1 0.418909\n",
      "2017-11-26T15:48:53.672781: step 19075, loss 0.694911, acc 0.538086, f1 0.376489\n",
      "2017-11-26T15:48:53.974064: step 19080, loss 0.688258, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:48:54.273623: step 19085, loss 0.685616, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:48:54.588434: step 19090, loss 0.69119, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:48:54.921505: step 19095, loss 0.689393, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:48:55.249243: step 19100, loss 0.687924, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:48:55.581824: step 19105, loss 0.686466, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:48:55.912910: step 19110, loss 0.683109, acc 0.579102, f1 0.424746\n",
      "2017-11-26T15:48:56.246741: step 19115, loss 0.689647, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:48:56.574461: step 19120, loss 0.681145, acc 0.583984, f1 0.430607\n",
      "2017-11-26T15:48:56.906332: step 19125, loss 0.686343, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:48:57.235595: step 19130, loss 0.690117, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:48:57.565318: step 19135, loss 0.677609, acc 0.595703, f1 0.444772\n",
      "2017-11-26T15:48:57.887822: step 19140, loss 0.692717, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:48:58.228314: step 19145, loss 0.684558, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:48:58.565755: step 19150, loss 0.696155, acc 0.523438, f1 0.359696\n",
      "2017-11-26T15:48:58.883115: step 19155, loss 0.695469, acc 0.527344, f1 0.36415\n",
      "2017-11-26T15:48:59.198210: step 19160, loss 0.683507, acc 0.574219, f1 0.418909\n",
      "2017-11-26T15:48:59.510277: step 19165, loss 0.689101, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:48:59.816834: step 19170, loss 0.683101, acc 0.578125, f1 0.423577\n",
      "2017-11-26T15:49:00.141970: step 19175, loss 0.69058, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:49:00.450491: step 19180, loss 0.690189, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:49:00.780307: step 19185, loss 0.686739, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:49:01.108988: step 19190, loss 0.681578, acc 0.591797, f1 0.440035\n",
      "2017-11-26T15:49:01.422917: step 19195, loss 0.694235, acc 0.53125, f1 0.368622\n",
      "2017-11-26T15:49:01.723018: step 19200, loss 0.685926, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:49:02.049619: step 19205, loss 0.689663, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:49:02.354212: step 19210, loss 0.68967, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:49:02.666123: step 19215, loss 0.687438, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:49:02.958684: step 19220, loss 0.688015, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:49:03.246856: step 19225, loss 0.689828, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:49:03.557908: step 19230, loss 0.685786, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:49:03.884630: step 19235, loss 0.685786, acc 0.578125, f1 0.423577\n",
      "2017-11-26T15:49:04.221602: step 19240, loss 0.692681, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:49:04.556170: step 19245, loss 0.685184, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:49:04.875584: step 19250, loss 0.687313, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:49:05.199735: step 19255, loss 0.686145, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:49:05.523917: step 19260, loss 0.684032, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:49:05.845091: step 19265, loss 0.676735, acc 0.59668, f1 0.445959\n",
      "2017-11-26T15:49:06.152098: step 19270, loss 0.688815, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:49:06.442747: step 19275, loss 0.682587, acc 0.581055, f1 0.427088\n",
      "2017-11-26T15:49:06.767636: step 19280, loss 0.690887, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:49:07.105312: step 19285, loss 0.694096, acc 0.532227, f1 0.369743\n",
      "2017-11-26T15:49:07.427932: step 19290, loss 0.693029, acc 0.529297, f1 0.366384\n",
      "2017-11-26T15:49:07.765025: step 19295, loss 0.684352, acc 0.574219, f1 0.418909\n",
      "2017-11-26T15:49:08.071256: step 19300, loss 0.686837, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:49:08.372934: step 19305, loss 0.687196, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:49:08.694134: step 19310, loss 0.690049, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:49:09.006042: step 19315, loss 0.68968, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:49:09.328973: step 19320, loss 0.69336, acc 0.526367, f1 0.363035\n",
      "2017-11-26T15:49:09.630892: step 19325, loss 0.68817, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:49:09.985315: step 19330, loss 0.685048, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:49:10.303680: step 19335, loss 0.691706, acc 0.536133, f1 0.374236\n",
      "2017-11-26T15:49:10.611609: step 19340, loss 0.691001, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:49:10.913439: step 19345, loss 0.696328, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:49:11.218256: step 19350, loss 0.690796, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:49:11.543386: step 19355, loss 0.689936, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:49:11.866928: step 19360, loss 0.688503, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:49:12.193756: step 19365, loss 0.683711, acc 0.574219, f1 0.418909\n",
      "2017-11-26T15:49:12.515874: step 19370, loss 0.685856, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:49:12.849840: step 19375, loss 0.689218, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:49:13.186021: step 19380, loss 0.691166, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:49:13.513068: step 19385, loss 0.689788, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:49:13.848107: step 19390, loss 0.684941, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:49:14.178941: step 19395, loss 0.691927, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:49:14.509580: step 19400, loss 0.685727, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:49:14.846433: step 19405, loss 0.687531, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:49:15.164848: step 19410, loss 0.688168, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:49:15.498748: step 19415, loss 0.691831, acc 0.538086, f1 0.376489\n",
      "2017-11-26T15:49:15.819763: step 19420, loss 0.684545, acc 0.573242, f1 0.417744\n",
      "2017-11-26T15:49:16.143785: step 19425, loss 0.68835, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:49:16.448060: step 19430, loss 0.686665, acc 0.561523, f1 0.403847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T15:49:16.759212: step 19435, loss 0.689996, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:49:17.062181: step 19440, loss 0.693959, acc 0.53418, f1 0.371988\n",
      "2017-11-26T15:49:17.388842: step 19445, loss 0.683133, acc 0.579102, f1 0.424746\n",
      "2017-11-26T15:49:17.682691: step 19450, loss 0.695585, acc 0.518555, f1 0.354151\n",
      "2017-11-26T15:49:17.982909: step 19455, loss 0.687394, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:49:18.289808: step 19460, loss 0.689216, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:49:18.623091: step 19465, loss 0.678527, acc 0.59668, f1 0.445959\n",
      "2017-11-26T15:49:18.946053: step 19470, loss 0.681286, acc 0.59375, f1 0.442402\n",
      "2017-11-26T15:49:19.248900: step 19475, loss 0.685357, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:49:19.559422: step 19480, loss 0.690942, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:49:19.882863: step 19485, loss 0.687908, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:49:20.193286: step 19490, loss 0.692701, acc 0.535156, f1 0.373111\n",
      "2017-11-26T15:49:20.528591: step 19495, loss 0.689086, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:49:20.844601: step 19500, loss 0.683637, acc 0.576172, f1 0.421241\n",
      "2017-11-26T15:49:21.157141: step 19505, loss 0.679961, acc 0.589844, f1 0.437673\n",
      "2017-11-26T15:49:21.456103: step 19510, loss 0.687111, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:49:21.769443: step 19515, loss 0.687342, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:49:22.096823: step 19520, loss 0.681908, acc 0.585938, f1 0.432959\n",
      "2017-11-26T15:49:22.417508: step 19525, loss 0.689699, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:49:22.723573: step 19530, loss 0.685712, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:49:23.025622: step 19535, loss 0.683889, acc 0.573242, f1 0.417744\n",
      "2017-11-26T15:49:23.339714: step 19540, loss 0.684212, acc 0.575195, f1 0.420074\n",
      "2017-11-26T15:49:23.657845: step 19545, loss 0.68718, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:49:23.981201: step 19550, loss 0.689406, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:49:24.313569: step 19555, loss 0.691975, acc 0.540039, f1 0.378746\n",
      "2017-11-26T15:49:24.643585: step 19560, loss 0.692029, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:49:24.968114: step 19565, loss 0.685353, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:49:25.290444: step 19570, loss 0.690241, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:49:25.600859: step 19575, loss 0.690593, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:49:25.938954: step 19580, loss 0.688681, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:49:26.394567: step 19585, loss 0.685318, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:49:26.730423: step 19590, loss 0.69055, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:49:27.043096: step 19595, loss 0.690589, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:49:27.348841: step 19600, loss 0.693312, acc 0.537109, f1 0.375362\n",
      "2017-11-26T15:49:27.670612: step 19605, loss 0.686709, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:49:27.975286: step 19610, loss 0.686882, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:49:28.291152: step 19615, loss 0.694122, acc 0.533203, f1 0.370865\n",
      "2017-11-26T15:49:28.622180: step 19620, loss 0.687308, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:49:28.953061: step 19625, loss 0.681958, acc 0.583008, f1 0.429433\n",
      "2017-11-26T15:49:29.285745: step 19630, loss 0.691115, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:49:29.603691: step 19635, loss 0.694826, acc 0.53418, f1 0.371988\n",
      "2017-11-26T15:49:29.939592: step 19640, loss 0.69397, acc 0.532227, f1 0.369743\n",
      "2017-11-26T15:49:30.253926: step 19645, loss 0.683281, acc 0.574219, f1 0.418909\n",
      "2017-11-26T15:49:30.577887: step 19650, loss 0.689625, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:49:30.878634: step 19655, loss 0.685693, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:49:31.192844: step 19660, loss 0.695001, acc 0.521484, f1 0.357475\n",
      "2017-11-26T15:49:31.505536: step 19665, loss 0.695421, acc 0.523438, f1 0.359696\n",
      "2017-11-26T15:49:31.813644: step 19670, loss 0.685599, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:49:32.115609: step 19675, loss 0.687538, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:49:32.432379: step 19680, loss 0.692609, acc 0.53418, f1 0.371988\n",
      "2017-11-26T15:49:32.724263: step 19685, loss 0.688305, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:49:33.034872: step 19690, loss 0.683189, acc 0.581055, f1 0.427088\n",
      "2017-11-26T15:49:33.357311: step 19695, loss 0.687658, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:49:33.655018: step 19700, loss 0.686599, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:49:33.965040: step 19705, loss 0.693133, acc 0.53418, f1 0.371988\n",
      "2017-11-26T15:49:34.291611: step 19710, loss 0.686442, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:49:34.611028: step 19715, loss 0.684179, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:49:34.921442: step 19720, loss 0.688243, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:49:35.224001: step 19725, loss 0.685327, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:49:35.539455: step 19730, loss 0.686222, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:49:35.849491: step 19735, loss 0.688316, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:49:36.153931: step 19740, loss 0.692137, acc 0.538086, f1 0.376489\n",
      "2017-11-26T15:49:36.462588: step 19745, loss 0.683025, acc 0.577148, f1 0.422408\n",
      "2017-11-26T15:49:36.787384: step 19750, loss 0.690614, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:49:37.121481: step 19755, loss 0.695048, acc 0.526367, f1 0.363035\n",
      "2017-11-26T15:49:37.430850: step 19760, loss 0.688885, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:49:37.767019: step 19765, loss 0.689713, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:49:38.087067: step 19770, loss 0.693249, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:49:38.423438: step 19775, loss 0.685459, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:49:38.761617: step 19780, loss 0.690051, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:49:39.092052: step 19785, loss 0.684686, acc 0.575195, f1 0.420074\n",
      "2017-11-26T15:49:39.415415: step 19790, loss 0.684142, acc 0.582031, f1 0.42826\n",
      "2017-11-26T15:49:39.737645: step 19795, loss 0.689247, acc 0.551758, f1 0.392377\n",
      "\n",
      "Evaluation:\n",
      "loss 0.68742, acc 0.55893, f1 0.400903\n",
      "\n",
      "Current epoch:  19\n",
      "2017-11-26T15:49:42.383859: step 19800, loss 0.695372, acc 0.520508, f1 0.356366\n",
      "2017-11-26T15:49:42.709225: step 19805, loss 0.68969, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:49:43.019422: step 19810, loss 0.686101, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:49:43.341581: step 19815, loss 0.683857, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:49:43.678879: step 19820, loss 0.686951, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:49:43.998263: step 19825, loss 0.687636, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:49:44.313805: step 19830, loss 0.687711, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:49:44.637519: step 19835, loss 0.688536, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:49:44.973650: step 19840, loss 0.690813, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:49:45.293802: step 19845, loss 0.689322, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:49:45.617029: step 19850, loss 0.688649, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:49:45.924490: step 19855, loss 0.694117, acc 0.533203, f1 0.370865\n",
      "2017-11-26T15:49:46.248110: step 19860, loss 0.684073, acc 0.578125, f1 0.423577\n",
      "2017-11-26T15:49:46.562236: step 19865, loss 0.685395, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:49:46.849150: step 19870, loss 0.685376, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:49:47.133017: step 19875, loss 0.689347, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:49:47.434084: step 19880, loss 0.69571, acc 0.526367, f1 0.363035\n",
      "2017-11-26T15:49:47.726036: step 19885, loss 0.688037, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:49:48.031412: step 19890, loss 0.683397, acc 0.573242, f1 0.417744\n",
      "2017-11-26T15:49:48.352057: step 19895, loss 0.689663, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:49:48.662073: step 19900, loss 0.687771, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:49:48.956591: step 19905, loss 0.68174, acc 0.585938, f1 0.432959\n",
      "2017-11-26T15:49:49.259109: step 19910, loss 0.689617, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:49:49.582542: step 19915, loss 0.688037, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:49:49.912431: step 19920, loss 0.687173, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:49:50.235983: step 19925, loss 0.686273, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:49:50.561243: step 19930, loss 0.688714, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:49:50.887990: step 19935, loss 0.691173, acc 0.540039, f1 0.378746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T15:49:51.221555: step 19940, loss 0.690465, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:49:51.536942: step 19945, loss 0.682065, acc 0.589844, f1 0.437673\n",
      "2017-11-26T15:49:51.842700: step 19950, loss 0.689967, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:49:52.143860: step 19955, loss 0.692286, acc 0.536133, f1 0.374236\n",
      "2017-11-26T15:49:52.420972: step 19960, loss 0.688317, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:49:52.755617: step 19965, loss 0.689276, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:49:53.080097: step 19970, loss 0.685657, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:49:53.402251: step 19975, loss 0.682275, acc 0.579102, f1 0.424746\n",
      "2017-11-26T15:49:53.737405: step 19980, loss 0.691985, acc 0.537109, f1 0.375362\n",
      "2017-11-26T15:49:54.055434: step 19985, loss 0.690153, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:49:54.358804: step 19990, loss 0.689898, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:49:54.677206: step 19995, loss 0.683466, acc 0.574219, f1 0.418909\n",
      "2017-11-26T15:49:54.970516: step 20000, loss 0.685762, acc 0.573242, f1 0.417744\n",
      "2017-11-26T15:49:55.266281: step 20005, loss 0.690287, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:49:55.589258: step 20010, loss 0.689241, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:49:55.912588: step 20015, loss 0.687515, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:49:56.243586: step 20020, loss 0.684502, acc 0.574219, f1 0.418909\n",
      "2017-11-26T15:49:56.558833: step 20025, loss 0.691037, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:49:56.890093: step 20030, loss 0.690812, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:49:57.221511: step 20035, loss 0.683955, acc 0.574219, f1 0.418909\n",
      "2017-11-26T15:49:57.557420: step 20040, loss 0.684148, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:49:57.885464: step 20045, loss 0.686802, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:49:58.210482: step 20050, loss 0.690289, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:49:58.544176: step 20055, loss 0.687461, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:49:58.878336: step 20060, loss 0.697019, acc 0.509766, f1 0.34424\n",
      "2017-11-26T15:49:59.197304: step 20065, loss 0.691439, acc 0.543945, f1 0.383273\n",
      "2017-11-26T15:49:59.535747: step 20070, loss 0.688412, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:49:59.864708: step 20075, loss 0.692384, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:50:00.185683: step 20080, loss 0.688902, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:50:00.506603: step 20085, loss 0.687914, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:50:00.830753: step 20090, loss 0.693907, acc 0.532227, f1 0.369743\n",
      "2017-11-26T15:50:01.155301: step 20095, loss 0.681859, acc 0.588867, f1 0.436493\n",
      "2017-11-26T15:50:01.471078: step 20100, loss 0.687656, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:50:01.801037: step 20105, loss 0.687049, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:50:02.135271: step 20110, loss 0.682855, acc 0.578125, f1 0.423577\n",
      "2017-11-26T15:50:02.465341: step 20115, loss 0.692374, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:50:02.768670: step 20120, loss 0.694459, acc 0.536133, f1 0.374236\n",
      "2017-11-26T15:50:03.061671: step 20125, loss 0.686974, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:50:03.379377: step 20130, loss 0.688874, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:50:03.688676: step 20135, loss 0.687772, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:50:03.995266: step 20140, loss 0.68925, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:50:04.298749: step 20145, loss 0.688737, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:50:04.590880: step 20150, loss 0.686458, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:50:04.903576: step 20155, loss 0.693373, acc 0.532227, f1 0.369743\n",
      "2017-11-26T15:50:05.233416: step 20160, loss 0.689083, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:50:05.554422: step 20165, loss 0.687732, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:50:05.875779: step 20170, loss 0.686694, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:50:06.199600: step 20175, loss 0.688768, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:50:06.515225: step 20180, loss 0.684842, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:50:06.834461: step 20185, loss 0.688306, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:50:07.142756: step 20190, loss 0.688881, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:50:07.464883: step 20195, loss 0.68883, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:50:07.778698: step 20200, loss 0.689345, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:50:08.102490: step 20205, loss 0.688003, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:50:08.432032: step 20210, loss 0.692508, acc 0.533203, f1 0.370865\n",
      "2017-11-26T15:50:08.748261: step 20215, loss 0.681901, acc 0.583008, f1 0.429433\n",
      "2017-11-26T15:50:09.075141: step 20220, loss 0.683983, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:50:09.405370: step 20225, loss 0.685468, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:50:09.731809: step 20230, loss 0.688214, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:50:10.056901: step 20235, loss 0.688193, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:50:10.383516: step 20240, loss 0.681464, acc 0.580078, f1 0.425916\n",
      "2017-11-26T15:50:10.701315: step 20245, loss 0.6809, acc 0.587891, f1 0.435314\n",
      "2017-11-26T15:50:11.024861: step 20250, loss 0.683297, acc 0.579102, f1 0.424746\n",
      "2017-11-26T15:50:11.335724: step 20255, loss 0.688389, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:50:11.659737: step 20260, loss 0.689948, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:50:11.979594: step 20265, loss 0.687493, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:50:12.312801: step 20270, loss 0.685484, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:50:12.643584: step 20275, loss 0.689631, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:50:12.979130: step 20280, loss 0.687354, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:50:13.302033: step 20285, loss 0.686249, acc 0.569336, f1 0.413096\n",
      "2017-11-26T15:50:13.632954: step 20290, loss 0.692128, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:50:13.961269: step 20295, loss 0.68482, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:50:14.287539: step 20300, loss 0.686132, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:50:14.617618: step 20305, loss 0.684139, acc 0.574219, f1 0.418909\n",
      "2017-11-26T15:50:14.956018: step 20310, loss 0.684621, acc 0.570312, f1 0.414257\n",
      "2017-11-26T15:50:15.267829: step 20315, loss 0.693787, acc 0.532227, f1 0.369743\n",
      "2017-11-26T15:50:15.565976: step 20320, loss 0.677002, acc 0.600586, f1 0.450714\n",
      "2017-11-26T15:50:15.889552: step 20325, loss 0.690704, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:50:16.227578: step 20330, loss 0.69025, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:50:16.537727: step 20335, loss 0.691271, acc 0.541992, f1 0.381008\n",
      "2017-11-26T15:50:16.874193: step 20340, loss 0.68429, acc 0.574219, f1 0.418909\n",
      "2017-11-26T15:50:17.202911: step 20345, loss 0.690622, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:50:17.526533: step 20350, loss 0.687423, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:50:17.872730: step 20355, loss 0.688882, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:50:18.198055: step 20360, loss 0.687045, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:50:18.493317: step 20365, loss 0.688135, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:50:18.802565: step 20370, loss 0.687862, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:50:19.113331: step 20375, loss 0.688027, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:50:19.409073: step 20380, loss 0.694307, acc 0.537109, f1 0.375362\n",
      "2017-11-26T15:50:19.723722: step 20385, loss 0.689018, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:50:20.041472: step 20390, loss 0.694014, acc 0.527344, f1 0.36415\n",
      "2017-11-26T15:50:20.361878: step 20395, loss 0.689456, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:50:20.665013: step 20400, loss 0.688924, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:50:20.973028: step 20405, loss 0.689024, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:50:21.273782: step 20410, loss 0.687055, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:50:21.607007: step 20415, loss 0.686573, acc 0.5625, f1 0.405\n",
      "2017-11-26T15:50:21.913263: step 20420, loss 0.691487, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:50:22.240219: step 20425, loss 0.68625, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:50:22.569959: step 20430, loss 0.68608, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:50:22.883247: step 20435, loss 0.691649, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:50:23.207947: step 20440, loss 0.688955, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:50:23.546567: step 20445, loss 0.686928, acc 0.560547, f1 0.402696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T15:50:23.871462: step 20450, loss 0.693961, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:50:24.196382: step 20455, loss 0.689951, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:50:24.516124: step 20460, loss 0.689073, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:50:24.829199: step 20465, loss 0.690917, acc 0.551758, f1 0.392377\n",
      "2017-11-26T15:50:25.131650: step 20470, loss 0.693165, acc 0.536133, f1 0.374236\n",
      "2017-11-26T15:50:25.449175: step 20475, loss 0.688269, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:50:25.767519: step 20480, loss 0.687369, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:50:26.087487: step 20485, loss 0.68649, acc 0.564453, f1 0.407308\n",
      "2017-11-26T15:50:26.418398: step 20490, loss 0.688765, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:50:26.743896: step 20495, loss 0.689876, acc 0.548828, f1 0.388955\n",
      "2017-11-26T15:50:27.065291: step 20500, loss 0.688224, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:50:27.373593: step 20505, loss 0.685563, acc 0.568359, f1 0.411937\n",
      "2017-11-26T15:50:27.706433: step 20510, loss 0.688535, acc 0.553711, f1 0.394663\n",
      "2017-11-26T15:50:28.039711: step 20515, loss 0.685002, acc 0.567383, f1 0.410778\n",
      "2017-11-26T15:50:28.356108: step 20520, loss 0.692416, acc 0.537109, f1 0.375362\n",
      "2017-11-26T15:50:28.819354: step 20525, loss 0.687937, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:50:29.144813: step 20530, loss 0.679795, acc 0.587891, f1 0.435314\n",
      "2017-11-26T15:50:29.478877: step 20535, loss 0.684766, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:50:29.808437: step 20540, loss 0.688437, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:50:30.134371: step 20545, loss 0.689419, acc 0.550781, f1 0.391235\n",
      "2017-11-26T15:50:30.439995: step 20550, loss 0.686996, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:50:30.730186: step 20555, loss 0.69279, acc 0.536133, f1 0.374236\n",
      "2017-11-26T15:50:31.029869: step 20560, loss 0.684136, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:50:31.330748: step 20565, loss 0.690138, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:50:31.653494: step 20570, loss 0.687491, acc 0.557617, f1 0.399247\n",
      "2017-11-26T15:50:31.954431: step 20575, loss 0.688276, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:50:32.266145: step 20580, loss 0.686015, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:50:32.587496: step 20585, loss 0.683903, acc 0.580078, f1 0.425916\n",
      "2017-11-26T15:50:32.914824: step 20590, loss 0.686834, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:50:33.245960: step 20595, loss 0.686101, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:50:33.562803: step 20600, loss 0.680969, acc 0.584961, f1 0.431783\n",
      "2017-11-26T15:50:33.884153: step 20605, loss 0.685013, acc 0.578125, f1 0.423577\n",
      "2017-11-26T15:50:34.201338: step 20610, loss 0.690433, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:50:34.507335: step 20615, loss 0.689394, acc 0.549805, f1 0.390095\n",
      "2017-11-26T15:50:34.835755: step 20620, loss 0.68739, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:50:35.162586: step 20625, loss 0.686555, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:50:35.489164: step 20630, loss 0.687527, acc 0.558594, f1 0.400396\n",
      "2017-11-26T15:50:35.816655: step 20635, loss 0.689187, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:50:36.160663: step 20640, loss 0.691872, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:50:36.492432: step 20645, loss 0.683304, acc 0.580078, f1 0.425916\n",
      "2017-11-26T15:50:36.820592: step 20650, loss 0.680383, acc 0.583984, f1 0.430607\n",
      "2017-11-26T15:50:37.145621: step 20655, loss 0.681911, acc 0.578125, f1 0.423577\n",
      "2017-11-26T15:50:37.438745: step 20660, loss 0.688187, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:50:37.760572: step 20665, loss 0.690706, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:50:38.091157: step 20670, loss 0.693906, acc 0.529297, f1 0.366384\n",
      "2017-11-26T15:50:38.419698: step 20675, loss 0.685691, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:50:38.740728: step 20680, loss 0.687173, acc 0.563477, f1 0.406154\n",
      "2017-11-26T15:50:39.038845: step 20685, loss 0.685883, acc 0.566406, f1 0.40962\n",
      "2017-11-26T15:50:39.367752: step 20690, loss 0.685383, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:50:39.695696: step 20695, loss 0.69054, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:50:40.001307: step 20700, loss 0.688565, acc 0.552734, f1 0.393519\n",
      "2017-11-26T15:50:40.338340: step 20705, loss 0.685044, acc 0.571289, f1 0.415418\n",
      "2017-11-26T15:50:40.680364: step 20710, loss 0.683082, acc 0.575195, f1 0.420074\n",
      "2017-11-26T15:50:40.992156: step 20715, loss 0.692328, acc 0.535156, f1 0.373111\n",
      "2017-11-26T15:50:41.317135: step 20720, loss 0.684546, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:50:41.617246: step 20725, loss 0.689744, acc 0.547852, f1 0.387817\n",
      "2017-11-26T15:50:41.928541: step 20730, loss 0.687879, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:50:42.251729: step 20735, loss 0.688076, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:50:42.565710: step 20740, loss 0.686949, acc 0.561523, f1 0.403847\n",
      "2017-11-26T15:50:42.874452: step 20745, loss 0.683764, acc 0.575195, f1 0.420074\n",
      "2017-11-26T15:50:43.191399: step 20750, loss 0.690974, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:50:43.478043: step 20755, loss 0.68829, acc 0.554688, f1 0.395807\n",
      "2017-11-26T15:50:43.772386: step 20760, loss 0.690166, acc 0.544922, f1 0.384408\n",
      "2017-11-26T15:50:44.099496: step 20765, loss 0.687215, acc 0.560547, f1 0.402696\n",
      "2017-11-26T15:50:44.429963: step 20770, loss 0.68448, acc 0.572266, f1 0.416581\n",
      "2017-11-26T15:50:44.755318: step 20775, loss 0.692532, acc 0.542969, f1 0.38214\n",
      "2017-11-26T15:50:45.070899: step 20780, loss 0.693607, acc 0.53125, f1 0.368622\n",
      "2017-11-26T15:50:45.404916: step 20785, loss 0.696555, acc 0.512695, f1 0.347534\n",
      "2017-11-26T15:50:45.733899: step 20790, loss 0.687885, acc 0.556641, f1 0.398099\n",
      "2017-11-26T15:50:46.056422: step 20795, loss 0.687557, acc 0.55957, f1 0.401545\n",
      "2017-11-26T15:50:46.381563: step 20800, loss 0.690403, acc 0.545898, f1 0.385543\n",
      "2017-11-26T15:50:46.703409: step 20805, loss 0.694585, acc 0.532227, f1 0.369743\n",
      "2017-11-26T15:50:47.012930: step 20810, loss 0.690507, acc 0.546875, f1 0.386679\n",
      "2017-11-26T15:50:47.303182: step 20815, loss 0.682696, acc 0.583008, f1 0.429433\n",
      "2017-11-26T15:50:47.611576: step 20820, loss 0.688071, acc 0.555664, f1 0.396953\n",
      "2017-11-26T15:50:47.924940: step 20825, loss 0.686653, acc 0.56543, f1 0.408464\n",
      "2017-11-26T15:50:48.238500: step 20830, loss 0.691946, acc 0.541016, f1 0.379877\n",
      "2017-11-26T15:50:48.578783: step 20835, loss 0.69211, acc 0.539062, f1 0.377617\n",
      "2017-11-26T15:50:48.864900: step 20840, loss 0.681008, acc 0.587189, f1 0.434467\n",
      "\n",
      "Evaluation:\n",
      "loss 0.687738, acc 0.55893, f1 0.400903\n",
      "\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "# Define input data parameters\n",
    "\n",
    "# put Word2Vec on 590 million English Tweets using 52 dimensions.\n",
    "vocabulary_size = final_embeddings.shape[0]\n",
    "embedding_size = final_embeddings.shape[1]  # Dimension of the embedding vector.\n",
    "sequence_length=x_train_distance.shape[1]\n",
    "num_classes=3\n",
    "\n",
    "### Define Hyperparameters\n",
    "\n",
    "# filter_sizes: The number of words we want our convolutional filters to cover. \n",
    "# We will have num_filters for each size specified here. \n",
    "# For example, [3, 4, 5] means that we will have filters that slide over 3, 4 and 5 words respectively, for a total of 3 * num_filters filters.\n",
    "first_layer_filter_sizes = [4]\n",
    "second_layer_filter_sizes = [3]\n",
    "\n",
    "first_pool_window_sizes = [4]\n",
    "first_pool_strides = [2]\n",
    "\n",
    "num_filters = 200\n",
    "\n",
    "l2_reg_lambda=0.0  # No L2 norm\n",
    "\n",
    "batch_size = 1024\n",
    "num_epochs = 20\n",
    "\n",
    "num_checkpoints = 5\n",
    "print_train_every = 5\n",
    "evaluate_every = 10000000\n",
    "checkpoint_every = 10000000\n",
    "allow_soft_placement=True\n",
    "log_device_placement=False\n",
    "\n",
    "\n",
    "with graph.as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=allow_soft_placement,\n",
    "      log_device_placement=log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = SentimentTwoLayerCNNModel(\n",
    "            sequence_length=sequence_length,\n",
    "            num_classes=num_classes,\n",
    "            vocabulary_size=vocabulary_size,\n",
    "            embedding_size=embedding_size,\n",
    "            first_layer_filter_sizes=first_layer_filter_sizes,\n",
    "            second_layer_filter_sizes=second_layer_filter_sizes,\n",
    "            first_pool_strides=first_pool_strides,\n",
    "            num_filters = num_filters,\n",
    "            first_pool_window_sizes=first_pool_window_sizes,\n",
    "            l2_reg_lambda=0.01)\n",
    "        \n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdadeltaOptimizer(1.0)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=num_checkpoints)\n",
    "\n",
    "       # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "            }\n",
    "            _, step, cur_loss, cur_accuracy, y_pred = sess.run(\n",
    "                [train_op, global_step, cnn.loss, cnn.accuracy, cnn.predictions],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "\n",
    "            if step % print_train_every == 0:\n",
    "                f1 = f1_score(y_batch, y_pred, average = 'weighted')\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}, f1 {:g}\".format(time_str, step, cur_loss, cur_accuracy,\n",
    "                                                                    f1))\n",
    "\n",
    "\n",
    "        def dev_step_batch(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "            }\n",
    "            \n",
    "            step, cur_loss, cur_accuracy, y_pred = sess.run(\n",
    "                [global_step, cnn.loss, cnn.accuracy, cnn.predictions],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            f1 = f1_score(y_batch, y_pred, average = 'weighted')\n",
    "\n",
    "            return cur_loss, cur_accuracy, f1\n",
    "        \n",
    "        def dev_step_batch(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "            }\n",
    "            step, cur_loss, cur_accuracy, y_pred = sess.run(\n",
    "                [global_step, cnn.loss, cnn.accuracy, cnn.predictions],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            f1 = f1_score(y_batch, y_pred, average = 'weighted')\n",
    "            return cur_loss, cur_accuracy, f1\n",
    "        \n",
    "        \n",
    "        sess.run(cnn.embedding_init, feed_dict={cnn.embedding_placeholder: final_embeddings})\n",
    "        # Generate batches\n",
    "        batches = batch_iter(\n",
    "            list(zip(x_train_distance, y_train_distance)), batch_size, num_epochs)\n",
    "        \n",
    "        batches_test = list(batch_iter(\n",
    "            list(zip(x_test_distance, y_test_distance)), batch_size, 1))\n",
    "        \n",
    "        # Training loop. For each batch...\n",
    "        for batch, end_of_epoch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            \n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if end_of_epoch:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                total_loss=0\n",
    "                total_f1=0\n",
    "                total_accuracy=0\n",
    "                len_of_batch = int(len(batches_test))\n",
    "                for batch_test, end_of_epoch_test in batches_test:\n",
    "                    x_batch_test, y_batch_test = zip(*batch_test)\n",
    "                    cur_loss, cur_accuracy, cur_f1 = dev_step_batch(x_batch_test, y_batch_test)\n",
    "                    total_loss+=cur_loss\n",
    "                    total_accuracy+=cur_accuracy\n",
    "                    total_f1+=cur_f1\n",
    "                print(\"loss {:g}, acc {:g}, f1 {:g}\".format(total_loss/len_of_batch, total_accuracy/len_of_batch, total_f1/len_of_batch))\n",
    "                print(\"\")\n",
    "            if current_step % checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "        final_embeddings = cnn.embeddings_words.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.3149592442675012&quot;).pbtxt = 'node {\\n  name: &quot;input_x&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: -1\\n        }\\n        dim {\\n          size: 65\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;input_y&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: -1\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 46692\\n          }\\n          dim {\\n            size: 52\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;embedding_words&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 46692\\n        }\\n        dim {\\n          size: 52\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;embedding_words/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;embedding_words&quot;\\n  input: &quot;Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding_words&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;embedding_words/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;embedding_words&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding_words&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Const_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n          dim {\\n            size: 52\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;embedding_padding&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 1\\n        }\\n        dim {\\n          size: 52\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;embedding_padding/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;embedding_padding&quot;\\n  input: &quot;Const_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding_padding&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;embedding_padding/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;embedding_padding&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding_padding&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;word_embedding_placeholder&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 46692\\n        }\\n        dim {\\n          size: 52\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;embedding_words&quot;\\n  input: &quot;word_embedding_placeholder&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding_words&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;embedding/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;embedding&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;embedding_words/read&quot;\\n  input: &quot;embedding_padding/read&quot;\\n  input: &quot;embedding/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;embedding_lookup&quot;\\n  op: &quot;Gather&quot;\\n  input: &quot;embedding&quot;\\n  input: &quot;input_x&quot;\\n  attr {\\n    key: &quot;Tindices&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tparams&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_indices&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ExpandDims/dim&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: -1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ExpandDims&quot;\\n  op: &quot;ExpandDims&quot;\\n  input: &quot;embedding_lookup&quot;\\n  input: &quot;ExpandDims/dim&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tdim&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Const_2&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/truncated_normal/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\004\\\\000\\\\000\\\\0004\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\310\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/truncated_normal/mean&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/truncated_normal/stddev&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.10000000149011612\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/truncated_normal/TruncatedNormal&quot;\\n  op: &quot;TruncatedNormal&quot;\\n  input: &quot;conv-maxpool-1/truncated_normal/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/truncated_normal/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;conv-maxpool-1/truncated_normal/TruncatedNormal&quot;\\n  input: &quot;conv-maxpool-1/truncated_normal/stddev&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/truncated_normal&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;conv-maxpool-1/truncated_normal/mul&quot;\\n  input: &quot;conv-maxpool-1/truncated_normal/mean&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/W&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 4\\n        }\\n        dim {\\n          size: 52\\n        }\\n        dim {\\n          size: 1\\n        }\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/W/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;conv-maxpool-1/W&quot;\\n  input: &quot;conv-maxpool-1/truncated_normal&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-1/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/W/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;conv-maxpool-1/W&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-1/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 200\\n          }\\n        }\\n        float_val: 0.10000000149011612\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/b&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/b/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;conv-maxpool-1/b&quot;\\n  input: &quot;conv-maxpool-1/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-1/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/b/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;conv-maxpool-1/b&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-1/b&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/conv&quot;\\n  op: &quot;Conv2D&quot;\\n  input: &quot;ExpandDims&quot;\\n  input: &quot;conv-maxpool-1/W/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;VALID&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;conv-maxpool-1/conv&quot;\\n  input: &quot;conv-maxpool-1/b/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;conv-maxpool-1/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/pool&quot;\\n  op: &quot;MaxPool&quot;\\n  input: &quot;conv-maxpool-1/relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;ksize&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 4\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;VALID&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 2\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/truncated_normal/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\003\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\310\\\\000\\\\000\\\\000\\\\310\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/truncated_normal/mean&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/truncated_normal/stddev&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.10000000149011612\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/truncated_normal/TruncatedNormal&quot;\\n  op: &quot;TruncatedNormal&quot;\\n  input: &quot;conv-maxpool-2/truncated_normal/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/truncated_normal/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;conv-maxpool-2/truncated_normal/TruncatedNormal&quot;\\n  input: &quot;conv-maxpool-2/truncated_normal/stddev&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/truncated_normal&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;conv-maxpool-2/truncated_normal/mul&quot;\\n  input: &quot;conv-maxpool-2/truncated_normal/mean&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/W&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 3\\n        }\\n        dim {\\n          size: 1\\n        }\\n        dim {\\n          size: 200\\n        }\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/W/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;conv-maxpool-2/W&quot;\\n  input: &quot;conv-maxpool-2/truncated_normal&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-2/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/W/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;conv-maxpool-2/W&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-2/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 200\\n          }\\n        }\\n        float_val: 0.10000000149011612\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/b&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/b/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;conv-maxpool-2/b&quot;\\n  input: &quot;conv-maxpool-2/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-2/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/b/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;conv-maxpool-2/b&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-2/b&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/conv&quot;\\n  op: &quot;Conv2D&quot;\\n  input: &quot;conv-maxpool-1/pool&quot;\\n  input: &quot;conv-maxpool-2/W/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;VALID&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;conv-maxpool-2/conv&quot;\\n  input: &quot;conv-maxpool-2/b/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;conv-maxpool-2/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/pool&quot;\\n  op: &quot;MaxPool&quot;\\n  input: &quot;conv-maxpool-2/relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;ksize&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 28\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;VALID&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\310\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;conv-maxpool-2/pool&quot;\\n  input: &quot;Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\310\\\\000\\\\000\\\\000\\\\310\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.12247448414564133\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.12247448414564133\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform/max&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform/mul&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 200\\n        }\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden/W&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden/W&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 200\\n          }\\n        }\\n        float_val: 0.10000000149011612\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/b&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/b/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden/hidden/b&quot;\\n  input: &quot;hidden/hidden/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/hidden/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/b/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden/hidden/b&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/hidden/b&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/L2Loss&quot;\\n  op: &quot;L2Loss&quot;\\n  input: &quot;hidden/W/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Const_2&quot;\\n  input: &quot;hidden/hidden/L2Loss&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/L2Loss_1&quot;\\n  op: &quot;L2Loss&quot;\\n  input: &quot;hidden/hidden/b/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/add_1&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden/hidden/add&quot;\\n  input: &quot;hidden/hidden/L2Loss_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/xw_plus_b/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Reshape&quot;\\n  input: &quot;hidden/W/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/xw_plus_b&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;hidden/hidden/xw_plus_b/MatMul&quot;\\n  input: &quot;hidden/hidden/b/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;hidden/hidden/xw_plus_b&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\310\\\\000\\\\000\\\\000\\\\003\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.17192047834396362\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.17192047834396362\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;output/W/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;output/W/Initializer/random_uniform/max&quot;\\n  input: &quot;output/W/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;output/W/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;output/W/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;output/W/Initializer/random_uniform/mul&quot;\\n  input: &quot;output/W/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 200\\n        }\\n        dim {\\n          size: 3\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;output/W&quot;\\n  input: &quot;output/W/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;output/W&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        float_val: 0.10000000149011612\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/b&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 3\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/b/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;output/output/b&quot;\\n  input: &quot;output/output/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/output/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/b/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;output/output/b&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/output/b&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/L2Loss&quot;\\n  op: &quot;L2Loss&quot;\\n  input: &quot;output/W/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden/hidden/add_1&quot;\\n  input: &quot;output/output/L2Loss&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/L2Loss_1&quot;\\n  op: &quot;L2Loss&quot;\\n  input: &quot;output/output/b/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/add_1&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;output/output/add&quot;\\n  input: &quot;output/output/L2Loss_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/scores/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;hidden/hidden/Relu&quot;\\n  input: &quot;output/W/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/scores&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;output/output/scores/MatMul&quot;\\n  input: &quot;output/output/b/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/predictions/dimension&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/predictions&quot;\\n  op: &quot;ArgMax&quot;\\n  input: &quot;output/output/scores&quot;\\n  input: &quot;output/output/predictions/dimension&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;output_type&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;input_y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  op: &quot;SparseSoftmaxCrossEntropyWithLogits&quot;\\n  input: &quot;output/output/scores&quot;\\n  input: &quot;input_y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tlabels&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/Mean&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  input: &quot;loss/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/mul/x&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.009999999776482582\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;loss/mul/x&quot;\\n  input: &quot;output/output/add_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;loss/Mean&quot;\\n  input: &quot;loss/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;accuracy/Equal&quot;\\n  op: &quot;Equal&quot;\\n  input: &quot;output/output/predictions&quot;\\n  input: &quot;input_y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;accuracy/Cast&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;accuracy/Equal&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_BOOL\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;accuracy/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;accuracy/accuracy&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;accuracy/Cast&quot;\\n  input: &quot;accuracy/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;global_step/initial_value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;global_step&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;global_step/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;global_step&quot;\\n  input: &quot;global_step/initial_value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@global_step&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;global_step/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;global_step&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@global_step&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/Shape&quot;\\n  input: &quot;gradients/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/add_grad/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/add_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/add_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/loss/add_grad/Shape&quot;\\n  input: &quot;gradients/loss/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/add_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Fill&quot;\\n  input: &quot;gradients/loss/add_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/add_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/loss/add_grad/Sum&quot;\\n  input: &quot;gradients/loss/add_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/add_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Fill&quot;\\n  input: &quot;gradients/loss/add_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/add_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/loss/add_grad/Sum_1&quot;\\n  input: &quot;gradients/loss/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/add_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/loss/add_grad/Reshape&quot;\\n  input: &quot;^gradients/loss/add_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/loss/add_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/loss/add_grad/Reshape&quot;\\n  input: &quot;^gradients/loss/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/loss/add_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/add_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/loss/add_grad/Reshape_1&quot;\\n  input: &quot;^gradients/loss/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/loss/add_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/Mean_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/Mean_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/loss/add_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/loss/Mean_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/Mean_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/Mean_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/loss/Mean_grad/Reshape&quot;\\n  input: &quot;gradients/loss/Mean_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/Mean_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/Mean_grad/Shape_2&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/Mean_grad/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/Mean_grad/Prod&quot;\\n  op: &quot;Prod&quot;\\n  input: &quot;gradients/loss/Mean_grad/Shape_1&quot;\\n  input: &quot;gradients/loss/Mean_grad/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/Mean_grad/Const_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/Mean_grad/Prod_1&quot;\\n  op: &quot;Prod&quot;\\n  input: &quot;gradients/loss/Mean_grad/Shape_2&quot;\\n  input: &quot;gradients/loss/Mean_grad/Const_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/Mean_grad/Maximum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/Mean_grad/Maximum&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;gradients/loss/Mean_grad/Prod_1&quot;\\n  input: &quot;gradients/loss/Mean_grad/Maximum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/Mean_grad/floordiv&quot;\\n  op: &quot;FloorDiv&quot;\\n  input: &quot;gradients/loss/Mean_grad/Prod&quot;\\n  input: &quot;gradients/loss/Mean_grad/Maximum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/Mean_grad/Cast&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;gradients/loss/Mean_grad/floordiv&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/Mean_grad/truediv&quot;\\n  op: &quot;RealDiv&quot;\\n  input: &quot;gradients/loss/Mean_grad/Tile&quot;\\n  input: &quot;gradients/loss/Mean_grad/Cast&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/mul_grad/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/mul_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/mul_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/loss/mul_grad/Shape&quot;\\n  input: &quot;gradients/loss/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/mul_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/loss/add_grad/tuple/control_dependency_1&quot;\\n  input: &quot;output/output/add_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/mul_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/loss/mul_grad/mul&quot;\\n  input: &quot;gradients/loss/mul_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/mul_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/loss/mul_grad/Sum&quot;\\n  input: &quot;gradients/loss/mul_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/mul_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;loss/mul/x&quot;\\n  input: &quot;gradients/loss/add_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/mul_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/loss/mul_grad/mul_1&quot;\\n  input: &quot;gradients/loss/mul_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/mul_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/loss/mul_grad/Sum_1&quot;\\n  input: &quot;gradients/loss/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/mul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/loss/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/loss/mul_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/loss/mul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/loss/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/loss/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/loss/mul_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/mul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/loss/mul_grad/Reshape_1&quot;\\n  input: &quot;^gradients/loss/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/loss/mul_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/zeros_like&quot;\\n  op: &quot;ZerosLike&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient&quot;\\n  op: &quot;PreventGradient&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;message&quot;\\n    value {\\n      s: &quot;Currently there is no way to take the second derivative of sparse_softmax_cross_entropy_with_logits due to the fused implementation\\\\\\'s interaction with tf.gradients()&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: -1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims&quot;\\n  op: &quot;ExpandDims&quot;\\n  input: &quot;gradients/loss/Mean_grad/truediv&quot;\\n  input: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tdim&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims&quot;\\n  input: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/output/output/add_1_grad/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/output/output/add_1_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/output/output/add_1_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/output/output/add_1_grad/Shape&quot;\\n  input: &quot;gradients/output/output/add_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/output/output/add_1_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/loss/mul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/output/output/add_1_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/output/output/add_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/output/output/add_1_grad/Sum&quot;\\n  input: &quot;gradients/output/output/add_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/output/output/add_1_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/loss/mul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/output/output/add_1_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/output/output/add_1_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/output/output/add_1_grad/Sum_1&quot;\\n  input: &quot;gradients/output/output/add_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/output/output/add_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/output/output/add_1_grad/Reshape&quot;\\n  input: &quot;^gradients/output/output/add_1_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/output/output/add_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/output/output/add_1_grad/Reshape&quot;\\n  input: &quot;^gradients/output/output/add_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/output/output/add_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/output/output/add_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/output/output/add_1_grad/Reshape_1&quot;\\n  input: &quot;^gradients/output/output/add_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/output/output/add_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/output/output/scores_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/output/output/scores_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n  input: &quot;^gradients/output/output/scores_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/output/output/scores_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n  input: &quot;^gradients/output/output/scores_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/output/output/scores_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/output/output/scores_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/output/output/scores_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/output/output/scores_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/output/output/add_grad/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/output/output/add_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/output/output/add_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/output/output/add_grad/Shape&quot;\\n  input: &quot;gradients/output/output/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/output/output/add_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/output/output/add_1_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/output/output/add_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/output/output/add_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/output/output/add_grad/Sum&quot;\\n  input: &quot;gradients/output/output/add_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/output/output/add_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/output/output/add_1_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/output/output/add_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/output/output/add_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/output/output/add_grad/Sum_1&quot;\\n  input: &quot;gradients/output/output/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/output/output/add_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/output/output/add_grad/Reshape&quot;\\n  input: &quot;^gradients/output/output/add_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/output/output/add_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/output/output/add_grad/Reshape&quot;\\n  input: &quot;^gradients/output/output/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/output/output/add_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/output/output/add_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/output/output/add_grad/Reshape_1&quot;\\n  input: &quot;^gradients/output/output/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/output/output/add_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/output/output/L2Loss_1_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;output/output/b/read&quot;\\n  input: &quot;gradients/output/output/add_1_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/output/output/scores/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/output/output/scores_grad/tuple/control_dependency&quot;\\n  input: &quot;output/W/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/output/output/scores/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;hidden/hidden/Relu&quot;\\n  input: &quot;gradients/output/output/scores_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/output/output/scores/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/output/output/scores/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/output/output/scores/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/output/output/scores/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/output/output/scores/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/output/output/scores/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/output/output/scores/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/output/output/scores/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/output/output/scores/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/output/output/scores/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/output/output/scores/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hidden/hidden/add_1_grad/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hidden/hidden/add_1_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hidden/hidden/add_1_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/hidden/hidden/add_1_grad/Shape&quot;\\n  input: &quot;gradients/hidden/hidden/add_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hidden/hidden/add_1_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/output/output/add_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/hidden/hidden/add_1_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hidden/hidden/add_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/hidden/hidden/add_1_grad/Sum&quot;\\n  input: &quot;gradients/hidden/hidden/add_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hidden/hidden/add_1_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/output/output/add_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/hidden/hidden/add_1_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hidden/hidden/add_1_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/hidden/hidden/add_1_grad/Sum_1&quot;\\n  input: &quot;gradients/hidden/hidden/add_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hidden/hidden/add_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/hidden/hidden/add_1_grad/Reshape&quot;\\n  input: &quot;^gradients/hidden/hidden/add_1_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/hidden/hidden/add_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/hidden/hidden/add_1_grad/Reshape&quot;\\n  input: &quot;^gradients/hidden/hidden/add_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/hidden/hidden/add_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hidden/hidden/add_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/hidden/hidden/add_1_grad/Reshape_1&quot;\\n  input: &quot;^gradients/hidden/hidden/add_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/hidden/hidden/add_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/output/output/L2Loss_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;output/W/read&quot;\\n  input: &quot;gradients/output/output/add_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/output/output/scores_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/output/output/L2Loss_1_grad/mul&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/output/output/scores_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hidden/hidden/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/output/output/scores/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden/hidden/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hidden/hidden/add_grad/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hidden/hidden/add_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hidden/hidden/add_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/hidden/hidden/add_grad/Shape&quot;\\n  input: &quot;gradients/hidden/hidden/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hidden/hidden/add_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/hidden/hidden/add_1_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/hidden/hidden/add_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hidden/hidden/add_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/hidden/hidden/add_grad/Sum&quot;\\n  input: &quot;gradients/hidden/hidden/add_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hidden/hidden/add_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/hidden/hidden/add_1_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/hidden/hidden/add_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hidden/hidden/add_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/hidden/hidden/add_grad/Sum_1&quot;\\n  input: &quot;gradients/hidden/hidden/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hidden/hidden/add_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/hidden/hidden/add_grad/Reshape&quot;\\n  input: &quot;^gradients/hidden/hidden/add_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/hidden/hidden/add_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/hidden/hidden/add_grad/Reshape&quot;\\n  input: &quot;^gradients/hidden/hidden/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/hidden/hidden/add_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hidden/hidden/add_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/hidden/hidden/add_grad/Reshape_1&quot;\\n  input: &quot;^gradients/hidden/hidden/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/hidden/hidden/add_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hidden/hidden/L2Loss_1_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden/hidden/b/read&quot;\\n  input: &quot;gradients/hidden/hidden/add_1_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_1&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/output/output/scores/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/output/output/L2Loss_grad/mul&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/output/output/scores/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hidden/hidden/xw_plus_b_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/hidden/hidden/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hidden/hidden/xw_plus_b_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/hidden/hidden/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/hidden/hidden/xw_plus_b_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/hidden/hidden/xw_plus_b_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/hidden/hidden/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/hidden/hidden/xw_plus_b_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/hidden/hidden/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hidden/hidden/xw_plus_b_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/hidden/hidden/xw_plus_b_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/hidden/hidden/xw_plus_b_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/hidden/hidden/xw_plus_b_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hidden/hidden/L2Loss_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden/W/read&quot;\\n  input: &quot;gradients/hidden/hidden/add_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hidden/hidden/xw_plus_b/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/hidden/hidden/xw_plus_b_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden/W/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hidden/hidden/xw_plus_b/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Reshape&quot;\\n  input: &quot;gradients/hidden/hidden/xw_plus_b_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hidden/hidden/xw_plus_b/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/hidden/hidden/xw_plus_b/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/hidden/hidden/xw_plus_b/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/hidden/hidden/xw_plus_b/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/hidden/hidden/xw_plus_b/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/hidden/hidden/xw_plus_b/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/hidden/hidden/xw_plus_b/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hidden/hidden/xw_plus_b/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/hidden/hidden/xw_plus_b/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/hidden/hidden/xw_plus_b/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/hidden/hidden/xw_plus_b/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_2&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/hidden/hidden/L2Loss_1_grad/mul&quot;\\n  input: &quot;gradients/hidden/hidden/xw_plus_b_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/hidden/hidden/L2Loss_1_grad/mul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Reshape_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;conv-maxpool-2/pool&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Reshape_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/hidden/hidden/xw_plus_b/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Reshape_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_3&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/hidden/hidden/L2Loss_grad/mul&quot;\\n  input: &quot;gradients/hidden/hidden/xw_plus_b/MatMul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/hidden/hidden/L2Loss_grad/mul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/conv-maxpool-2/pool_grad/MaxPoolGrad&quot;\\n  op: &quot;MaxPoolGrad&quot;\\n  input: &quot;conv-maxpool-2/relu&quot;\\n  input: &quot;conv-maxpool-2/pool&quot;\\n  input: &quot;gradients/Reshape_grad/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;ksize&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 28\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;VALID&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/conv-maxpool-2/relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/conv-maxpool-2/pool_grad/MaxPoolGrad&quot;\\n  input: &quot;conv-maxpool-2/relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/conv-maxpool-2/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/conv-maxpool-2/relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/conv-maxpool-2/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/conv-maxpool-2/relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/conv-maxpool-2/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/conv-maxpool-2/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/conv-maxpool-2/relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/conv-maxpool-2/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/conv-maxpool-2/relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/conv-maxpool-2/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/conv-maxpool-2/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/conv-maxpool-2/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/conv-maxpool-2/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/conv-maxpool-2/conv_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;conv-maxpool-1/pool&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/conv-maxpool-2/conv_grad/Conv2DBackpropInput&quot;\\n  op: &quot;Conv2DBackpropInput&quot;\\n  input: &quot;gradients/conv-maxpool-2/conv_grad/Shape&quot;\\n  input: &quot;conv-maxpool-2/W/read&quot;\\n  input: &quot;gradients/conv-maxpool-2/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;VALID&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/conv-maxpool-2/conv_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\003\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\310\\\\000\\\\000\\\\000\\\\310\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/conv-maxpool-2/conv_grad/Conv2DBackpropFilter&quot;\\n  op: &quot;Conv2DBackpropFilter&quot;\\n  input: &quot;conv-maxpool-1/pool&quot;\\n  input: &quot;gradients/conv-maxpool-2/conv_grad/Shape_1&quot;\\n  input: &quot;gradients/conv-maxpool-2/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;VALID&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/conv-maxpool-2/conv_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/conv-maxpool-2/conv_grad/Conv2DBackpropInput&quot;\\n  input: &quot;^gradients/conv-maxpool-2/conv_grad/Conv2DBackpropFilter&quot;\\n}\\nnode {\\n  name: &quot;gradients/conv-maxpool-2/conv_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/conv-maxpool-2/conv_grad/Conv2DBackpropInput&quot;\\n  input: &quot;^gradients/conv-maxpool-2/conv_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/conv-maxpool-2/conv_grad/Conv2DBackpropInput&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/conv-maxpool-2/conv_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/conv-maxpool-2/conv_grad/Conv2DBackpropFilter&quot;\\n  input: &quot;^gradients/conv-maxpool-2/conv_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/conv-maxpool-2/conv_grad/Conv2DBackpropFilter&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/conv-maxpool-1/pool_grad/MaxPoolGrad&quot;\\n  op: &quot;MaxPoolGrad&quot;\\n  input: &quot;conv-maxpool-1/relu&quot;\\n  input: &quot;conv-maxpool-1/pool&quot;\\n  input: &quot;gradients/conv-maxpool-2/conv_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;ksize&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 4\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;VALID&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 2\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/conv-maxpool-1/relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/conv-maxpool-1/pool_grad/MaxPoolGrad&quot;\\n  input: &quot;conv-maxpool-1/relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/conv-maxpool-1/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/conv-maxpool-1/relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/conv-maxpool-1/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/conv-maxpool-1/relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/conv-maxpool-1/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/conv-maxpool-1/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/conv-maxpool-1/relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/conv-maxpool-1/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/conv-maxpool-1/relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/conv-maxpool-1/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/conv-maxpool-1/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/conv-maxpool-1/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/conv-maxpool-1/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/conv-maxpool-1/conv_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;ExpandDims&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/conv-maxpool-1/conv_grad/Conv2DBackpropInput&quot;\\n  op: &quot;Conv2DBackpropInput&quot;\\n  input: &quot;gradients/conv-maxpool-1/conv_grad/Shape&quot;\\n  input: &quot;conv-maxpool-1/W/read&quot;\\n  input: &quot;gradients/conv-maxpool-1/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;VALID&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/conv-maxpool-1/conv_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\004\\\\000\\\\000\\\\0004\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\310\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/conv-maxpool-1/conv_grad/Conv2DBackpropFilter&quot;\\n  op: &quot;Conv2DBackpropFilter&quot;\\n  input: &quot;ExpandDims&quot;\\n  input: &quot;gradients/conv-maxpool-1/conv_grad/Shape_1&quot;\\n  input: &quot;gradients/conv-maxpool-1/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;VALID&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/conv-maxpool-1/conv_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/conv-maxpool-1/conv_grad/Conv2DBackpropInput&quot;\\n  input: &quot;^gradients/conv-maxpool-1/conv_grad/Conv2DBackpropFilter&quot;\\n}\\nnode {\\n  name: &quot;gradients/conv-maxpool-1/conv_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/conv-maxpool-1/conv_grad/Conv2DBackpropInput&quot;\\n  input: &quot;^gradients/conv-maxpool-1/conv_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/conv-maxpool-1/conv_grad/Conv2DBackpropInput&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/conv-maxpool-1/conv_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/conv-maxpool-1/conv_grad/Conv2DBackpropFilter&quot;\\n  input: &quot;^gradients/conv-maxpool-1/conv_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/conv-maxpool-1/conv_grad/Conv2DBackpropFilter&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/ExpandDims_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;embedding_lookup&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/ExpandDims_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/conv-maxpool-1/conv_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/ExpandDims_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/embedding_lookup_grad/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT64\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;e\\\\266\\\\000\\\\000\\\\000\\\\000\\\\000\\\\0004\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/embedding_lookup_grad/ToInt32&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;gradients/embedding_lookup_grad/Shape&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/embedding_lookup_grad/Size&quot;\\n  op: &quot;Size&quot;\\n  input: &quot;input_x&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/embedding_lookup_grad/ExpandDims/dim&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/embedding_lookup_grad/ExpandDims&quot;\\n  op: &quot;ExpandDims&quot;\\n  input: &quot;gradients/embedding_lookup_grad/Size&quot;\\n  input: &quot;gradients/embedding_lookup_grad/ExpandDims/dim&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tdim&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/embedding_lookup_grad/strided_slice/stack&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/embedding_lookup_grad/strided_slice/stack_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/embedding_lookup_grad/strided_slice/stack_2&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/embedding_lookup_grad/strided_slice&quot;\\n  op: &quot;StridedSlice&quot;\\n  input: &quot;gradients/embedding_lookup_grad/ToInt32&quot;\\n  input: &quot;gradients/embedding_lookup_grad/strided_slice/stack&quot;\\n  input: &quot;gradients/embedding_lookup_grad/strided_slice/stack_1&quot;\\n  input: &quot;gradients/embedding_lookup_grad/strided_slice/stack_2&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;begin_mask&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;ellipsis_mask&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;end_mask&quot;\\n    value {\\n      i: 1\\n    }\\n  }\\n  attr {\\n    key: &quot;new_axis_mask&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;shrink_axis_mask&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/embedding_lookup_grad/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/embedding_lookup_grad/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;gradients/embedding_lookup_grad/ExpandDims&quot;\\n  input: &quot;gradients/embedding_lookup_grad/strided_slice&quot;\\n  input: &quot;gradients/embedding_lookup_grad/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/embedding_lookup_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/ExpandDims_grad/Reshape&quot;\\n  input: &quot;gradients/embedding_lookup_grad/concat&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/embedding_lookup_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;input_x&quot;\\n  input: &quot;gradients/embedding_lookup_grad/ExpandDims&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/embedding_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/embedding_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;embedding/axis&quot;\\n  input: &quot;gradients/embedding_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/embedding_grad/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;d\\\\266\\\\000\\\\0004\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/embedding_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\0004\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/embedding_grad/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/embedding_grad/Gather&quot;\\n  op: &quot;Gather&quot;\\n  input: &quot;gradients/embedding_grad/Shape&quot;\\n  input: &quot;gradients/embedding_grad/mod&quot;\\n  attr {\\n    key: &quot;Tindices&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tparams&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_indices&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/embedding_grad/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;gradients/embedding_grad/Const&quot;\\n  input: &quot;gradients/embedding_grad/Gather&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/embedding_grad/GreaterEqual&quot;\\n  op: &quot;GreaterEqual&quot;\\n  input: &quot;gradients/embedding_lookup_grad/Reshape_1&quot;\\n  input: &quot;gradients/embedding_grad/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/embedding_grad/Less&quot;\\n  op: &quot;Less&quot;\\n  input: &quot;gradients/embedding_lookup_grad/Reshape_1&quot;\\n  input: &quot;gradients/embedding_grad/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/embedding_grad/LogicalAnd&quot;\\n  op: &quot;LogicalAnd&quot;\\n  input: &quot;gradients/embedding_grad/GreaterEqual&quot;\\n  input: &quot;gradients/embedding_grad/Less&quot;\\n}\\nnode {\\n  name: &quot;gradients/embedding_grad/Where&quot;\\n  op: &quot;Where&quot;\\n  input: &quot;gradients/embedding_grad/LogicalAnd&quot;\\n}\\nnode {\\n  name: &quot;gradients/embedding_grad/Squeeze&quot;\\n  op: &quot;Squeeze&quot;\\n  input: &quot;gradients/embedding_grad/Where&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n  attr {\\n    key: &quot;squeeze_dims&quot;\\n    value {\\n      list {\\n        i: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/embedding_grad/Gather_1&quot;\\n  op: &quot;Gather&quot;\\n  input: &quot;gradients/embedding_lookup_grad/Reshape_1&quot;\\n  input: &quot;gradients/embedding_grad/Squeeze&quot;\\n  attr {\\n    key: &quot;Tindices&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n  attr {\\n    key: &quot;Tparams&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_indices&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/embedding_grad/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;gradients/embedding_grad/Gather_1&quot;\\n  input: &quot;gradients/embedding_grad/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/embedding_grad/Gather_2&quot;\\n  op: &quot;Gather&quot;\\n  input: &quot;gradients/embedding_lookup_grad/Reshape&quot;\\n  input: &quot;gradients/embedding_grad/Squeeze&quot;\\n  attr {\\n    key: &quot;Tindices&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n  attr {\\n    key: &quot;Tparams&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_indices&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/embedding_grad/Gather_3&quot;\\n  op: &quot;Gather&quot;\\n  input: &quot;gradients/embedding_grad/Shape_1&quot;\\n  input: &quot;gradients/embedding_grad/mod&quot;\\n  attr {\\n    key: &quot;Tindices&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tparams&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_indices&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/embedding_grad/add_1&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;gradients/embedding_grad/add&quot;\\n  input: &quot;gradients/embedding_grad/Gather_3&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/embedding_grad/GreaterEqual_1&quot;\\n  op: &quot;GreaterEqual&quot;\\n  input: &quot;gradients/embedding_lookup_grad/Reshape_1&quot;\\n  input: &quot;gradients/embedding_grad/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/embedding_grad/Less_1&quot;\\n  op: &quot;Less&quot;\\n  input: &quot;gradients/embedding_lookup_grad/Reshape_1&quot;\\n  input: &quot;gradients/embedding_grad/add_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/embedding_grad/LogicalAnd_1&quot;\\n  op: &quot;LogicalAnd&quot;\\n  input: &quot;gradients/embedding_grad/GreaterEqual_1&quot;\\n  input: &quot;gradients/embedding_grad/Less_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/embedding_grad/Where_1&quot;\\n  op: &quot;Where&quot;\\n  input: &quot;gradients/embedding_grad/LogicalAnd_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/embedding_grad/Squeeze_1&quot;\\n  op: &quot;Squeeze&quot;\\n  input: &quot;gradients/embedding_grad/Where_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n  attr {\\n    key: &quot;squeeze_dims&quot;\\n    value {\\n      list {\\n        i: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/embedding_grad/Gather_4&quot;\\n  op: &quot;Gather&quot;\\n  input: &quot;gradients/embedding_lookup_grad/Reshape_1&quot;\\n  input: &quot;gradients/embedding_grad/Squeeze_1&quot;\\n  attr {\\n    key: &quot;Tindices&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n  attr {\\n    key: &quot;Tparams&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_indices&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/embedding_grad/sub_1&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;gradients/embedding_grad/Gather_4&quot;\\n  input: &quot;gradients/embedding_grad/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/embedding_grad/Gather_5&quot;\\n  op: &quot;Gather&quot;\\n  input: &quot;gradients/embedding_lookup_grad/Reshape&quot;\\n  input: &quot;gradients/embedding_grad/Squeeze_1&quot;\\n  attr {\\n    key: &quot;Tindices&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n  attr {\\n    key: &quot;Tparams&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_indices&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/embedding_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/embedding_grad/Gather_2&quot;\\n  input: &quot;^gradients/embedding_grad/Gather_5&quot;\\n}\\nnode {\\n  name: &quot;gradients/embedding_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/embedding_grad/Gather_2&quot;\\n  input: &quot;^gradients/embedding_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/embedding_grad/Gather_2&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/embedding_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/embedding_grad/Gather_5&quot;\\n  input: &quot;^gradients/embedding_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/embedding_grad/Gather_5&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;embedding_words/Adadelta/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding_words&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 46692\\n          }\\n          dim {\\n            size: 52\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;embedding_words/Adadelta&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding_words&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 46692\\n        }\\n        dim {\\n          size: 52\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;embedding_words/Adadelta/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;embedding_words/Adadelta&quot;\\n  input: &quot;embedding_words/Adadelta/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding_words&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;embedding_words/Adadelta/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;embedding_words/Adadelta&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding_words&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;embedding_words/Adadelta_1/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding_words&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 46692\\n          }\\n          dim {\\n            size: 52\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;embedding_words/Adadelta_1&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding_words&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 46692\\n        }\\n        dim {\\n          size: 52\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;embedding_words/Adadelta_1/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;embedding_words/Adadelta_1&quot;\\n  input: &quot;embedding_words/Adadelta_1/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding_words&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;embedding_words/Adadelta_1/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;embedding_words/Adadelta_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding_words&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/W/Adadelta/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-1/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n          dim {\\n            size: 52\\n          }\\n          dim {\\n            size: 1\\n          }\\n          dim {\\n            size: 200\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/W/Adadelta&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-1/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 4\\n        }\\n        dim {\\n          size: 52\\n        }\\n        dim {\\n          size: 1\\n        }\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/W/Adadelta/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;conv-maxpool-1/W/Adadelta&quot;\\n  input: &quot;conv-maxpool-1/W/Adadelta/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-1/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/W/Adadelta/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;conv-maxpool-1/W/Adadelta&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-1/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/W/Adadelta_1/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-1/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n          dim {\\n            size: 52\\n          }\\n          dim {\\n            size: 1\\n          }\\n          dim {\\n            size: 200\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/W/Adadelta_1&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-1/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 4\\n        }\\n        dim {\\n          size: 52\\n        }\\n        dim {\\n          size: 1\\n        }\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/W/Adadelta_1/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;conv-maxpool-1/W/Adadelta_1&quot;\\n  input: &quot;conv-maxpool-1/W/Adadelta_1/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-1/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/W/Adadelta_1/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;conv-maxpool-1/W/Adadelta_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-1/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/b/Adadelta/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-1/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 200\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/b/Adadelta&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-1/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/b/Adadelta/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;conv-maxpool-1/b/Adadelta&quot;\\n  input: &quot;conv-maxpool-1/b/Adadelta/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-1/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/b/Adadelta/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;conv-maxpool-1/b/Adadelta&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-1/b&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/b/Adadelta_1/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-1/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 200\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/b/Adadelta_1&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-1/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/b/Adadelta_1/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;conv-maxpool-1/b/Adadelta_1&quot;\\n  input: &quot;conv-maxpool-1/b/Adadelta_1/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-1/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/b/Adadelta_1/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;conv-maxpool-1/b/Adadelta_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-1/b&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/W/Adadelta/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-2/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n          dim {\\n            size: 1\\n          }\\n          dim {\\n            size: 200\\n          }\\n          dim {\\n            size: 200\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/W/Adadelta&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-2/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 3\\n        }\\n        dim {\\n          size: 1\\n        }\\n        dim {\\n          size: 200\\n        }\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/W/Adadelta/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;conv-maxpool-2/W/Adadelta&quot;\\n  input: &quot;conv-maxpool-2/W/Adadelta/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-2/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/W/Adadelta/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;conv-maxpool-2/W/Adadelta&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-2/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/W/Adadelta_1/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-2/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n          dim {\\n            size: 1\\n          }\\n          dim {\\n            size: 200\\n          }\\n          dim {\\n            size: 200\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/W/Adadelta_1&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-2/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 3\\n        }\\n        dim {\\n          size: 1\\n        }\\n        dim {\\n          size: 200\\n        }\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/W/Adadelta_1/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;conv-maxpool-2/W/Adadelta_1&quot;\\n  input: &quot;conv-maxpool-2/W/Adadelta_1/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-2/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/W/Adadelta_1/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;conv-maxpool-2/W/Adadelta_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-2/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/b/Adadelta/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-2/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 200\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/b/Adadelta&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-2/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/b/Adadelta/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;conv-maxpool-2/b/Adadelta&quot;\\n  input: &quot;conv-maxpool-2/b/Adadelta/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-2/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/b/Adadelta/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;conv-maxpool-2/b/Adadelta&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-2/b&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/b/Adadelta_1/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-2/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 200\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/b/Adadelta_1&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-2/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/b/Adadelta_1/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;conv-maxpool-2/b/Adadelta_1&quot;\\n  input: &quot;conv-maxpool-2/b/Adadelta_1/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-2/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/b/Adadelta_1/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;conv-maxpool-2/b/Adadelta_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-2/b&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Adadelta/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 200\\n          }\\n          dim {\\n            size: 200\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Adadelta&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 200\\n        }\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Adadelta/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden/W/Adadelta&quot;\\n  input: &quot;hidden/W/Adadelta/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Adadelta/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden/W/Adadelta&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Adadelta_1/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 200\\n          }\\n          dim {\\n            size: 200\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Adadelta_1&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 200\\n        }\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Adadelta_1/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden/W/Adadelta_1&quot;\\n  input: &quot;hidden/W/Adadelta_1/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Adadelta_1/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden/W/Adadelta_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/b/Adadelta/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/hidden/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 200\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/b/Adadelta&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/hidden/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/b/Adadelta/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden/hidden/b/Adadelta&quot;\\n  input: &quot;hidden/hidden/b/Adadelta/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/hidden/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/b/Adadelta/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden/hidden/b/Adadelta&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/hidden/b&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/b/Adadelta_1/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/hidden/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 200\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/b/Adadelta_1&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/hidden/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/b/Adadelta_1/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden/hidden/b/Adadelta_1&quot;\\n  input: &quot;hidden/hidden/b/Adadelta_1/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/hidden/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/b/Adadelta_1/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden/hidden/b/Adadelta_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/hidden/b&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Adadelta/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 200\\n          }\\n          dim {\\n            size: 3\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Adadelta&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 200\\n        }\\n        dim {\\n          size: 3\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Adadelta/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;output/W/Adadelta&quot;\\n  input: &quot;output/W/Adadelta/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Adadelta/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;output/W/Adadelta&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Adadelta_1/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 200\\n          }\\n          dim {\\n            size: 3\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Adadelta_1&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 200\\n        }\\n        dim {\\n          size: 3\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Adadelta_1/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;output/W/Adadelta_1&quot;\\n  input: &quot;output/W/Adadelta_1/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Adadelta_1/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;output/W/Adadelta_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/b/Adadelta/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/output/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/b/Adadelta&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/output/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 3\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/b/Adadelta/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;output/output/b/Adadelta&quot;\\n  input: &quot;output/output/b/Adadelta/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/output/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/b/Adadelta/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;output/output/b/Adadelta&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/output/b&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/b/Adadelta_1/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/output/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/b/Adadelta_1&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/output/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 3\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/b/Adadelta_1/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;output/output/b/Adadelta_1&quot;\\n  input: &quot;output/output/b/Adadelta_1/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/output/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/b/Adadelta_1/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;output/output/b/Adadelta_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/output/b&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adadelta/lr&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adadelta/rho&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.949999988079071\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adadelta/epsilon&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 9.99999993922529e-09\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adadelta/update_embedding_words/Unique&quot;\\n  op: &quot;Unique&quot;\\n  input: &quot;gradients/embedding_grad/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding_words&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;out_idx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adadelta/update_embedding_words/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Adadelta/update_embedding_words/Unique&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding_words&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adadelta/update_embedding_words/strided_slice/stack&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding_words&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adadelta/update_embedding_words/strided_slice/stack_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding_words&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adadelta/update_embedding_words/strided_slice/stack_2&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding_words&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adadelta/update_embedding_words/strided_slice&quot;\\n  op: &quot;StridedSlice&quot;\\n  input: &quot;Adadelta/update_embedding_words/Shape&quot;\\n  input: &quot;Adadelta/update_embedding_words/strided_slice/stack&quot;\\n  input: &quot;Adadelta/update_embedding_words/strided_slice/stack_1&quot;\\n  input: &quot;Adadelta/update_embedding_words/strided_slice/stack_2&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding_words&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;begin_mask&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;ellipsis_mask&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;end_mask&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;new_axis_mask&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;shrink_axis_mask&quot;\\n    value {\\n      i: 1\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adadelta/update_embedding_words/UnsortedSegmentSum&quot;\\n  op: &quot;UnsortedSegmentSum&quot;\\n  input: &quot;gradients/embedding_grad/tuple/control_dependency&quot;\\n  input: &quot;Adadelta/update_embedding_words/Unique:1&quot;\\n  input: &quot;Adadelta/update_embedding_words/strided_slice&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tindices&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding_words&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adadelta/update_embedding_words/SparseApplyAdadelta&quot;\\n  op: &quot;SparseApplyAdadelta&quot;\\n  input: &quot;embedding_words&quot;\\n  input: &quot;embedding_words/Adadelta&quot;\\n  input: &quot;embedding_words/Adadelta_1&quot;\\n  input: &quot;Adadelta/lr&quot;\\n  input: &quot;Adadelta/rho&quot;\\n  input: &quot;Adadelta/epsilon&quot;\\n  input: &quot;Adadelta/update_embedding_words/UnsortedSegmentSum&quot;\\n  input: &quot;Adadelta/update_embedding_words/Unique&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tindices&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding_words&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adadelta/update_conv-maxpool-1/W/ApplyAdadelta&quot;\\n  op: &quot;ApplyAdadelta&quot;\\n  input: &quot;conv-maxpool-1/W&quot;\\n  input: &quot;conv-maxpool-1/W/Adadelta&quot;\\n  input: &quot;conv-maxpool-1/W/Adadelta_1&quot;\\n  input: &quot;Adadelta/lr&quot;\\n  input: &quot;Adadelta/rho&quot;\\n  input: &quot;Adadelta/epsilon&quot;\\n  input: &quot;gradients/conv-maxpool-1/conv_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-1/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adadelta/update_conv-maxpool-1/b/ApplyAdadelta&quot;\\n  op: &quot;ApplyAdadelta&quot;\\n  input: &quot;conv-maxpool-1/b&quot;\\n  input: &quot;conv-maxpool-1/b/Adadelta&quot;\\n  input: &quot;conv-maxpool-1/b/Adadelta_1&quot;\\n  input: &quot;Adadelta/lr&quot;\\n  input: &quot;Adadelta/rho&quot;\\n  input: &quot;Adadelta/epsilon&quot;\\n  input: &quot;gradients/conv-maxpool-1/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-1/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adadelta/update_conv-maxpool-2/W/ApplyAdadelta&quot;\\n  op: &quot;ApplyAdadelta&quot;\\n  input: &quot;conv-maxpool-2/W&quot;\\n  input: &quot;conv-maxpool-2/W/Adadelta&quot;\\n  input: &quot;conv-maxpool-2/W/Adadelta_1&quot;\\n  input: &quot;Adadelta/lr&quot;\\n  input: &quot;Adadelta/rho&quot;\\n  input: &quot;Adadelta/epsilon&quot;\\n  input: &quot;gradients/conv-maxpool-2/conv_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-2/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adadelta/update_conv-maxpool-2/b/ApplyAdadelta&quot;\\n  op: &quot;ApplyAdadelta&quot;\\n  input: &quot;conv-maxpool-2/b&quot;\\n  input: &quot;conv-maxpool-2/b/Adadelta&quot;\\n  input: &quot;conv-maxpool-2/b/Adadelta_1&quot;\\n  input: &quot;Adadelta/lr&quot;\\n  input: &quot;Adadelta/rho&quot;\\n  input: &quot;Adadelta/epsilon&quot;\\n  input: &quot;gradients/conv-maxpool-2/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-2/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adadelta/update_hidden/W/ApplyAdadelta&quot;\\n  op: &quot;ApplyAdadelta&quot;\\n  input: &quot;hidden/W&quot;\\n  input: &quot;hidden/W/Adadelta&quot;\\n  input: &quot;hidden/W/Adadelta_1&quot;\\n  input: &quot;Adadelta/lr&quot;\\n  input: &quot;Adadelta/rho&quot;\\n  input: &quot;Adadelta/epsilon&quot;\\n  input: &quot;gradients/AddN_3&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adadelta/update_hidden/hidden/b/ApplyAdadelta&quot;\\n  op: &quot;ApplyAdadelta&quot;\\n  input: &quot;hidden/hidden/b&quot;\\n  input: &quot;hidden/hidden/b/Adadelta&quot;\\n  input: &quot;hidden/hidden/b/Adadelta_1&quot;\\n  input: &quot;Adadelta/lr&quot;\\n  input: &quot;Adadelta/rho&quot;\\n  input: &quot;Adadelta/epsilon&quot;\\n  input: &quot;gradients/AddN_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/hidden/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adadelta/update_output/W/ApplyAdadelta&quot;\\n  op: &quot;ApplyAdadelta&quot;\\n  input: &quot;output/W&quot;\\n  input: &quot;output/W/Adadelta&quot;\\n  input: &quot;output/W/Adadelta_1&quot;\\n  input: &quot;Adadelta/lr&quot;\\n  input: &quot;Adadelta/rho&quot;\\n  input: &quot;Adadelta/epsilon&quot;\\n  input: &quot;gradients/AddN_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adadelta/update_output/output/b/ApplyAdadelta&quot;\\n  op: &quot;ApplyAdadelta&quot;\\n  input: &quot;output/output/b&quot;\\n  input: &quot;output/output/b/Adadelta&quot;\\n  input: &quot;output/output/b/Adadelta_1&quot;\\n  input: &quot;Adadelta/lr&quot;\\n  input: &quot;Adadelta/rho&quot;\\n  input: &quot;Adadelta/epsilon&quot;\\n  input: &quot;gradients/AddN&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/output/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adadelta/update&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^Adadelta/update_embedding_words/SparseApplyAdadelta&quot;\\n  input: &quot;^Adadelta/update_conv-maxpool-1/W/ApplyAdadelta&quot;\\n  input: &quot;^Adadelta/update_conv-maxpool-1/b/ApplyAdadelta&quot;\\n  input: &quot;^Adadelta/update_conv-maxpool-2/W/ApplyAdadelta&quot;\\n  input: &quot;^Adadelta/update_conv-maxpool-2/b/ApplyAdadelta&quot;\\n  input: &quot;^Adadelta/update_hidden/W/ApplyAdadelta&quot;\\n  input: &quot;^Adadelta/update_hidden/hidden/b/ApplyAdadelta&quot;\\n  input: &quot;^Adadelta/update_output/W/ApplyAdadelta&quot;\\n  input: &quot;^Adadelta/update_output/output/b/ApplyAdadelta&quot;\\n}\\nnode {\\n  name: &quot;Adadelta/value&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^Adadelta/update&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@global_step&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adadelta&quot;\\n  op: &quot;AssignAdd&quot;\\n  input: &quot;global_step&quot;\\n  input: &quot;Adadelta/value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@global_step&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n        }\\n        string_val: &quot;model&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 29\\n          }\\n        }\\n        string_val: &quot;conv-maxpool-1/W&quot;\\n        string_val: &quot;conv-maxpool-1/W/Adadelta&quot;\\n        string_val: &quot;conv-maxpool-1/W/Adadelta_1&quot;\\n        string_val: &quot;conv-maxpool-1/b&quot;\\n        string_val: &quot;conv-maxpool-1/b/Adadelta&quot;\\n        string_val: &quot;conv-maxpool-1/b/Adadelta_1&quot;\\n        string_val: &quot;conv-maxpool-2/W&quot;\\n        string_val: &quot;conv-maxpool-2/W/Adadelta&quot;\\n        string_val: &quot;conv-maxpool-2/W/Adadelta_1&quot;\\n        string_val: &quot;conv-maxpool-2/b&quot;\\n        string_val: &quot;conv-maxpool-2/b/Adadelta&quot;\\n        string_val: &quot;conv-maxpool-2/b/Adadelta_1&quot;\\n        string_val: &quot;embedding_padding&quot;\\n        string_val: &quot;embedding_words&quot;\\n        string_val: &quot;embedding_words/Adadelta&quot;\\n        string_val: &quot;embedding_words/Adadelta_1&quot;\\n        string_val: &quot;global_step&quot;\\n        string_val: &quot;hidden/W&quot;\\n        string_val: &quot;hidden/W/Adadelta&quot;\\n        string_val: &quot;hidden/W/Adadelta_1&quot;\\n        string_val: &quot;hidden/hidden/b&quot;\\n        string_val: &quot;hidden/hidden/b/Adadelta&quot;\\n        string_val: &quot;hidden/hidden/b/Adadelta_1&quot;\\n        string_val: &quot;output/W&quot;\\n        string_val: &quot;output/W/Adadelta&quot;\\n        string_val: &quot;output/W/Adadelta_1&quot;\\n        string_val: &quot;output/output/b&quot;\\n        string_val: &quot;output/output/b/Adadelta&quot;\\n        string_val: &quot;output/output/b/Adadelta_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 29\\n          }\\n        }\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2&quot;\\n  op: &quot;SaveV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/SaveV2/tensor_names&quot;\\n  input: &quot;save/SaveV2/shape_and_slices&quot;\\n  input: &quot;conv-maxpool-1/W&quot;\\n  input: &quot;conv-maxpool-1/W/Adadelta&quot;\\n  input: &quot;conv-maxpool-1/W/Adadelta_1&quot;\\n  input: &quot;conv-maxpool-1/b&quot;\\n  input: &quot;conv-maxpool-1/b/Adadelta&quot;\\n  input: &quot;conv-maxpool-1/b/Adadelta_1&quot;\\n  input: &quot;conv-maxpool-2/W&quot;\\n  input: &quot;conv-maxpool-2/W/Adadelta&quot;\\n  input: &quot;conv-maxpool-2/W/Adadelta_1&quot;\\n  input: &quot;conv-maxpool-2/b&quot;\\n  input: &quot;conv-maxpool-2/b/Adadelta&quot;\\n  input: &quot;conv-maxpool-2/b/Adadelta_1&quot;\\n  input: &quot;embedding_padding&quot;\\n  input: &quot;embedding_words&quot;\\n  input: &quot;embedding_words/Adadelta&quot;\\n  input: &quot;embedding_words/Adadelta_1&quot;\\n  input: &quot;global_step&quot;\\n  input: &quot;hidden/W&quot;\\n  input: &quot;hidden/W/Adadelta&quot;\\n  input: &quot;hidden/W/Adadelta_1&quot;\\n  input: &quot;hidden/hidden/b&quot;\\n  input: &quot;hidden/hidden/b/Adadelta&quot;\\n  input: &quot;hidden/hidden/b/Adadelta_1&quot;\\n  input: &quot;output/W&quot;\\n  input: &quot;output/W/Adadelta&quot;\\n  input: &quot;output/W/Adadelta_1&quot;\\n  input: &quot;output/output/b&quot;\\n  input: &quot;output/output/b/Adadelta&quot;\\n  input: &quot;output/output/b/Adadelta_1&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_INT32\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;^save/SaveV2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@save/Const&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;conv-maxpool-1/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2/tensor_names&quot;\\n  input: &quot;save/RestoreV2/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;conv-maxpool-1/W&quot;\\n  input: &quot;save/RestoreV2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-1/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_1/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;conv-maxpool-1/W/Adadelta&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_1/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_1&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_1/tensor_names&quot;\\n  input: &quot;save/RestoreV2_1/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_1&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;conv-maxpool-1/W/Adadelta&quot;\\n  input: &quot;save/RestoreV2_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-1/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_2/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;conv-maxpool-1/W/Adadelta_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_2/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_2&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_2/tensor_names&quot;\\n  input: &quot;save/RestoreV2_2/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_2&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;conv-maxpool-1/W/Adadelta_1&quot;\\n  input: &quot;save/RestoreV2_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-1/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_3/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;conv-maxpool-1/b&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_3/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_3&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_3/tensor_names&quot;\\n  input: &quot;save/RestoreV2_3/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_3&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;conv-maxpool-1/b&quot;\\n  input: &quot;save/RestoreV2_3&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-1/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_4/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;conv-maxpool-1/b/Adadelta&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_4/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_4&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_4/tensor_names&quot;\\n  input: &quot;save/RestoreV2_4/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_4&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;conv-maxpool-1/b/Adadelta&quot;\\n  input: &quot;save/RestoreV2_4&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-1/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_5/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;conv-maxpool-1/b/Adadelta_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_5/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_5&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_5/tensor_names&quot;\\n  input: &quot;save/RestoreV2_5/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_5&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;conv-maxpool-1/b/Adadelta_1&quot;\\n  input: &quot;save/RestoreV2_5&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-1/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_6/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;conv-maxpool-2/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_6/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_6&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_6/tensor_names&quot;\\n  input: &quot;save/RestoreV2_6/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_6&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;conv-maxpool-2/W&quot;\\n  input: &quot;save/RestoreV2_6&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-2/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_7/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;conv-maxpool-2/W/Adadelta&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_7/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_7&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_7/tensor_names&quot;\\n  input: &quot;save/RestoreV2_7/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_7&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;conv-maxpool-2/W/Adadelta&quot;\\n  input: &quot;save/RestoreV2_7&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-2/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_8/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;conv-maxpool-2/W/Adadelta_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_8/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_8&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_8/tensor_names&quot;\\n  input: &quot;save/RestoreV2_8/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_8&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;conv-maxpool-2/W/Adadelta_1&quot;\\n  input: &quot;save/RestoreV2_8&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-2/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_9/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;conv-maxpool-2/b&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_9/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_9&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_9/tensor_names&quot;\\n  input: &quot;save/RestoreV2_9/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_9&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;conv-maxpool-2/b&quot;\\n  input: &quot;save/RestoreV2_9&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-2/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_10/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;conv-maxpool-2/b/Adadelta&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_10/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_10&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_10/tensor_names&quot;\\n  input: &quot;save/RestoreV2_10/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_10&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;conv-maxpool-2/b/Adadelta&quot;\\n  input: &quot;save/RestoreV2_10&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-2/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_11/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;conv-maxpool-2/b/Adadelta_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_11/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_11&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_11/tensor_names&quot;\\n  input: &quot;save/RestoreV2_11/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_11&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;conv-maxpool-2/b/Adadelta_1&quot;\\n  input: &quot;save/RestoreV2_11&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-2/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_12/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;embedding_padding&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_12/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_12&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_12/tensor_names&quot;\\n  input: &quot;save/RestoreV2_12/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_12&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;embedding_padding&quot;\\n  input: &quot;save/RestoreV2_12&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding_padding&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_13/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;embedding_words&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_13/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_13&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_13/tensor_names&quot;\\n  input: &quot;save/RestoreV2_13/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_13&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;embedding_words&quot;\\n  input: &quot;save/RestoreV2_13&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding_words&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_14/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;embedding_words/Adadelta&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_14/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_14&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_14/tensor_names&quot;\\n  input: &quot;save/RestoreV2_14/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_14&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;embedding_words/Adadelta&quot;\\n  input: &quot;save/RestoreV2_14&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding_words&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_15/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;embedding_words/Adadelta_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_15/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_15&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_15/tensor_names&quot;\\n  input: &quot;save/RestoreV2_15/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_15&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;embedding_words/Adadelta_1&quot;\\n  input: &quot;save/RestoreV2_15&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding_words&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_16/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;global_step&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_16/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_16&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_16/tensor_names&quot;\\n  input: &quot;save/RestoreV2_16/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_INT32\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_16&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;global_step&quot;\\n  input: &quot;save/RestoreV2_16&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@global_step&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_17/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;hidden/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_17/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_17&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_17/tensor_names&quot;\\n  input: &quot;save/RestoreV2_17/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_17&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden/W&quot;\\n  input: &quot;save/RestoreV2_17&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_18/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;hidden/W/Adadelta&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_18/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_18&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_18/tensor_names&quot;\\n  input: &quot;save/RestoreV2_18/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_18&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden/W/Adadelta&quot;\\n  input: &quot;save/RestoreV2_18&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_19/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;hidden/W/Adadelta_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_19/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_19&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_19/tensor_names&quot;\\n  input: &quot;save/RestoreV2_19/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_19&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden/W/Adadelta_1&quot;\\n  input: &quot;save/RestoreV2_19&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_20/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;hidden/hidden/b&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_20/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_20&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_20/tensor_names&quot;\\n  input: &quot;save/RestoreV2_20/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_20&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden/hidden/b&quot;\\n  input: &quot;save/RestoreV2_20&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/hidden/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_21/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;hidden/hidden/b/Adadelta&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_21/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_21&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_21/tensor_names&quot;\\n  input: &quot;save/RestoreV2_21/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_21&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden/hidden/b/Adadelta&quot;\\n  input: &quot;save/RestoreV2_21&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/hidden/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_22/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;hidden/hidden/b/Adadelta_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_22/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_22&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_22/tensor_names&quot;\\n  input: &quot;save/RestoreV2_22/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_22&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden/hidden/b/Adadelta_1&quot;\\n  input: &quot;save/RestoreV2_22&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/hidden/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_23/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;output/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_23/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_23&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_23/tensor_names&quot;\\n  input: &quot;save/RestoreV2_23/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_23&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;output/W&quot;\\n  input: &quot;save/RestoreV2_23&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_24/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;output/W/Adadelta&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_24/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_24&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_24/tensor_names&quot;\\n  input: &quot;save/RestoreV2_24/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_24&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;output/W/Adadelta&quot;\\n  input: &quot;save/RestoreV2_24&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_25/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;output/W/Adadelta_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_25/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_25&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_25/tensor_names&quot;\\n  input: &quot;save/RestoreV2_25/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_25&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;output/W/Adadelta_1&quot;\\n  input: &quot;save/RestoreV2_25&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_26/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;output/output/b&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_26/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_26&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_26/tensor_names&quot;\\n  input: &quot;save/RestoreV2_26/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_26&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;output/output/b&quot;\\n  input: &quot;save/RestoreV2_26&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/output/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_27/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;output/output/b/Adadelta&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_27/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_27&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_27/tensor_names&quot;\\n  input: &quot;save/RestoreV2_27/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_27&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;output/output/b/Adadelta&quot;\\n  input: &quot;save/RestoreV2_27&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/output/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_28/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;output/output/b/Adadelta_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_28/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_28&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_28/tensor_names&quot;\\n  input: &quot;save/RestoreV2_28/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_28&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;output/output/b/Adadelta_1&quot;\\n  input: &quot;save/RestoreV2_28&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/output/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/restore_all&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^save/Assign&quot;\\n  input: &quot;^save/Assign_1&quot;\\n  input: &quot;^save/Assign_2&quot;\\n  input: &quot;^save/Assign_3&quot;\\n  input: &quot;^save/Assign_4&quot;\\n  input: &quot;^save/Assign_5&quot;\\n  input: &quot;^save/Assign_6&quot;\\n  input: &quot;^save/Assign_7&quot;\\n  input: &quot;^save/Assign_8&quot;\\n  input: &quot;^save/Assign_9&quot;\\n  input: &quot;^save/Assign_10&quot;\\n  input: &quot;^save/Assign_11&quot;\\n  input: &quot;^save/Assign_12&quot;\\n  input: &quot;^save/Assign_13&quot;\\n  input: &quot;^save/Assign_14&quot;\\n  input: &quot;^save/Assign_15&quot;\\n  input: &quot;^save/Assign_16&quot;\\n  input: &quot;^save/Assign_17&quot;\\n  input: &quot;^save/Assign_18&quot;\\n  input: &quot;^save/Assign_19&quot;\\n  input: &quot;^save/Assign_20&quot;\\n  input: &quot;^save/Assign_21&quot;\\n  input: &quot;^save/Assign_22&quot;\\n  input: &quot;^save/Assign_23&quot;\\n  input: &quot;^save/Assign_24&quot;\\n  input: &quot;^save/Assign_25&quot;\\n  input: &quot;^save/Assign_26&quot;\\n  input: &quot;^save/Assign_27&quot;\\n  input: &quot;^save/Assign_28&quot;\\n}\\nnode {\\n  name: &quot;init&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^embedding_words/Assign&quot;\\n  input: &quot;^embedding_padding/Assign&quot;\\n  input: &quot;^conv-maxpool-1/W/Assign&quot;\\n  input: &quot;^conv-maxpool-1/b/Assign&quot;\\n  input: &quot;^conv-maxpool-2/W/Assign&quot;\\n  input: &quot;^conv-maxpool-2/b/Assign&quot;\\n  input: &quot;^hidden/W/Assign&quot;\\n  input: &quot;^hidden/hidden/b/Assign&quot;\\n  input: &quot;^output/W/Assign&quot;\\n  input: &quot;^output/output/b/Assign&quot;\\n  input: &quot;^global_step/Assign&quot;\\n  input: &quot;^embedding_words/Adadelta/Assign&quot;\\n  input: &quot;^embedding_words/Adadelta_1/Assign&quot;\\n  input: &quot;^conv-maxpool-1/W/Adadelta/Assign&quot;\\n  input: &quot;^conv-maxpool-1/W/Adadelta_1/Assign&quot;\\n  input: &quot;^conv-maxpool-1/b/Adadelta/Assign&quot;\\n  input: &quot;^conv-maxpool-1/b/Adadelta_1/Assign&quot;\\n  input: &quot;^conv-maxpool-2/W/Adadelta/Assign&quot;\\n  input: &quot;^conv-maxpool-2/W/Adadelta_1/Assign&quot;\\n  input: &quot;^conv-maxpool-2/b/Adadelta/Assign&quot;\\n  input: &quot;^conv-maxpool-2/b/Adadelta_1/Assign&quot;\\n  input: &quot;^hidden/W/Adadelta/Assign&quot;\\n  input: &quot;^hidden/W/Adadelta_1/Assign&quot;\\n  input: &quot;^hidden/hidden/b/Adadelta/Assign&quot;\\n  input: &quot;^hidden/hidden/b/Adadelta_1/Assign&quot;\\n  input: &quot;^output/W/Adadelta/Assign&quot;\\n  input: &quot;^output/W/Adadelta_1/Assign&quot;\\n  input: &quot;^output/output/b/Adadelta/Assign&quot;\\n  input: &quot;^output/output/b/Adadelta_1/Assign&quot;\\n}\\nversions {\\n  producer: 24\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.3149592442675012&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_graph(graph.as_graph_def())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# del x_train_distance, x_test_distance, y_test_distance, y_train_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46692, 52)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save embeddings\n",
    "# np.save('final_embeddings_52_distance_modified.npy', final_embeddings)\n",
    "# with open('./data/embed_tweets_en_590M_52D_data/vocabulary_dict_52_modified.pickle', 'wb') as myfile:\n",
    "#     pickle.dump(word_dict, myfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load embeddings from previous work\n",
    "# final_embeddings = np.load('./final_embeddings_52_distance_modified.npy')\n",
    "# word_dict = {}\n",
    "# with open('./data/embed_tweets_en_590M_52D_data/vocabulary_dict_52_modified.pickle', 'rb') as myfile:\n",
    "#     word_dict = pickle.load(myfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46692, 52)\n",
      "46691\n"
     ]
    }
   ],
   "source": [
    "print(final_embeddings.shape)\n",
    "print(len(word_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First CNN filter (4, 52, 1, 200)\n",
      "output of first cnn (?, 50, 1, 200)\n",
      "Second CNN filter (3, 1, 200, 200)\n",
      "output of second cnn (?, 22, 1, 200)\n",
      "h_pool_flat (?, 200)\n",
      "scores (?, 3)\n",
      "predictions (?,)\n",
      "losses (?,)\n",
      "Writing to /home/phejimlin/Documents/Machine-learning/Milestone2/runs/1511682868\n",
      "\n",
      "Current epoch:  0\n",
      "Current epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phejimlin/anaconda3/envs/tensorflow_3.5/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T15:54:29.437384: step 5, loss 2.04545, acc 0.453125, f1 0.383099\n",
      "2017-11-26T15:54:29.580913: step 10, loss 1.97774, acc 0.507812, f1 0.442097\n",
      "2017-11-26T15:54:29.727518: step 15, loss 1.94329, acc 0.429688, f1 0.409886\n",
      "2017-11-26T15:54:29.872352: step 20, loss 1.90091, acc 0.570312, f1 0.514503\n",
      "2017-11-26T15:54:30.013326: step 25, loss 2.00495, acc 0.429688, f1 0.258282\n",
      "2017-11-26T15:54:30.163649: step 30, loss 1.93526, acc 0.5, f1 0.398158\n",
      "2017-11-26T15:54:30.306525: step 35, loss 1.97693, acc 0.398438, f1 0.252916\n",
      "2017-11-26T15:54:30.448495: step 40, loss 1.87718, acc 0.476562, f1 0.391032\n",
      "2017-11-26T15:54:30.581263: step 45, loss 1.91099, acc 0.4375, f1 0.303021\n",
      "2017-11-26T15:54:30.713397: step 50, loss 1.73735, acc 0.578125, f1 0.549031\n",
      "2017-11-26T15:54:30.855336: step 55, loss 1.81307, acc 0.546875, f1 0.494429\n",
      "2017-11-26T15:54:31.000415: step 60, loss 1.77743, acc 0.507812, f1 0.457651\n",
      "2017-11-26T15:54:31.141659: step 65, loss 1.86924, acc 0.4375, f1 0.33812\n",
      "2017-11-26T15:54:31.283862: step 70, loss 1.76991, acc 0.484375, f1 0.447088\n",
      "2017-11-26T15:54:31.426600: step 75, loss 1.741, acc 0.523438, f1 0.479167\n",
      "2017-11-26T15:54:31.573727: step 80, loss 1.72454, acc 0.53125, f1 0.489484\n",
      "2017-11-26T15:54:31.707309: step 85, loss 1.73937, acc 0.5, f1 0.457834\n",
      "2017-11-26T15:54:31.861229: step 90, loss 1.73091, acc 0.484375, f1 0.420628\n",
      "2017-11-26T15:54:32.001543: step 95, loss 1.71214, acc 0.5, f1 0.445135\n",
      "2017-11-26T15:54:32.128926: step 100, loss 1.71736, acc 0.453125, f1 0.405739\n",
      "2017-11-26T15:54:32.271970: step 105, loss 1.68979, acc 0.46875, f1 0.374249\n",
      "2017-11-26T15:54:32.403656: step 110, loss 1.61592, acc 0.492188, f1 0.465746\n",
      "2017-11-26T15:54:32.544418: step 115, loss 1.67714, acc 0.453125, f1 0.378769\n",
      "2017-11-26T15:54:32.690485: step 120, loss 1.66898, acc 0.492188, f1 0.442996\n",
      "2017-11-26T15:54:32.829421: step 125, loss 1.57323, acc 0.523438, f1 0.487715\n",
      "2017-11-26T15:54:32.974774: step 130, loss 1.59032, acc 0.5, f1 0.470084\n",
      "2017-11-26T15:54:33.117644: step 135, loss 1.61076, acc 0.476562, f1 0.417344\n",
      "2017-11-26T15:54:33.261797: step 140, loss 1.55182, acc 0.546875, f1 0.474602\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T15:54:34.065963: step 141, loss 1.57033, acc 0.55283, f1 0.483311\n",
      "\n",
      "\n",
      "Current epoch:  1\n",
      "2017-11-26T15:54:34.209196: step 145, loss 1.60651, acc 0.40625, f1 0.344696\n",
      "2017-11-26T15:54:34.359979: step 150, loss 1.57144, acc 0.492188, f1 0.450763\n",
      "2017-11-26T15:54:34.506073: step 155, loss 1.47595, acc 0.5625, f1 0.526053\n",
      "2017-11-26T15:54:34.654953: step 160, loss 1.62592, acc 0.4375, f1 0.359422\n",
      "2017-11-26T15:54:34.797118: step 165, loss 1.55765, acc 0.546875, f1 0.496175\n",
      "2017-11-26T15:54:34.940158: step 170, loss 1.51312, acc 0.539062, f1 0.491622\n",
      "2017-11-26T15:54:35.084881: step 175, loss 1.48826, acc 0.539062, f1 0.480063\n",
      "2017-11-26T15:54:35.237746: step 180, loss 1.59577, acc 0.40625, f1 0.306906\n",
      "2017-11-26T15:54:35.368011: step 185, loss 1.51569, acc 0.546875, f1 0.469421\n",
      "2017-11-26T15:54:35.506782: step 190, loss 1.4958, acc 0.539062, f1 0.485677\n",
      "2017-11-26T15:54:35.659073: step 195, loss 1.44851, acc 0.515625, f1 0.449195\n",
      "2017-11-26T15:54:35.797604: step 200, loss 1.48103, acc 0.515625, f1 0.425203\n",
      "2017-11-26T15:54:35.928627: step 205, loss 1.55637, acc 0.421875, f1 0.329605\n",
      "2017-11-26T15:54:36.070424: step 210, loss 1.41418, acc 0.515625, f1 0.479167\n",
      "2017-11-26T15:54:36.210491: step 215, loss 1.44635, acc 0.546875, f1 0.490626\n",
      "2017-11-26T15:54:36.348418: step 220, loss 1.45608, acc 0.492188, f1 0.449467\n",
      "2017-11-26T15:54:36.490248: step 225, loss 1.38684, acc 0.570312, f1 0.524673\n",
      "2017-11-26T15:54:36.632160: step 230, loss 1.53062, acc 0.515625, f1 0.459532\n",
      "2017-11-26T15:54:36.774463: step 235, loss 1.48419, acc 0.5, f1 0.44109\n",
      "2017-11-26T15:54:36.921839: step 240, loss 1.39366, acc 0.523438, f1 0.459213\n",
      "2017-11-26T15:54:37.078565: step 245, loss 1.39252, acc 0.492188, f1 0.45863\n",
      "2017-11-26T15:54:37.221191: step 250, loss 1.40502, acc 0.460938, f1 0.397148\n",
      "2017-11-26T15:54:37.352225: step 255, loss 1.37788, acc 0.578125, f1 0.52329\n",
      "2017-11-26T15:54:37.489579: step 260, loss 1.40542, acc 0.507812, f1 0.457918\n",
      "2017-11-26T15:54:37.630056: step 265, loss 1.384, acc 0.484375, f1 0.453537\n",
      "2017-11-26T15:54:37.762973: step 270, loss 1.41424, acc 0.460938, f1 0.423326\n",
      "2017-11-26T15:54:37.913134: step 275, loss 1.40547, acc 0.492188, f1 0.443363\n",
      "2017-11-26T15:54:38.058389: step 280, loss 1.38933, acc 0.460938, f1 0.423688\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T15:54:38.272479: step 282, loss 1.35463, acc 0.552879, f1 0.491433\n",
      "\n",
      "\n",
      "Current epoch:  2\n",
      "2017-11-26T15:54:38.393463: step 285, loss 1.39512, acc 0.484375, f1 0.429802\n",
      "2017-11-26T15:54:38.538367: step 290, loss 1.37299, acc 0.5, f1 0.452886\n",
      "2017-11-26T15:54:38.679060: step 295, loss 1.32605, acc 0.578125, f1 0.5423\n",
      "2017-11-26T15:54:38.813919: step 300, loss 1.37589, acc 0.523438, f1 0.465097\n",
      "2017-11-26T15:54:38.959242: step 305, loss 1.31055, acc 0.546875, f1 0.494212\n",
      "2017-11-26T15:54:39.077917: step 310, loss 1.32333, acc 0.515625, f1 0.472055\n",
      "2017-11-26T15:54:39.208810: step 315, loss 1.32667, acc 0.53125, f1 0.475616\n",
      "2017-11-26T15:54:39.353608: step 320, loss 1.36534, acc 0.453125, f1 0.406819\n",
      "2017-11-26T15:54:39.473131: step 325, loss 1.33458, acc 0.539062, f1 0.492528\n",
      "2017-11-26T15:54:39.617799: step 330, loss 1.29432, acc 0.53125, f1 0.484583\n",
      "2017-11-26T15:54:39.762817: step 335, loss 1.33294, acc 0.507812, f1 0.462489\n",
      "2017-11-26T15:54:39.905494: step 340, loss 1.34804, acc 0.484375, f1 0.438803\n",
      "2017-11-26T15:54:40.052439: step 345, loss 1.22502, acc 0.578125, f1 0.530188\n",
      "2017-11-26T15:54:40.197264: step 350, loss 1.29711, acc 0.523438, f1 0.483297\n",
      "2017-11-26T15:54:40.338618: step 355, loss 1.28445, acc 0.539062, f1 0.495754\n",
      "2017-11-26T15:54:40.482907: step 360, loss 1.3255, acc 0.53125, f1 0.484235\n",
      "2017-11-26T15:54:40.628341: step 365, loss 1.30327, acc 0.484375, f1 0.435829\n",
      "2017-11-26T15:54:40.765050: step 370, loss 1.26374, acc 0.515625, f1 0.449462\n",
      "2017-11-26T15:54:40.912153: step 375, loss 1.32028, acc 0.40625, f1 0.357838\n",
      "2017-11-26T15:54:41.047749: step 380, loss 1.22422, acc 0.578125, f1 0.538676\n",
      "2017-11-26T15:54:41.198585: step 385, loss 1.27468, acc 0.507812, f1 0.44533\n",
      "2017-11-26T15:54:41.342441: step 390, loss 1.26712, acc 0.515625, f1 0.475024\n",
      "2017-11-26T15:54:41.485265: step 395, loss 1.23356, acc 0.546875, f1 0.4948\n",
      "2017-11-26T15:54:41.615645: step 400, loss 1.25355, acc 0.460938, f1 0.437013\n",
      "2017-11-26T15:54:41.748337: step 405, loss 1.28708, acc 0.476562, f1 0.410443\n",
      "2017-11-26T15:54:41.884552: step 410, loss 1.28532, acc 0.445312, f1 0.392949\n",
      "2017-11-26T15:54:42.021004: step 415, loss 1.1991, acc 0.617188, f1 0.563312\n",
      "2017-11-26T15:54:42.168900: step 420, loss 1.27091, acc 0.46875, f1 0.408581\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T15:54:42.391482: step 423, loss 1.24861, acc 0.532571, f1 0.489544\n",
      "\n",
      "\n",
      "Current epoch:  3\n",
      "2017-11-26T15:54:42.456423: step 425, loss 1.30746, acc 0.445312, f1 0.397939\n",
      "2017-11-26T15:54:42.598580: step 430, loss 1.21819, acc 0.507812, f1 0.463025\n",
      "2017-11-26T15:54:42.733896: step 435, loss 1.22671, acc 0.554688, f1 0.520587\n",
      "2017-11-26T15:54:42.880865: step 440, loss 1.22791, acc 0.601562, f1 0.545627\n",
      "2017-11-26T15:54:43.024076: step 445, loss 1.18664, acc 0.578125, f1 0.529266\n",
      "2017-11-26T15:54:43.172526: step 450, loss 1.14228, acc 0.578125, f1 0.547307\n",
      "2017-11-26T15:54:43.323369: step 455, loss 1.27123, acc 0.492188, f1 0.419264\n",
      "2017-11-26T15:54:43.445726: step 460, loss 1.21752, acc 0.539062, f1 0.47863\n",
      "2017-11-26T15:54:43.581294: step 465, loss 1.23394, acc 0.546875, f1 0.493873\n",
      "2017-11-26T15:54:43.726440: step 470, loss 1.19078, acc 0.539062, f1 0.498153\n",
      "2017-11-26T15:54:43.865398: step 475, loss 1.18401, acc 0.539062, f1 0.499547\n",
      "2017-11-26T15:54:44.007311: step 480, loss 1.24294, acc 0.476562, f1 0.404099\n",
      "2017-11-26T15:54:44.136789: step 485, loss 1.20284, acc 0.507812, f1 0.460461\n",
      "2017-11-26T15:54:44.277223: step 490, loss 1.16471, acc 0.546875, f1 0.522721\n",
      "2017-11-26T15:54:44.414195: step 495, loss 1.23507, acc 0.5, f1 0.454701\n",
      "2017-11-26T15:54:44.557565: step 500, loss 1.26647, acc 0.445312, f1 0.399734\n",
      "2017-11-26T15:54:44.705301: step 505, loss 1.19726, acc 0.546875, f1 0.486911\n",
      "2017-11-26T15:54:44.855940: step 510, loss 1.19084, acc 0.523438, f1 0.469703\n",
      "2017-11-26T15:54:45.001843: step 515, loss 1.1625, acc 0.570312, f1 0.534212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T15:54:45.134336: step 520, loss 1.15847, acc 0.570312, f1 0.518371\n",
      "2017-11-26T15:54:45.279753: step 525, loss 1.19046, acc 0.523438, f1 0.467135\n",
      "2017-11-26T15:54:45.412513: step 530, loss 1.17667, acc 0.484375, f1 0.441186\n",
      "2017-11-26T15:54:45.551452: step 535, loss 1.20663, acc 0.46875, f1 0.42832\n",
      "2017-11-26T15:54:45.701759: step 540, loss 1.20689, acc 0.476562, f1 0.419323\n",
      "2017-11-26T15:54:45.838495: step 545, loss 1.15811, acc 0.53125, f1 0.492045\n",
      "2017-11-26T15:54:45.984625: step 550, loss 1.23902, acc 0.4375, f1 0.378141\n",
      "2017-11-26T15:54:46.138727: step 555, loss 1.13812, acc 0.578125, f1 0.544079\n",
      "2017-11-26T15:54:46.290310: step 560, loss 1.15297, acc 0.554688, f1 0.505506\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T15:54:46.559452: step 564, loss 1.16035, acc 0.554478, f1 0.501459\n",
      "\n",
      "\n",
      "Current epoch:  4\n",
      "2017-11-26T15:54:46.617380: step 565, loss 1.17045, acc 0.539062, f1 0.491667\n",
      "2017-11-26T15:54:46.759594: step 570, loss 1.13282, acc 0.570312, f1 0.526089\n",
      "2017-11-26T15:54:46.898950: step 575, loss 1.24459, acc 0.453125, f1 0.391636\n",
      "2017-11-26T15:54:47.048405: step 580, loss 1.16803, acc 0.453125, f1 0.404674\n",
      "2017-11-26T15:54:47.193767: step 585, loss 1.14462, acc 0.5, f1 0.448052\n",
      "2017-11-26T15:54:47.324485: step 590, loss 1.13175, acc 0.554688, f1 0.512784\n",
      "2017-11-26T15:54:47.467308: step 595, loss 1.11777, acc 0.546875, f1 0.509417\n",
      "2017-11-26T15:54:47.607126: step 600, loss 1.15596, acc 0.5, f1 0.460572\n",
      "2017-11-26T15:54:47.752228: step 605, loss 1.21035, acc 0.390625, f1 0.351243\n",
      "2017-11-26T15:54:47.908633: step 610, loss 1.19377, acc 0.453125, f1 0.433512\n",
      "2017-11-26T15:54:48.052059: step 615, loss 1.12339, acc 0.515625, f1 0.481141\n",
      "2017-11-26T15:54:48.202959: step 620, loss 1.15512, acc 0.5, f1 0.455454\n",
      "2017-11-26T15:54:48.346594: step 625, loss 1.17338, acc 0.445312, f1 0.398148\n",
      "2017-11-26T15:54:48.493483: step 630, loss 1.28322, acc 0.40625, f1 0.35487\n",
      "2017-11-26T15:54:48.637747: step 635, loss 1.11461, acc 0.59375, f1 0.55163\n",
      "2017-11-26T15:54:48.778493: step 640, loss 1.14657, acc 0.539062, f1 0.477359\n",
      "2017-11-26T15:54:48.920634: step 645, loss 1.15881, acc 0.507812, f1 0.460718\n",
      "2017-11-26T15:54:49.064206: step 650, loss 1.13997, acc 0.570312, f1 0.509288\n",
      "2017-11-26T15:54:49.206088: step 655, loss 1.08132, acc 0.546875, f1 0.504997\n",
      "2017-11-26T15:54:49.355647: step 660, loss 1.12247, acc 0.546875, f1 0.491987\n",
      "2017-11-26T15:54:49.504343: step 665, loss 1.05708, acc 0.578125, f1 0.539608\n",
      "2017-11-26T15:54:49.641695: step 670, loss 1.06329, acc 0.5625, f1 0.519523\n",
      "2017-11-26T15:54:49.780083: step 675, loss 1.06484, acc 0.515625, f1 0.478112\n",
      "2017-11-26T15:54:49.927704: step 680, loss 1.12148, acc 0.53125, f1 0.48109\n",
      "2017-11-26T15:54:50.075613: step 685, loss 1.17593, acc 0.53125, f1 0.458203\n",
      "2017-11-26T15:54:50.216051: step 690, loss 1.11967, acc 0.570312, f1 0.519334\n",
      "2017-11-26T15:54:50.369713: step 695, loss 1.07333, acc 0.539062, f1 0.492853\n",
      "2017-11-26T15:54:50.513281: step 700, loss 1.02531, acc 0.507812, f1 0.456013\n",
      "2017-11-26T15:54:50.664728: step 705, loss 1.10068, acc 0.5, f1 0.45405\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T15:54:50.816287: step 705, loss 1.11349, acc 0.552103, f1 0.502286\n",
      "\n",
      "\n",
      "Current epoch:  5\n",
      "2017-11-26T15:54:50.989178: step 710, loss 1.14458, acc 0.53125, f1 0.483982\n",
      "2017-11-26T15:54:51.133978: step 715, loss 1.10761, acc 0.460938, f1 0.421259\n",
      "2017-11-26T15:54:51.278881: step 720, loss 1.0957, acc 0.53125, f1 0.48888\n",
      "2017-11-26T15:54:51.435492: step 725, loss 1.05252, acc 0.585938, f1 0.550787\n",
      "2017-11-26T15:54:51.576781: step 730, loss 1.08233, acc 0.5625, f1 0.51875\n",
      "2017-11-26T15:54:51.722292: step 735, loss 1.13878, acc 0.476562, f1 0.428673\n",
      "2017-11-26T15:54:51.868085: step 740, loss 1.15989, acc 0.445312, f1 0.407492\n",
      "2017-11-26T15:54:52.018870: step 745, loss 1.08549, acc 0.53125, f1 0.475709\n",
      "2017-11-26T15:54:52.149876: step 750, loss 1.08129, acc 0.492188, f1 0.446675\n",
      "2017-11-26T15:54:52.304618: step 755, loss 1.00719, acc 0.578125, f1 0.547379\n",
      "2017-11-26T15:54:52.456379: step 760, loss 1.0784, acc 0.546875, f1 0.500126\n",
      "2017-11-26T15:54:52.604565: step 765, loss 1.10283, acc 0.507812, f1 0.47644\n",
      "2017-11-26T15:54:52.755930: step 770, loss 1.06866, acc 0.5, f1 0.472781\n",
      "2017-11-26T15:54:52.904674: step 775, loss 1.07981, acc 0.554688, f1 0.504162\n",
      "2017-11-26T15:54:53.053723: step 780, loss 1.03167, acc 0.59375, f1 0.548112\n",
      "2017-11-26T15:54:53.199997: step 785, loss 1.05432, acc 0.585938, f1 0.544235\n",
      "2017-11-26T15:54:53.347168: step 790, loss 1.08745, acc 0.53125, f1 0.46792\n",
      "2017-11-26T15:54:53.494631: step 795, loss 1.09076, acc 0.476562, f1 0.439838\n",
      "2017-11-26T15:54:53.645478: step 800, loss 1.06477, acc 0.539062, f1 0.485894\n",
      "2017-11-26T15:54:53.797059: step 805, loss 1.00044, acc 0.59375, f1 0.564981\n",
      "2017-11-26T15:54:53.933453: step 810, loss 1.10928, acc 0.484375, f1 0.449543\n",
      "2017-11-26T15:54:54.081949: step 815, loss 1.0111, acc 0.53125, f1 0.494656\n",
      "2017-11-26T15:54:54.222691: step 820, loss 1.06413, acc 0.523438, f1 0.488687\n",
      "2017-11-26T15:54:54.368460: step 825, loss 1.10867, acc 0.523438, f1 0.474653\n",
      "2017-11-26T15:54:54.515337: step 830, loss 1.14205, acc 0.46875, f1 0.39874\n",
      "2017-11-26T15:54:54.661227: step 835, loss 1.02302, acc 0.601562, f1 0.56314\n",
      "2017-11-26T15:54:54.806188: step 840, loss 1.03145, acc 0.515625, f1 0.465138\n",
      "2017-11-26T15:54:54.946291: step 845, loss 1.09312, acc 0.46875, f1 0.420673\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T15:54:55.126243: step 846, loss 1.07014, acc 0.556611, f1 0.491778\n",
      "\n",
      "\n",
      "Current epoch:  6\n",
      "2017-11-26T15:54:55.276585: step 850, loss 0.99763, acc 0.546875, f1 0.508352\n",
      "2017-11-26T15:54:55.425601: step 855, loss 1.09245, acc 0.515625, f1 0.455844\n",
      "2017-11-26T15:54:55.561190: step 860, loss 1.07701, acc 0.5625, f1 0.526687\n",
      "2017-11-26T15:54:55.710758: step 865, loss 1.06669, acc 0.578125, f1 0.518319\n",
      "2017-11-26T15:54:55.859486: step 870, loss 1.08075, acc 0.5, f1 0.450079\n",
      "2017-11-26T15:54:56.003020: step 875, loss 1.07303, acc 0.539062, f1 0.492643\n",
      "2017-11-26T15:54:56.150334: step 880, loss 1.10776, acc 0.554688, f1 0.500021\n",
      "2017-11-26T15:54:56.288133: step 885, loss 1.06673, acc 0.507812, f1 0.47229\n",
      "2017-11-26T15:54:56.441987: step 890, loss 1.01352, acc 0.546875, f1 0.51209\n",
      "2017-11-26T15:54:56.590437: step 895, loss 1.0877, acc 0.554688, f1 0.496712\n",
      "2017-11-26T15:54:56.735456: step 900, loss 1.05303, acc 0.546875, f1 0.500516\n",
      "2017-11-26T15:54:56.885414: step 905, loss 1.0906, acc 0.46875, f1 0.404477\n",
      "2017-11-26T15:54:57.036634: step 910, loss 1.07103, acc 0.53125, f1 0.475781\n",
      "2017-11-26T15:54:57.181693: step 915, loss 1.1087, acc 0.46875, f1 0.420313\n",
      "2017-11-26T15:54:57.317340: step 920, loss 1.04256, acc 0.546875, f1 0.505075\n",
      "2017-11-26T15:54:57.455485: step 925, loss 1.06613, acc 0.492188, f1 0.457057\n",
      "2017-11-26T15:54:57.611936: step 930, loss 1.0452, acc 0.539062, f1 0.481759\n",
      "2017-11-26T15:54:57.752037: step 935, loss 1.08447, acc 0.523438, f1 0.461839\n",
      "2017-11-26T15:54:57.880651: step 940, loss 1.03918, acc 0.5625, f1 0.512946\n",
      "2017-11-26T15:54:58.025905: step 945, loss 1.06528, acc 0.46875, f1 0.434081\n",
      "2017-11-26T15:54:58.158272: step 950, loss 1.08383, acc 0.523438, f1 0.470788\n",
      "2017-11-26T15:54:58.275503: step 955, loss 0.971246, acc 0.585938, f1 0.552649\n",
      "2017-11-26T15:54:58.414334: step 960, loss 1.05131, acc 0.523438, f1 0.496506\n",
      "2017-11-26T15:54:58.547248: step 965, loss 0.961379, acc 0.632812, f1 0.587025\n",
      "2017-11-26T15:54:58.696545: step 970, loss 1.08861, acc 0.492188, f1 0.435987\n",
      "2017-11-26T15:54:58.849099: step 975, loss 1.04171, acc 0.539062, f1 0.498498\n",
      "2017-11-26T15:54:58.986451: step 980, loss 1.08435, acc 0.460938, f1 0.413164\n",
      "2017-11-26T15:54:59.115661: step 985, loss 1.0719, acc 0.53125, f1 0.476899\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T15:54:59.334541: step 987, loss 1.05515, acc 0.542797, f1 0.497964\n",
      "\n",
      "\n",
      "Current epoch:  7\n",
      "2017-11-26T15:54:59.444467: step 990, loss 1.05111, acc 0.5, f1 0.463822\n",
      "2017-11-26T15:54:59.585133: step 995, loss 1.10994, acc 0.429688, f1 0.385086\n",
      "2017-11-26T15:54:59.726672: step 1000, loss 1.01631, acc 0.539062, f1 0.487353\n",
      "2017-11-26T15:54:59.866264: step 1005, loss 1.04223, acc 0.554688, f1 0.47701\n",
      "2017-11-26T15:55:00.011144: step 1010, loss 0.948844, acc 0.609375, f1 0.568773\n",
      "2017-11-26T15:55:00.150297: step 1015, loss 1.0666, acc 0.53125, f1 0.472179\n",
      "2017-11-26T15:55:00.291422: step 1020, loss 1.029, acc 0.523438, f1 0.47336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T15:55:00.434541: step 1025, loss 1.05353, acc 0.507812, f1 0.472098\n",
      "2017-11-26T15:55:00.584369: step 1030, loss 1.01892, acc 0.554688, f1 0.507071\n",
      "2017-11-26T15:55:00.730210: step 1035, loss 1.02692, acc 0.492188, f1 0.438361\n",
      "2017-11-26T15:55:00.881233: step 1040, loss 1.00084, acc 0.5625, f1 0.516341\n",
      "2017-11-26T15:55:01.021175: step 1045, loss 1.03003, acc 0.578125, f1 0.515977\n",
      "2017-11-26T15:55:01.162768: step 1050, loss 1.04611, acc 0.585938, f1 0.528448\n",
      "2017-11-26T15:55:01.306995: step 1055, loss 1.08373, acc 0.507812, f1 0.458173\n",
      "2017-11-26T15:55:01.445581: step 1060, loss 0.994117, acc 0.585938, f1 0.554983\n",
      "2017-11-26T15:55:01.588983: step 1065, loss 1.04209, acc 0.554688, f1 0.497673\n",
      "2017-11-26T15:55:01.722965: step 1070, loss 1.02418, acc 0.53125, f1 0.49766\n",
      "2017-11-26T15:55:01.858335: step 1075, loss 1.0344, acc 0.5625, f1 0.501811\n",
      "2017-11-26T15:55:01.997742: step 1080, loss 1.04547, acc 0.53125, f1 0.460989\n",
      "2017-11-26T15:55:02.149616: step 1085, loss 1.0483, acc 0.5, f1 0.434974\n",
      "2017-11-26T15:55:02.296176: step 1090, loss 0.975877, acc 0.585938, f1 0.547416\n",
      "2017-11-26T15:55:02.437168: step 1095, loss 1.01643, acc 0.554688, f1 0.518159\n",
      "2017-11-26T15:55:02.560639: step 1100, loss 1.05212, acc 0.53125, f1 0.482664\n",
      "2017-11-26T15:55:02.698468: step 1105, loss 1.05282, acc 0.539062, f1 0.472147\n",
      "2017-11-26T15:55:02.828259: step 1110, loss 1.05959, acc 0.515625, f1 0.468294\n",
      "2017-11-26T15:55:02.969289: step 1115, loss 1.06653, acc 0.484375, f1 0.435058\n",
      "2017-11-26T15:55:03.108035: step 1120, loss 1.0561, acc 0.53125, f1 0.458616\n",
      "2017-11-26T15:55:03.239697: step 1125, loss 1.08468, acc 0.484375, f1 0.43392\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T15:55:03.469687: step 1128, loss 1.05141, acc 0.518321, f1 0.476864\n",
      "\n",
      "\n",
      "Current epoch:  8\n",
      "2017-11-26T15:55:03.545184: step 1130, loss 1.00268, acc 0.585938, f1 0.547971\n",
      "2017-11-26T15:55:03.677931: step 1135, loss 1.09708, acc 0.476562, f1 0.398744\n",
      "2017-11-26T15:55:03.821006: step 1140, loss 0.996814, acc 0.523438, f1 0.479718\n",
      "2017-11-26T15:55:03.957733: step 1145, loss 1.01707, acc 0.523438, f1 0.482946\n",
      "2017-11-26T15:55:04.097346: step 1150, loss 1.0638, acc 0.53125, f1 0.479274\n",
      "2017-11-26T15:55:04.242184: step 1155, loss 1.0368, acc 0.507812, f1 0.46317\n",
      "2017-11-26T15:55:04.392072: step 1160, loss 0.967004, acc 0.617188, f1 0.569106\n",
      "2017-11-26T15:55:04.544914: step 1165, loss 1.01698, acc 0.515625, f1 0.478896\n",
      "2017-11-26T15:55:04.689475: step 1170, loss 1.0655, acc 0.515625, f1 0.467503\n",
      "2017-11-26T15:55:04.830407: step 1175, loss 1.04472, acc 0.554688, f1 0.496614\n",
      "2017-11-26T15:55:04.965117: step 1180, loss 0.956128, acc 0.609375, f1 0.573175\n",
      "2017-11-26T15:55:05.119439: step 1185, loss 1.0885, acc 0.4375, f1 0.383525\n",
      "2017-11-26T15:55:05.266104: step 1190, loss 0.929182, acc 0.585938, f1 0.5391\n",
      "2017-11-26T15:55:05.418562: step 1195, loss 1.09763, acc 0.476562, f1 0.427296\n",
      "2017-11-26T15:55:05.558296: step 1200, loss 1.01552, acc 0.523438, f1 0.470752\n",
      "2017-11-26T15:55:05.705838: step 1205, loss 0.987557, acc 0.53125, f1 0.471869\n",
      "2017-11-26T15:55:05.834147: step 1210, loss 1.02056, acc 0.554688, f1 0.50016\n",
      "2017-11-26T15:55:05.981480: step 1215, loss 1.05546, acc 0.484375, f1 0.4308\n",
      "2017-11-26T15:55:06.135678: step 1220, loss 1.00362, acc 0.539062, f1 0.491411\n",
      "2017-11-26T15:55:06.280354: step 1225, loss 0.966986, acc 0.585938, f1 0.539279\n",
      "2017-11-26T15:55:06.426073: step 1230, loss 1.0373, acc 0.53125, f1 0.48087\n",
      "2017-11-26T15:55:06.570783: step 1235, loss 0.961883, acc 0.539062, f1 0.497866\n",
      "2017-11-26T15:55:06.708963: step 1240, loss 0.996293, acc 0.570312, f1 0.522665\n",
      "2017-11-26T15:55:06.848222: step 1245, loss 0.975538, acc 0.59375, f1 0.548845\n",
      "2017-11-26T15:55:06.990820: step 1250, loss 0.981506, acc 0.53125, f1 0.484499\n",
      "2017-11-26T15:55:07.130680: step 1255, loss 0.963388, acc 0.554688, f1 0.507689\n",
      "2017-11-26T15:55:07.279832: step 1260, loss 1.01267, acc 0.539062, f1 0.485965\n",
      "2017-11-26T15:55:07.411889: step 1265, loss 1.01764, acc 0.53125, f1 0.485054\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T15:55:07.671218: step 1269, loss 1.03561, acc 0.52341, f1 0.48212\n",
      "\n",
      "\n",
      "Current epoch:  9\n",
      "2017-11-26T15:55:07.730272: step 1270, loss 0.959222, acc 0.585938, f1 0.529489\n",
      "2017-11-26T15:55:07.877442: step 1275, loss 1.01899, acc 0.5, f1 0.456647\n",
      "2017-11-26T15:55:08.040015: step 1280, loss 1.00277, acc 0.523438, f1 0.482176\n",
      "2017-11-26T15:55:08.179303: step 1285, loss 1.0253, acc 0.515625, f1 0.471693\n",
      "2017-11-26T15:55:08.315525: step 1290, loss 1.03068, acc 0.546875, f1 0.501801\n",
      "2017-11-26T15:55:08.437884: step 1295, loss 1.09227, acc 0.398438, f1 0.36024\n",
      "2017-11-26T15:55:08.570613: step 1300, loss 1.03474, acc 0.515625, f1 0.464428\n",
      "2017-11-26T15:55:08.707468: step 1305, loss 0.992497, acc 0.53125, f1 0.488786\n",
      "2017-11-26T15:55:08.826591: step 1310, loss 1.00742, acc 0.523438, f1 0.474743\n",
      "2017-11-26T15:55:08.966734: step 1315, loss 1.02721, acc 0.484375, f1 0.431774\n",
      "2017-11-26T15:55:09.113464: step 1320, loss 1.01151, acc 0.515625, f1 0.47457\n",
      "2017-11-26T15:55:09.245709: step 1325, loss 0.991871, acc 0.523438, f1 0.488907\n",
      "2017-11-26T15:55:09.372928: step 1330, loss 1.01147, acc 0.578125, f1 0.537556\n",
      "2017-11-26T15:55:09.503397: step 1335, loss 0.981982, acc 0.507812, f1 0.456338\n",
      "2017-11-26T15:55:09.634787: step 1340, loss 0.940335, acc 0.554688, f1 0.511297\n",
      "2017-11-26T15:55:09.778569: step 1345, loss 0.977781, acc 0.5625, f1 0.517195\n",
      "2017-11-26T15:55:09.917939: step 1350, loss 0.96062, acc 0.5625, f1 0.525301\n",
      "2017-11-26T15:55:10.053887: step 1355, loss 1.02911, acc 0.53125, f1 0.464181\n",
      "2017-11-26T15:55:10.205578: step 1360, loss 0.966741, acc 0.578125, f1 0.549746\n",
      "2017-11-26T15:55:10.325068: step 1365, loss 0.991459, acc 0.585938, f1 0.540517\n",
      "2017-11-26T15:55:10.471975: step 1370, loss 0.977845, acc 0.570312, f1 0.525884\n",
      "2017-11-26T15:55:10.618717: step 1375, loss 1.02018, acc 0.546875, f1 0.492491\n",
      "2017-11-26T15:55:10.765533: step 1380, loss 0.941527, acc 0.601562, f1 0.56662\n",
      "2017-11-26T15:55:10.896456: step 1385, loss 1.09761, acc 0.445312, f1 0.406425\n",
      "2017-11-26T15:55:11.044636: step 1390, loss 0.971344, acc 0.570312, f1 0.519479\n",
      "2017-11-26T15:55:11.198470: step 1395, loss 0.978514, acc 0.53125, f1 0.48095\n",
      "2017-11-26T15:55:11.344042: step 1400, loss 0.93145, acc 0.523438, f1 0.493134\n",
      "2017-11-26T15:55:11.480012: step 1405, loss 1.05712, acc 0.492188, f1 0.448112\n",
      "2017-11-26T15:55:11.612597: step 1410, loss 1.02049, acc 0.443548, f1 0.4025\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T15:55:11.767025: step 1410, loss 1.00207, acc 0.55758, f1 0.49837\n",
      "\n",
      "\n",
      "Current epoch:  10\n",
      "2017-11-26T15:55:11.933678: step 1415, loss 1.06637, acc 0.460938, f1 0.394706\n",
      "2017-11-26T15:55:12.057357: step 1420, loss 0.961035, acc 0.578125, f1 0.523805\n",
      "2017-11-26T15:55:12.193565: step 1425, loss 0.965941, acc 0.601562, f1 0.555127\n",
      "2017-11-26T15:55:12.341537: step 1430, loss 0.986463, acc 0.554688, f1 0.502773\n",
      "2017-11-26T15:55:12.479282: step 1435, loss 0.986021, acc 0.515625, f1 0.473307\n",
      "2017-11-26T15:55:12.621960: step 1440, loss 1.01333, acc 0.46875, f1 0.43771\n",
      "2017-11-26T15:55:12.771339: step 1445, loss 1.00127, acc 0.59375, f1 0.530185\n",
      "2017-11-26T15:55:12.921173: step 1450, loss 0.985556, acc 0.523438, f1 0.480967\n",
      "2017-11-26T15:55:13.069612: step 1455, loss 0.958402, acc 0.585938, f1 0.542286\n",
      "2017-11-26T15:55:13.212642: step 1460, loss 0.97633, acc 0.53125, f1 0.488199\n",
      "2017-11-26T15:55:13.357508: step 1465, loss 1.0035, acc 0.460938, f1 0.415125\n",
      "2017-11-26T15:55:13.500423: step 1470, loss 0.970146, acc 0.585938, f1 0.533964\n",
      "2017-11-26T15:55:13.661087: step 1475, loss 0.98057, acc 0.570312, f1 0.538114\n",
      "2017-11-26T15:55:13.806681: step 1480, loss 1.0665, acc 0.507812, f1 0.451596\n",
      "2017-11-26T15:55:13.954287: step 1485, loss 0.953, acc 0.585938, f1 0.536908\n",
      "2017-11-26T15:55:14.095750: step 1490, loss 0.963288, acc 0.601562, f1 0.553554\n",
      "2017-11-26T15:55:14.234384: step 1495, loss 1.01074, acc 0.546875, f1 0.499187\n",
      "2017-11-26T15:55:14.385162: step 1500, loss 1.05993, acc 0.492188, f1 0.441137\n",
      "2017-11-26T15:55:14.535648: step 1505, loss 0.971269, acc 0.570312, f1 0.529406\n",
      "2017-11-26T15:55:14.678452: step 1510, loss 0.94792, acc 0.554688, f1 0.513023\n",
      "2017-11-26T15:55:14.819140: step 1515, loss 0.900335, acc 0.59375, f1 0.562017\n",
      "2017-11-26T15:55:14.942761: step 1520, loss 0.937273, acc 0.5625, f1 0.524857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T15:55:15.086533: step 1525, loss 0.937913, acc 0.609375, f1 0.55173\n",
      "2017-11-26T15:55:15.225662: step 1530, loss 1.08023, acc 0.476562, f1 0.413848\n",
      "2017-11-26T15:55:15.361218: step 1535, loss 0.97346, acc 0.617188, f1 0.574224\n",
      "2017-11-26T15:55:15.509231: step 1540, loss 0.930112, acc 0.578125, f1 0.547815\n",
      "2017-11-26T15:55:15.657940: step 1545, loss 0.965803, acc 0.546875, f1 0.500072\n",
      "2017-11-26T15:55:15.809770: step 1550, loss 0.969812, acc 0.5, f1 0.459645\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T15:55:15.988815: step 1551, loss 1.01318, acc 0.540859, f1 0.495753\n",
      "\n",
      "\n",
      "Current epoch:  11\n",
      "2017-11-26T15:55:16.127641: step 1555, loss 0.956347, acc 0.59375, f1 0.542806\n",
      "2017-11-26T15:55:16.259900: step 1560, loss 0.979669, acc 0.554688, f1 0.502436\n",
      "2017-11-26T15:55:16.407872: step 1565, loss 1.0213, acc 0.492188, f1 0.437275\n",
      "2017-11-26T15:55:16.543694: step 1570, loss 0.976151, acc 0.539062, f1 0.483411\n",
      "2017-11-26T15:55:16.681523: step 1575, loss 0.960346, acc 0.539062, f1 0.491534\n",
      "2017-11-26T15:55:16.835433: step 1580, loss 1.02956, acc 0.539062, f1 0.481461\n",
      "2017-11-26T15:55:16.975212: step 1585, loss 0.995388, acc 0.546875, f1 0.488254\n",
      "2017-11-26T15:55:17.109278: step 1590, loss 0.929039, acc 0.578125, f1 0.545403\n",
      "2017-11-26T15:55:17.269186: step 1595, loss 1.03911, acc 0.46875, f1 0.422269\n",
      "2017-11-26T15:55:17.422701: step 1600, loss 0.969003, acc 0.515625, f1 0.463078\n",
      "2017-11-26T15:55:17.578083: step 1605, loss 0.983364, acc 0.554688, f1 0.489379\n",
      "2017-11-26T15:55:17.722715: step 1610, loss 1.0126, acc 0.523438, f1 0.476844\n",
      "2017-11-26T15:55:17.875256: step 1615, loss 0.88789, acc 0.671875, f1 0.637494\n",
      "2017-11-26T15:55:18.030425: step 1620, loss 1.00685, acc 0.484375, f1 0.441405\n",
      "2017-11-26T15:55:18.179184: step 1625, loss 1.05342, acc 0.453125, f1 0.401002\n",
      "2017-11-26T15:55:18.328645: step 1630, loss 1.04039, acc 0.507812, f1 0.458212\n",
      "2017-11-26T15:55:18.478513: step 1635, loss 1.0096, acc 0.546875, f1 0.514834\n",
      "2017-11-26T15:55:18.622285: step 1640, loss 1.02827, acc 0.46875, f1 0.412282\n",
      "2017-11-26T15:55:18.764644: step 1645, loss 0.917178, acc 0.578125, f1 0.545271\n",
      "2017-11-26T15:55:18.906995: step 1650, loss 1.02955, acc 0.460938, f1 0.414782\n",
      "2017-11-26T15:55:19.045535: step 1655, loss 0.978623, acc 0.492188, f1 0.474236\n",
      "2017-11-26T15:55:19.190871: step 1660, loss 0.934427, acc 0.578125, f1 0.531576\n",
      "2017-11-26T15:55:19.345170: step 1665, loss 0.932531, acc 0.554688, f1 0.517158\n",
      "2017-11-26T15:55:19.488475: step 1670, loss 0.989827, acc 0.523438, f1 0.471284\n",
      "2017-11-26T15:55:19.629560: step 1675, loss 1.00784, acc 0.554688, f1 0.494271\n",
      "2017-11-26T15:55:19.784761: step 1680, loss 0.973448, acc 0.53125, f1 0.474656\n",
      "2017-11-26T15:55:19.936480: step 1685, loss 1.07218, acc 0.5, f1 0.431151\n",
      "2017-11-26T15:55:20.059414: step 1690, loss 0.991915, acc 0.507812, f1 0.444754\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T15:55:20.265206: step 1692, loss 0.99533, acc 0.554091, f1 0.502056\n",
      "\n",
      "\n",
      "Current epoch:  12\n",
      "2017-11-26T15:55:20.330270: step 1695, loss 1.00923, acc 0.523438, f1 0.485344\n",
      "2017-11-26T15:55:20.470559: step 1700, loss 0.952878, acc 0.53125, f1 0.498702\n",
      "2017-11-26T15:55:20.605447: step 1705, loss 0.98053, acc 0.5625, f1 0.521245\n",
      "2017-11-26T15:55:20.738331: step 1710, loss 0.927941, acc 0.570312, f1 0.525647\n",
      "2017-11-26T15:55:20.878698: step 1715, loss 0.949445, acc 0.53125, f1 0.479397\n",
      "2017-11-26T15:55:21.016549: step 1720, loss 0.919064, acc 0.578125, f1 0.542756\n",
      "2017-11-26T15:55:21.156649: step 1725, loss 0.997215, acc 0.515625, f1 0.464098\n",
      "2017-11-26T15:55:21.300805: step 1730, loss 0.933093, acc 0.578125, f1 0.557735\n",
      "2017-11-26T15:55:21.449150: step 1735, loss 0.960696, acc 0.578125, f1 0.542079\n",
      "2017-11-26T15:55:21.588690: step 1740, loss 0.990954, acc 0.546875, f1 0.500874\n",
      "2017-11-26T15:55:21.737933: step 1745, loss 0.939326, acc 0.578125, f1 0.538573\n",
      "2017-11-26T15:55:21.892064: step 1750, loss 1.02042, acc 0.507812, f1 0.465823\n",
      "2017-11-26T15:55:22.027993: step 1755, loss 1.02088, acc 0.53125, f1 0.463708\n",
      "2017-11-26T15:55:22.166488: step 1760, loss 0.983433, acc 0.585938, f1 0.539986\n",
      "2017-11-26T15:55:22.319339: step 1765, loss 1.01331, acc 0.484375, f1 0.43307\n",
      "2017-11-26T15:55:22.456627: step 1770, loss 0.965265, acc 0.585938, f1 0.538449\n",
      "2017-11-26T15:55:22.599463: step 1775, loss 0.996229, acc 0.523438, f1 0.47314\n",
      "2017-11-26T15:55:22.738447: step 1780, loss 0.952386, acc 0.570312, f1 0.521808\n",
      "2017-11-26T15:55:22.887261: step 1785, loss 0.990962, acc 0.523438, f1 0.481704\n",
      "2017-11-26T15:55:23.034175: step 1790, loss 1.04414, acc 0.46875, f1 0.414634\n",
      "2017-11-26T15:55:23.181693: step 1795, loss 1.01361, acc 0.53125, f1 0.473235\n",
      "2017-11-26T15:55:23.332910: step 1800, loss 1.01011, acc 0.59375, f1 0.542559\n",
      "2017-11-26T15:55:23.478100: step 1805, loss 1.0117, acc 0.5625, f1 0.498308\n",
      "2017-11-26T15:55:23.618877: step 1810, loss 1.00124, acc 0.507812, f1 0.467722\n",
      "2017-11-26T15:55:23.748788: step 1815, loss 0.947907, acc 0.5625, f1 0.518188\n",
      "2017-11-26T15:55:23.887206: step 1820, loss 0.985844, acc 0.5, f1 0.459313\n",
      "2017-11-26T15:55:24.023399: step 1825, loss 0.985581, acc 0.59375, f1 0.528244\n",
      "2017-11-26T15:55:24.148049: step 1830, loss 1.01965, acc 0.460938, f1 0.419299\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T15:55:24.385683: step 1833, loss 0.985888, acc 0.554139, f1 0.48782\n",
      "\n",
      "\n",
      "Current epoch:  13\n",
      "2017-11-26T15:55:24.452593: step 1835, loss 0.909292, acc 0.507812, f1 0.47201\n",
      "2017-11-26T15:55:24.584562: step 1840, loss 0.96994, acc 0.585938, f1 0.548553\n",
      "2017-11-26T15:55:24.722921: step 1845, loss 1.0171, acc 0.585938, f1 0.523028\n",
      "2017-11-26T15:55:24.867704: step 1850, loss 0.998726, acc 0.507812, f1 0.461772\n",
      "2017-11-26T15:55:24.999105: step 1855, loss 0.943376, acc 0.539062, f1 0.494422\n",
      "2017-11-26T15:55:25.144295: step 1860, loss 0.980104, acc 0.546875, f1 0.490042\n",
      "2017-11-26T15:55:25.289146: step 1865, loss 0.950188, acc 0.53125, f1 0.486248\n",
      "2017-11-26T15:55:25.417767: step 1870, loss 1.04146, acc 0.484375, f1 0.410229\n",
      "2017-11-26T15:55:25.575746: step 1875, loss 0.99096, acc 0.492188, f1 0.447207\n",
      "2017-11-26T15:55:25.715297: step 1880, loss 0.945394, acc 0.554688, f1 0.530181\n",
      "2017-11-26T15:55:25.851023: step 1885, loss 1.00936, acc 0.515625, f1 0.466558\n",
      "2017-11-26T15:55:25.995926: step 1890, loss 0.955016, acc 0.585938, f1 0.534548\n",
      "2017-11-26T15:55:26.139798: step 1895, loss 0.928584, acc 0.554688, f1 0.510741\n",
      "2017-11-26T15:55:26.290068: step 1900, loss 1.04852, acc 0.492188, f1 0.441221\n",
      "2017-11-26T15:55:26.438777: step 1905, loss 0.991754, acc 0.53125, f1 0.488632\n",
      "2017-11-26T15:55:26.594200: step 1910, loss 1.00924, acc 0.5, f1 0.456847\n",
      "2017-11-26T15:55:26.736828: step 1915, loss 0.985349, acc 0.570312, f1 0.510057\n",
      "2017-11-26T15:55:26.878277: step 1920, loss 0.920279, acc 0.625, f1 0.571912\n",
      "2017-11-26T15:55:27.027844: step 1925, loss 0.87848, acc 0.648438, f1 0.617025\n",
      "2017-11-26T15:55:27.184697: step 1930, loss 0.978996, acc 0.578125, f1 0.522003\n",
      "2017-11-26T15:55:27.331648: step 1935, loss 1.00473, acc 0.507812, f1 0.458754\n",
      "2017-11-26T15:55:27.479478: step 1940, loss 0.882591, acc 0.625, f1 0.592415\n",
      "2017-11-26T15:55:27.629228: step 1945, loss 1.01885, acc 0.507812, f1 0.4539\n",
      "2017-11-26T15:55:27.765760: step 1950, loss 1.02205, acc 0.5, f1 0.460406\n",
      "2017-11-26T15:55:27.910940: step 1955, loss 1.03497, acc 0.5, f1 0.451251\n",
      "2017-11-26T15:55:28.044068: step 1960, loss 0.985423, acc 0.578125, f1 0.531789\n",
      "2017-11-26T15:55:28.196361: step 1965, loss 1.01455, acc 0.507812, f1 0.452323\n",
      "2017-11-26T15:55:28.330335: step 1970, loss 1.00001, acc 0.546875, f1 0.490392\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T15:55:28.593368: step 1974, loss 1.0023, acc 0.546529, f1 0.511056\n",
      "\n",
      "\n",
      "Current epoch:  14\n",
      "2017-11-26T15:55:28.655648: step 1975, loss 0.97124, acc 0.554688, f1 0.508486\n",
      "2017-11-26T15:55:28.802939: step 1980, loss 0.958724, acc 0.59375, f1 0.539118\n",
      "2017-11-26T15:55:28.939917: step 1985, loss 0.896065, acc 0.640625, f1 0.609695\n",
      "2017-11-26T15:55:29.077163: step 1990, loss 0.956141, acc 0.546875, f1 0.504669\n",
      "2017-11-26T15:55:29.219961: step 1995, loss 0.962561, acc 0.53125, f1 0.486369\n",
      "2017-11-26T15:55:29.363504: step 2000, loss 1.01848, acc 0.515625, f1 0.467489\n",
      "2017-11-26T15:55:29.497994: step 2005, loss 0.931198, acc 0.617188, f1 0.565413\n",
      "2017-11-26T15:55:29.640342: step 2010, loss 1.02505, acc 0.445312, f1 0.406486\n",
      "2017-11-26T15:55:29.781999: step 2015, loss 0.877512, acc 0.609375, f1 0.581563\n",
      "2017-11-26T15:55:29.926512: step 2020, loss 1.02365, acc 0.5, f1 0.457081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T15:55:30.068217: step 2025, loss 1.00594, acc 0.53125, f1 0.488012\n",
      "2017-11-26T15:55:30.206725: step 2030, loss 0.865405, acc 0.625, f1 0.60409\n",
      "2017-11-26T15:55:30.350439: step 2035, loss 0.930071, acc 0.617188, f1 0.595363\n",
      "2017-11-26T15:55:30.487977: step 2040, loss 1.01759, acc 0.507812, f1 0.452098\n",
      "2017-11-26T15:55:30.643671: step 2045, loss 1.04569, acc 0.5, f1 0.45722\n",
      "2017-11-26T15:55:30.782998: step 2050, loss 0.948918, acc 0.570312, f1 0.529062\n",
      "2017-11-26T15:55:30.925487: step 2055, loss 0.945159, acc 0.523438, f1 0.490741\n",
      "2017-11-26T15:55:31.070824: step 2060, loss 1.05909, acc 0.453125, f1 0.396066\n",
      "2017-11-26T15:55:31.221518: step 2065, loss 0.999273, acc 0.429688, f1 0.383295\n",
      "2017-11-26T15:55:31.363202: step 2070, loss 0.989893, acc 0.515625, f1 0.464391\n",
      "2017-11-26T15:55:31.512766: step 2075, loss 1.08543, acc 0.351562, f1 0.301303\n",
      "2017-11-26T15:55:31.654328: step 2080, loss 0.990252, acc 0.515625, f1 0.476284\n",
      "2017-11-26T15:55:31.797194: step 2085, loss 0.984472, acc 0.484375, f1 0.422801\n",
      "2017-11-26T15:55:31.946104: step 2090, loss 0.994938, acc 0.515625, f1 0.458132\n",
      "2017-11-26T15:55:32.096260: step 2095, loss 0.884355, acc 0.5625, f1 0.523904\n",
      "2017-11-26T15:55:32.237900: step 2100, loss 1.03459, acc 0.507812, f1 0.460369\n",
      "2017-11-26T15:55:32.380757: step 2105, loss 0.981871, acc 0.5625, f1 0.49881\n",
      "2017-11-26T15:55:32.530095: step 2110, loss 0.91179, acc 0.570312, f1 0.528655\n",
      "2017-11-26T15:55:32.681916: step 2115, loss 0.921965, acc 0.596774, f1 0.554068\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T15:55:32.833671: step 2115, loss 0.982067, acc 0.553994, f1 0.479491\n",
      "\n",
      "\n",
      "Current epoch:  15\n",
      "2017-11-26T15:55:33.025262: step 2120, loss 1.01083, acc 0.492188, f1 0.447567\n",
      "2017-11-26T15:55:33.159812: step 2125, loss 0.994831, acc 0.554688, f1 0.499208\n",
      "2017-11-26T15:55:33.300568: step 2130, loss 0.87114, acc 0.625, f1 0.587441\n",
      "2017-11-26T15:55:33.449600: step 2135, loss 0.921379, acc 0.601562, f1 0.546786\n",
      "2017-11-26T15:55:33.575019: step 2140, loss 0.952283, acc 0.59375, f1 0.559096\n",
      "2017-11-26T15:55:33.730003: step 2145, loss 0.980853, acc 0.507812, f1 0.472677\n",
      "2017-11-26T15:55:33.877236: step 2150, loss 1.06721, acc 0.46875, f1 0.410917\n",
      "2017-11-26T15:55:34.019595: step 2155, loss 0.932261, acc 0.5625, f1 0.524684\n",
      "2017-11-26T15:55:34.172736: step 2160, loss 1.04616, acc 0.484375, f1 0.429038\n",
      "2017-11-26T15:55:34.333134: step 2165, loss 0.923939, acc 0.601562, f1 0.554084\n",
      "2017-11-26T15:55:34.472923: step 2170, loss 0.88843, acc 0.65625, f1 0.622393\n",
      "2017-11-26T15:55:34.595642: step 2175, loss 0.982033, acc 0.515625, f1 0.464978\n",
      "2017-11-26T15:55:34.731927: step 2180, loss 1.07571, acc 0.445312, f1 0.399912\n",
      "2017-11-26T15:55:34.862400: step 2185, loss 1.00449, acc 0.523438, f1 0.479239\n",
      "2017-11-26T15:55:34.996814: step 2190, loss 1.00566, acc 0.523438, f1 0.483362\n",
      "2017-11-26T15:55:35.145976: step 2195, loss 0.945282, acc 0.523438, f1 0.481217\n",
      "2017-11-26T15:55:35.290698: step 2200, loss 0.972599, acc 0.523438, f1 0.472569\n",
      "2017-11-26T15:55:35.431765: step 2205, loss 0.965447, acc 0.609375, f1 0.55845\n",
      "2017-11-26T15:55:35.572685: step 2210, loss 1.01004, acc 0.492188, f1 0.453672\n",
      "2017-11-26T15:55:35.707293: step 2215, loss 0.907318, acc 0.539062, f1 0.509574\n",
      "2017-11-26T15:55:35.850120: step 2220, loss 0.989355, acc 0.53125, f1 0.477892\n",
      "2017-11-26T15:55:35.992708: step 2225, loss 0.923291, acc 0.601562, f1 0.531487\n",
      "2017-11-26T15:55:36.137404: step 2230, loss 1.04212, acc 0.476562, f1 0.41318\n",
      "2017-11-26T15:55:36.285749: step 2235, loss 0.949805, acc 0.59375, f1 0.542092\n",
      "2017-11-26T15:55:36.431317: step 2240, loss 0.932654, acc 0.554688, f1 0.548952\n",
      "2017-11-26T15:55:36.582877: step 2245, loss 0.945464, acc 0.570312, f1 0.526894\n",
      "2017-11-26T15:55:36.736014: step 2250, loss 0.931398, acc 0.546875, f1 0.517042\n",
      "2017-11-26T15:55:36.876643: step 2255, loss 0.951759, acc 0.546875, f1 0.508776\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T15:55:37.053676: step 2256, loss 1.02258, acc 0.497819, f1 0.456506\n",
      "\n",
      "\n",
      "Current epoch:  16\n",
      "2017-11-26T15:55:37.195921: step 2260, loss 0.934834, acc 0.601562, f1 0.562572\n",
      "2017-11-26T15:55:37.341274: step 2265, loss 0.953223, acc 0.53125, f1 0.483124\n",
      "2017-11-26T15:55:37.483700: step 2270, loss 0.883299, acc 0.632812, f1 0.599451\n",
      "2017-11-26T15:55:37.624178: step 2275, loss 0.989799, acc 0.492188, f1 0.461931\n",
      "2017-11-26T15:55:37.764249: step 2280, loss 0.961988, acc 0.539062, f1 0.487706\n",
      "2017-11-26T15:55:37.902287: step 2285, loss 0.959891, acc 0.492188, f1 0.446488\n",
      "2017-11-26T15:55:38.045678: step 2290, loss 0.933249, acc 0.578125, f1 0.533335\n",
      "2017-11-26T15:55:38.191917: step 2295, loss 1.00634, acc 0.546875, f1 0.481887\n",
      "2017-11-26T15:55:38.338592: step 2300, loss 0.969459, acc 0.570312, f1 0.540672\n",
      "2017-11-26T15:55:38.478196: step 2305, loss 0.951361, acc 0.5625, f1 0.501415\n",
      "2017-11-26T15:55:38.622096: step 2310, loss 1.01711, acc 0.507812, f1 0.49746\n",
      "2017-11-26T15:55:38.766789: step 2315, loss 0.928574, acc 0.546875, f1 0.500225\n",
      "2017-11-26T15:55:38.914163: step 2320, loss 0.96149, acc 0.59375, f1 0.534647\n",
      "2017-11-26T15:55:39.057728: step 2325, loss 0.98207, acc 0.539062, f1 0.507091\n",
      "2017-11-26T15:55:39.203346: step 2330, loss 1.07104, acc 0.492188, f1 0.414125\n",
      "2017-11-26T15:55:39.345962: step 2335, loss 0.924525, acc 0.59375, f1 0.551496\n",
      "2017-11-26T15:55:39.493136: step 2340, loss 0.959238, acc 0.578125, f1 0.521111\n",
      "2017-11-26T15:55:39.643345: step 2345, loss 0.925277, acc 0.601562, f1 0.568621\n",
      "2017-11-26T15:55:39.788010: step 2350, loss 0.968315, acc 0.507812, f1 0.453657\n",
      "2017-11-26T15:55:39.931176: step 2355, loss 0.880451, acc 0.59375, f1 0.571384\n",
      "2017-11-26T15:55:40.061428: step 2360, loss 0.900755, acc 0.570312, f1 0.532475\n",
      "2017-11-26T15:55:40.213690: step 2365, loss 0.997598, acc 0.539062, f1 0.498501\n",
      "2017-11-26T15:55:40.347025: step 2370, loss 0.934412, acc 0.5625, f1 0.529472\n",
      "2017-11-26T15:55:40.498973: step 2375, loss 0.95772, acc 0.539062, f1 0.510221\n",
      "2017-11-26T15:55:40.655350: step 2380, loss 0.923212, acc 0.59375, f1 0.548913\n",
      "2017-11-26T15:55:40.788496: step 2385, loss 0.932258, acc 0.539062, f1 0.495597\n",
      "2017-11-26T15:55:40.936339: step 2390, loss 0.980657, acc 0.546875, f1 0.512595\n",
      "2017-11-26T15:55:41.085744: step 2395, loss 0.986109, acc 0.570312, f1 0.554629\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T15:55:41.303245: step 2397, loss 1.01171, acc 0.508966, f1 0.471928\n",
      "\n",
      "\n",
      "Current epoch:  17\n",
      "2017-11-26T15:55:41.424698: step 2400, loss 0.944392, acc 0.554688, f1 0.519004\n",
      "2017-11-26T15:55:41.567512: step 2405, loss 0.945462, acc 0.507812, f1 0.470215\n",
      "2017-11-26T15:55:41.696633: step 2410, loss 0.96144, acc 0.5625, f1 0.522062\n",
      "2017-11-26T15:55:41.845994: step 2415, loss 0.984487, acc 0.539062, f1 0.488092\n",
      "2017-11-26T15:55:41.995771: step 2420, loss 0.943492, acc 0.632812, f1 0.582161\n",
      "2017-11-26T15:55:42.137717: step 2425, loss 0.949029, acc 0.5625, f1 0.506801\n",
      "2017-11-26T15:55:42.275298: step 2430, loss 0.877701, acc 0.609375, f1 0.581342\n",
      "2017-11-26T15:55:42.421841: step 2435, loss 1.02891, acc 0.476562, f1 0.424591\n",
      "2017-11-26T15:55:42.578965: step 2440, loss 0.970009, acc 0.507812, f1 0.475927\n",
      "2017-11-26T15:55:42.727786: step 2445, loss 0.968479, acc 0.5625, f1 0.545636\n",
      "2017-11-26T15:55:42.871199: step 2450, loss 1.12254, acc 0.421875, f1 0.354821\n",
      "2017-11-26T15:55:43.015809: step 2455, loss 0.96432, acc 0.507812, f1 0.474228\n",
      "2017-11-26T15:55:43.166512: step 2460, loss 0.976521, acc 0.578125, f1 0.520024\n",
      "2017-11-26T15:55:43.316479: step 2465, loss 0.973724, acc 0.523438, f1 0.477389\n",
      "2017-11-26T15:55:43.465614: step 2470, loss 0.917986, acc 0.578125, f1 0.539474\n",
      "2017-11-26T15:55:43.621190: step 2475, loss 0.985204, acc 0.578125, f1 0.515511\n",
      "2017-11-26T15:55:43.771891: step 2480, loss 0.863145, acc 0.554688, f1 0.520247\n",
      "2017-11-26T15:55:43.917743: step 2485, loss 0.939796, acc 0.539062, f1 0.517823\n",
      "2017-11-26T15:55:44.067209: step 2490, loss 0.985162, acc 0.539062, f1 0.488624\n",
      "2017-11-26T15:55:44.203309: step 2495, loss 0.959691, acc 0.515625, f1 0.483705\n",
      "2017-11-26T15:55:44.356683: step 2500, loss 0.874879, acc 0.59375, f1 0.559561\n",
      "2017-11-26T15:55:44.506444: step 2505, loss 0.992873, acc 0.460938, f1 0.4175\n",
      "2017-11-26T15:55:44.643759: step 2510, loss 1.04586, acc 0.46875, f1 0.405927\n",
      "2017-11-26T15:55:44.786055: step 2515, loss 0.97146, acc 0.546875, f1 0.502637\n",
      "2017-11-26T15:55:44.935712: step 2520, loss 0.971959, acc 0.484375, f1 0.447472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T15:55:45.081595: step 2525, loss 0.944411, acc 0.578125, f1 0.542259\n",
      "2017-11-26T15:55:45.213447: step 2530, loss 0.974095, acc 0.53125, f1 0.498453\n",
      "2017-11-26T15:55:45.347290: step 2535, loss 1.01056, acc 0.539062, f1 0.492188\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T15:55:45.584348: step 2538, loss 1.00212, acc 0.523943, f1 0.486029\n",
      "\n",
      "\n",
      "Current epoch:  18\n",
      "2017-11-26T15:55:45.665482: step 2540, loss 0.980529, acc 0.570312, f1 0.535073\n",
      "2017-11-26T15:55:45.819531: step 2545, loss 0.979924, acc 0.5625, f1 0.518357\n",
      "2017-11-26T15:55:45.965136: step 2550, loss 0.928549, acc 0.539062, f1 0.508918\n",
      "2017-11-26T15:55:46.105144: step 2555, loss 0.903685, acc 0.578125, f1 0.552104\n",
      "2017-11-26T15:55:46.242908: step 2560, loss 1.03063, acc 0.507812, f1 0.456549\n",
      "2017-11-26T15:55:46.393446: step 2565, loss 0.923086, acc 0.609375, f1 0.575342\n",
      "2017-11-26T15:55:46.535398: step 2570, loss 0.945528, acc 0.625, f1 0.592773\n",
      "2017-11-26T15:55:46.684650: step 2575, loss 0.91964, acc 0.570312, f1 0.534387\n",
      "2017-11-26T15:55:46.827639: step 2580, loss 1.06039, acc 0.492188, f1 0.434048\n",
      "2017-11-26T15:55:46.965830: step 2585, loss 0.901206, acc 0.585938, f1 0.561031\n",
      "2017-11-26T15:55:47.101597: step 2590, loss 0.915819, acc 0.601562, f1 0.554489\n",
      "2017-11-26T15:55:47.246420: step 2595, loss 0.947347, acc 0.578125, f1 0.528815\n",
      "2017-11-26T15:55:47.384172: step 2600, loss 0.943919, acc 0.570312, f1 0.524729\n",
      "2017-11-26T15:55:47.544420: step 2605, loss 0.93782, acc 0.554688, f1 0.544868\n",
      "2017-11-26T15:55:47.691647: step 2610, loss 0.915159, acc 0.625, f1 0.606268\n",
      "2017-11-26T15:55:47.840000: step 2615, loss 1.04367, acc 0.460938, f1 0.439426\n",
      "2017-11-26T15:55:47.984062: step 2620, loss 0.923893, acc 0.601562, f1 0.551347\n",
      "2017-11-26T15:55:48.138402: step 2625, loss 0.884784, acc 0.554688, f1 0.523271\n",
      "2017-11-26T15:55:48.281188: step 2630, loss 0.957206, acc 0.507812, f1 0.467238\n",
      "2017-11-26T15:55:48.416377: step 2635, loss 0.906188, acc 0.570312, f1 0.529756\n",
      "2017-11-26T15:55:48.561012: step 2640, loss 0.951751, acc 0.53125, f1 0.487615\n",
      "2017-11-26T15:55:48.712087: step 2645, loss 0.961545, acc 0.5, f1 0.43147\n",
      "2017-11-26T15:55:48.846524: step 2650, loss 1.02277, acc 0.484375, f1 0.374232\n",
      "2017-11-26T15:55:49.003432: step 2655, loss 0.958068, acc 0.554688, f1 0.510165\n",
      "2017-11-26T15:55:49.159396: step 2660, loss 0.940407, acc 0.523438, f1 0.494068\n",
      "2017-11-26T15:55:49.304730: step 2665, loss 1.03343, acc 0.445312, f1 0.418955\n",
      "2017-11-26T15:55:49.450123: step 2670, loss 0.958589, acc 0.523438, f1 0.48219\n",
      "2017-11-26T15:55:49.599501: step 2675, loss 0.899826, acc 0.578125, f1 0.542332\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T15:55:49.865756: step 2679, loss 0.99736, acc 0.530099, f1 0.491715\n",
      "\n",
      "\n",
      "Current epoch:  19\n",
      "2017-11-26T15:55:49.942954: step 2680, loss 0.934301, acc 0.570312, f1 0.539828\n",
      "2017-11-26T15:55:50.081588: step 2685, loss 1.02696, acc 0.484375, f1 0.422728\n",
      "2017-11-26T15:55:50.228553: step 2690, loss 0.836241, acc 0.648438, f1 0.618188\n",
      "2017-11-26T15:55:50.373679: step 2695, loss 0.940089, acc 0.59375, f1 0.560014\n",
      "2017-11-26T15:55:50.518848: step 2700, loss 0.999893, acc 0.5, f1 0.463764\n",
      "2017-11-26T15:55:50.659277: step 2705, loss 0.951091, acc 0.5625, f1 0.516563\n",
      "2017-11-26T15:55:50.795968: step 2710, loss 0.996526, acc 0.5, f1 0.444329\n",
      "2017-11-26T15:55:50.917969: step 2715, loss 0.964341, acc 0.523438, f1 0.477236\n",
      "2017-11-26T15:55:51.054067: step 2720, loss 1.01118, acc 0.523438, f1 0.476353\n",
      "2017-11-26T15:55:51.191269: step 2725, loss 0.949156, acc 0.476562, f1 0.439002\n",
      "2017-11-26T15:55:51.340769: step 2730, loss 0.954151, acc 0.53125, f1 0.479647\n",
      "2017-11-26T15:55:51.492619: step 2735, loss 0.945529, acc 0.5625, f1 0.506586\n",
      "2017-11-26T15:55:51.641980: step 2740, loss 0.94675, acc 0.539062, f1 0.507902\n",
      "2017-11-26T15:55:51.785123: step 2745, loss 0.956184, acc 0.484375, f1 0.444018\n",
      "2017-11-26T15:55:51.928202: step 2750, loss 0.993723, acc 0.460938, f1 0.418249\n",
      "2017-11-26T15:55:52.072236: step 2755, loss 0.965252, acc 0.523438, f1 0.474512\n",
      "2017-11-26T15:55:52.218541: step 2760, loss 0.991443, acc 0.523438, f1 0.478145\n",
      "2017-11-26T15:55:52.370540: step 2765, loss 0.896486, acc 0.601562, f1 0.55947\n",
      "2017-11-26T15:55:52.504024: step 2770, loss 0.9491, acc 0.601562, f1 0.559239\n",
      "2017-11-26T15:55:52.652364: step 2775, loss 0.951503, acc 0.539062, f1 0.482975\n",
      "2017-11-26T15:55:52.792859: step 2780, loss 0.962102, acc 0.523438, f1 0.481829\n",
      "2017-11-26T15:55:52.919477: step 2785, loss 0.969763, acc 0.585938, f1 0.579683\n",
      "2017-11-26T15:55:53.062979: step 2790, loss 0.878951, acc 0.65625, f1 0.619974\n",
      "2017-11-26T15:55:53.209942: step 2795, loss 0.978046, acc 0.53125, f1 0.478523\n",
      "2017-11-26T15:55:53.364002: step 2800, loss 0.94603, acc 0.507812, f1 0.463211\n",
      "2017-11-26T15:55:53.505228: step 2805, loss 0.970714, acc 0.53125, f1 0.441539\n",
      "2017-11-26T15:55:53.647431: step 2810, loss 0.932971, acc 0.578125, f1 0.542575\n",
      "2017-11-26T15:55:53.792287: step 2815, loss 0.918586, acc 0.554688, f1 0.527888\n",
      "2017-11-26T15:55:53.942654: step 2820, loss 0.909861, acc 0.612903, f1 0.587017\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T15:55:54.094515: step 2820, loss 1.02069, acc 0.500872, f1 0.45931\n",
      "\n",
      "\n",
      "Current epoch:  20\n",
      "2017-11-26T15:55:54.281593: step 2825, loss 0.966389, acc 0.554688, f1 0.518072\n",
      "2017-11-26T15:55:54.428561: step 2830, loss 0.896858, acc 0.578125, f1 0.539705\n",
      "2017-11-26T15:55:54.567909: step 2835, loss 1.00269, acc 0.554688, f1 0.499757\n",
      "2017-11-26T15:55:54.722194: step 2840, loss 0.966191, acc 0.546875, f1 0.517355\n",
      "2017-11-26T15:55:54.863021: step 2845, loss 0.972175, acc 0.59375, f1 0.52472\n",
      "2017-11-26T15:55:55.000728: step 2850, loss 0.960284, acc 0.476562, f1 0.441275\n",
      "2017-11-26T15:55:55.145350: step 2855, loss 0.951985, acc 0.539062, f1 0.509868\n",
      "2017-11-26T15:55:55.302725: step 2860, loss 0.824215, acc 0.664062, f1 0.628262\n",
      "2017-11-26T15:55:55.448375: step 2865, loss 1.07918, acc 0.492188, f1 0.42953\n",
      "2017-11-26T15:55:55.594779: step 2870, loss 0.932585, acc 0.5625, f1 0.512771\n",
      "2017-11-26T15:55:55.739100: step 2875, loss 0.89929, acc 0.59375, f1 0.545178\n",
      "2017-11-26T15:55:55.886736: step 2880, loss 0.879046, acc 0.59375, f1 0.559512\n",
      "2017-11-26T15:55:56.029647: step 2885, loss 0.979219, acc 0.554688, f1 0.537329\n",
      "2017-11-26T15:55:56.167137: step 2890, loss 0.972472, acc 0.546875, f1 0.506981\n",
      "2017-11-26T15:55:56.312979: step 2895, loss 0.860272, acc 0.664062, f1 0.62268\n",
      "2017-11-26T15:55:56.461580: step 2900, loss 0.953153, acc 0.523438, f1 0.470243\n",
      "2017-11-26T15:55:56.599820: step 2905, loss 0.988358, acc 0.554688, f1 0.491319\n",
      "2017-11-26T15:55:56.743518: step 2910, loss 0.984365, acc 0.515625, f1 0.456881\n",
      "2017-11-26T15:55:56.894626: step 2915, loss 0.869491, acc 0.609375, f1 0.559716\n",
      "2017-11-26T15:55:57.041646: step 2920, loss 0.920826, acc 0.554688, f1 0.495585\n",
      "2017-11-26T15:55:57.183844: step 2925, loss 1.07284, acc 0.429688, f1 0.357806\n",
      "2017-11-26T15:55:57.321827: step 2930, loss 0.844261, acc 0.632812, f1 0.576852\n",
      "2017-11-26T15:55:57.470608: step 2935, loss 0.899514, acc 0.585938, f1 0.536756\n",
      "2017-11-26T15:55:57.622047: step 2940, loss 0.957506, acc 0.523438, f1 0.470131\n",
      "2017-11-26T15:55:57.771008: step 2945, loss 0.962666, acc 0.539062, f1 0.486439\n",
      "2017-11-26T15:55:57.919646: step 2950, loss 1.02044, acc 0.46875, f1 0.432221\n",
      "2017-11-26T15:55:58.076137: step 2955, loss 0.926048, acc 0.578125, f1 0.545686\n",
      "2017-11-26T15:55:58.219037: step 2960, loss 0.971275, acc 0.523438, f1 0.460292\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T15:55:58.399408: step 2961, loss 0.990896, acc 0.537951, f1 0.497316\n",
      "\n",
      "\n",
      "Current epoch:  21\n",
      "2017-11-26T15:55:58.524266: step 2965, loss 0.933031, acc 0.515625, f1 0.486574\n",
      "2017-11-26T15:55:58.669383: step 2970, loss 0.871201, acc 0.609375, f1 0.562737\n",
      "2017-11-26T15:55:58.807506: step 2975, loss 0.934295, acc 0.570312, f1 0.532995\n",
      "2017-11-26T15:55:58.945482: step 2980, loss 0.980325, acc 0.578125, f1 0.513274\n",
      "2017-11-26T15:55:59.084897: step 2985, loss 0.827428, acc 0.664062, f1 0.6286\n",
      "2017-11-26T15:55:59.224327: step 2990, loss 0.940028, acc 0.53125, f1 0.487812\n",
      "2017-11-26T15:55:59.365156: step 2995, loss 0.974305, acc 0.429688, f1 0.403849\n",
      "2017-11-26T15:55:59.504155: step 3000, loss 0.920616, acc 0.546875, f1 0.472538\n",
      "2017-11-26T15:55:59.644472: step 3005, loss 0.946902, acc 0.585938, f1 0.538842\n",
      "2017-11-26T15:55:59.782203: step 3010, loss 1.05383, acc 0.46875, f1 0.394326\n",
      "2017-11-26T15:55:59.928896: step 3015, loss 0.976401, acc 0.539062, f1 0.482244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T15:56:00.061252: step 3020, loss 0.995687, acc 0.546875, f1 0.505447\n",
      "2017-11-26T15:56:00.203972: step 3025, loss 0.910039, acc 0.554688, f1 0.541554\n",
      "2017-11-26T15:56:00.346782: step 3030, loss 0.96638, acc 0.53125, f1 0.471246\n",
      "2017-11-26T15:56:00.465289: step 3035, loss 0.952094, acc 0.523438, f1 0.475096\n",
      "2017-11-26T15:56:00.589949: step 3040, loss 0.899886, acc 0.554688, f1 0.523082\n",
      "2017-11-26T15:56:00.721560: step 3045, loss 0.883048, acc 0.617188, f1 0.590748\n",
      "2017-11-26T15:56:00.853534: step 3050, loss 0.896356, acc 0.5625, f1 0.526545\n",
      "2017-11-26T15:56:00.993647: step 3055, loss 0.90965, acc 0.585938, f1 0.544921\n",
      "2017-11-26T15:56:01.121702: step 3060, loss 0.9152, acc 0.601562, f1 0.564593\n",
      "2017-11-26T15:56:01.262498: step 3065, loss 0.980338, acc 0.5, f1 0.450791\n",
      "2017-11-26T15:56:01.392284: step 3070, loss 0.871894, acc 0.585938, f1 0.550218\n",
      "2017-11-26T15:56:01.519565: step 3075, loss 0.958573, acc 0.554688, f1 0.491077\n",
      "2017-11-26T15:56:01.676523: step 3080, loss 0.96812, acc 0.515625, f1 0.441303\n",
      "2017-11-26T15:56:01.819431: step 3085, loss 0.928549, acc 0.625, f1 0.584216\n",
      "2017-11-26T15:56:01.968671: step 3090, loss 0.971532, acc 0.632812, f1 0.62966\n",
      "2017-11-26T15:56:02.113823: step 3095, loss 1.02167, acc 0.460938, f1 0.364927\n",
      "2017-11-26T15:56:02.264841: step 3100, loss 0.970072, acc 0.460938, f1 0.405456\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T15:56:02.470000: step 3102, loss 1.01303, acc 0.508288, f1 0.482332\n",
      "\n",
      "\n",
      "Current epoch:  22\n",
      "2017-11-26T15:56:02.591239: step 3105, loss 0.958883, acc 0.539062, f1 0.489828\n",
      "2017-11-26T15:56:02.739578: step 3110, loss 0.927844, acc 0.585938, f1 0.562057\n",
      "2017-11-26T15:56:02.888437: step 3115, loss 0.937692, acc 0.554688, f1 0.497936\n",
      "2017-11-26T15:56:03.037197: step 3120, loss 0.971071, acc 0.523438, f1 0.475387\n",
      "2017-11-26T15:56:03.191083: step 3125, loss 0.919806, acc 0.601562, f1 0.54995\n",
      "2017-11-26T15:56:03.336552: step 3130, loss 0.915477, acc 0.601562, f1 0.587264\n",
      "2017-11-26T15:56:03.478752: step 3135, loss 0.913503, acc 0.554688, f1 0.512238\n",
      "2017-11-26T15:56:03.635172: step 3140, loss 0.881254, acc 0.617188, f1 0.573171\n",
      "2017-11-26T15:56:03.780327: step 3145, loss 1.03597, acc 0.492188, f1 0.428717\n",
      "2017-11-26T15:56:03.919691: step 3150, loss 0.991627, acc 0.546875, f1 0.515924\n",
      "2017-11-26T15:56:04.061644: step 3155, loss 0.882719, acc 0.625, f1 0.589375\n",
      "2017-11-26T15:56:04.209922: step 3160, loss 0.867224, acc 0.5625, f1 0.518211\n",
      "2017-11-26T15:56:04.350528: step 3165, loss 0.912198, acc 0.609375, f1 0.565232\n",
      "2017-11-26T15:56:04.495756: step 3170, loss 0.964398, acc 0.554688, f1 0.510177\n",
      "2017-11-26T15:56:04.642684: step 3175, loss 0.947076, acc 0.546875, f1 0.495975\n",
      "2017-11-26T15:56:04.789031: step 3180, loss 0.97893, acc 0.492188, f1 0.433816\n",
      "2017-11-26T15:56:04.928909: step 3185, loss 0.960156, acc 0.570312, f1 0.516334\n",
      "2017-11-26T15:56:05.073434: step 3190, loss 1.00135, acc 0.507812, f1 0.456349\n",
      "2017-11-26T15:56:05.210857: step 3195, loss 0.941396, acc 0.53125, f1 0.471115\n",
      "2017-11-26T15:56:05.348257: step 3200, loss 0.900426, acc 0.609375, f1 0.568535\n",
      "2017-11-26T15:56:05.496935: step 3205, loss 0.935974, acc 0.570312, f1 0.541505\n",
      "2017-11-26T15:56:05.637692: step 3210, loss 0.971534, acc 0.492188, f1 0.440762\n",
      "2017-11-26T15:56:05.787143: step 3215, loss 1.04552, acc 0.421875, f1 0.366856\n",
      "2017-11-26T15:56:05.925538: step 3220, loss 0.918104, acc 0.554688, f1 0.519525\n",
      "2017-11-26T15:56:06.061655: step 3225, loss 0.99622, acc 0.5, f1 0.406152\n",
      "2017-11-26T15:56:06.207699: step 3230, loss 0.987102, acc 0.484375, f1 0.463038\n",
      "2017-11-26T15:56:06.352543: step 3235, loss 0.963802, acc 0.515625, f1 0.476588\n",
      "2017-11-26T15:56:06.487179: step 3240, loss 0.912593, acc 0.578125, f1 0.541045\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T15:56:06.708359: step 3243, loss 0.987755, acc 0.547596, f1 0.49845\n",
      "\n",
      "\n",
      "Current epoch:  23\n",
      "2017-11-26T15:56:06.805291: step 3245, loss 0.913343, acc 0.546875, f1 0.520088\n",
      "2017-11-26T15:56:06.945281: step 3250, loss 0.958852, acc 0.554688, f1 0.544892\n",
      "2017-11-26T15:56:07.081362: step 3255, loss 0.862869, acc 0.609375, f1 0.576921\n",
      "2017-11-26T15:56:07.237598: step 3260, loss 0.959669, acc 0.53125, f1 0.489282\n",
      "2017-11-26T15:56:07.382266: step 3265, loss 1.05409, acc 0.445312, f1 0.390875\n",
      "2017-11-26T15:56:07.522060: step 3270, loss 0.900727, acc 0.625, f1 0.604094\n",
      "2017-11-26T15:56:07.671294: step 3275, loss 0.883288, acc 0.601562, f1 0.564891\n",
      "2017-11-26T15:56:07.804260: step 3280, loss 0.970934, acc 0.5625, f1 0.524854\n",
      "2017-11-26T15:56:07.943144: step 3285, loss 0.915647, acc 0.53125, f1 0.482559\n",
      "2017-11-26T15:56:08.081878: step 3290, loss 1.01212, acc 0.507812, f1 0.454357\n",
      "2017-11-26T15:56:08.211468: step 3295, loss 0.965265, acc 0.601562, f1 0.558546\n",
      "2017-11-26T15:56:08.360128: step 3300, loss 0.981062, acc 0.507812, f1 0.470154\n",
      "2017-11-26T15:56:08.507629: step 3305, loss 0.964735, acc 0.5625, f1 0.529886\n",
      "2017-11-26T15:56:08.654786: step 3310, loss 0.990572, acc 0.546875, f1 0.489083\n",
      "2017-11-26T15:56:08.795805: step 3315, loss 0.965774, acc 0.515625, f1 0.478826\n",
      "2017-11-26T15:56:08.947484: step 3320, loss 1.01198, acc 0.5, f1 0.46311\n",
      "2017-11-26T15:56:09.089490: step 3325, loss 0.942226, acc 0.585938, f1 0.546774\n",
      "2017-11-26T15:56:09.229026: step 3330, loss 0.998427, acc 0.546875, f1 0.492338\n",
      "2017-11-26T15:56:09.369443: step 3335, loss 0.99393, acc 0.507812, f1 0.473687\n",
      "2017-11-26T15:56:09.510537: step 3340, loss 0.963667, acc 0.523438, f1 0.501306\n",
      "2017-11-26T15:56:09.650127: step 3345, loss 0.931782, acc 0.523438, f1 0.501004\n",
      "2017-11-26T15:56:09.795130: step 3350, loss 0.986617, acc 0.53125, f1 0.486543\n",
      "2017-11-26T15:56:09.943405: step 3355, loss 0.947788, acc 0.507812, f1 0.444267\n",
      "2017-11-26T15:56:10.086498: step 3360, loss 0.893443, acc 0.5625, f1 0.531971\n",
      "2017-11-26T15:56:10.223188: step 3365, loss 0.919058, acc 0.539062, f1 0.501855\n",
      "2017-11-26T15:56:10.363214: step 3370, loss 0.906518, acc 0.570312, f1 0.550515\n",
      "2017-11-26T15:56:10.490347: step 3375, loss 0.92907, acc 0.609375, f1 0.596276\n",
      "2017-11-26T15:56:10.639976: step 3380, loss 0.975343, acc 0.515625, f1 0.437341\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T15:56:10.890092: step 3384, loss 0.990459, acc 0.540616, f1 0.499943\n",
      "\n",
      "\n",
      "Current epoch:  24\n",
      "2017-11-26T15:56:10.938789: step 3385, loss 0.935358, acc 0.578125, f1 0.527532\n",
      "2017-11-26T15:56:11.088211: step 3390, loss 0.940978, acc 0.492188, f1 0.42761\n",
      "2017-11-26T15:56:11.230757: step 3395, loss 0.962896, acc 0.578125, f1 0.519631\n",
      "2017-11-26T15:56:11.380191: step 3400, loss 0.964688, acc 0.578125, f1 0.539628\n",
      "2017-11-26T15:56:11.520657: step 3405, loss 0.929738, acc 0.632812, f1 0.593467\n",
      "2017-11-26T15:56:11.652797: step 3410, loss 0.903935, acc 0.601562, f1 0.550204\n",
      "2017-11-26T15:56:11.791798: step 3415, loss 0.951347, acc 0.546875, f1 0.500389\n",
      "2017-11-26T15:56:11.929539: step 3420, loss 1.02664, acc 0.492188, f1 0.431727\n",
      "2017-11-26T15:56:12.066261: step 3425, loss 0.948327, acc 0.539062, f1 0.516741\n",
      "2017-11-26T15:56:12.220489: step 3430, loss 0.940801, acc 0.59375, f1 0.544965\n",
      "2017-11-26T15:56:12.372374: step 3435, loss 0.959418, acc 0.507812, f1 0.481403\n",
      "2017-11-26T15:56:12.515910: step 3440, loss 0.917548, acc 0.546875, f1 0.522304\n",
      "2017-11-26T15:56:12.648192: step 3445, loss 1.04654, acc 0.476562, f1 0.420855\n",
      "2017-11-26T15:56:12.789785: step 3450, loss 0.925296, acc 0.609375, f1 0.566125\n",
      "2017-11-26T15:56:12.941986: step 3455, loss 0.973971, acc 0.515625, f1 0.476567\n",
      "2017-11-26T15:56:13.091344: step 3460, loss 0.998884, acc 0.554688, f1 0.517283\n",
      "2017-11-26T15:56:13.218330: step 3465, loss 0.877725, acc 0.539062, f1 0.507219\n",
      "2017-11-26T15:56:13.358802: step 3470, loss 0.89203, acc 0.585938, f1 0.556209\n",
      "2017-11-26T15:56:13.513395: step 3475, loss 0.965281, acc 0.539062, f1 0.469845\n",
      "2017-11-26T15:56:13.669274: step 3480, loss 0.946459, acc 0.5, f1 0.42832\n",
      "2017-11-26T15:56:13.810630: step 3485, loss 0.96306, acc 0.601562, f1 0.575671\n",
      "2017-11-26T15:56:13.950196: step 3490, loss 0.933374, acc 0.578125, f1 0.549919\n",
      "2017-11-26T15:56:14.082120: step 3495, loss 0.900354, acc 0.578125, f1 0.55779\n",
      "2017-11-26T15:56:14.220939: step 3500, loss 0.883739, acc 0.5625, f1 0.519866\n",
      "2017-11-26T15:56:14.362596: step 3505, loss 1.01037, acc 0.507812, f1 0.45457\n",
      "2017-11-26T15:56:14.511328: step 3510, loss 0.934765, acc 0.546875, f1 0.500787\n",
      "2017-11-26T15:56:14.654184: step 3515, loss 0.931638, acc 0.617188, f1 0.573084\n",
      "2017-11-26T15:56:14.791397: step 3520, loss 0.948267, acc 0.484375, f1 0.436101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T15:56:14.928597: step 3525, loss 1.04542, acc 0.451613, f1 0.38299\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T15:56:15.079984: step 3525, loss 0.993547, acc 0.541973, f1 0.497999\n",
      "\n",
      "\n",
      "Current epoch:  25\n",
      "2017-11-26T15:56:15.246496: step 3530, loss 0.932669, acc 0.570312, f1 0.538821\n",
      "2017-11-26T15:56:15.396544: step 3535, loss 0.812019, acc 0.65625, f1 0.617816\n",
      "2017-11-26T15:56:15.532581: step 3540, loss 0.968497, acc 0.484375, f1 0.427346\n",
      "2017-11-26T15:56:15.664224: step 3545, loss 0.929676, acc 0.578125, f1 0.564845\n",
      "2017-11-26T15:56:15.795846: step 3550, loss 0.943067, acc 0.554688, f1 0.499604\n",
      "2017-11-26T15:56:15.931933: step 3555, loss 0.921401, acc 0.585938, f1 0.539315\n",
      "2017-11-26T15:56:16.075762: step 3560, loss 0.925444, acc 0.578125, f1 0.569993\n",
      "2017-11-26T15:56:16.217887: step 3565, loss 0.9239, acc 0.578125, f1 0.541349\n",
      "2017-11-26T15:56:16.377831: step 3570, loss 0.943247, acc 0.53125, f1 0.523922\n",
      "2017-11-26T15:56:16.526424: step 3575, loss 1.00353, acc 0.476562, f1 0.456818\n",
      "2017-11-26T15:56:16.678819: step 3580, loss 0.933509, acc 0.539062, f1 0.516392\n",
      "2017-11-26T15:56:16.828433: step 3585, loss 1.02226, acc 0.484375, f1 0.436917\n",
      "2017-11-26T15:56:16.971955: step 3590, loss 0.979065, acc 0.585938, f1 0.514025\n",
      "2017-11-26T15:56:17.110814: step 3595, loss 0.988295, acc 0.53125, f1 0.480668\n",
      "2017-11-26T15:56:17.266102: step 3600, loss 0.951318, acc 0.53125, f1 0.495826\n",
      "2017-11-26T15:56:17.409863: step 3605, loss 0.933146, acc 0.5625, f1 0.514467\n",
      "2017-11-26T15:56:17.554311: step 3610, loss 0.977087, acc 0.53125, f1 0.498091\n",
      "2017-11-26T15:56:17.677788: step 3615, loss 0.938948, acc 0.578125, f1 0.520882\n",
      "2017-11-26T15:56:17.824381: step 3620, loss 0.940779, acc 0.539062, f1 0.499669\n",
      "2017-11-26T15:56:17.968526: step 3625, loss 0.916147, acc 0.578125, f1 0.556077\n",
      "2017-11-26T15:56:18.100380: step 3630, loss 0.962629, acc 0.585938, f1 0.530025\n",
      "2017-11-26T15:56:18.243079: step 3635, loss 0.963201, acc 0.554688, f1 0.507693\n",
      "2017-11-26T15:56:18.389925: step 3640, loss 0.935802, acc 0.53125, f1 0.479254\n",
      "2017-11-26T15:56:18.528016: step 3645, loss 0.885053, acc 0.609375, f1 0.576885\n",
      "2017-11-26T15:56:18.665966: step 3650, loss 0.94977, acc 0.570312, f1 0.527739\n",
      "2017-11-26T15:56:18.809857: step 3655, loss 0.853098, acc 0.617188, f1 0.579298\n",
      "2017-11-26T15:56:18.941563: step 3660, loss 0.891448, acc 0.59375, f1 0.567735\n",
      "2017-11-26T15:56:19.087125: step 3665, loss 0.903226, acc 0.578125, f1 0.531181\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T15:56:19.258731: step 3666, loss 0.997358, acc 0.546287, f1 0.496839\n",
      "\n",
      "\n",
      "Current epoch:  26\n",
      "2017-11-26T15:56:19.400627: step 3670, loss 0.942196, acc 0.601562, f1 0.55822\n",
      "2017-11-26T15:56:19.540491: step 3675, loss 0.957887, acc 0.59375, f1 0.548171\n",
      "2017-11-26T15:56:19.665972: step 3680, loss 0.916562, acc 0.585938, f1 0.527859\n",
      "2017-11-26T15:56:19.818391: step 3685, loss 0.838622, acc 0.640625, f1 0.624094\n",
      "2017-11-26T15:56:19.955916: step 3690, loss 0.880314, acc 0.585938, f1 0.555655\n",
      "2017-11-26T15:56:20.095307: step 3695, loss 0.983616, acc 0.523438, f1 0.501232\n",
      "2017-11-26T15:56:20.238635: step 3700, loss 0.962286, acc 0.570312, f1 0.5415\n",
      "2017-11-26T15:56:20.373280: step 3705, loss 0.952846, acc 0.554688, f1 0.556898\n",
      "2017-11-26T15:56:20.496020: step 3710, loss 1.09251, acc 0.4375, f1 0.343784\n",
      "2017-11-26T15:56:20.631609: step 3715, loss 0.97042, acc 0.585938, f1 0.537398\n",
      "2017-11-26T15:56:20.775776: step 3720, loss 1.03054, acc 0.492188, f1 0.422039\n",
      "2017-11-26T15:56:20.909123: step 3725, loss 0.874529, acc 0.648438, f1 0.615483\n",
      "2017-11-26T15:56:21.049533: step 3730, loss 0.930672, acc 0.53125, f1 0.488942\n",
      "2017-11-26T15:56:21.191258: step 3735, loss 0.979681, acc 0.546875, f1 0.50739\n",
      "2017-11-26T15:56:21.331867: step 3740, loss 0.947127, acc 0.523438, f1 0.516575\n",
      "2017-11-26T15:56:21.476401: step 3745, loss 0.923274, acc 0.554688, f1 0.509533\n",
      "2017-11-26T15:56:21.622605: step 3750, loss 0.972823, acc 0.554688, f1 0.504886\n",
      "2017-11-26T15:56:21.771603: step 3755, loss 0.929511, acc 0.554688, f1 0.528408\n",
      "2017-11-26T15:56:21.920209: step 3760, loss 0.938753, acc 0.554688, f1 0.510397\n",
      "2017-11-26T15:56:22.057538: step 3765, loss 1.00246, acc 0.492188, f1 0.448014\n",
      "2017-11-26T15:56:22.195902: step 3770, loss 0.85476, acc 0.632812, f1 0.607026\n",
      "2017-11-26T15:56:22.340675: step 3775, loss 0.97767, acc 0.5625, f1 0.531106\n",
      "2017-11-26T15:56:22.482778: step 3780, loss 0.990741, acc 0.554688, f1 0.515548\n",
      "2017-11-26T15:56:22.624382: step 3785, loss 0.90535, acc 0.570312, f1 0.516704\n",
      "2017-11-26T15:56:22.763175: step 3790, loss 1.02262, acc 0.453125, f1 0.40254\n",
      "2017-11-26T15:56:22.893800: step 3795, loss 0.930981, acc 0.585938, f1 0.537946\n",
      "2017-11-26T15:56:23.041085: step 3800, loss 0.956905, acc 0.570312, f1 0.554397\n",
      "2017-11-26T15:56:23.179196: step 3805, loss 0.903763, acc 0.5625, f1 0.502929\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T15:56:23.383773: step 3807, loss 0.993988, acc 0.554672, f1 0.497038\n",
      "\n",
      "\n",
      "Current epoch:  27\n",
      "2017-11-26T15:56:23.497601: step 3810, loss 0.880758, acc 0.578125, f1 0.549041\n",
      "2017-11-26T15:56:23.644597: step 3815, loss 0.820374, acc 0.679688, f1 0.645934\n",
      "2017-11-26T15:56:23.785444: step 3820, loss 0.837417, acc 0.585938, f1 0.523074\n",
      "2017-11-26T15:56:23.926593: step 3825, loss 0.960535, acc 0.585938, f1 0.55734\n",
      "2017-11-26T15:56:24.067154: step 3830, loss 0.910173, acc 0.5, f1 0.456093\n",
      "2017-11-26T15:56:24.194196: step 3835, loss 0.938443, acc 0.539062, f1 0.49046\n",
      "2017-11-26T15:56:24.337731: step 3840, loss 0.91202, acc 0.539062, f1 0.479277\n",
      "2017-11-26T15:56:24.484518: step 3845, loss 0.868821, acc 0.617188, f1 0.588774\n",
      "2017-11-26T15:56:24.613426: step 3850, loss 0.942464, acc 0.53125, f1 0.512306\n",
      "2017-11-26T15:56:24.757630: step 3855, loss 0.95876, acc 0.53125, f1 0.481824\n",
      "2017-11-26T15:56:24.904663: step 3860, loss 0.872815, acc 0.609375, f1 0.579269\n",
      "2017-11-26T15:56:25.039593: step 3865, loss 0.926456, acc 0.546875, f1 0.531912\n",
      "2017-11-26T15:56:25.187181: step 3870, loss 0.893225, acc 0.570312, f1 0.525589\n",
      "2017-11-26T15:56:25.339095: step 3875, loss 0.949061, acc 0.570312, f1 0.510993\n",
      "2017-11-26T15:56:25.489834: step 3880, loss 0.935728, acc 0.570312, f1 0.540201\n",
      "2017-11-26T15:56:25.630737: step 3885, loss 1.09327, acc 0.429688, f1 0.370017\n",
      "2017-11-26T15:56:25.782054: step 3890, loss 0.988214, acc 0.546875, f1 0.494668\n",
      "2017-11-26T15:56:25.933889: step 3895, loss 0.834972, acc 0.671875, f1 0.647341\n",
      "2017-11-26T15:56:26.087931: step 3900, loss 0.963848, acc 0.554688, f1 0.512511\n",
      "2017-11-26T15:56:26.232714: step 3905, loss 0.967679, acc 0.546875, f1 0.534248\n",
      "2017-11-26T15:56:26.380149: step 3910, loss 0.940271, acc 0.59375, f1 0.530705\n",
      "2017-11-26T15:56:26.522937: step 3915, loss 0.953288, acc 0.53125, f1 0.522284\n",
      "2017-11-26T15:56:26.645902: step 3920, loss 0.938361, acc 0.554688, f1 0.48836\n",
      "2017-11-26T15:56:26.786450: step 3925, loss 0.937017, acc 0.507812, f1 0.449245\n",
      "2017-11-26T15:56:26.926261: step 3930, loss 0.941622, acc 0.554688, f1 0.52884\n",
      "2017-11-26T15:56:27.068999: step 3935, loss 0.892432, acc 0.5625, f1 0.515649\n",
      "2017-11-26T15:56:27.216197: step 3940, loss 0.933921, acc 0.578125, f1 0.521217\n",
      "2017-11-26T15:56:27.356646: step 3945, loss 0.882859, acc 0.601562, f1 0.560167\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T15:56:27.579461: step 3948, loss 0.990353, acc 0.547353, f1 0.480453\n",
      "\n",
      "\n",
      "Current epoch:  28\n",
      "2017-11-26T15:56:27.643441: step 3950, loss 0.941179, acc 0.523438, f1 0.473186\n",
      "2017-11-26T15:56:27.786914: step 3955, loss 0.839677, acc 0.679688, f1 0.629609\n",
      "2017-11-26T15:56:27.933608: step 3960, loss 0.921183, acc 0.554688, f1 0.520576\n",
      "2017-11-26T15:56:28.085448: step 3965, loss 0.977535, acc 0.492188, f1 0.465297\n",
      "2017-11-26T15:56:28.236949: step 3970, loss 0.818027, acc 0.59375, f1 0.543204\n",
      "2017-11-26T15:56:28.373036: step 3975, loss 0.836176, acc 0.59375, f1 0.556081\n",
      "2017-11-26T15:56:28.522645: step 3980, loss 0.890071, acc 0.59375, f1 0.545956\n",
      "2017-11-26T15:56:28.681183: step 3985, loss 0.936363, acc 0.554688, f1 0.524088\n",
      "2017-11-26T15:56:28.844862: step 3990, loss 1.04636, acc 0.5, f1 0.464697\n",
      "2017-11-26T15:56:29.006919: step 3995, loss 0.890413, acc 0.5625, f1 0.500064\n",
      "2017-11-26T15:56:29.161086: step 4000, loss 0.904849, acc 0.578125, f1 0.569883\n",
      "2017-11-26T15:56:29.293587: step 4005, loss 0.879606, acc 0.617188, f1 0.602989\n",
      "2017-11-26T15:56:29.427616: step 4010, loss 0.930124, acc 0.578125, f1 0.536111\n",
      "2017-11-26T15:56:29.572763: step 4015, loss 0.914139, acc 0.609375, f1 0.600592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T15:56:29.726342: step 4020, loss 0.898264, acc 0.523438, f1 0.504973\n",
      "2017-11-26T15:56:29.865589: step 4025, loss 0.992084, acc 0.46875, f1 0.447124\n",
      "2017-11-26T15:56:30.012856: step 4030, loss 0.912563, acc 0.554688, f1 0.552919\n",
      "2017-11-26T15:56:30.165627: step 4035, loss 0.925467, acc 0.609375, f1 0.554759\n",
      "2017-11-26T15:56:30.297876: step 4040, loss 0.91872, acc 0.59375, f1 0.578295\n",
      "2017-11-26T15:56:30.441195: step 4045, loss 0.941232, acc 0.609375, f1 0.561316\n",
      "2017-11-26T15:56:30.593079: step 4050, loss 0.913149, acc 0.578125, f1 0.54836\n",
      "2017-11-26T15:56:30.745117: step 4055, loss 0.962856, acc 0.546875, f1 0.512225\n",
      "2017-11-26T15:56:30.889109: step 4060, loss 0.88934, acc 0.617188, f1 0.590538\n",
      "2017-11-26T15:56:31.027040: step 4065, loss 0.988521, acc 0.5, f1 0.472782\n",
      "2017-11-26T15:56:31.177569: step 4070, loss 0.863034, acc 0.546875, f1 0.526266\n",
      "2017-11-26T15:56:31.321284: step 4075, loss 0.877316, acc 0.585938, f1 0.534853\n",
      "2017-11-26T15:56:31.456696: step 4080, loss 0.953878, acc 0.4375, f1 0.341357\n",
      "2017-11-26T15:56:31.602757: step 4085, loss 0.858483, acc 0.59375, f1 0.545933\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T15:56:31.870534: step 4089, loss 1.00253, acc 0.529081, f1 0.503678\n",
      "\n",
      "\n",
      "Current epoch:  29\n",
      "2017-11-26T15:56:31.907184: step 4090, loss 0.872866, acc 0.640625, f1 0.614987\n",
      "2017-11-26T15:56:32.038557: step 4095, loss 0.899338, acc 0.546875, f1 0.50971\n",
      "2017-11-26T15:56:32.169223: step 4100, loss 0.893476, acc 0.601562, f1 0.553742\n",
      "2017-11-26T15:56:32.295576: step 4105, loss 0.886617, acc 0.617188, f1 0.594105\n",
      "2017-11-26T15:56:32.440157: step 4110, loss 0.983169, acc 0.546875, f1 0.500117\n",
      "2017-11-26T15:56:32.587923: step 4115, loss 0.925197, acc 0.507812, f1 0.485727\n",
      "2017-11-26T15:56:32.730566: step 4120, loss 0.929489, acc 0.5625, f1 0.530377\n",
      "2017-11-26T15:56:32.863678: step 4125, loss 0.980164, acc 0.601562, f1 0.579391\n",
      "2017-11-26T15:56:33.010419: step 4130, loss 0.948166, acc 0.523438, f1 0.488201\n",
      "2017-11-26T15:56:33.156203: step 4135, loss 0.815968, acc 0.625, f1 0.590153\n",
      "2017-11-26T15:56:33.299053: step 4140, loss 1.00452, acc 0.46875, f1 0.432366\n",
      "2017-11-26T15:56:33.442768: step 4145, loss 0.883191, acc 0.585938, f1 0.549378\n",
      "2017-11-26T15:56:33.590502: step 4150, loss 0.97446, acc 0.5625, f1 0.52034\n",
      "2017-11-26T15:56:33.732050: step 4155, loss 0.937124, acc 0.578125, f1 0.56363\n",
      "2017-11-26T15:56:33.879006: step 4160, loss 0.965941, acc 0.539062, f1 0.497332\n",
      "2017-11-26T15:56:34.037085: step 4165, loss 0.921281, acc 0.507812, f1 0.483623\n",
      "2017-11-26T15:56:34.174071: step 4170, loss 0.912001, acc 0.578125, f1 0.557281\n",
      "2017-11-26T15:56:34.318279: step 4175, loss 0.898856, acc 0.59375, f1 0.547352\n",
      "2017-11-26T15:56:34.465973: step 4180, loss 0.940588, acc 0.523438, f1 0.483675\n",
      "2017-11-26T15:56:34.616875: step 4185, loss 0.915867, acc 0.601562, f1 0.584994\n",
      "2017-11-26T15:56:34.748575: step 4190, loss 0.883884, acc 0.632812, f1 0.612136\n",
      "2017-11-26T15:56:34.893403: step 4195, loss 0.855193, acc 0.617188, f1 0.596725\n",
      "2017-11-26T15:56:35.037154: step 4200, loss 0.892525, acc 0.5625, f1 0.524868\n",
      "2017-11-26T15:56:35.165337: step 4205, loss 0.935842, acc 0.554688, f1 0.512737\n",
      "2017-11-26T15:56:35.317197: step 4210, loss 0.882421, acc 0.601562, f1 0.557709\n",
      "2017-11-26T15:56:35.468330: step 4215, loss 0.90608, acc 0.570312, f1 0.534976\n",
      "2017-11-26T15:56:35.615972: step 4220, loss 0.889732, acc 0.578125, f1 0.531259\n",
      "2017-11-26T15:56:35.763927: step 4225, loss 1.06461, acc 0.484375, f1 0.437021\n",
      "2017-11-26T15:56:35.911009: step 4230, loss 0.84409, acc 0.637097, f1 0.616699\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T15:56:36.061025: step 4230, loss 1.00075, acc 0.545609, f1 0.470246\n",
      "\n",
      "\n",
      "Current epoch:  30\n",
      "2017-11-26T15:56:36.219391: step 4235, loss 0.846278, acc 0.617188, f1 0.587929\n",
      "2017-11-26T15:56:36.367000: step 4240, loss 0.891365, acc 0.617188, f1 0.579809\n",
      "2017-11-26T15:56:36.504929: step 4245, loss 0.951082, acc 0.523438, f1 0.483128\n",
      "2017-11-26T15:56:36.654959: step 4250, loss 0.905058, acc 0.578125, f1 0.536125\n",
      "2017-11-26T15:56:36.810571: step 4255, loss 0.948292, acc 0.53125, f1 0.484997\n",
      "2017-11-26T15:56:36.956823: step 4260, loss 0.85153, acc 0.640625, f1 0.618501\n",
      "2017-11-26T15:56:37.091891: step 4265, loss 0.986933, acc 0.523438, f1 0.4601\n",
      "2017-11-26T15:56:37.232629: step 4270, loss 0.905099, acc 0.609375, f1 0.561252\n",
      "2017-11-26T15:56:37.380240: step 4275, loss 0.983177, acc 0.515625, f1 0.464951\n",
      "2017-11-26T15:56:37.527954: step 4280, loss 0.854139, acc 0.617188, f1 0.580408\n",
      "2017-11-26T15:56:37.674678: step 4285, loss 0.941651, acc 0.570312, f1 0.560385\n",
      "2017-11-26T15:56:37.822543: step 4290, loss 0.94241, acc 0.585938, f1 0.547349\n",
      "2017-11-26T15:56:37.958651: step 4295, loss 0.996596, acc 0.445312, f1 0.394979\n",
      "2017-11-26T15:56:38.104091: step 4300, loss 0.922972, acc 0.578125, f1 0.530845\n",
      "2017-11-26T15:56:38.254833: step 4305, loss 0.90929, acc 0.59375, f1 0.557685\n",
      "2017-11-26T15:56:38.383128: step 4310, loss 0.91298, acc 0.617188, f1 0.55451\n",
      "2017-11-26T15:56:38.524000: step 4315, loss 0.970708, acc 0.476562, f1 0.392536\n",
      "2017-11-26T15:56:38.671821: step 4320, loss 0.943021, acc 0.539062, f1 0.497688\n",
      "2017-11-26T15:56:38.821300: step 4325, loss 1.04438, acc 0.539062, f1 0.477658\n",
      "2017-11-26T15:56:38.971564: step 4330, loss 0.839277, acc 0.648438, f1 0.6198\n",
      "2017-11-26T15:56:39.118300: step 4335, loss 0.903824, acc 0.601562, f1 0.56313\n",
      "2017-11-26T15:56:39.247545: step 4340, loss 0.821105, acc 0.664062, f1 0.645275\n",
      "2017-11-26T15:56:39.384813: step 4345, loss 0.974054, acc 0.476562, f1 0.417302\n",
      "2017-11-26T15:56:39.531998: step 4350, loss 0.997618, acc 0.515625, f1 0.480287\n",
      "2017-11-26T15:56:39.671233: step 4355, loss 0.963868, acc 0.523438, f1 0.509672\n",
      "2017-11-26T15:56:39.826139: step 4360, loss 0.929694, acc 0.546875, f1 0.532619\n",
      "2017-11-26T15:56:39.977349: step 4365, loss 0.964653, acc 0.507812, f1 0.492112\n",
      "2017-11-26T15:56:40.125121: step 4370, loss 0.82541, acc 0.65625, f1 0.632749\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T15:56:40.295289: step 4371, loss 1.00139, acc 0.535236, f1 0.491681\n",
      "\n",
      "\n",
      "Current epoch:  31\n",
      "2017-11-26T15:56:40.420353: step 4375, loss 0.905362, acc 0.625, f1 0.586159\n",
      "2017-11-26T15:56:40.563993: step 4380, loss 0.937658, acc 0.554688, f1 0.52127\n",
      "2017-11-26T15:56:40.715754: step 4385, loss 0.850855, acc 0.65625, f1 0.639915\n",
      "2017-11-26T15:56:40.862734: step 4390, loss 0.888761, acc 0.59375, f1 0.561898\n",
      "2017-11-26T15:56:41.011694: step 4395, loss 0.968998, acc 0.476562, f1 0.395833\n",
      "2017-11-26T15:56:41.167167: step 4400, loss 0.912096, acc 0.59375, f1 0.580484\n",
      "2017-11-26T15:56:41.319406: step 4405, loss 0.966897, acc 0.5, f1 0.4694\n",
      "2017-11-26T15:56:41.464693: step 4410, loss 0.870196, acc 0.570312, f1 0.52534\n",
      "2017-11-26T15:56:41.611392: step 4415, loss 0.931522, acc 0.585938, f1 0.557426\n",
      "2017-11-26T15:56:41.745151: step 4420, loss 0.899327, acc 0.601562, f1 0.559322\n",
      "2017-11-26T15:56:41.895048: step 4425, loss 1.05322, acc 0.523438, f1 0.438494\n",
      "2017-11-26T15:56:42.042036: step 4430, loss 1.00079, acc 0.515625, f1 0.490349\n",
      "2017-11-26T15:56:42.177564: step 4435, loss 0.949314, acc 0.53125, f1 0.485162\n",
      "2017-11-26T15:56:42.324264: step 4440, loss 0.960121, acc 0.539062, f1 0.507013\n",
      "2017-11-26T15:56:42.466705: step 4445, loss 0.822844, acc 0.625, f1 0.580375\n",
      "2017-11-26T15:56:42.609219: step 4450, loss 0.906326, acc 0.578125, f1 0.537653\n",
      "2017-11-26T15:56:42.760390: step 4455, loss 0.854062, acc 0.554688, f1 0.545472\n",
      "2017-11-26T15:56:42.906839: step 4460, loss 0.984973, acc 0.46875, f1 0.427375\n",
      "2017-11-26T15:56:43.054348: step 4465, loss 0.90407, acc 0.617188, f1 0.576924\n",
      "2017-11-26T15:56:43.190556: step 4470, loss 0.955728, acc 0.53125, f1 0.468848\n",
      "2017-11-26T15:56:43.323580: step 4475, loss 0.836335, acc 0.609375, f1 0.547519\n",
      "2017-11-26T15:56:43.472623: step 4480, loss 0.908432, acc 0.523438, f1 0.455301\n",
      "2017-11-26T15:56:43.622160: step 4485, loss 0.864955, acc 0.609375, f1 0.569637\n",
      "2017-11-26T15:56:43.760366: step 4490, loss 0.901907, acc 0.609375, f1 0.568883\n",
      "2017-11-26T15:56:43.902643: step 4495, loss 0.783731, acc 0.664062, f1 0.633253\n",
      "2017-11-26T15:56:44.047711: step 4500, loss 0.940155, acc 0.554688, f1 0.516488\n",
      "2017-11-26T15:56:44.200831: step 4505, loss 0.956899, acc 0.539062, f1 0.478883\n",
      "2017-11-26T15:56:44.344508: step 4510, loss 0.907192, acc 0.570312, f1 0.550332\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T15:56:44.551891: step 4512, loss 1.0495, acc 0.509354, f1 0.469447\n",
      "\n",
      "\n",
      "Current epoch:  32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T15:56:44.646539: step 4515, loss 0.904349, acc 0.53125, f1 0.500609\n",
      "2017-11-26T15:56:44.793395: step 4520, loss 0.973996, acc 0.507812, f1 0.453329\n",
      "2017-11-26T15:56:44.939960: step 4525, loss 0.904019, acc 0.570312, f1 0.546184\n",
      "2017-11-26T15:56:45.091967: step 4530, loss 0.920935, acc 0.523438, f1 0.498772\n",
      "2017-11-26T15:56:45.235955: step 4535, loss 0.8162, acc 0.609375, f1 0.578494\n",
      "2017-11-26T15:56:45.378309: step 4540, loss 0.942622, acc 0.601562, f1 0.600699\n",
      "2017-11-26T15:56:45.533432: step 4545, loss 0.84698, acc 0.617188, f1 0.599762\n",
      "2017-11-26T15:56:45.673457: step 4550, loss 0.960524, acc 0.507812, f1 0.466763\n",
      "2017-11-26T15:56:45.805770: step 4555, loss 0.837722, acc 0.65625, f1 0.628387\n",
      "2017-11-26T15:56:45.957474: step 4560, loss 0.855942, acc 0.632812, f1 0.617191\n",
      "2017-11-26T15:56:46.103621: step 4565, loss 0.916207, acc 0.585938, f1 0.529183\n",
      "2017-11-26T15:56:46.238361: step 4570, loss 0.980461, acc 0.507812, f1 0.436047\n",
      "2017-11-26T15:56:46.385051: step 4575, loss 0.84962, acc 0.625, f1 0.592477\n",
      "2017-11-26T15:56:46.525662: step 4580, loss 0.935651, acc 0.578125, f1 0.536565\n",
      "2017-11-26T15:56:46.651793: step 4585, loss 0.9334, acc 0.53125, f1 0.45463\n",
      "2017-11-26T15:56:46.793206: step 4590, loss 0.910784, acc 0.554688, f1 0.532964\n",
      "2017-11-26T15:56:46.940997: step 4595, loss 0.965196, acc 0.609375, f1 0.546346\n",
      "2017-11-26T15:56:47.074430: step 4600, loss 0.896166, acc 0.640625, f1 0.614396\n",
      "2017-11-26T15:56:47.219288: step 4605, loss 0.864437, acc 0.585938, f1 0.570128\n",
      "2017-11-26T15:56:47.362249: step 4610, loss 0.918614, acc 0.5625, f1 0.485562\n",
      "2017-11-26T15:56:47.509517: step 4615, loss 0.951065, acc 0.554688, f1 0.500816\n",
      "2017-11-26T15:56:47.661319: step 4620, loss 0.890542, acc 0.578125, f1 0.523727\n",
      "2017-11-26T15:56:47.801955: step 4625, loss 0.926409, acc 0.554688, f1 0.530814\n",
      "2017-11-26T15:56:47.938289: step 4630, loss 0.895608, acc 0.585938, f1 0.566596\n",
      "2017-11-26T15:56:48.085271: step 4635, loss 0.91653, acc 0.609375, f1 0.586443\n",
      "2017-11-26T15:56:48.222456: step 4640, loss 0.872087, acc 0.585938, f1 0.556472\n",
      "2017-11-26T15:56:48.364287: step 4645, loss 0.954328, acc 0.539062, f1 0.478147\n",
      "2017-11-26T15:56:48.506606: step 4650, loss 0.815484, acc 0.710938, f1 0.704723\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T15:56:48.752263: step 4653, loss 1.02369, acc 0.50727, f1 0.47085\n",
      "\n",
      "\n",
      "Current epoch:  33\n",
      "2017-11-26T15:56:48.850299: step 4655, loss 0.861406, acc 0.617188, f1 0.593005\n",
      "2017-11-26T15:56:48.986629: step 4660, loss 0.880693, acc 0.5625, f1 0.518888\n",
      "2017-11-26T15:56:49.128180: step 4665, loss 0.873075, acc 0.617188, f1 0.613077\n",
      "2017-11-26T15:56:49.284454: step 4670, loss 1.00171, acc 0.515625, f1 0.525189\n",
      "2017-11-26T15:56:49.427359: step 4675, loss 0.937356, acc 0.554688, f1 0.529062\n",
      "2017-11-26T15:56:49.574364: step 4680, loss 0.916029, acc 0.539062, f1 0.508335\n",
      "2017-11-26T15:56:49.723245: step 4685, loss 0.910607, acc 0.578125, f1 0.54746\n",
      "2017-11-26T15:56:49.872513: step 4690, loss 0.983816, acc 0.523438, f1 0.470541\n",
      "2017-11-26T15:56:50.010705: step 4695, loss 0.887218, acc 0.570312, f1 0.51685\n",
      "2017-11-26T15:56:50.142817: step 4700, loss 1.00884, acc 0.507812, f1 0.5097\n",
      "2017-11-26T15:56:50.294671: step 4705, loss 0.987992, acc 0.539062, f1 0.526028\n",
      "2017-11-26T15:56:50.441748: step 4710, loss 0.97283, acc 0.554688, f1 0.520924\n",
      "2017-11-26T15:56:50.585257: step 4715, loss 0.92081, acc 0.578125, f1 0.535484\n",
      "2017-11-26T15:56:50.738558: step 4720, loss 0.907959, acc 0.59375, f1 0.57208\n",
      "2017-11-26T15:56:50.882197: step 4725, loss 0.928895, acc 0.578125, f1 0.542282\n",
      "2017-11-26T15:56:51.025119: step 4730, loss 0.920003, acc 0.523438, f1 0.488344\n",
      "2017-11-26T15:56:51.167454: step 4735, loss 0.923821, acc 0.554688, f1 0.517353\n",
      "2017-11-26T15:56:51.311037: step 4740, loss 0.900562, acc 0.554688, f1 0.508094\n",
      "2017-11-26T15:56:51.454052: step 4745, loss 0.90468, acc 0.5625, f1 0.529162\n",
      "2017-11-26T15:56:51.601512: step 4750, loss 0.840538, acc 0.601562, f1 0.564027\n",
      "2017-11-26T15:56:51.749399: step 4755, loss 0.893787, acc 0.5625, f1 0.500888\n",
      "2017-11-26T15:56:51.893399: step 4760, loss 0.87867, acc 0.609375, f1 0.564437\n",
      "2017-11-26T15:56:52.033905: step 4765, loss 0.977567, acc 0.5625, f1 0.516367\n",
      "2017-11-26T15:56:52.178028: step 4770, loss 0.862035, acc 0.609375, f1 0.591492\n",
      "2017-11-26T15:56:52.320653: step 4775, loss 0.916107, acc 0.578125, f1 0.544838\n",
      "2017-11-26T15:56:52.460066: step 4780, loss 0.936014, acc 0.546875, f1 0.505428\n",
      "2017-11-26T15:56:52.592450: step 4785, loss 0.815679, acc 0.679688, f1 0.642065\n",
      "2017-11-26T15:56:52.733215: step 4790, loss 0.942251, acc 0.5, f1 0.434491\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T15:56:52.985577: step 4794, loss 1.0513, acc 0.492633, f1 0.453603\n",
      "\n",
      "\n",
      "Current epoch:  34\n",
      "2017-11-26T15:56:53.048483: step 4795, loss 0.896025, acc 0.554688, f1 0.510743\n",
      "2017-11-26T15:56:53.196249: step 4800, loss 0.920379, acc 0.617188, f1 0.576929\n",
      "2017-11-26T15:56:53.347984: step 4805, loss 0.938496, acc 0.578125, f1 0.574914\n",
      "2017-11-26T15:56:53.493885: step 4810, loss 0.864762, acc 0.601562, f1 0.587667\n",
      "2017-11-26T15:56:53.643114: step 4815, loss 1.00472, acc 0.46875, f1 0.421845\n",
      "2017-11-26T15:56:53.784887: step 4820, loss 0.831238, acc 0.640625, f1 0.619735\n",
      "2017-11-26T15:56:53.925392: step 4825, loss 0.918926, acc 0.554688, f1 0.467187\n",
      "2017-11-26T15:56:54.074354: step 4830, loss 0.848575, acc 0.601562, f1 0.576165\n",
      "2017-11-26T15:56:54.207361: step 4835, loss 0.941013, acc 0.570312, f1 0.565144\n",
      "2017-11-26T15:56:54.362250: step 4840, loss 0.89107, acc 0.59375, f1 0.547078\n",
      "2017-11-26T15:56:54.508321: step 4845, loss 0.856649, acc 0.59375, f1 0.555036\n",
      "2017-11-26T15:56:54.651219: step 4850, loss 0.898361, acc 0.554688, f1 0.485232\n",
      "2017-11-26T15:56:54.777117: step 4855, loss 0.931209, acc 0.53125, f1 0.490028\n",
      "2017-11-26T15:56:54.905883: step 4860, loss 0.92038, acc 0.546875, f1 0.496388\n",
      "2017-11-26T15:56:55.043330: step 4865, loss 0.908633, acc 0.53125, f1 0.425801\n",
      "2017-11-26T15:56:55.175265: step 4870, loss 0.93878, acc 0.609375, f1 0.583477\n",
      "2017-11-26T15:56:55.315167: step 4875, loss 0.977952, acc 0.484375, f1 0.438662\n",
      "2017-11-26T15:56:55.454505: step 4880, loss 0.915525, acc 0.5625, f1 0.535929\n",
      "2017-11-26T15:56:55.607323: step 4885, loss 0.918753, acc 0.585938, f1 0.536301\n",
      "2017-11-26T15:56:55.743084: step 4890, loss 0.866152, acc 0.625, f1 0.600539\n",
      "2017-11-26T15:56:55.885660: step 4895, loss 0.93262, acc 0.578125, f1 0.57451\n",
      "2017-11-26T15:56:55.999426: step 4900, loss 0.870103, acc 0.601562, f1 0.566505\n",
      "2017-11-26T15:56:56.137681: step 4905, loss 0.881885, acc 0.570312, f1 0.560696\n",
      "2017-11-26T15:56:56.266749: step 4910, loss 0.874347, acc 0.609375, f1 0.572616\n",
      "2017-11-26T15:56:56.425973: step 4915, loss 0.834465, acc 0.609375, f1 0.587395\n",
      "2017-11-26T15:56:56.582757: step 4920, loss 0.892867, acc 0.523438, f1 0.470757\n",
      "2017-11-26T15:56:56.730689: step 4925, loss 0.932743, acc 0.5625, f1 0.530213\n",
      "2017-11-26T15:56:56.876624: step 4930, loss 0.96361, acc 0.601562, f1 0.561483\n",
      "2017-11-26T15:56:57.013671: step 4935, loss 0.877265, acc 0.596774, f1 0.583413\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T15:56:57.164644: step 4935, loss 1.00531, acc 0.545948, f1 0.484713\n",
      "\n",
      "\n",
      "Current epoch:  35\n",
      "2017-11-26T15:56:57.314386: step 4940, loss 0.935015, acc 0.570312, f1 0.497899\n",
      "2017-11-26T15:56:57.459891: step 4945, loss 0.831719, acc 0.632812, f1 0.622743\n",
      "2017-11-26T15:56:57.594404: step 4950, loss 0.981346, acc 0.515625, f1 0.475321\n",
      "2017-11-26T15:56:57.731658: step 4955, loss 0.892145, acc 0.578125, f1 0.565725\n",
      "2017-11-26T15:56:57.870399: step 4960, loss 0.976369, acc 0.523438, f1 0.498596\n",
      "2017-11-26T15:56:58.003686: step 4965, loss 0.870238, acc 0.625, f1 0.58833\n",
      "2017-11-26T15:56:58.142226: step 4970, loss 0.831024, acc 0.648438, f1 0.62987\n",
      "2017-11-26T15:56:58.280768: step 4975, loss 0.815915, acc 0.648438, f1 0.61527\n",
      "2017-11-26T15:56:58.419310: step 4980, loss 0.92662, acc 0.5625, f1 0.531851\n",
      "2017-11-26T15:56:58.573289: step 4985, loss 1.13523, acc 0.476562, f1 0.370437\n",
      "2017-11-26T15:56:58.721757: step 4990, loss 0.88315, acc 0.648438, f1 0.643387\n",
      "2017-11-26T15:56:58.859770: step 4995, loss 0.98682, acc 0.53125, f1 0.48164\n",
      "2017-11-26T15:56:59.002556: step 5000, loss 0.904895, acc 0.578125, f1 0.54166\n",
      "2017-11-26T15:56:59.147886: step 5005, loss 0.933639, acc 0.617188, f1 0.56491\n",
      "2017-11-26T15:56:59.301754: step 5010, loss 0.884204, acc 0.570312, f1 0.536989\n",
      "2017-11-26T15:56:59.429245: step 5015, loss 0.931462, acc 0.5625, f1 0.524182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T15:56:59.567569: step 5020, loss 0.94528, acc 0.523438, f1 0.478345\n",
      "2017-11-26T15:56:59.704559: step 5025, loss 0.906873, acc 0.5625, f1 0.514008\n",
      "2017-11-26T15:56:59.837170: step 5030, loss 0.869056, acc 0.585938, f1 0.56018\n",
      "2017-11-26T15:56:59.981615: step 5035, loss 0.908314, acc 0.578125, f1 0.584901\n",
      "2017-11-26T15:57:00.137713: step 5040, loss 0.862715, acc 0.671875, f1 0.656298\n",
      "2017-11-26T15:57:00.283488: step 5045, loss 0.898212, acc 0.554688, f1 0.523706\n",
      "2017-11-26T15:57:00.428103: step 5050, loss 0.905123, acc 0.578125, f1 0.523946\n",
      "2017-11-26T15:57:00.563925: step 5055, loss 0.957847, acc 0.570312, f1 0.54582\n",
      "2017-11-26T15:57:00.705952: step 5060, loss 0.85409, acc 0.632812, f1 0.607661\n",
      "2017-11-26T15:57:00.846588: step 5065, loss 0.920753, acc 0.492188, f1 0.443006\n",
      "2017-11-26T15:57:00.993981: step 5070, loss 0.931732, acc 0.546875, f1 0.508089\n",
      "2017-11-26T15:57:01.140206: step 5075, loss 0.947581, acc 0.570312, f1 0.510977\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T15:57:01.316984: step 5076, loss 1.07278, acc 0.458753, f1 0.416532\n",
      "\n",
      "\n",
      "Current epoch:  36\n",
      "2017-11-26T15:57:01.463355: step 5080, loss 0.938589, acc 0.59375, f1 0.554614\n",
      "2017-11-26T15:57:01.564254: step 5085, loss 0.847584, acc 0.640625, f1 0.602197\n",
      "2017-11-26T15:57:01.720610: step 5090, loss 0.821628, acc 0.671875, f1 0.64947\n",
      "2017-11-26T15:57:01.859606: step 5095, loss 0.886969, acc 0.554688, f1 0.533974\n",
      "2017-11-26T15:57:02.000207: step 5100, loss 0.845999, acc 0.640625, f1 0.621885\n",
      "2017-11-26T15:57:02.142992: step 5105, loss 0.98471, acc 0.515625, f1 0.473698\n",
      "2017-11-26T15:57:02.279651: step 5110, loss 0.887555, acc 0.59375, f1 0.539867\n",
      "2017-11-26T15:57:02.415901: step 5115, loss 0.927399, acc 0.53125, f1 0.481777\n",
      "2017-11-26T15:57:02.560031: step 5120, loss 0.949109, acc 0.546875, f1 0.531586\n",
      "2017-11-26T15:57:02.708732: step 5125, loss 0.890708, acc 0.625, f1 0.591419\n",
      "2017-11-26T15:57:02.862343: step 5130, loss 0.874978, acc 0.640625, f1 0.623877\n",
      "2017-11-26T15:57:03.018108: step 5135, loss 0.855426, acc 0.554688, f1 0.522921\n",
      "2017-11-26T15:57:03.170002: step 5140, loss 0.839592, acc 0.578125, f1 0.547877\n",
      "2017-11-26T15:57:03.307831: step 5145, loss 0.89087, acc 0.617188, f1 0.554688\n",
      "2017-11-26T15:57:03.445571: step 5150, loss 0.934814, acc 0.5625, f1 0.519695\n",
      "2017-11-26T15:57:03.598422: step 5155, loss 0.87871, acc 0.664062, f1 0.621344\n",
      "2017-11-26T15:57:03.744177: step 5160, loss 0.914496, acc 0.5625, f1 0.518478\n",
      "2017-11-26T15:57:03.881570: step 5165, loss 1.01517, acc 0.4375, f1 0.332508\n",
      "2017-11-26T15:57:04.034789: step 5170, loss 0.938988, acc 0.53125, f1 0.492604\n",
      "2017-11-26T15:57:04.152486: step 5175, loss 0.924261, acc 0.570312, f1 0.549252\n",
      "2017-11-26T15:57:04.317202: step 5180, loss 0.951253, acc 0.523438, f1 0.491145\n",
      "2017-11-26T15:57:04.464297: step 5185, loss 0.920773, acc 0.578125, f1 0.538682\n",
      "2017-11-26T15:57:04.599282: step 5190, loss 0.839524, acc 0.671875, f1 0.640883\n",
      "2017-11-26T15:57:04.734476: step 5195, loss 0.902601, acc 0.578125, f1 0.561324\n",
      "2017-11-26T15:57:04.874603: step 5200, loss 0.916949, acc 0.5625, f1 0.559437\n",
      "2017-11-26T15:57:05.014529: step 5205, loss 0.937465, acc 0.554688, f1 0.498969\n",
      "2017-11-26T15:57:05.145031: step 5210, loss 0.878546, acc 0.609375, f1 0.572386\n",
      "2017-11-26T15:57:05.286424: step 5215, loss 0.847931, acc 0.625, f1 0.610787\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T15:57:05.493770: step 5217, loss 1.01599, acc 0.540762, f1 0.488545\n",
      "\n",
      "\n",
      "Current epoch:  37\n",
      "2017-11-26T15:57:05.620714: step 5220, loss 0.890999, acc 0.546875, f1 0.507\n",
      "2017-11-26T15:57:05.758872: step 5225, loss 0.944563, acc 0.5625, f1 0.542732\n",
      "2017-11-26T15:57:05.889178: step 5230, loss 0.806327, acc 0.664062, f1 0.634474\n",
      "2017-11-26T15:57:06.026241: step 5235, loss 0.887861, acc 0.59375, f1 0.571203\n",
      "2017-11-26T15:57:06.165128: step 5240, loss 0.887611, acc 0.632812, f1 0.622619\n",
      "2017-11-26T15:57:06.281164: step 5245, loss 0.899446, acc 0.59375, f1 0.53987\n",
      "2017-11-26T15:57:06.424337: step 5250, loss 0.926283, acc 0.585938, f1 0.581637\n",
      "2017-11-26T15:57:06.554564: step 5255, loss 0.936882, acc 0.554688, f1 0.550695\n",
      "2017-11-26T15:57:06.680534: step 5260, loss 0.84793, acc 0.585938, f1 0.566703\n",
      "2017-11-26T15:57:06.819125: step 5265, loss 0.871064, acc 0.640625, f1 0.617525\n",
      "2017-11-26T15:57:06.964119: step 5270, loss 0.933202, acc 0.578125, f1 0.55663\n",
      "2017-11-26T15:57:07.102226: step 5275, loss 0.893195, acc 0.59375, f1 0.558924\n",
      "2017-11-26T15:57:07.231395: step 5280, loss 0.93367, acc 0.554688, f1 0.5026\n",
      "2017-11-26T15:57:07.368480: step 5285, loss 0.982656, acc 0.515625, f1 0.468075\n",
      "2017-11-26T15:57:07.501375: step 5290, loss 1.03995, acc 0.484375, f1 0.411547\n",
      "2017-11-26T15:57:07.648349: step 5295, loss 0.849461, acc 0.640625, f1 0.634517\n",
      "2017-11-26T15:57:07.778784: step 5300, loss 0.97416, acc 0.554688, f1 0.527362\n",
      "2017-11-26T15:57:07.919297: step 5305, loss 0.796021, acc 0.6875, f1 0.67818\n",
      "2017-11-26T15:57:08.068619: step 5310, loss 0.909718, acc 0.492188, f1 0.466042\n",
      "2017-11-26T15:57:08.211940: step 5315, loss 0.83688, acc 0.625, f1 0.576859\n",
      "2017-11-26T15:57:08.345119: step 5320, loss 0.869552, acc 0.53125, f1 0.483936\n",
      "2017-11-26T15:57:08.492385: step 5325, loss 0.886134, acc 0.609375, f1 0.574028\n",
      "2017-11-26T15:57:08.636997: step 5330, loss 1.02857, acc 0.476562, f1 0.43457\n",
      "2017-11-26T15:57:08.778825: step 5335, loss 0.877068, acc 0.617188, f1 0.611279\n",
      "2017-11-26T15:57:08.916880: step 5340, loss 0.886371, acc 0.585938, f1 0.570678\n",
      "2017-11-26T15:57:09.052210: step 5345, loss 0.891784, acc 0.570312, f1 0.557722\n",
      "2017-11-26T15:57:09.182242: step 5350, loss 0.958196, acc 0.578125, f1 0.54189\n",
      "2017-11-26T15:57:09.324509: step 5355, loss 0.914094, acc 0.5625, f1 0.533581\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T15:57:09.558088: step 5358, loss 1.23936, acc 0.371946, f1 0.258121\n",
      "\n",
      "\n",
      "Current epoch:  38\n",
      "2017-11-26T15:57:09.627328: step 5360, loss 0.876751, acc 0.578125, f1 0.558779\n",
      "2017-11-26T15:57:09.784170: step 5365, loss 0.919331, acc 0.546875, f1 0.49757\n",
      "2017-11-26T15:57:09.923156: step 5370, loss 0.835611, acc 0.59375, f1 0.553678\n",
      "2017-11-26T15:57:10.072651: step 5375, loss 0.827811, acc 0.640625, f1 0.606732\n",
      "2017-11-26T15:57:10.214737: step 5380, loss 0.868121, acc 0.617188, f1 0.589785\n",
      "2017-11-26T15:57:10.357573: step 5385, loss 0.860254, acc 0.585938, f1 0.536523\n",
      "2017-11-26T15:57:10.494878: step 5390, loss 0.95597, acc 0.554688, f1 0.533379\n",
      "2017-11-26T15:57:10.630712: step 5395, loss 0.802985, acc 0.648438, f1 0.617982\n",
      "2017-11-26T15:57:10.773626: step 5400, loss 0.940635, acc 0.554688, f1 0.5363\n",
      "2017-11-26T15:57:10.913131: step 5405, loss 1.04603, acc 0.445312, f1 0.346656\n",
      "2017-11-26T15:57:11.013945: step 5410, loss 0.873243, acc 0.578125, f1 0.54378\n",
      "2017-11-26T15:57:11.146763: step 5415, loss 0.887099, acc 0.585938, f1 0.542152\n",
      "2017-11-26T15:57:11.287908: step 5420, loss 0.806213, acc 0.671875, f1 0.646838\n",
      "2017-11-26T15:57:11.418849: step 5425, loss 0.796796, acc 0.671875, f1 0.655367\n",
      "2017-11-26T15:57:11.556856: step 5430, loss 0.896934, acc 0.601562, f1 0.573413\n",
      "2017-11-26T15:57:11.695589: step 5435, loss 1.12693, acc 0.421875, f1 0.332406\n",
      "2017-11-26T15:57:11.845457: step 5440, loss 0.853829, acc 0.640625, f1 0.631757\n",
      "2017-11-26T15:57:12.001595: step 5445, loss 0.925905, acc 0.578125, f1 0.571149\n",
      "2017-11-26T15:57:12.145848: step 5450, loss 0.906614, acc 0.578125, f1 0.580586\n",
      "2017-11-26T15:57:12.294800: step 5455, loss 0.909389, acc 0.617188, f1 0.576629\n",
      "2017-11-26T15:57:12.438470: step 5460, loss 0.889071, acc 0.546875, f1 0.509786\n",
      "2017-11-26T15:57:12.589740: step 5465, loss 0.909153, acc 0.570312, f1 0.528884\n",
      "2017-11-26T15:57:12.724899: step 5470, loss 0.909131, acc 0.570312, f1 0.514923\n",
      "2017-11-26T15:57:12.875044: step 5475, loss 0.915251, acc 0.585938, f1 0.580538\n",
      "2017-11-26T15:57:13.019919: step 5480, loss 0.857237, acc 0.59375, f1 0.558041\n",
      "2017-11-26T15:57:13.160088: step 5485, loss 0.878902, acc 0.5625, f1 0.549326\n",
      "2017-11-26T15:57:13.310474: step 5490, loss 0.963822, acc 0.546875, f1 0.49517\n",
      "2017-11-26T15:57:13.453172: step 5495, loss 0.965027, acc 0.546875, f1 0.487156\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T15:57:13.717070: step 5499, loss 1.10902, acc 0.439172, f1 0.406641\n",
      "\n",
      "\n",
      "Current epoch:  39\n",
      "2017-11-26T15:57:13.790673: step 5500, loss 0.89097, acc 0.578125, f1 0.559662\n",
      "2017-11-26T15:57:13.929570: step 5505, loss 0.839216, acc 0.632812, f1 0.593972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T15:57:14.084594: step 5510, loss 0.965873, acc 0.492188, f1 0.417894\n",
      "2017-11-26T15:57:14.221240: step 5515, loss 0.843366, acc 0.679688, f1 0.614145\n",
      "2017-11-26T15:57:14.365108: step 5520, loss 0.890976, acc 0.585938, f1 0.535\n",
      "2017-11-26T15:57:14.504591: step 5525, loss 0.921792, acc 0.625, f1 0.612\n",
      "2017-11-26T15:57:14.650498: step 5530, loss 0.951835, acc 0.585938, f1 0.577439\n",
      "2017-11-26T15:57:14.795137: step 5535, loss 0.848254, acc 0.648438, f1 0.613385\n",
      "2017-11-26T15:57:14.932120: step 5540, loss 0.885933, acc 0.640625, f1 0.589954\n",
      "2017-11-26T15:57:15.082034: step 5545, loss 0.950015, acc 0.578125, f1 0.52363\n",
      "2017-11-26T15:57:15.234446: step 5550, loss 0.814793, acc 0.648438, f1 0.620083\n",
      "2017-11-26T15:57:15.383450: step 5555, loss 0.865833, acc 0.609375, f1 0.584156\n",
      "2017-11-26T15:57:15.529194: step 5560, loss 0.820861, acc 0.640625, f1 0.623061\n",
      "2017-11-26T15:57:15.669765: step 5565, loss 0.919402, acc 0.585938, f1 0.532704\n",
      "2017-11-26T15:57:15.807632: step 5570, loss 0.821748, acc 0.695312, f1 0.687585\n",
      "2017-11-26T15:57:15.955064: step 5575, loss 0.827815, acc 0.648438, f1 0.624115\n",
      "2017-11-26T15:57:16.100515: step 5580, loss 1.04047, acc 0.4375, f1 0.37443\n",
      "2017-11-26T15:57:16.241434: step 5585, loss 0.821256, acc 0.625, f1 0.592547\n",
      "2017-11-26T15:57:16.382130: step 5590, loss 0.83965, acc 0.625, f1 0.605718\n",
      "2017-11-26T15:57:16.503744: step 5595, loss 0.857789, acc 0.648438, f1 0.61533\n",
      "2017-11-26T15:57:16.645058: step 5600, loss 0.880015, acc 0.59375, f1 0.552003\n",
      "2017-11-26T15:57:16.778747: step 5605, loss 0.906394, acc 0.585938, f1 0.555734\n",
      "2017-11-26T15:57:16.912082: step 5610, loss 0.86611, acc 0.617188, f1 0.586058\n",
      "2017-11-26T15:57:17.058018: step 5615, loss 0.865761, acc 0.59375, f1 0.586324\n",
      "2017-11-26T15:57:17.205090: step 5620, loss 0.951726, acc 0.523438, f1 0.504795\n",
      "2017-11-26T15:57:17.345293: step 5625, loss 0.906849, acc 0.632812, f1 0.616714\n",
      "2017-11-26T15:57:17.493066: step 5630, loss 0.930637, acc 0.601562, f1 0.565755\n",
      "2017-11-26T15:57:17.638669: step 5635, loss 0.901997, acc 0.601562, f1 0.554278\n",
      "2017-11-26T15:57:17.775112: step 5640, loss 0.869568, acc 0.677419, f1 0.637736\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T15:57:17.915882: step 5640, loss 1.0387, acc 0.503877, f1 0.47994\n",
      "\n",
      "\n",
      "Current epoch:  40\n",
      "2017-11-26T15:57:18.064020: step 5645, loss 0.876365, acc 0.617188, f1 0.576119\n",
      "2017-11-26T15:57:18.198260: step 5650, loss 0.891217, acc 0.59375, f1 0.580471\n",
      "2017-11-26T15:57:18.329708: step 5655, loss 0.812128, acc 0.640625, f1 0.632972\n",
      "2017-11-26T15:57:18.474444: step 5660, loss 0.900403, acc 0.625, f1 0.62601\n",
      "2017-11-26T15:57:18.616489: step 5665, loss 0.841049, acc 0.609375, f1 0.553553\n",
      "2017-11-26T15:57:18.764029: step 5670, loss 0.887684, acc 0.601562, f1 0.571922\n",
      "2017-11-26T15:57:18.906235: step 5675, loss 0.954671, acc 0.5625, f1 0.504329\n",
      "2017-11-26T15:57:19.052534: step 5680, loss 0.883736, acc 0.59375, f1 0.549577\n",
      "2017-11-26T15:57:19.199992: step 5685, loss 0.950952, acc 0.507812, f1 0.445582\n",
      "2017-11-26T15:57:19.327864: step 5690, loss 0.859408, acc 0.648438, f1 0.597108\n",
      "2017-11-26T15:57:19.472462: step 5695, loss 0.910148, acc 0.539062, f1 0.492335\n",
      "2017-11-26T15:57:19.606273: step 5700, loss 0.901154, acc 0.601562, f1 0.600624\n",
      "2017-11-26T15:57:19.757396: step 5705, loss 0.901181, acc 0.601562, f1 0.565902\n",
      "2017-11-26T15:57:19.904861: step 5710, loss 0.838747, acc 0.601562, f1 0.588455\n",
      "2017-11-26T15:57:20.058164: step 5715, loss 0.924258, acc 0.570312, f1 0.51904\n",
      "2017-11-26T15:57:20.191556: step 5720, loss 0.790799, acc 0.65625, f1 0.642519\n",
      "2017-11-26T15:57:20.314567: step 5725, loss 1.02812, acc 0.445312, f1 0.374383\n",
      "2017-11-26T15:57:20.450191: step 5730, loss 0.933845, acc 0.5625, f1 0.537898\n",
      "2017-11-26T15:57:20.579635: step 5735, loss 0.85031, acc 0.601562, f1 0.559387\n",
      "2017-11-26T15:57:20.718013: step 5740, loss 0.878362, acc 0.617188, f1 0.567036\n",
      "2017-11-26T15:57:20.853851: step 5745, loss 0.872217, acc 0.59375, f1 0.575184\n",
      "2017-11-26T15:57:20.989372: step 5750, loss 0.79971, acc 0.75, f1 0.753252\n",
      "2017-11-26T15:57:21.119068: step 5755, loss 0.876387, acc 0.632812, f1 0.621242\n",
      "2017-11-26T15:57:21.261201: step 5760, loss 0.883685, acc 0.59375, f1 0.576699\n",
      "2017-11-26T15:57:21.404039: step 5765, loss 0.963299, acc 0.515625, f1 0.466059\n",
      "2017-11-26T15:57:21.544857: step 5770, loss 0.903845, acc 0.585938, f1 0.530274\n",
      "2017-11-26T15:57:21.672341: step 5775, loss 0.849258, acc 0.648438, f1 0.648523\n",
      "2017-11-26T15:57:21.808709: step 5780, loss 0.856588, acc 0.640625, f1 0.606412\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T15:57:21.983614: step 5781, loss 1.05324, acc 0.48764, f1 0.470766\n",
      "\n",
      "\n",
      "Current epoch:  41\n",
      "2017-11-26T15:57:22.094572: step 5785, loss 0.78394, acc 0.695312, f1 0.676804\n",
      "2017-11-26T15:57:22.257885: step 5790, loss 0.912304, acc 0.554688, f1 0.509823\n",
      "2017-11-26T15:57:22.410361: step 5795, loss 0.887202, acc 0.671875, f1 0.618904\n",
      "2017-11-26T15:57:22.553508: step 5800, loss 0.912821, acc 0.59375, f1 0.549054\n",
      "2017-11-26T15:57:22.695560: step 5805, loss 0.872585, acc 0.617188, f1 0.586538\n",
      "2017-11-26T15:57:22.831579: step 5810, loss 0.977803, acc 0.515625, f1 0.460493\n",
      "2017-11-26T15:57:22.962400: step 5815, loss 0.822995, acc 0.6875, f1 0.667484\n",
      "2017-11-26T15:57:23.110911: step 5820, loss 0.77942, acc 0.640625, f1 0.614113\n",
      "2017-11-26T15:57:23.256431: step 5825, loss 0.77387, acc 0.648438, f1 0.61887\n",
      "2017-11-26T15:57:23.390945: step 5830, loss 0.865567, acc 0.59375, f1 0.553952\n",
      "2017-11-26T15:57:23.522615: step 5835, loss 0.884014, acc 0.59375, f1 0.553977\n",
      "2017-11-26T15:57:23.663319: step 5840, loss 0.848283, acc 0.617188, f1 0.594269\n",
      "2017-11-26T15:57:23.804199: step 5845, loss 0.909995, acc 0.578125, f1 0.555443\n",
      "2017-11-26T15:57:23.953287: step 5850, loss 0.876001, acc 0.664062, f1 0.637206\n",
      "2017-11-26T15:57:24.097897: step 5855, loss 0.8158, acc 0.640625, f1 0.626859\n",
      "2017-11-26T15:57:24.232189: step 5860, loss 0.865744, acc 0.617188, f1 0.567242\n",
      "2017-11-26T15:57:24.373883: step 5865, loss 0.990199, acc 0.515625, f1 0.446648\n",
      "2017-11-26T15:57:24.527908: step 5870, loss 0.927927, acc 0.601562, f1 0.601833\n",
      "2017-11-26T15:57:24.671747: step 5875, loss 0.910712, acc 0.609375, f1 0.568576\n",
      "2017-11-26T15:57:24.819950: step 5880, loss 0.987128, acc 0.515625, f1 0.458363\n",
      "2017-11-26T15:57:24.964408: step 5885, loss 0.896901, acc 0.59375, f1 0.547857\n",
      "2017-11-26T15:57:25.108074: step 5890, loss 0.947309, acc 0.523438, f1 0.462158\n",
      "2017-11-26T15:57:25.244629: step 5895, loss 0.844847, acc 0.640625, f1 0.626243\n",
      "2017-11-26T15:57:25.378154: step 5900, loss 0.941548, acc 0.648438, f1 0.597715\n",
      "2017-11-26T15:57:25.513378: step 5905, loss 0.833499, acc 0.664062, f1 0.630776\n",
      "2017-11-26T15:57:25.653122: step 5910, loss 0.879474, acc 0.601562, f1 0.568415\n",
      "2017-11-26T15:57:25.794233: step 5915, loss 0.905612, acc 0.585938, f1 0.559377\n",
      "2017-11-26T15:57:25.953732: step 5920, loss 0.883291, acc 0.609375, f1 0.578382\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T15:57:26.168170: step 5922, loss 1.0472, acc 0.487543, f1 0.47271\n",
      "\n",
      "\n",
      "Current epoch:  42\n",
      "2017-11-26T15:57:26.304082: step 5925, loss 0.850816, acc 0.617188, f1 0.5975\n",
      "2017-11-26T15:57:26.446882: step 5930, loss 0.923293, acc 0.554688, f1 0.530814\n",
      "2017-11-26T15:57:26.602514: step 5935, loss 0.856447, acc 0.640625, f1 0.62843\n",
      "2017-11-26T15:57:26.748170: step 5940, loss 0.850351, acc 0.640625, f1 0.626973\n",
      "2017-11-26T15:57:26.888697: step 5945, loss 0.876662, acc 0.59375, f1 0.559384\n",
      "2017-11-26T15:57:27.032789: step 5950, loss 0.903381, acc 0.554688, f1 0.536699\n",
      "2017-11-26T15:57:27.173348: step 5955, loss 0.81966, acc 0.6875, f1 0.67616\n",
      "2017-11-26T15:57:27.320160: step 5960, loss 0.917487, acc 0.585938, f1 0.596098\n",
      "2017-11-26T15:57:27.447167: step 5965, loss 0.805461, acc 0.6875, f1 0.653434\n",
      "2017-11-26T15:57:27.584081: step 5970, loss 0.847588, acc 0.640625, f1 0.594906\n",
      "2017-11-26T15:57:27.722344: step 5975, loss 0.84208, acc 0.671875, f1 0.666306\n",
      "2017-11-26T15:57:27.873593: step 5980, loss 0.834361, acc 0.65625, f1 0.625266\n",
      "2017-11-26T15:57:28.014236: step 5985, loss 0.921856, acc 0.570312, f1 0.525245\n",
      "2017-11-26T15:57:28.156387: step 5990, loss 0.898943, acc 0.578125, f1 0.533736\n",
      "2017-11-26T15:57:28.306891: step 5995, loss 0.837766, acc 0.609375, f1 0.604093\n",
      "2017-11-26T15:57:28.439890: step 6000, loss 0.838708, acc 0.632812, f1 0.626725\n",
      "2017-11-26T15:57:28.577300: step 6005, loss 0.863039, acc 0.65625, f1 0.64972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T15:57:28.711226: step 6010, loss 0.843129, acc 0.617188, f1 0.568212\n",
      "2017-11-26T15:57:28.844453: step 6015, loss 0.804943, acc 0.625, f1 0.606523\n",
      "2017-11-26T15:57:28.993847: step 6020, loss 0.910986, acc 0.570312, f1 0.529769\n",
      "2017-11-26T15:57:29.129721: step 6025, loss 0.900646, acc 0.570312, f1 0.565788\n",
      "2017-11-26T15:57:29.262047: step 6030, loss 0.836581, acc 0.6875, f1 0.649561\n",
      "2017-11-26T15:57:29.404559: step 6035, loss 0.846797, acc 0.609375, f1 0.561627\n",
      "2017-11-26T15:57:29.551960: step 6040, loss 0.857678, acc 0.585938, f1 0.544656\n",
      "2017-11-26T15:57:29.702841: step 6045, loss 0.787968, acc 0.695312, f1 0.696978\n",
      "2017-11-26T15:57:29.848308: step 6050, loss 0.912365, acc 0.609375, f1 0.603613\n",
      "2017-11-26T15:57:29.980359: step 6055, loss 0.819107, acc 0.65625, f1 0.631322\n",
      "2017-11-26T15:57:30.124112: step 6060, loss 0.873451, acc 0.578125, f1 0.542752\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T15:57:30.357376: step 6063, loss 1.04626, acc 0.52341, f1 0.481529\n",
      "\n",
      "\n",
      "Current epoch:  43\n",
      "2017-11-26T15:57:30.430208: step 6065, loss 0.889437, acc 0.59375, f1 0.575562\n",
      "2017-11-26T15:57:30.563898: step 6070, loss 0.933748, acc 0.484375, f1 0.424473\n",
      "2017-11-26T15:57:30.706886: step 6075, loss 0.827916, acc 0.609375, f1 0.583343\n",
      "2017-11-26T15:57:30.851797: step 6080, loss 0.797778, acc 0.640625, f1 0.61009\n",
      "2017-11-26T15:57:30.992144: step 6085, loss 0.899418, acc 0.5625, f1 0.511982\n",
      "2017-11-26T15:57:31.130129: step 6090, loss 0.814629, acc 0.664062, f1 0.639103\n",
      "2017-11-26T15:57:31.232800: step 6095, loss 0.836282, acc 0.601562, f1 0.553133\n",
      "2017-11-26T15:57:31.379984: step 6100, loss 0.896193, acc 0.617188, f1 0.616408\n",
      "2017-11-26T15:57:31.526601: step 6105, loss 0.906925, acc 0.609375, f1 0.598609\n",
      "2017-11-26T15:57:31.668458: step 6110, loss 0.905806, acc 0.625, f1 0.589749\n",
      "2017-11-26T15:57:31.806973: step 6115, loss 0.901535, acc 0.59375, f1 0.55486\n",
      "2017-11-26T15:57:31.948232: step 6120, loss 0.967063, acc 0.492188, f1 0.423614\n",
      "2017-11-26T15:57:32.083629: step 6125, loss 0.932752, acc 0.5625, f1 0.527931\n",
      "2017-11-26T15:57:32.243783: step 6130, loss 0.891809, acc 0.59375, f1 0.528332\n",
      "2017-11-26T15:57:32.392874: step 6135, loss 0.824913, acc 0.640625, f1 0.614454\n",
      "2017-11-26T15:57:32.536447: step 6140, loss 0.838488, acc 0.664062, f1 0.667035\n",
      "2017-11-26T15:57:32.670200: step 6145, loss 0.883205, acc 0.539062, f1 0.524537\n",
      "2017-11-26T15:57:32.827376: step 6150, loss 0.930971, acc 0.5625, f1 0.497378\n",
      "2017-11-26T15:57:32.967314: step 6155, loss 0.899884, acc 0.59375, f1 0.563683\n",
      "2017-11-26T15:57:33.118162: step 6160, loss 0.859845, acc 0.523438, f1 0.484632\n",
      "2017-11-26T15:57:33.257789: step 6165, loss 0.889921, acc 0.570312, f1 0.526837\n",
      "2017-11-26T15:57:33.404937: step 6170, loss 0.891358, acc 0.523438, f1 0.444426\n",
      "2017-11-26T15:57:33.543131: step 6175, loss 0.873773, acc 0.585938, f1 0.558821\n",
      "2017-11-26T15:57:33.686060: step 6180, loss 0.923964, acc 0.570312, f1 0.572441\n",
      "2017-11-26T15:57:33.814007: step 6185, loss 0.879795, acc 0.59375, f1 0.597597\n",
      "2017-11-26T15:57:33.955539: step 6190, loss 0.805936, acc 0.617188, f1 0.593646\n",
      "2017-11-26T15:57:34.104247: step 6195, loss 0.863609, acc 0.578125, f1 0.517882\n",
      "2017-11-26T15:57:34.244406: step 6200, loss 0.815205, acc 0.648438, f1 0.604648\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T15:57:34.512826: step 6204, loss 1.113, acc 0.463164, f1 0.421544\n",
      "\n",
      "\n",
      "Current epoch:  44\n",
      "2017-11-26T15:57:34.572369: step 6205, loss 0.846381, acc 0.617188, f1 0.564497\n",
      "2017-11-26T15:57:34.701624: step 6210, loss 0.863496, acc 0.601562, f1 0.550795\n",
      "2017-11-26T15:57:34.850366: step 6215, loss 0.816965, acc 0.648438, f1 0.649589\n",
      "2017-11-26T15:57:34.993043: step 6220, loss 0.920129, acc 0.53125, f1 0.454358\n",
      "2017-11-26T15:57:35.132051: step 6225, loss 0.796992, acc 0.648438, f1 0.635229\n",
      "2017-11-26T15:57:35.263885: step 6230, loss 0.770413, acc 0.695312, f1 0.677885\n",
      "2017-11-26T15:57:35.390440: step 6235, loss 0.813137, acc 0.640625, f1 0.630459\n",
      "2017-11-26T15:57:35.522742: step 6240, loss 0.888289, acc 0.632812, f1 0.611245\n",
      "2017-11-26T15:57:35.654916: step 6245, loss 0.84857, acc 0.640625, f1 0.613094\n",
      "2017-11-26T15:57:35.797817: step 6250, loss 0.884081, acc 0.539062, f1 0.541004\n",
      "2017-11-26T15:57:35.937685: step 6255, loss 0.863091, acc 0.632812, f1 0.594547\n",
      "2017-11-26T15:57:36.071889: step 6260, loss 0.782728, acc 0.679688, f1 0.636357\n",
      "2017-11-26T15:57:36.204774: step 6265, loss 0.959571, acc 0.515625, f1 0.468333\n",
      "2017-11-26T15:57:36.348535: step 6270, loss 0.816067, acc 0.640625, f1 0.607621\n",
      "2017-11-26T15:57:36.497943: step 6275, loss 0.943686, acc 0.5625, f1 0.522137\n",
      "2017-11-26T15:57:36.646975: step 6280, loss 0.831086, acc 0.632812, f1 0.615134\n",
      "2017-11-26T15:57:36.794553: step 6285, loss 0.834435, acc 0.632812, f1 0.625072\n",
      "2017-11-26T15:57:36.931325: step 6290, loss 0.981504, acc 0.53125, f1 0.498808\n",
      "2017-11-26T15:57:37.076265: step 6295, loss 0.876272, acc 0.601562, f1 0.581288\n",
      "2017-11-26T15:57:37.209931: step 6300, loss 0.935668, acc 0.578125, f1 0.566596\n",
      "2017-11-26T15:57:37.349350: step 6305, loss 0.898265, acc 0.515625, f1 0.481268\n",
      "2017-11-26T15:57:37.496195: step 6310, loss 0.930839, acc 0.507812, f1 0.45629\n",
      "2017-11-26T15:57:37.639465: step 6315, loss 0.85422, acc 0.640625, f1 0.632889\n",
      "2017-11-26T15:57:37.783676: step 6320, loss 0.889425, acc 0.625, f1 0.590211\n",
      "2017-11-26T15:57:37.924064: step 6325, loss 0.919637, acc 0.570312, f1 0.510173\n",
      "2017-11-26T15:57:38.062062: step 6330, loss 0.924255, acc 0.539062, f1 0.527209\n",
      "2017-11-26T15:57:38.195310: step 6335, loss 0.913097, acc 0.53125, f1 0.498792\n",
      "2017-11-26T15:57:38.336531: step 6340, loss 0.873052, acc 0.601562, f1 0.578131\n",
      "2017-11-26T15:57:38.482712: step 6345, loss 1.08714, acc 0.427419, f1 0.317762\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T15:57:38.634048: step 6345, loss 1.04489, acc 0.528063, f1 0.45731\n",
      "\n",
      "\n",
      "Current epoch:  45\n",
      "2017-11-26T15:57:38.816643: step 6350, loss 0.914613, acc 0.65625, f1 0.654437\n",
      "2017-11-26T15:57:38.965354: step 6355, loss 0.831368, acc 0.601562, f1 0.574945\n",
      "2017-11-26T15:57:39.115502: step 6360, loss 0.890992, acc 0.625, f1 0.568551\n",
      "2017-11-26T15:57:39.264722: step 6365, loss 0.800571, acc 0.648438, f1 0.637752\n",
      "2017-11-26T15:57:39.419141: step 6370, loss 0.909562, acc 0.601562, f1 0.544774\n",
      "2017-11-26T15:57:39.561709: step 6375, loss 0.893203, acc 0.570312, f1 0.528606\n",
      "2017-11-26T15:57:39.713762: step 6380, loss 0.813491, acc 0.6875, f1 0.665372\n",
      "2017-11-26T15:57:39.851549: step 6385, loss 0.835226, acc 0.664062, f1 0.642724\n",
      "2017-11-26T15:57:39.978083: step 6390, loss 0.904107, acc 0.578125, f1 0.559624\n",
      "2017-11-26T15:57:40.121498: step 6395, loss 0.878468, acc 0.59375, f1 0.584738\n",
      "2017-11-26T15:57:40.254022: step 6400, loss 0.808984, acc 0.648438, f1 0.629554\n",
      "2017-11-26T15:57:40.388817: step 6405, loss 1.01565, acc 0.445312, f1 0.367564\n",
      "2017-11-26T15:57:40.529266: step 6410, loss 0.838087, acc 0.679688, f1 0.674867\n",
      "2017-11-26T15:57:40.651807: step 6415, loss 0.8703, acc 0.617188, f1 0.615424\n",
      "2017-11-26T15:57:40.780626: step 6420, loss 0.832402, acc 0.59375, f1 0.557937\n",
      "2017-11-26T15:57:40.914492: step 6425, loss 0.842621, acc 0.640625, f1 0.611002\n",
      "2017-11-26T15:57:41.053771: step 6430, loss 0.847312, acc 0.601562, f1 0.575159\n",
      "2017-11-26T15:57:41.191097: step 6435, loss 0.837905, acc 0.65625, f1 0.636807\n",
      "2017-11-26T15:57:41.333633: step 6440, loss 1.01926, acc 0.523438, f1 0.432242\n",
      "2017-11-26T15:57:41.461076: step 6445, loss 0.895657, acc 0.585938, f1 0.540921\n",
      "2017-11-26T15:57:41.599412: step 6450, loss 0.860787, acc 0.625, f1 0.61784\n",
      "2017-11-26T15:57:41.738633: step 6455, loss 0.851563, acc 0.585938, f1 0.569426\n",
      "2017-11-26T15:57:41.887735: step 6460, loss 0.89942, acc 0.585938, f1 0.572277\n",
      "2017-11-26T15:57:42.028599: step 6465, loss 0.920675, acc 0.585938, f1 0.588347\n",
      "2017-11-26T15:57:42.178504: step 6470, loss 0.820488, acc 0.640625, f1 0.628042\n",
      "2017-11-26T15:57:42.304635: step 6475, loss 0.914064, acc 0.625, f1 0.611944\n",
      "2017-11-26T15:57:42.445422: step 6480, loss 0.92699, acc 0.539062, f1 0.530853\n",
      "2017-11-26T15:57:42.584951: step 6485, loss 0.934851, acc 0.539062, f1 0.512419\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T15:57:42.762190: step 6486, loss 1.04171, acc 0.525155, f1 0.483211\n",
      "\n",
      "\n",
      "Current epoch:  46\n",
      "2017-11-26T15:57:42.889070: step 6490, loss 0.778934, acc 0.671875, f1 0.655962\n",
      "2017-11-26T15:57:43.024308: step 6495, loss 0.9042, acc 0.585938, f1 0.574403\n",
      "2017-11-26T15:57:43.174487: step 6500, loss 0.851592, acc 0.609375, f1 0.573248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T15:57:43.316997: step 6505, loss 0.846158, acc 0.5625, f1 0.539104\n",
      "2017-11-26T15:57:43.456443: step 6510, loss 0.874016, acc 0.601562, f1 0.564819\n",
      "2017-11-26T15:57:43.603483: step 6515, loss 0.902596, acc 0.59375, f1 0.606467\n",
      "2017-11-26T15:57:43.748866: step 6520, loss 0.822743, acc 0.664062, f1 0.663132\n",
      "2017-11-26T15:57:43.892317: step 6525, loss 0.874535, acc 0.648438, f1 0.613096\n",
      "2017-11-26T15:57:44.042889: step 6530, loss 0.792223, acc 0.671875, f1 0.662621\n",
      "2017-11-26T15:57:44.192563: step 6535, loss 0.848241, acc 0.648438, f1 0.633921\n",
      "2017-11-26T15:57:44.344871: step 6540, loss 0.973405, acc 0.554688, f1 0.550597\n",
      "2017-11-26T15:57:44.494065: step 6545, loss 0.953364, acc 0.515625, f1 0.486161\n",
      "2017-11-26T15:57:44.646872: step 6550, loss 0.833404, acc 0.601562, f1 0.560464\n",
      "2017-11-26T15:57:44.787537: step 6555, loss 0.817304, acc 0.679688, f1 0.676336\n",
      "2017-11-26T15:57:44.945340: step 6560, loss 0.900378, acc 0.585938, f1 0.556326\n",
      "2017-11-26T15:57:45.097003: step 6565, loss 0.796149, acc 0.625, f1 0.600292\n",
      "2017-11-26T15:57:45.245146: step 6570, loss 0.852779, acc 0.640625, f1 0.621501\n",
      "2017-11-26T15:57:45.397901: step 6575, loss 0.821037, acc 0.640625, f1 0.610448\n",
      "2017-11-26T15:57:45.533782: step 6580, loss 0.865598, acc 0.640625, f1 0.614071\n",
      "2017-11-26T15:57:45.685301: step 6585, loss 0.869855, acc 0.601562, f1 0.556469\n",
      "2017-11-26T15:57:45.826381: step 6590, loss 0.829014, acc 0.648438, f1 0.645663\n",
      "2017-11-26T15:57:45.955609: step 6595, loss 0.787716, acc 0.671875, f1 0.622497\n",
      "2017-11-26T15:57:46.083322: step 6600, loss 0.854489, acc 0.65625, f1 0.670115\n",
      "2017-11-26T15:57:46.228462: step 6605, loss 0.850726, acc 0.632812, f1 0.619396\n",
      "2017-11-26T15:57:46.363220: step 6610, loss 0.809343, acc 0.632812, f1 0.611586\n",
      "2017-11-26T15:57:46.502201: step 6615, loss 0.795898, acc 0.679688, f1 0.670731\n",
      "2017-11-26T15:57:46.644847: step 6620, loss 0.928854, acc 0.601562, f1 0.564373\n",
      "2017-11-26T15:57:46.784158: step 6625, loss 0.903545, acc 0.609375, f1 0.541181\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T15:57:46.994599: step 6627, loss 1.04821, acc 0.531601, f1 0.484557\n",
      "\n",
      "\n",
      "Current epoch:  47\n",
      "2017-11-26T15:57:47.084932: step 6630, loss 0.942014, acc 0.585938, f1 0.589698\n",
      "2017-11-26T15:57:47.224894: step 6635, loss 0.88301, acc 0.570312, f1 0.533677\n",
      "2017-11-26T15:57:47.364921: step 6640, loss 0.78631, acc 0.664062, f1 0.626217\n",
      "2017-11-26T15:57:47.508762: step 6645, loss 0.913446, acc 0.609375, f1 0.582415\n",
      "2017-11-26T15:57:47.651519: step 6650, loss 0.804239, acc 0.695312, f1 0.686982\n",
      "2017-11-26T15:57:47.801202: step 6655, loss 0.909988, acc 0.578125, f1 0.537776\n",
      "2017-11-26T15:57:47.942574: step 6660, loss 0.819617, acc 0.601562, f1 0.577442\n",
      "2017-11-26T15:57:48.083252: step 6665, loss 0.877863, acc 0.632812, f1 0.585534\n",
      "2017-11-26T15:57:48.222877: step 6670, loss 0.850934, acc 0.609375, f1 0.540774\n",
      "2017-11-26T15:57:48.359538: step 6675, loss 0.815418, acc 0.65625, f1 0.620216\n",
      "2017-11-26T15:57:48.502259: step 6680, loss 0.818777, acc 0.640625, f1 0.601987\n",
      "2017-11-26T15:57:48.655358: step 6685, loss 0.793388, acc 0.609375, f1 0.571671\n",
      "2017-11-26T15:57:48.806188: step 6690, loss 0.787082, acc 0.65625, f1 0.599837\n",
      "2017-11-26T15:57:48.953734: step 6695, loss 0.927698, acc 0.617188, f1 0.598301\n",
      "2017-11-26T15:57:49.102648: step 6700, loss 0.785135, acc 0.671875, f1 0.652298\n",
      "2017-11-26T15:57:49.256615: step 6705, loss 0.844833, acc 0.617188, f1 0.606771\n",
      "2017-11-26T15:57:49.398799: step 6710, loss 0.819781, acc 0.601562, f1 0.573763\n",
      "2017-11-26T15:57:49.545356: step 6715, loss 0.814842, acc 0.609375, f1 0.585336\n",
      "2017-11-26T15:57:49.702166: step 6720, loss 0.9736, acc 0.554688, f1 0.524633\n",
      "2017-11-26T15:57:49.849433: step 6725, loss 0.876891, acc 0.570312, f1 0.548655\n",
      "2017-11-26T15:57:49.988105: step 6730, loss 0.870317, acc 0.617188, f1 0.598179\n",
      "2017-11-26T15:57:50.130505: step 6735, loss 0.825843, acc 0.601562, f1 0.561238\n",
      "2017-11-26T15:57:50.284703: step 6740, loss 0.88274, acc 0.640625, f1 0.62981\n",
      "2017-11-26T15:57:50.426666: step 6745, loss 0.863058, acc 0.585938, f1 0.539311\n",
      "2017-11-26T15:57:50.570191: step 6750, loss 0.865937, acc 0.640625, f1 0.616247\n",
      "2017-11-26T15:57:50.717212: step 6755, loss 0.789496, acc 0.671875, f1 0.660225\n",
      "2017-11-26T15:57:50.863479: step 6760, loss 0.853059, acc 0.640625, f1 0.61291\n",
      "2017-11-26T15:57:51.006563: step 6765, loss 0.965408, acc 0.546875, f1 0.490978\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T15:57:51.243526: step 6768, loss 1.148, acc 0.421675, f1 0.374886\n",
      "\n",
      "\n",
      "Current epoch:  48\n",
      "2017-11-26T15:57:51.321359: step 6770, loss 0.911854, acc 0.5625, f1 0.519899\n",
      "2017-11-26T15:57:51.477926: step 6775, loss 0.868721, acc 0.554688, f1 0.51438\n",
      "2017-11-26T15:57:51.622960: step 6780, loss 0.829474, acc 0.632812, f1 0.604211\n",
      "2017-11-26T15:57:51.754722: step 6785, loss 0.869506, acc 0.578125, f1 0.537501\n",
      "2017-11-26T15:57:51.896912: step 6790, loss 0.863171, acc 0.617188, f1 0.623953\n",
      "2017-11-26T15:57:52.039448: step 6795, loss 0.844553, acc 0.679688, f1 0.670021\n",
      "2017-11-26T15:57:52.157704: step 6800, loss 0.845398, acc 0.585938, f1 0.548136\n",
      "2017-11-26T15:57:52.294275: step 6805, loss 0.827272, acc 0.601562, f1 0.585966\n",
      "2017-11-26T15:57:52.433001: step 6810, loss 0.845542, acc 0.671875, f1 0.656439\n",
      "2017-11-26T15:57:52.563162: step 6815, loss 0.898628, acc 0.601562, f1 0.588838\n",
      "2017-11-26T15:57:52.707212: step 6820, loss 0.840218, acc 0.617188, f1 0.592324\n",
      "2017-11-26T15:57:52.859568: step 6825, loss 1.0259, acc 0.484375, f1 0.401467\n",
      "2017-11-26T15:57:53.002652: step 6830, loss 0.849872, acc 0.671875, f1 0.65023\n",
      "2017-11-26T15:57:53.143948: step 6835, loss 0.837419, acc 0.617188, f1 0.588645\n",
      "2017-11-26T15:57:53.291011: step 6840, loss 0.825905, acc 0.578125, f1 0.554442\n",
      "2017-11-26T15:57:53.427757: step 6845, loss 0.92655, acc 0.578125, f1 0.539679\n",
      "2017-11-26T15:57:53.560362: step 6850, loss 0.875662, acc 0.53125, f1 0.504418\n",
      "2017-11-26T15:57:53.705832: step 6855, loss 0.854465, acc 0.632812, f1 0.60232\n",
      "2017-11-26T15:57:53.833497: step 6860, loss 0.829216, acc 0.625, f1 0.58416\n",
      "2017-11-26T15:57:53.980152: step 6865, loss 0.819023, acc 0.632812, f1 0.602711\n",
      "2017-11-26T15:57:54.124854: step 6870, loss 0.833394, acc 0.625, f1 0.596117\n",
      "2017-11-26T15:57:54.254649: step 6875, loss 0.829297, acc 0.65625, f1 0.627241\n",
      "2017-11-26T15:57:54.403834: step 6880, loss 0.914062, acc 0.546875, f1 0.516019\n",
      "2017-11-26T15:57:54.557892: step 6885, loss 0.905128, acc 0.59375, f1 0.566667\n",
      "2017-11-26T15:57:54.691485: step 6890, loss 0.997413, acc 0.570312, f1 0.539557\n",
      "2017-11-26T15:57:54.835246: step 6895, loss 1.06357, acc 0.46875, f1 0.3599\n",
      "2017-11-26T15:57:54.982160: step 6900, loss 0.837198, acc 0.640625, f1 0.604429\n",
      "2017-11-26T15:57:55.122357: step 6905, loss 0.829892, acc 0.664062, f1 0.646566\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T15:57:55.388500: step 6909, loss 1.15578, acc 0.430108, f1 0.395723\n",
      "\n",
      "\n",
      "Current epoch:  49\n",
      "2017-11-26T15:57:55.442812: step 6910, loss 0.859855, acc 0.617188, f1 0.590906\n",
      "2017-11-26T15:57:55.587727: step 6915, loss 0.824069, acc 0.625, f1 0.610263\n",
      "2017-11-26T15:57:55.738412: step 6920, loss 0.819437, acc 0.625, f1 0.611173\n",
      "2017-11-26T15:57:55.881373: step 6925, loss 0.879147, acc 0.648438, f1 0.654528\n",
      "2017-11-26T15:57:56.026764: step 6930, loss 1.01876, acc 0.460938, f1 0.415083\n",
      "2017-11-26T15:57:56.164418: step 6935, loss 0.830048, acc 0.601562, f1 0.532121\n",
      "2017-11-26T15:57:56.310169: step 6940, loss 0.778009, acc 0.65625, f1 0.642048\n",
      "2017-11-26T15:57:56.454972: step 6945, loss 0.791744, acc 0.625, f1 0.56307\n",
      "2017-11-26T15:57:56.595250: step 6950, loss 0.82813, acc 0.664062, f1 0.654852\n",
      "2017-11-26T15:57:56.733192: step 6955, loss 0.773069, acc 0.664062, f1 0.625105\n",
      "2017-11-26T15:57:56.879145: step 6960, loss 0.977809, acc 0.53125, f1 0.518566\n",
      "2017-11-26T15:57:57.018438: step 6965, loss 0.981334, acc 0.515625, f1 0.455278\n",
      "2017-11-26T15:57:57.153517: step 6970, loss 0.839644, acc 0.648438, f1 0.641751\n",
      "2017-11-26T15:57:57.289258: step 6975, loss 0.792155, acc 0.671875, f1 0.666814\n",
      "2017-11-26T15:57:57.428152: step 6980, loss 0.82161, acc 0.664062, f1 0.658655\n",
      "2017-11-26T15:57:57.573570: step 6985, loss 0.861313, acc 0.625, f1 0.585653\n",
      "2017-11-26T15:57:57.709362: step 6990, loss 0.770515, acc 0.632812, f1 0.610896\n",
      "2017-11-26T15:57:57.835243: step 6995, loss 0.779675, acc 0.734375, f1 0.716666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T15:57:57.964468: step 7000, loss 0.830858, acc 0.59375, f1 0.580878\n",
      "2017-11-26T15:57:58.091870: step 7005, loss 0.827472, acc 0.632812, f1 0.608594\n",
      "2017-11-26T15:57:58.227237: step 7010, loss 0.845769, acc 0.648438, f1 0.625579\n",
      "2017-11-26T15:57:58.370009: step 7015, loss 0.912292, acc 0.5625, f1 0.526175\n",
      "2017-11-26T15:57:58.518702: step 7020, loss 1.00055, acc 0.539062, f1 0.495456\n",
      "2017-11-26T15:57:58.651480: step 7025, loss 0.919903, acc 0.546875, f1 0.537127\n",
      "2017-11-26T15:57:58.784970: step 7030, loss 0.838688, acc 0.578125, f1 0.556658\n",
      "2017-11-26T15:57:58.926087: step 7035, loss 0.886165, acc 0.625, f1 0.615678\n",
      "2017-11-26T15:57:59.074848: step 7040, loss 0.924926, acc 0.554688, f1 0.537831\n",
      "2017-11-26T15:57:59.224604: step 7045, loss 0.818585, acc 0.664062, f1 0.641668\n",
      "2017-11-26T15:57:59.369724: step 7050, loss 0.916484, acc 0.596774, f1 0.598049\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T15:57:59.521938: step 7050, loss 1.09658, acc 0.510517, f1 0.470381\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "### Define input data parameters\n",
    "\n",
    "# put Word2Vec on 590 million English Tweets using 52 dimensions.\n",
    "vocabulary_size = final_embeddings.shape[0]\n",
    "embedding_size = final_embeddings.shape[1]  # Dimension of the embedding vector.\n",
    "sequence_length=x_train.shape[1]\n",
    "num_classes=3\n",
    "\n",
    "### Define Hyperparameters\n",
    "\n",
    "# filter_sizes: The number of words we want our convolutional filters to cover. \n",
    "# We will have num_filters for each size specified here. \n",
    "# For example, [3, 4, 5] means that we will have filters that slide over 3, 4 and 5 words respectively, for a total of 3 * num_filters filters.\n",
    "first_layer_filter_sizes = [4]\n",
    "second_layer_filter_sizes = [3]\n",
    "\n",
    "first_pool_window_sizes = [4]\n",
    "first_pool_strides = [2]\n",
    "\n",
    "num_filters = 200\n",
    "\n",
    "l2_reg_lambda=0.0  # No L2 norm\n",
    "\n",
    "batch_size = 128\n",
    "num_epochs = 50\n",
    "\n",
    "num_checkpoints = 5\n",
    "print_train_every = 5\n",
    "evaluate_every = 10000000\n",
    "checkpoint_every = 10000000\n",
    "allow_soft_placement=True\n",
    "log_device_placement=False\n",
    "\n",
    "### Define record list for plot acc/epoch and loss/epoch\n",
    "train_accuracy_temp = []\n",
    "train_accuracy_list = []\n",
    "test_accuracy_list = []\n",
    "\n",
    "train_loss_temp = []\n",
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "\n",
    "epoch_list = []\n",
    "epoch_counter = 0\n",
    "\n",
    "with graph.as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=allow_soft_placement,\n",
    "      log_device_placement=log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = SentimentTwoLayerCNNModel(\n",
    "            sequence_length=sequence_length,\n",
    "            num_classes=num_classes,\n",
    "            vocabulary_size=vocabulary_size,\n",
    "            embedding_size=embedding_size,\n",
    "            first_layer_filter_sizes=first_layer_filter_sizes,\n",
    "            second_layer_filter_sizes=second_layer_filter_sizes,\n",
    "            first_pool_strides=first_pool_strides,\n",
    "            num_filters = num_filters,\n",
    "            first_pool_window_sizes=first_pool_window_sizes,\n",
    "            l2_reg_lambda=0.01)\n",
    "        \n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdadeltaOptimizer(1.0)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=num_checkpoints)\n",
    "\n",
    "       # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "            }\n",
    "            _, step, cur_loss, cur_accuracy, y_pred = sess.run(\n",
    "                [train_op, global_step, cnn.loss, cnn.accuracy, cnn.predictions],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "\n",
    "            if step % print_train_every == 0:\n",
    "                f1 = f1_score(y_batch, y_pred, average = 'weighted')\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}, f1 {:g}\".format(time_str, step, cur_loss, cur_accuracy,\n",
    "                                                                    f1))\n",
    "            # For record accuracy and loss\n",
    "            train_accuracy_temp.append(cur_accuracy)\n",
    "            train_loss_temp.append(cur_loss)\n",
    "\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "            }\n",
    "            step, cur_loss, cur_accuracy, y_pred = sess.run(\n",
    "                [global_step, cnn.loss, cnn.accuracy, cnn.predictions],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            f1 = f1_score(y_batch, y_pred, average = 'weighted')\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}, f1 {:g}\".format(time_str, step, cur_loss, cur_accuracy,\n",
    "                                                                    f1))\n",
    "            # for record history\n",
    "            test_accuracy_list.append(cur_accuracy)\n",
    "            test_loss_list.append(cur_loss)\n",
    "        \n",
    "        \n",
    "        sess.run(cnn.embedding_init, feed_dict={cnn.embedding_placeholder: final_embeddings})\n",
    "        # Generate batches\n",
    "        batches = batch_iter(\n",
    "            list(zip(x_train, y_train)), batch_size, num_epochs)\n",
    "        \n",
    "        batches_test = list(batch_iter(\n",
    "            list(zip(x_test, y_test)), batch_size, 1))\n",
    "        \n",
    "        # Training loop. For each batch...\n",
    "        for batch, end_of_epoch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            \n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if end_of_epoch:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_test, y_test)\n",
    "                print(\"\")\n",
    "                \n",
    "                ### For record training history\n",
    "                # Take average of accuracy and loss per epoch\n",
    "                train_accuracy_list.append(sum(train_accuracy_temp)/len(train_accuracy_temp))\n",
    "                train_loss_list.append(sum(train_loss_temp)/len(train_loss_temp))\n",
    "                # clear temp list\n",
    "                train_accuracy_temp = []\n",
    "                train_loss_temp = []\n",
    "                epoch_list.append(epoch_counter)\n",
    "                epoch_counter+=1\n",
    "                print(\"\")\n",
    "            if current_step % checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "        final_embeddings = cnn.embeddings_words.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show_graph(graph.as_graph_def())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "50\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "print(len(test_accuracy_list))\n",
    "print(len(train_accuracy_list))\n",
    "print(len(epoch_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Plot word2vec distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_plot(words, matrix, v_dic, nbest=15):\n",
    "    word_vectors = []\n",
    "    for word in words:\n",
    "        index = v_dic[word]\n",
    "        word_vectors.append(matrix[index])  \n",
    "    pca = PCA(n_components=2)  \n",
    "    pca.fit(word_vectors)\n",
    "    X = pca.transform(word_vectors)\n",
    "    \n",
    "    xs = X[:, 0]\n",
    "    ys = X[:, 1]\n",
    "\n",
    "    # draw\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.scatter(xs, ys, marker = 'o')\n",
    "    for i, w in enumerate(words):\n",
    "        plt.annotate(w, (xs[i], ys[i]))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2vec with distance supervised learning\n",
      "\n",
      "Before\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAu8AAAHdCAYAAACt2ZdqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f356eea0198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAu8AAAHdCAYAAACt2ZdqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3551ff0b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "words = ['sadness', 'sobbing', 'cry', 'weep', 'horrible', 'worst', 'awful', 'enjoyable', 'wonderful',\n",
    "         'bad', 'fantastic', 'great', 'good', 'beautiful', 'stunning','gorgeous','glad','well','dumb']\n",
    "\n",
    "plt.rcParams['font.size'] = 18\n",
    "\n",
    "print(\"Word2vec with distance supervised learning\\n\")\n",
    "print(\"Before\")\n",
    "draw_plot(words, original_embeddings, original_word_dict)\n",
    "print(\"After\")\n",
    "draw_plot(words, final_embeddings, word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot loss/epoch and acc/epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaUAAAEsCAYAAABjbay+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzsnXd4VFXawH9n0nunhJbQkSpNiiBNwV7XXmCt7OKq67rr\nquviru1zV9ayWEARu1hQF1EBlSKi0ovSQwKkkJCE9J453x/nDplMpmYmmQTO73nmuZnT752b+973\nnPd9j5BSotFoNBpNW8Dk7wFoNBqNRmNBCyWNRqPRtBm0UNJoNBpNm0ELJY1Go9G0GbRQ0mg0Gk2b\nQQsljUaj0bQZtFDS+BQhxBohhBRCTPJBWzONthZ7P7KWx5fnbqftj422U3zdtkbTltBCSXNKIYSY\nazy85/p7LL5CCBEKzAC2SykzjLQWE4AejGuxMYaZ/hqD5tQj0N8D0GhOIW4GwoEjPm73XCAC+MzH\n7Wo0bQ4tlDQaHyGl9LUwsnCZcdRCSXPqI6XUn3bwAaT6uSTAbcA2oALIBl4EIo28eOAF1Nt6FbAb\nmOmk3Q7As8B+o3wRsA711i8c1IkC/gUcBqqBdON7BLDGGOskO/VMwI3Ad0ChUfcQ8DzQ0U75mUZb\ni928RhmW62TnM9f2WgICmA1sAcqAIqsy5wIvATuNsVYZY30F6OGgf7vnbp0OjAW+Nq5zBbAemOrk\nnExAHnDI+J7i5Bzt9d0DmA8ctPp9VwNXOOivs/Fb/gqUGNflMPA5cJWda2jv4/B+s6ofBNwELDHu\nvTLjswN4FIhwUjcK+CuwCSg2ruNB4C1gXHPLW13bDAf9TjLy1zhKByKB/zParwY+a43zBboDdUA+\nEOLkf73auAcc9ufvj98HoD9u/lAN//D/Nh4uX6HenAuM9FVAgnHDZxo3/1rAbOTfbKfNvkCWkX/U\nqPOV0b4E3sVGMBn/IFuN/ELgE+OBVQr8DGzA/sMxyBivNMquNuqmGWmZQE+bOjPxTCj9G9hu1NkO\nLLb6XGbnWr4E1KKE5PvAD1ZlDgKVwGZjnP9DPZylcc372el/jYNzt6T/y+hvM/ABsMtIrwUmOjin\niUaZecb3RON8jhnpX9ucZ3+rutNQgkUCe43zWGOclwSetOmrs1W7h4BPgQ+N37Qc+Nqq7GLjGkmU\nYLUew9lu/FZdra7leuN6rEQ9MKVxjcLs1EsFDhhlioAvUPftT6gH7uLmlsd7ofQz6gWn2LhfPgJe\naa3zNX4vCdzoYPx/NfJf8PfzzOm94e8B6I+bP1TDgzQH6GuV3gX1Ji2BX1AP12Cr/DstDxk7bW4y\n8hbb1OlHg7CabVPnOat/wDir9GRgn9U4J9nUe4YG4dnJKt0EPGHkrbOpM9MyPg+u01xsNCMn17IQ\nONNBmUuBGJu0AOAxo+7XduqscXDulnQzcK1VukBpuRL4zsE45hn5E93py+b3OAHUWPdp5PWnQauc\nYpX+dyPtJTvtRQJjbdIW46ZmZKe9KOAiINAmPQZYbrT7oE2eiYaXjveAKJv8RKwEYjPKp+CdULII\nl0Q/ne80o+x6O/2bUDMaEjjD09+rNT9+H4D+uPlDNdz0t9nJ+4+RVwwk2OQFoFR6idW0Ew1v4AW2\nN7uRP9PIP2iVFo6acpDAaDt1LrYa5ySr9ATU23mh7fiMfOt/viF2xrDYg+s0F/eF0oPutmtTPxOo\nt/OQWGN77jbpH9hpK9HIqwaC7OSnAceBAHf6ssq3vAQ85iD/CiN/qVXafCPtMnt17LSxmGYKJRft\n9jHa3WSTfjkNWl+Ta2WnHU/Lp+C9UBrjr/M16uwx6gyySb/A3tjb4kcbOrQ/VtpJSzOOW6SUBdYZ\nUsp6IUQGSjAko6agQAklgE+llKV22nwHWAD0EkJ0kVJmASNQ60YHpZQbbStIKZcJIYqAWJusSUAo\nsNx2fEY9sxBiPTAUGINax2kNnBoOCCF6ABeipjmjUAIe1FSkCeiNWttzl69sE6SU+UKIQtRaYCJK\nE7b0PwToCSySUtZ70A/A+cbxIwf564zjGKu0zcbxKSGEGfhGSlnhYb8eIYQYBUxGrX2Fo7RHYWT3\ntSk+wzi+LaWsdaN5T8t7S66U8idnBVr4fEFNSb8A3AXMsUqfbRxfcbMdv6GFUvsj005amZM86/wQ\nq7QuxjHdXgUpZZ0Q4gjQyyibZVUnw8n4DtNUKPU0jlcKIaSTugBJLvJ9yWFHGUKIx4EHaRBE9oj2\nsL+jDtJLUUIpxCb9cuPYHKs7yzXfJYRwVs76er+JeoG4GbVOWCeE2IHSyt6RUm5vxjjsIoSIRK2r\nXOikmO317W4c97nZjaflvcXZ/dQa5wvqN3wSuFEI8RcpZbkQojtKU8oDlnrQll/QQqmdIaU0O8l2\nludPLA/23ah1LGf82sJjOYmUstJeuhDiKuBhlJHAvSijjBwpZbWRvwFlRef0aW8HT3+fy1AGBqs8\nrAcN1/w9lCGFS4x76xYhxP+h1j8mA+NQGvL9Qoh/SikfbcZY7PE06gH9K/AXlJZWKKWsFUIEo6Yz\nmwzRwz48Le8KV8EG7N5PBq1xvkgpS4QQb6M0o+uA14A7jLG/LqWs8bTN1kYLpdOXLOPY016mECKQ\nhje1LJtjDyft2suzaAhbpZQzPRijv7jKOD4spXzDTn7vlh6AMXU4DLXmU9WMJo6ixvmolDLNVWFr\npJS7US8Qzxj3wVWo9aNHhBDvSSn3NmM8tliu8bVSyl9s8hxdX4sfmO00lyM8LW95YEc6yO/mZjv2\naI3ztTAfJZTuEkK8CdyKeiFa4GE7fkGHGTp9sawpXCaEiLKTfwNq7STNWE8CZe5aDvQRQoy0rSCE\nuJCmU3cA36Le1mcY0xgtieXB4s0LV7xxbDLdJoSYSutMMbpymHV1nl8bx6sc5LuFlLJOSvkB6n4R\nwGAPxuAMh9cY9YZvD8t66k1CiCA3+vC0fD7qPk0QQiTayT/PjTYc0RrnC4CU8lfUlOsIlGVrJ+Ar\naYSoautooXSaIqVchxIy8cAL1je9EKIP6mYG5VhrqVMBLDK+viiEiLWq0xnlJ2Svr2PAy6iF/E+F\nEE20MyFErBDiTuPN3BssAnSAF21YNIHbba5LCuo8WoPLUM6QXzjId3We/0atVc0VQtwqhGi0NiYU\no4QQ51ql3SyEONO2ISFEV5QRCjQOoeTNtbZc49/Z9DUNuN9Bnc9RRjD9gUW2LzhCiEQhxNnNLW9M\nbf1gfH3UpuzNOBYe7tAa52vNf43jA8axzRs4nMTf5n/6494Hw+TUQd5MnJhO49hU2dp59ghqIfZL\nGpxn38O+86zFfLsA+Bj1Nl8KbMSx82wwynlTot6wf0Y5AX6EEo61Rl6ou+fl4Fw7obQ5iXq7fwM1\nr36JO9fSyO+NMq+XKKOOD1GaRyXKIfkHB+fo6DrbTbfKzzDyU4zvCSiB9I2TMV5i1KlCOWq+Znz6\nWZWZhvJVkqg39K9RDtFf0+Ak+7RVeYtz8xFgGcoCcyUNzrZLbMYwDGUaXw+sAF43xtAkqoKd8V9t\n+R1QztjvWd07Tzn6jVCGN4eM/BPGOD/AsfOsp+XPsboXdxr3504j7V84Nwlf4+/ztaoXaPzmlnvY\n1NLPKF99/D4A/XHzh2oBoWTkdUA5aB4wbvIS4HvgFhyHGYpGvYkfMeocNtqIdNaXUfdy4x/rGEo4\nHUeFWnkZmO7JeTm5VpNRxglFNES0mOvOtbQq09t4IGWhHsp7UT5QIY7O0dN0q/wMGgsly3nPcTHG\n2agXhAqrB55t38mosDc7UVaYFcZDbiVwD5BsVXYiKuTTJiDX+G2PAt8A12LjK2XUuQr1gCy1GsNM\nN3+nqagXhxNG/Z+Am9y432NQjr47UC8g5cb9uxg7fkLNKD8V9T9Qjvp/+AYYjxthhtrC+VrVe9to\n9yFP/n/8/RHG4DUaTRtBCPEZKqJENymlIzN/jcYhQogI1AtVGNBdSpnr5yG5jba+02jaHhtQb91a\nIGmayx9RGtZb7UkgAVpT0mg0mlMBIUQ/lGFDV5SlYCUq3FC6XwfmIVpT0mg0mlODziifpCqUc+6f\n25tAAq0paTQajaYNoTUlD0lMTJQpKSn+HoZGo9G0K7Zs2ZIvpXTpeK6FkoekpKSwefNm1wU1Go1G\ncxIhhMOAtdboiA4ajUajaTNooaTRaDSaNoMWShqNRqNpM2ihpNFoNJo2gxZKGo1Go2kzaKGk0Wg0\nmjaDFkoajUajaTNoP6UWorq6msLCQkpLS6mvr/f3cDQ2BAcHk5iYSExMjL+HotG0OjV1ZpZsOsLY\nXgn07mBv42n/oYVSC1BdXc2RI0eIi4sjJSWFoKAghBD+HpbGQEpJZWUlmZmZhISEEBoa6u8haTSt\nRr1Zct+H21m+Mwch4OIhyfxhau82I5y0UGoBCgsLiYuLIzEx0d9D0dhBCEF4eDiJiYkcP36cbt26\n+XtIGk2rIKXk4U93sXxnDvdO60N1nZk3N2SwbGc2Fw1J5g9TetOno3+FkxZKLUBpaSk6Pl7bJyoq\nioKCAn8PQ6NpFaSUPL58Dx9sOsrdU3pz77S+ANw+oScLvz/Emxsy+MIQTpcMTaZvx0i6xYVjMrXu\nLI8WSi1AfX09QUFB/h6GxgWBgYHU1dX5exgaTavw/LcHeH19OjPHpfDHc/ueTI+PCOYvM/o3Ek7L\ndmQDEBpkoneHSPp2iKJ3x0gGJccwsa/LmKpeoYVSC6HXkNo++jfSnC689v0hnvvmAFeN6MqjF51h\n9963CKc5k3uzL7eUA7ml7M8tY39uKRvSCli6LYvh3WO1UNJoNBpN81my6QiPL9/D+YM68fQVg11O\nx0WEBDK8exzDu8c1Si+urKW4orYlhwr42U9JCGESQtwnhNgrhKgSQhwVQjwrhIjwoI1AIcQfhBBb\nhRDlQohi4+877ZSNEUK8KITIMvr7VQgxW+hXZo1GcwqRV1rFkk1HuOOtzTy4dBfn9E3iuWuHERjQ\n/Ed+TFgQ3RPCfThK+/hbU/oP8AfgU+BZYIDx/UwhxDQppdlZZSFEMPA/YDLwLvAK6pz6AD3slF0F\nnAm8COwBzgdeAjoCc311UhrfM3PmTN588030TskaTVOklPyaXcK3e/L4bm8uOzKLAUiOCWXmuBT+\nPL0/IYEBfh6le/hNKAkhBgJ3A0ullFdapacDLwDXAu+5aOZvwDTgXCnlahdlbwNGAX+QUr5opC0U\nQnwCPCSEeENK6dYmVJrGbN++nc8++4yZM2dqq0ONppWRUvLgJ7tYsvkoQsCwbrH86by+TB3Qkf6d\notrd2qk/p++uAwTwnE36QqACuNFZZWOK7x7gcynlaqFwZmB/vdHuQpv054Ag4BoPxq6xYvv27Tz2\n2GNkZGS0WB8LFy6ksrKyxdrXaFqKE+U1fLotk6ralons8taPh1my+Si/HZ/Kxoem8envxjNnSh8G\ndI5udwIJ/CuURgFmYKN1opSyCthu5DtjAhAFbBFCPA+UACVCiONCiCeFECe1QCGECRgObDPat2Yj\nIN3oT+MD6uvrqaio8LheUFCQjrygaXeYzZI572/lviU7mPHcOtbuP+7T9n8+VMA/v9jNtAEdeeTC\nASRFhfi0fX/gT6GUDORLKavt5GUBicY6kCP6Gcd7gSuBP6O0nQ3AX4HXrcrGAWFGu40w+s8Hujjq\nSAhxhxBisxBi8/Hjvr2p2jtz585l1qxZAEyePBkhBEIIZs6cyeLFixFC8M033/DPf/6TXr16ERoa\nyocffgjAypUrueaaa+jZsydhYWHExsZy3nnnsXbt2ib9zJw5s8lbnyWtuLiY2bNn06FDB0JDQxk/\nfjw///xzy5+8RuOCl9em8cPBAm49OxUhBLcs2sjsd7aQXeRY688rqWL5zhyynJQByC6q5HfvbqV7\nQjj/uWZoqzu5thT+NHQIB+wJJIAqqzI1DspYpurigYFSyn3G9w+FEKuBm4UQT0sp9xjt4KI/h2Yl\nUsoFwAKAkSNH6pV2K6644gpycnJYsGABDz30EAMGDACgV69e7NunfpI//elP1NbWcvvttxMdHU2/\nfup9YvHixRQWFnLzzTfTtWtXsrKyeO2115g6dSqrV69mwoQJbo1h+vTpJCUl8eijj1JQUMC8efO4\n8MILSU9PJyqqbcTz0px+bDl8gnmr9nPRkM48cuEA/jyjHwvWHuK/qw+ydv9x7pnah9+enYpZSjal\nn2DdgeOs23+cvcdKAQgPDuCvFwzghtHdmwicqtp67npnC9V1ZhbcNJKo0FPHWd+fQqkC6OAgL9Sq\njCMsrxE/WQkkC28Bk4zPHqt2HOm2oS768hmPLfuV3dklrdGV25yRHM3fLx7YrLpDhgxh7NixLFiw\ngHPPPZdJkyadzLMIpcrKSrZt20Z4eGO5v3DhQiIiGlv/33XXXQwcOJCnnnrKbaE0fPhwXnrppYbz\nOeMMrr76at577z3uvLOJZ4BG0+IUV9byh/e3kRwbypNXDEYIQUhgAHdP7cNlZ3bhsWW/8tRXe3lz\nQwYnKmqprK0nKEAwskc8f5nRn6HdYnh5TRp/++wXvtyZwzNXDaFbvPr/UfHrfmFnZjELbx5J7w6R\nfj5b3+JPoZQNnCGECLEzhdcFNbXnSEsCyDSOx+zk5RhHi/fXCZQQazJFJ4QIARKBpnNGGp8we/bs\nJgIJaCSQysrKqK6uJiAggLPOOouffvrJ7fbvu+++Rt+nTJkCwIEDB5o5Yo3GMeXVdQSYBKFB9k2s\npZQ8tHQXuSVVfHTXWKJttJhu8eG8dssovtmdyxsb0pmWFMnEPkmM7ZVAREjDI3lszwSWbDrK48v3\nMP25dTx4fn9uPKsHb/90mE+2ZnLP1D6ce0bHFj1Xf+BPobQJOA8YDXxvSRRChALDgHUu6lsMJLra\nybOk5QFIKc1CiK0o/ydbITgaZQW42eMzaAbN1UjaM3379rWbnpaWxsMPP8yKFSsoKipqlOeJ1VDP\nnj0bfU9ISADQwVY1PievpIrL5v9ATb2Z303qzfVndW8inD7YdJTlu3L484x+nGkTFcGaaWd0ZJoT\noSKE4NrR3ZnQN4m/Lt3Fo5//ytKtWfySVcy0AR24Z2ofn51XW8Kfhg5LUFZv99qk345a33nXkiCE\n6CyE6C+EOPm6LaVMB34ARgshhluVDTDaqANWWrX7vtHuHTb93WuUXeLtCWnsY09LKisrY+LEiXz9\n9dfcc889fPzxx6xYsYJVq1YxZcoUj5xkAwIcv7FqNL6iqrae29/eQlFlLT2TIvnHF7uZ9K81vPvz\nYWrqlJ//gdxSHlv2K2f3TuSuib180m+X2DDenDWKZ64cQlpeGd0Twpl3zbBTxrDBFr9pSlLKXUKI\n+cAcIcRS4EsaIjqspbHj7FPALajIDWus0u9GaVnfCCFeAApQFnijgX9IKY9YlV0IzALmCSFSUGtN\nFwCXA49LKTN8e4anD83xhfj222/Jzs5m0aJFJ633LDzyyCO+GppG4xOklDzw8U52Zhbxyo0jmD6w\nExsO5vPvlft4+NNfeGVtGndP6cOi9elEBAcy72rfWsMJIbh6VDemD+yEycQpZdhgi7/DDN0LZKC0\nlwtRptkvAo+6CjEEIKXcJoQYBzxutBWKEjazpJSLbcrWCCGmGWWvAxKANJRgm++j8zktiYxUC62F\nhYVu17FoN7bazMqVK7U5t6bN8eJ3B1m2I5u/zOjP9IGdABjXO5FPeiWwZv9xnl25jz9/vBOAxbNG\n0SG6ZXzqYsJPXWFkwa9CSUpZj4p596yLcjOBmQ7ydgKXuNlfETDH+Gh8xKhRozCZTDzxxBOcOHGC\niIgIUlNTndY5++yz6dSpE/fffz8ZGRl07dqV7du38/bbbzN48GB27drVSqPXaJyzfGcO81bt54rh\nXbjrnMbrl0IIJvfrwKS+SazanUtVnZlJ/RwZFWvcwa9RwjWnBt27d2fRokVUVlYye/ZsrrvuOl5+\n+WWndWJjY1mxYgVnnXUWL774Ivfffz+7d+/myy+/ZPjw4U7rajStxc7MIu7/aDsjesTxlGHabQ8h\nBOcN7MQlQ5NbeYSnHkIvBnvGyJEj5ebNzg319uzZc9KJVNO20b+VxhHHiqu4dP56Ak0mPp8znsTI\n9h/Cx58IIbZIKUe6KufvNSWNRqNpU0gp+elQIY8t+5Wyqjo+nj1OC6RWRAsljUZzyrAzs4j9uWV0\njA6hY3QoHaNCiQ4LdMtCtLbezJe7clj4/SF+ySohISKY/94wnAGdo1th5BoLWihpNJp2zy9Zxcxb\ntZ/v9uY1yQsJNNExOpROMaH0iA8nJTGC7vHhpCRE0CMxHJMQfLDxCG/8kEFWUSU9kyJ48vLBXDG8\ni8OoDZqWQwsljUbTbtl7rIT/rNrPil9ziQkL4oHp/Th/UCcKymvILakit6SavJIqjpVUkV1Uydr9\nx/loS2ajNgJNgjqzZHRqPI9dMpAp/Tucso6p7QEtlDQaTbvjQG4pz397gOW7cogMDuTeaSritiXO\nXM8kx3Urauo4UlhBRn4FhwvKKSyv4YLBnRnaLbaVRq9xhhZKGo2mXVBVW89Xv+Tw/sajbEwvJDw4\ngN9N6sXtE3oSG+5s67XGhAcH0r9TNP076bWitogWShqNxm/szy1lyaaj/HAwn65x4fTvFEXfTlH0\n7xRFamIEQQEm9h4r4YONR1m6NZOSqjpSEsL5y4z+XD2yKwnaKu6UQwsljUbTqpRV17FsRzZLNh1l\n+9EiggIEo1PjySgoZ/W+POrNyncyKEDQMTqUzBOVBAeYmDGoE9eO7saY1AS95nMKo4WSRqNpEcxm\nSWFFDXkl1eSWVpFXUsXmjBMs35VDRU09fTpE8siFA7j8zC4nNZ7qunrS8srZn1vKvtxSMvLLmTU+\nlSvO7EJchPtTdJr2ixZKGo3GZ9TUmXli+W5W7c4lr7SaOnPjiDERwQFcMjSZq0d148xusU38h0IC\nAzgjOZozkvV6z+mKFkoajcYnlFbVMvudraw/mM8FgzuRkhBBhyjlxNohOpSO0SF0iAolOFCH3NQ4\nRgsljUbjNTnFlcx6YxMH88r492+GctUIextCazSu0UJJo9F4xZ6cEma9sYmy6jremDWKCX2cOAlp\nNC7QerTGa7Zv387cuXPJyMho8b6ee+45Fi9e3OL9aNzjh4P5XP3Kj0gkH945VgskjddooaTxmu3b\nt/PYY49poXQaIaXkw01HuWXRRpJjw/j0d+O1cYLGJ+jpO41G4xE7jhbx+PLdbMo4wbheCbxy04iT\n4X00Gm/RmpLGK+bOncusWbMAmDx5MkIIhBDMnDkTgOrqap588kkGDhxIaGgosbGxXHzxxWzbtq1R\nO2azmeeee44hQ4YQFRVFdHQ0/fr149Zbb6W2thZQu3sePnyYtWvXnuxHCNEqGpoGMk9UcM8H27h0\n/g+k55fz5OWDeeu3o7VA0vgUrSlpvOKKK64gJyeHBQsW8NBDD53cxbVXr17U1tYyY8YMNmzYwE03\n3cScOXMoLi5m4cKFjB8/nnXr1jFypNqI8oknnuDRRx/l4osv5q677iIgIID09HT+97//UV1dTVBQ\nEG+//Tb33XcfiYmJPPzwwyfHkJSk1zHcpaCsmp2ZxdSZJfVmiVk2HAFiwoKIDQ8mLjyI2LBgokID\nKaup4+U1aby+Ph0BzJncm7sm9SIyRD8+NL5Hb4fuIV5vh/7Vg3BsVwuMzAs6DYbzn2529cWLFzNr\n1ixWr17NpEmTTqb/5z//4Y9//CNff/0106dPP5leUlLCoEGD6NmzJ2vWrAFg+PDhVFVVsXv3bqd9\npaSkkJKScrKet5wu26HnlVTx6rpDvPvzYapqzW7XMwkIDDBRU2fmijO78Kfp/UiODWvBkWpOVfR2\n6Bq/884779C/f39GjBhBfn5+o7xzzz2XN998k8rKSsLCwoiJiSEtLY3169dz9tln+2nE7QezWbJ0\nWxbPf7ufmLAgzumbxDl9O3Bm91iCAhpm5bOLKnl1bRrvbzpKvVly6bBkrhnZjbDgAExCEGBSH5MQ\ngKS4spaiilpOVNRSVFFDUUUtFTX1XH5mFwZ3jfHfCWtOG7RQam280EjaG3v27KGystLp9Fp+fj7d\nunXjySef5LLLLmPChAkkJyczadIkLrzwQq666iqCg3XMM2t2ZRbz6P9+YduRIoZ2jSEkMIBX1h5i\n/uo0okICGdc7gQl9ktidU8JHm48iJVw1oiu/m9Sb7gnh/h6+RuMULZQ0LYaUksGDBzNv3jyHZSwC\na+zYsaSlpbFixQpWr17N6tWree+993j88cdZv3498fHxrTXsNktBWTX/XrmPDzYdJSEimH9dNYQr\nh3fFZBKUVNWy4WA+a/fns27/cVb8mktwgIlrRnXjrnN60TVOCyNN+0ALJY3XnAyqKRuvVfTp04fj\nx48zZcoUTCbXhp6RkZFceeWVXHnllQC89NJL/P73v+f111/ngQceaNzXKUpdvZny6nrKauoor66j\nrFod9+aU8uJ3B6ioqee341O5Z1qfRlZv0aFBzBjUmRmDOiOlJD2/nKjQIJKi9H5DmvaFFkqnC7UV\nEBAKbggHT4mMjASgMG0bjB0BoWrt4eabb+aBBx5g3rx5/OlPf2pSLzc3l44dOwJqGi8xMbFR/vDh\nw1W7hYWN+rL+fqpgNkv+tXIfC9YdOrmfkC1n907k7xefQZ+OUU7bEkLQMymyJYap0bQ4Wiid6tRV\nQ3EmVJdAeALEdvd5F6NGjcJkMvHEC69xoqyKiE69SE1N5Z577mHVqlU88MADfPfdd0yZMoXo6GiO\nHDnCt99+S2hoKKtXrwZgwIABjBkzhrPOOovk5OSTZubBwcFce+21J/saM2YMr7/+On/7298YMGAA\nJpOJiy++mIiICJ+fV2tRVVvPHz/czpe7jnHJ0GSGdI0hMiSQiJDAk8e48CB6d4g85TVFjQYppd8+\nKOfd+4C9QBVwFHgWiHCz/hpAOviMtCk7yUnZL9wd84gRI6Qrdu/e7bKMV9RWS1mSI2V1mZRms/0y\n9fVSFmdLmbVNyuztUubtkzJrq5Q1FS0ypMUvPSsH9EmVQUGBEpC33HKLGmptrXz++eflyJEjZXh4\nuAwPD5cAZFSnAAAgAElEQVS9e/eW119/vVzx1ZdSVpVKWVUqn/rnXDlh/DiZlJQog4ODZdeuXeRV\nV10lt2zZ0qif3NxcecUVV8i4uDgphJCATE9Pl7K+1vG1cEKL/1YuOF5aJS+bv16mPPiFXLA2TZqb\ncQ4esXuZlCv/JmVdTcv2o9HYAGyWbjxj/eqnJIR4HvgD8CnwFTAAuBv4HpgmpXTqUCGEWAMMRAk2\nW76UUhZalZ0ErAYWGO1bkymlXOPOmJvtpyTNUFuFkoHW6cb3oDAwBbgzBCg6AhUF6m9TkJouC42B\nkEhAQFUxlGRBfQ2ExkFMMmCCvN0QHAEJvdzrxxPy9oApEGor1TjiezovL81wfB/UVTkoICAuFcLc\nMEOuKISiw0b5WI+G7U8/pYN5ZcxavJG8kmqeu2YY5w/u3LIdHvgG3r8GzHXQ7wL4zWII1GtOmtah\nzfspCSEGogTQUinllVbp6cALwLXAe240VS6lfMeDrn/0sLxvMNdD/j7H+aGxEJ/quh1phsoiQxDF\nQlURVBZCRT6IAPWQqa2AwFBI6A0hVusPkR2hNBuqSxune4u5TgmXqM5K6JXlqu+BoY7rlOerMtFd\nlEA+iTE9VZIFJ9JBpJ5co7JLRYES0qAEoodCyV3q6s38eKiAsKAAY9O6EEICG79ESCnJL6vhSGE5\nhwsqOFxQQUiQidSECFISI0hJiCAsWNX5Ma2AO9/eTHCgiQ/uGMOZ3eNaZNwnydwCH94MHQbA4Kth\n1d/g/evgmncgWFvmadoO/lxTug71BHrOJn0h8DRwI+4JJYQQJiASKJVuqH5CiAigXkrp6DXd95gC\nIK7nyWeu1R9KqFQWQX0dBLj4SapLQdar9aHQGAiPVwKvukwJqNoK9aCPSARhY9QQkQTlx6EkGxL7\ngq/WJ2oq1DEoXAmYsjwoOw6x3eyXr6+D0mNKMEYk2R9HQi8oSIPCdCWs7Qmm8gIoPgLBUUrA1Vf7\n5nxsKK6sZc57W/n+QGMH4LjwIDpGh5IQGUxBWQ1HCiuoqKk/mS9EgyJsoXNMKN3jw9l65AQ9EiJ4\nY+YousW3sFDIPwjv/UbdEzd8AlEdISwO/nc3vHc1XPeBoWVrNP7Hn0JpFGAGNlonSimrhBDbjXx3\n6AKUAWFAhRBiBfCQlHKvg/LPA28ACCEOAPOBF9wRZl4hTI6nogICofIEVJ1QD2lnVBYpjcha0zEF\nqLZdTXWZTBDdWWkWlSeUQPMFteXqGByhxhIer6bUojrbF7KlOUqwRndxLBhNgc4FU3k+FB9V1yGu\nJxSmQV2Nb87HiiMFFfz2zU1k5Jfz2CUDSUmMILekirySKnJLqjlWUsXx0mqSY8MY0zOBlIRweiRE\n0D0hnK5xYdTUmTlcUEF6fjnp+eVk5JeTXlDOeQM78eRlg4kJb+FgpqXH4J3LAQE3faoEEsDwm5RW\n/eld8M6VcMOHzjVSjaaV8KdQSgbypZT2Xm+zgHFCiGAppbMnTTrwA7ATqAfOAuYAU4UQZ0sprYPM\n1QL/A74Eso3+b0VpasOAWY46EULcAdwB0L27763XCAqHwDD1IHcmlMxmpQ2FxTXVgtwlLF5pMaU5\naqqrue1YU1Oupuosa2IRSWparSIfojo1LltbqdLDE22m7ezgSDCVH1cWhSHRah3JZFIP2Kpi78/F\nik0Zhdzx1mYk8PatZzG2V4LHbYQEBjCoSwyDuvjhgV9VDO9cpTTKmV80XUsccrW6bh//Ft66DG78\nxHcvKhpNM/Hn1hXhgKP5liqrMg6RUs6SUj4spVwipfxYSvkAcB5qKm+eTdkfpJSXSilflVIuk1K+\nCowBVgAzhRDjnfSzQEo5Uko5ssUiUofHq6m32krHZapL1JpSqBfrJkJAdLIygijPd13eFVKq6btg\nK5PsoDAlMMqPK0FqXbY4S2l6UW4u6lsEU1CoEkxFRxoEUnxqg99VQLBa2zLXO2/PoLC8hqwTlVTW\n1FNQ1vQ2/GRLJjcs/Jm48GA+/d34ZgkkjznyEyyaAXmOlHwPqK2CD26A43vgmrehy3D75c64VK0r\n5f4Cb13i/P7TaFoBfwqlCsCR6U+oVRmPkFJ+D6wDJgshnL6KG9Z9TxlfL/S0L58SFgcIpS05orJQ\nPaS9NVIIjVZtlB5TD3JvqKtWU3HBNn5CkR1U25VW51NdAjWlSntytXZmjSlQGW0EhSoNLCRGCSRr\nLc9iRVbvegqvqKKGzBMVFJbXUFBew4jHv2HGc+v4x7LdfLM7l3+t2Mv9H+1gRI84lv5uHKmJreQD\n9ctSOPIjvDEDMp1beDqlIE1Z2WV8D5e9DL2nOi/f73y4apGKXv/LUvf7Kc5Sv78/2Psl5DqPKK9p\nn/hTKGUDiUIIe4KpC2pqr7mLBBlAAOCOSVOGcUx0VshTPF6iCghSb/+VJ5qujoPSAKpKjCk3Hxgo\nRCcrYVKa6107NcZ6UpDNgzs4ssHoQUql4RVnQUCIWnD3FItgiu0O8SlNpx0DjNvIxUOyrKqWoycq\niQgJZEDnKDpEhfDA9H4kRobw7s+Hue2tzcxfnca1o7rx1q2jiQ1vxWCwx3YqA5TQGHjzEkj7zrP6\nlUWw4mGYf5YSahc/r6bo3KH/RZDUHza95l758gKYPxrevcpt7dRnSAmf3gk/PN+6/WpaBX8KpU1G\n/6OtE4UQoag1Hi9eFekD1AHuxKPpYxy9fDo3EBAQcHK3VI8IjwdzrbKws6WqGJDK78gXBIWr9aXy\n49697daWN5iiWyMERHRQFnHVJWqqsL4aYro0fx3LFKisDu3VDzSEhxMLvMqaOmWmHWiiR0I40lxP\neGgwv5/cm3duO4sdfz+PD+4Yw+JZo3jqisGNtoBoccxmpan0nAS/XaE0wXevhl8/c123vg42vQ4v\nDocf58PQa+DuLTBipvv9CwGjboPsrZC1xXX5TQuhpgzS18H6/7jfjy8ozzfuqbzW7VfTKvhTKC1B\neZLea5N+O2ot6V1LghCisxCivxAi3CotRgjRxNtUCHEhMB5YZW3yLYRosihgaGlzja/Lmn8qjYmK\niqKkpMTziqHR6gFvcYy1pvKEWjexnSbzBsu6TpkX8rimXPm52NPewuLUmEtzGkzAQ6Kb1Y2Ukqra\nek5U1FBVW99UEzUFqmvnwAKvpq6e9IIKTCZBSkIEgSYTpaWlhIY2+FKFBgUwpmcCk/p1aP1wPoWH\n1EO+0xA1vTlzOXQdCR/NhM2L7Ncpz4c9X8CrE2D5H5Wmc8cauHR+UwMTdxhyjdJ4Nznoz0JNBWxc\nAH1nwMArYPWTcORnz/trLoWH1LH8eOv1qWk1/GZ9J6XcJYSYD8wRQixFWcUNQEV4WEtjH6WngFuA\nyajQQhh/zxNCLAMOoTSj0Sj/pnyaCruvhRDZwBYarO9uRGlKL0opN+Ij4uPjOXJEOXRGR0cTFBTk\n3kNOmCA8Tk2NmOvUgxbUm3B1KUQ68OlpLoHBSnBUFkFMN8/btjjNOnJYFUJZ4pVkqe/OTMDtIKWk\nvKaekspaSqpqqalrMJoICjARacSGiwwJJCjQpM7HjqZUV28mPb8CKSU9kyIJChBUVFSQn5/fMtaU\nzeHYDnXsPEQdw2LhxqVKKH1xH5TkKIOP3F8g91f1sbxMxPaAq9+CAZd4d3+ERista/t7cN4/HVvi\n7XhPvTiN+wN0GqQ0q09ug7u+bzHn5UYUpqmjJ4Y6tVXKQCaxd8uMSeMz/B2Q9V7Ums4dKEODfOBF\n4FFXIYaAfagpvouAjkAQkAm8AjwppcyyKf8xcBkqikQsUA5sA/4upXzfFydjISQkhO7du1NYWEhG\nRgb19R7MudfVqIdNXm2DQ2N1mTIYiBIQ0AwNzBm1FeqfO7/eeQQGu3Wr1BRKBBBUZL+MNENJgdKm\nijPcaramzkxZdR1VtfWYpXrOhgSaCA0KIDjARG29mdI6Mzm19dRbojQFCOIoIVDWUZ1VRZBJEGAy\nASrKQk29mcTIENKL1eRASEgIHTt2bKQp+ZWcHSpkVJJVyKPgcLj2Xfj897DuGZUWEAId+kOvqdBx\noPr0GOe7cEGjblOa2fZ3YdzdTfPN9bDhv9BlpOpXCGUksWg6LLtHhS5qaS3TWlMym92LfL/lDfjm\nMXjwsA6t1Mbxq1CSUtajArA+66LcTGCmTdoewM1VXJBS/h/wfx4PspmEhITQuXNnOnf2MJ6ZlPDS\nnWqq67ZVKm3xRWr6a84m3//DV5fBM+fDyN96vivu2mfU1M2Dh507XtakKD8sFw8PKSXv/HyEfy7b\nTWiQiWkDOnLuGR2Z0DeJyJCmt6rZLNl7rJQNafn8dKCAc44u5praz+lf/SZmTIQGmYgNCya3tIqX\nbxjOsEEtHFvOG3J2qhBAgTaGFQFBcNkran0oPAHie3lmuegpHQdC93FqjWrM75v+ZnuWqfBP5z7W\ncC92HQlTHoFv5sLWNz1by2oOFqFkrlN+e+74VhWmQ12lut+1UGrT+FtT0tgiBAy7DlY9CvkH1BpS\nxno45y8t8wYaEqkW1/cthxlPedbH0Y1qHcNVJAA31sHKq+t46NNdfL49m4l9k3jummHERzi3fDOZ\nBGckR3NGcjS3TegJWybBsqX87+YUdpfHsi+3lPT8ch4c2p8Z/hBIH9wAHQfB5L86Lyelsrzrd779\nfJNJaSWtxahb4ZNb4dB30HtaQ7qUsOEFFWy3/0WN64y7Bw6tha8ehG5jlDbXUhSkNfxdnu+eULJM\nddaUQUQr+Jxpmo0/DR00jhhyjVpf2vG+YX0lYdCVLqs1m/4XKKfU3F/dr2M2Q+Ym9ZbsJQdyS7l0\n/g8s25HN/ef2ZfHMUS4Fkl3iUgAYFFrI1aO68beLzmDRzFFcdmYXr8foMTUVsO8r2LnEddmSLLVG\n03lYy4/LHQZcotYCN73eOP3wBrV+NPb3TSPam0xw+avqJefjWS3nhCul0noS+6rv7ho7WMrVeuz6\nqGllPBJKQojXhBBntdRgNAZRndSawY4PYNeH0GkwJPVtuf76ng8I2Pel+3UKDqqpk26jXZd1wqfb\nMrnkvz9QVFHDO7eexd1T+2AyNVMjtERZP5Hh1Zh8Qu4vyg/sRDoUHXVeNmenOnYa0vLjcofAYBh+\nC+z/uiECOyi/oPBEGHaD/XpRHZVgytsN3/6jZcZWUQjVxQ33nbtm4Sc1pfKWGZfGZ3iqKc0ENggh\nfhFC3GvPzFrjI4Zdr96gs7fBoKtatq+ojtB1FOxd7rJoeXUdacfLKD/0o0ro6r5QKqqoYXNGIR9s\nPMLjX+zm+oU/cd+SHQzuGsPyP0xgXG8v/ZejuyhjgRPp3rXjC7K2NvydYbt9lw3HdgJCree0FSzr\nQlsWq2PeXjiwAkbf4TxmYe+pamuM7e8qq1FfY7G86zZGHd21wCszhJcWSm0eT9eUuqIE0yxUbLmn\nhBCfA69LKVf5eGynN/0uUGs1VcUw6IqW76//BWqhujgTYrpSW29md3YJ+3NLOZBXpo65ZWQVqWmZ\nJwM/46KAcC54PZPE6AKSokJIjAxWMWPr6qmuNZ88VtbWk3migvyyBh+ikEATPZMiuXdaH+ZM7k2g\nLxxVTQEq4kNb0JSyt6n9q8x1kP69eslwRM4OSOzTtraPiO2mNOitb6n1zA0vKmOVUbe5rtv/QqXh\nZ270/VqYxcih6yhAuDd9V1upnG1BC6V2gEdCSUp5DLXX0dNCiImoKNtXAr8RQhwBFgGLpZQu5is0\nLgkKVX4gBQfVg7al6XchfDOX+j3LWRp4Ac99c+CkAAoONNErKZKRKXFc37E7ybGhnPPtUfIDBjO6\nSyLHS6s5UlDBtiMnCDAJQgIDCA1SJtwhgSaiQgOZ0r8DvTtEqk9SFF3iwgho7jSdM+JS1JqDv8ne\nBsnD1VRY+jq1FuLIiCRnJ3Qf07rjc4dRtyoDmJ9fVWtjI2a6ZyTQa7Lysdu/wvdCqSBNrbfG91TW\niO4IpTKrKb7TbU3p4LfKHP7yBe1mM8dmW99JKdcB64QQc1Ab9t2Kio7wqBBiFWrb8c9bfJ+iU5mJ\nf2q1rswJfaiITGXvqvd4oLwrg7vE8OD5/RnUJYbu8eGNBUhVCXx+kPhz/sK8yW1kcd5CfKp7YXI8\npSxPGZ2MmNnUbNuW6lLI36+MU8LjYffnakrR3hbx5QVQktngNNuW6DlZjXnVo0qgjv29e/VCY6D7\nWDiwUpmO+5LCQ8rROzBYGWOUubGmZC24asp8O562zKbX4Ms/q7XNc9LU2nQ7wBfWd6FAtPERKKfU\ns4BPgB1CiAFO6mr8jJSS1XvzuOjF9bxTNIih9b/w2jV9+d+c8Vw8NJnUxIimGk32VkBCN3f3YWxF\n4lKMLeJP+K7NrK2wYBJ89YDSHFyRswOQaruI1HNUWvo6+2VPRnIY6ouR+haTCUbeCki1xYXFkMQd\n+k5XBg+ujDw8pfBQg3CPSHRvTck6jFbNaaApmevh67/C8vsbfjNf/j+0MM3SlISKmXMBSju6EBVN\nYSNwG/ABKuTPjShn1YXA2b4YrMYzpJT8ml3CJ1sz+XJXDiWVdQSYBCahfHwChECi9hbqHh9O/0nX\nELR+GdMCd4Do47jho5vUsYv35uA+J87KAi/MB8Frd36otg2PSFJx4dK/h4GXO6+TvU0dOw9TD87I\njqqePafStmZ5Z8uZN8LRn2CSC18rW/pMh5WPKOMId9ah3EFKZehgMfyJ7ADZ213XaySU2vma0ndP\nQNZmGHo9DLioqdFJdZkK+bT/KzhrNgy9Fhaco0KJtRM8EkpCiF7Ab1Fx6DoDJahpugU2u7wCLDIC\nqP7bFwPVuM+x4io+257F0q2Z7M8tIzjAxJT+HegWH4ZZQr1ZIqWkXkrqzTCkawxXDu9KsEnCtiRl\nhTfYicVf5kZI7Nc6cc48xfBVojAdks9sfjvmemX4seEF6DFexZb7bDakr3VdN3ubmmKKNDaETJmg\nLPDsrSsd26nKttUdX8Ni1SaAnpLYR/0W+1f6TihVnlCGP5YddCOS3NSUjOk7EaCi2rdnfvlY3dtp\n36lp0sG/US8OnYdBSbbaRyv3V7jg3zD6dmW4BKe0pnTAOP4IPAwssY7EbYcMIKcZ49I0g9ySKv78\n8U7WHTiOlDCiRxxPXD6IiwYnExMe5F4j/c5Xayd1NfbXTqRUTrP9/bsnokMsQskbC7zKE+pt8+A3\n6oE642kV7if1HLVOUpyltuBwRNbWxgIxdaJ6mOTvh6R+jcvm7GibU3feIoTSlra+pazfnJmRu4vF\n8s56+q66WMVgDHISw7AsV23TIs3te/rObOxJNm4O9D4Xtr2jPpteU5FDKgqUpnT9R9DHiMRh2aW6\nHQklT9eUXgAGSynHSynfdCGQkFJ+IaX0YCJa01xKqmq5ZdFGNmcUcvfk3qz+0yQ+mT2OG87q4b5A\nAmWFV13i2LemIE3d4B74J7UqIZHqDbq5vkqF6bBwqgqZc9FzcOGzSiCBEi7g3O+o8oTqu5FQmqCO\ntutK1WXqerbVqTtv6XOeijeX7sJPy10s4YVOCiVDE61woS2V5aop1OCI9j19V2HsSRbbA3qeA1cu\nhPv3wYXz1D0angC3rmgQSKDO2RSk1lnbCR4JJSnlvVJKD2LRaFqD6rp67nxrCwfzynjlphH88bx+\nzd/Cu+c5agNAe9EdKgrh27nqby8jObQocanN15S++rOaErplGYyc1Tiv4yD1xn3IyRSeZY3DWijF\npUJ016bCLPcXQLZNyztfkHK2upcOrPRNe4WHANGgDUd0UEdXZuHlx9VUanBE25q+K0yHJ7s2rEG6\notgwGonp2pAWFqtM9+9YA7N/aOqALYQqc6pqSkKIq4UQbznJf1MI0cLhBzTWmM2SBz7ayY+HCnjm\nqiFM6JPkXYNBYdBrCuz9svG27Pu+gpfGqOOUv6mI1m2VuBQozPC83tFN6gF69r3QY2zTfJNJaT0W\nvyN7ZBuRHJKtTOWFUFpW+vdqCsZCThu2vPMFQaHGlOcKx9fLEwrTDHNwI8q3RVNyta5k0ZSCwtuW\nppS+DmpK3XdhsKwPWQsld7DsmdZO8HT67m7A2T5H9UYZTSvx9Nd7+d+ObP4yoz9XDPfwZnVE/4ug\nNFu9wVUWwaez4f1r1UPg9tWt6j/VLOJTle+Pg11oHbLmKTUFMvoOx2VSJ6q2LesbtmRvU9NLtpZ/\nqRPUnlh5uxvScnaqWHJRbXhLDW/pe56Kn3d8n/dtFR6CBCtfrwgjLJUrTaksz5i+i2xba0oWYWQd\nX9AZxVabZXpCWNypqymhdoZ1pmtuA85o/nA0nrBofToL1h3i5rE9uOscO46ZzaXvdGWptOZppR3t\nXAIT/6wEUnuYaopLUYvaxR74yBz5GdK+hfH3OA/3c9LvyMEUXvZ2+1Z/Kca6kvUU3rEd6nq29tbr\nrUmf89TxwArv27L2UYIGTcmZA211mYriEJGkIhq0JedZS3xEt4VSpnJL8NTVIfQUnr5D7THqbBtV\nCUQ1fzgad1m+M4d/Lt/N9IEd+fvFA93bbt1dwuMNj/wV6oa+/VuY8rDrSAZtBYuvkifhhtY8qR5c\nrsyXE3pDVLJ9Z9iy40oQ2hNKsd3UuCz16qpVkNNTderOQkxXtRa338t1pYpC9WC1FkohkWpKzpmm\nZPFRshg6tJUwQzXlDVqz20LpqLqenv6vh8W1K0MHT03C01GOsP91kH824OYV1nhCSVUtB3JL2Xes\njH3HSnh/41FGdI/j+WvPbJkYcuf+A47+rBZR29tOnSfNwt0USod/hENr4LzHXW9IaFkfOvhN0624\nLQvWycPt102dAL9+rnyg8vaAufbUtbyzps95atuLyqLm+7ZZXjDiezVOdxXVwaJFRXZQWkZbWVPK\n2anC/0R28kxT8nQ9CdrdmpKnQulT4EEhxCopZaMdwIQQvwV+A/zLV4M73VmzL4/FGzLYf6yU7OIG\n6/uI4ADG9krg+WuHERoU4KQFL+g6Qn3aI1GdIDDUfQu8NU8qS66Rt7pXPnUi7PxAvel2GtSQnr0N\nEI6nOFPPUX47OTsMyztOfU0JlFBaP085fDY34r2tj5KFiCTnmlK5lVBqSybhlvWkMy6Fja+qcbl6\nISrObF78urBY5eZRXwcBbX+zcU9H+DRwKbBACHEfYInxMRS1lrQPeNJ3wzt9OVpYwe/f3UpMWBCj\nU+Pp2ymKfh2j6Nsxii6xYc3fCO90QBhmw+4IpYz1akpt+lPuR1G2+Culr2sqlBL7QoiDGewUI9pW\nxvcqJlxwVMNU46lM11FqGvjASi+EUhqNzMEtRCSpfccccVJT6misKbUhoRTTTV2bja+q+8HZFvK1\nVUrAxnTzvC/LGlRVcbvYCt7TrStKhRDjgaeAa2gwajgBvAw8IqUs8e0QTz/MZsmfP1Yx0T68ayxd\n49pHyPk2RVyq6zUlKWH1k2oKxdYnyRmx3dQbe/o6GPu7hraytypzekdEdVJCK/179YDoNLjx9N+p\nSkAg9J4GB1Y1nfJ0l8JDaurKNnJDRFKDab09yvLUVhfhCUoTMdc6jlbSmmRtUQF7LdvSFB1xLpQs\ngre503eg1uTagVDy+O6QUhZLKX8HJAIdjU+ilHKOlLL9TFy2Yd75+TA/HirgkYvO0AKpuVg0JWf+\nMenr4PAPMOGPnofBSZ2o6lp2Vy3NUYvqruLtpU6EwxvU9F17sGT0FX2nq4gE2Vtdl7VH4SH7Ucot\n03eOfueyXGV2bwpQa0rgfwfa8nwoOqwCGsf1UGlFh53XOemj5KE5OLS7UEPNfk2TiuPGR++Z5CMy\n8st56su9TOybxLWjmqGqaxTxqerh42i9QUrllxSVDMNv8bz91HPUPH2OMYN90sjBhVBKmaDGVVtx\nehg5WOg9TWkszY3uUJBmfz+qiCS1u68j6zKLjxI0rNn421fJYgreZYRaywwIcW3s4AtNyVsLvB1L\nYOvb3rXhBs3duiIA6A/EYUewGRsAajzEbJY88PEOAgME/3flYN+aeZ9uWG9hEdmhaf6hNXDkRxVN\n2VkwT0dY/I7S10LXkUooiQBl/uxOPTg9jBwshMer9ZNfP1WGD8lnKu3FHSpPKMdjW8s7sPJVOm7f\nf6c8ryFa+0mh5GdNKWuLEtCdh6qpzNhuroWSRVPy1HEWGiwevdWUtr2tLEeH3+RdOy7wWFMSQvwF\nyAd2AmuB1XY+mmaw6Id0NmWc4O8XD6RzjA+iKp/OWG9hYUtdDaz8m4pHN/zm5rUfmQQdBjbEwcva\nCh3OcG0sEZGgBFdASNOI4ac6I2+F/APw2lR4pid8eAtsedP1RoCOLO/AdVQHe5qSv6fvsrZA0oAG\nJ+3Y7m5M3x1V59Ec94yTa0peakplufZf8HyMp/sp3YoyclgLrASeAP4D1KI2/DsEvOTjMZ4WpB0v\n418r9jG1fweuHN6MtyFNY2K7A8K+Bd66ZyB3F1z3gXc+WD3Pgc2LlGVU9jb3t/M46y44vrch+vjp\nwtBr1DRe+ho4+J0yEd/9mcrrNARu+BiiOjatZ3mxSLCjKUU6CcoqZeMHaZDxwuBPTUlKJZSs75XY\n7s6NNaD5PkrguzWlslzoOdm7NtzA0+m72cBPUsrJQogElFBaLqX8TgjxPMpEvIUcZ05d6s2S+z/c\nQWhQAE9doaftfEJQKEQnN3WgzdwC38+DYTeovaO8IXUi/PQS/PKJml5yd1PBFp7+aNNEJMCgK9VH\nSiWcD36jNlRc9y+40M6eoBZNydYcHKyCstoRSlXFUF/TEE082NBM/LmmdCJD3StdrHwAY3s07IXk\nKMRVcWbzgyAHBCr3A2+EUm2Vup6toCk1J/bdR8bfFuOGAAApZQ5qF9p73G1MCGESQtwnhNgrhKgS\nQhwVQjwrhHBr3wUhxBohhHTwabJXtxAiRgjxohAiy+jvVyHEbOFnKbDw+0NsP1rEPy4dSIfoZqxv\naOxju4VFbSV8dpcKgDrjKe/b7zFOrQ388Lz63sVBJAeNfYRQD9pxd8OZN8GWxfbXVgrS1FqKPQvJ\nsGJm9msAACAASURBVHhA2BdK1j5K0DC16ov4d8217bI4zTYSSoZZuKNYjVIampIXhk/ehhqyDtfU\nwngqlOoBi+5rOVobvmcAfTxo7z/APGA3Krr4R8AfgGVCCHfHlg/cZOfTKIyzECIYWAXcBSwx+tuH\nmm78uwdj9ilVtfXM/+4g0wZ05JKhyf4axqlJXErjNaXvHle7v176X7WVtLeExqiQQvn7ICBYrSlp\nmsfEB5SAX/t/TfNsA7FaExCojCjsCiXLg9Rm+s7b+HfvXg1f3Ne8ullbITCssdYTazELd2DsUHlC\njbk5Rg4WvN1TyVbAtyCeTt8dAVIBpJTVQoijwATgAyN/FFDoTkNCiIEowbBUSnmlVXo6aofba4H3\n3GiqXEr5jhvlbjPG9wcp5YtG2kIhxCfAQ0KIN6SULlYbfc+3e/Iora5j1vgUPW3na+JToOyYmq7J\n3gY/zlcBV3v5cF48dSJkbVabq7W3GIFtiZguKs7iz6/C+PsgsXdDXuEh5+t1ER3sCyXrEENgNX3n\n5ZrS0Z+gulRFlLfnO+WMrC3K6s56PdHagdYe9jb38xSvhZKNgG9BPNWU1gHWd8dHwJ1CiEVCiMWo\nB7+dLUvtch0ggOds0hcCFcCN7g7KmAaMdjENd73R7kKb9OeAIFSEilbns+1ZdIgKYUzPtu9p3e6w\nmIXn7YbPZivNadpjvu3DEnLI3fUkjWPOvk8J9jVWU6tVxcrp1pGmBI6DsjqcvvNCKFWVqDFJM/zo\nKC61A+prlV9bF5uYkpEdnMdqbO7mftZ4G5S1DU/fPQ/MF0JYJnf/jhJCt6CmzFYBD7rZ1ijUhoEb\nrROllFUog4lRbrbTBSgDioEyIcRSIUSjeB3GVOBwYJvRvjUbUetj7vbnM4oralmzL4+Lhya3TKTv\n0x2LUPpstnoLvexl53slNYfuY6DbGBhwiW/bPR2J7KAsE3/5BHJ/VWkWIwd7lncWIpLs76lUlgem\noAbrs8BQNUXojVCyOLFGdoRt7yj/KHfJ2w11VU3XHoVQ60UONSWL46yXa0peT9+JBsOSFsQjoSSl\n3CelfFVKWWl8L5dSXgLEAzFSyvOllG5N3wHJQL6UstpOXhaQaKwDOSMdeAaYhYpQ/hJwPvCzEMI6\nnG4cEGa0a3tO1ah1KYcTtkKIO4QQm4UQm48f9+AmdMGXv+RQWy+5bJg2AW8RLNZa+fth3Bz7W5x7\nS1AY3LrCt1OCpzPj7lYBbVcbcZ0L0tTRqaaU5FhTikhqiLUnhAo15M2akkVrmfqo2hNr46vu17Vn\n5GAhrofz6buAkAafrOZg2eivuQYaZbmq/1aIMu62UBJCRBrTdL+xzTPi4Xlq0hIO2BNIAFVWZRwi\npZwlpXxYSrlESvmxlPIB4DwgEmVAYd0XLvpz2JeUcoGUcqSUcmRSku/eFD7blkXPpAgGdYn2WZsa\nK8Lj1T9jYj+Y/Ii/R6Nxh/B4GDsH9n6hjAIshirOoqlHJkF1sRIS1thz9gyO8M76zrK+03MyDLgI\nNi5Q60vukLVFWQvaM22P7e5EKGU2b3M/a8LiVDDa5gpkayfkFsZtoWQInWsBXz1BKwBHK8OhVmU8\nQkr5PWrta7LVNKOlHWf9tarzQnZRJRszCrl0aBdt4NBSCAHXvQ83ftK8UEIa/zBmtnp4r35CTd9F\nJTuPlHHSV8lGWyq38yANDvfOT6k4U4WTiuqkDDKqilVUCnfI2qq0JHv/77Hdlf+SPQHnjeOsBW9D\nDbVSNAfwfE1pN5Dio76zUVN09gRFF9TUXk0z285A+U9ZgmGdACqxM0Vn9J+Inam9lmTZjmykhEuH\naTPwFqXHOBVbTNN+CI2Gs+9VTrUHVjifugMroWSzrlRmFffOgrcb/RVnKtNsU4DaBDNlgrLqrHPx\nqKouVbsN25u6AysLPDu+Sj4RSl6GGmqLmpLBM8BsIURfH/S9yeh/tHWiECIUGAZs9qLtPkAdhnm6\nlNIMbAXOtCMER6OsAL3pz2M+257NsG6xpCS65Ses0ZxejLpdPQQrCiDBXaFkpSmZzfYfpEER3sW+\nsxUQZ98Lpdmw60Pn9XJ2ANKJUHKwhUV9rdoWxWdCqRmakm24phbGU6HUHzgK7BJCfCKEeFoI8ajN\n529utrUEZfV2r0367aj1nXctCUKIzkKI/kKIcKu0GCNaeSOEEBcC44FVNpZ27xvt3mFT5V6UAFvi\n5ri9Zn9uKXtySrSWpNE4IjgcJvxJ/e22pmRlhFR5AmR9Q4ihk+16qykdbSwgek1VmzX+8LwShI44\naeTgIOqHIwfakmxAei+UvIl/V1UE9dWtpil5akox1+rvyx2UkcA/XTUkpdwlhJgPzBFCLEWZlg9A\nRXRYS2PH2adQZueTgTVG2mRgnhBiGSp6Qx1K67kRZU1nK+wWoqz05gkhUoA9wAXGeTwupcxwNWZf\n8fn2LAJMgouGaKGk0ThkxC1KczjjMufl7AklR86eweHGg74ZmOuhxEZrEQLG3wuf3Ar7vlTGD/bI\n2qIEjyMLuohEFenBVij5wkcJvNtTqRWjOYDnQslD92WX3Ita/7kD5ZSbD7wIPGpMuTljH2rK7SLU\n7rdBQCbwCvCklLLRGpGUskYIMQ14HOW4mwCkoaJKzPfR+bhESsnn27MZ3zuRpCgdAUCjcUhgCEx/\nwnW54Aj1QLcrlGwNHSKbrymV5SkLNlsBccZl8O0/YP1/VOQJe4YMWVvVflKOEML+FhYlPvBRAu+m\n71oxmgN4KJR8HYZHSlkPPGt8nJWbCcy0SdsDXO1hf0XAHOPjF7Yc/v/27j1azrq+9/j7k2RPLjsh\nCSRKAAPYiCClDRUQL1WxaKu0WitF0NiCIuoxcIDWrnOQYlHRVguiEU4h3qoHXaACinpA1OLiaBUQ\nOCgKpGIkO5GQcNnJzv3yPX/8nid7Mpnr3nPbM5/XWrMm8zzPzPP77ezMN7/b9/c0Q09v4cJXN2NY\nzsxQtqizeCFrHqBKv0gHZox9TGlPq6UkQEyeAi89D779d/DbH8FhL9v7/Ma1qdvvRe+u/vnlpoXn\nU9DHk/cOUuCeNGWMQam7W0o2Tjffv5ppA5N4zdEHdrooZr1jcF6d3XfjGFOqloNu8Vvhjn9O2UPm\nPT/NztPktHA3n/F2yD4bF+xtzsKUR3Gvew7BjANqbx5ZizT2VENtTDEEjW/y97k6LouIeMcYy9PT\nduzazbcf+B0nH/VsZk71/wfMmmbms/YeKxpZm9IKTS1ZVlkYTKl+du+qfzv2XLXxnYHp8KcfSYtp\nNz8Ju3em/Hi7d6UJF4e+DBYsrv75cw9NLZmtG9K0+Pye4x1Pyo011dDI2pRRohmZ9evQ6DfjmXVc\nE6RdaK3EnSvW8fTmHU4rZNZsg/Pgdw+Mvh5Zl2belY7v5Fuib980+sVfr+EhmDq78vv+4LT0GKvi\nbOEH/v7oPatls2jEtDFmCs+n1rdpkX+jue8mlT5IEwyeT5rd9hNGF6xaiZvvW8OcGQO8/IjWJzU0\n6yuD81P3XZ7brdK6mvHsqdTMVks55bawaHZLaUyz79q3RgkaX6e0j4jYFRErIuJdwJNAmV26bNO2\nndz+y7W87pgFFKaM+8duZsUG56eZcfmX7qZ15cdAxrOnUukapWYrXau0dRi2bWhiUBpnS6lNmv3t\neCvwpppX9aHN23fxl8cezKkvbOEvtVm/yhfJ5lkdRtbum2IIxrenUqtbSjMOSC25PCg1a41Sbvpc\n2DLc+Ps2Pt7WllKzR9v3J2XothLzZ03lo391TO0Lzaxx+aLUTevSGMym9RVaStmYUqPdd9s3pYSp\nrQxKUmot5WuVKk1BH6vpc1M29V0769+CYteONHFjorWUJM2RdCpwAfCzZnymmVnd8qwOI0+kL1Gi\nwphSPtGhwe0r9my01+KejuIFtM1uKeWphrY20FratJ6KP8sWaXRK+G7S7Lqyp0kJUC8cb6HMzBpS\nnGooX1dTmvcOimbfNdhSqrZGqZnmLIRVP8nuOZR2zm1WK6U41dDgAfW9p81rlKDx7rsvsm9QClIw\negT4SkTUueOVmVmTzDgAUPqffbUMBGMdU2p2q6WSOQtTS2bLM9k2GQtGd84dr7GkGmpzNgdoPM3Q\nmS0qh5nZ2E2eknat3bRudF+lcl1O+ey7RlMNDQ+BJsGsBeMrZy35tPDhVdnEiibuBTaWjf7yltKs\nCTamZGbWcYPzU0CqlkB0YBwtpVkLYPLA+MpYS/FapWbP9hvLRn/VukJbpKGgJOm9kr5X5fx3Jb1r\n/MUyM2vQ4PzR7rvCzNHxo2J7gtIYxpRa3XUHMPew9PzUb1KG8JYEpQa776bNhoFpzStHDY22lM4E\nVlQ5/wjw9jGXxsxsrPKsDiNPjE58KDVpUgpMDc++a/Eapdz0uSmgrvppypnXzHvmuesa7b5r43gS\nNB6Ungf8vMr5B7NrzMzaa09QqvFFWhhsbJ3S7t3Nb7VUku+r9Nsfp9fNHFOaPACFWY2lGpoAQWkA\nqNaOm1bjvJlZawzOTzPXhoeqr6sZmNHYmNKmdbBre3MDRDVzFsLmLDNFswNho6mG2pz3DhoPSo8A\nr65y/jWk3VzNzNorz+rw9MrqX6SN7j7brunguXyyA4x/c79S0+c0ONGhvXnvoPGg9BXgNZI+JKmQ\nH5Q0IOlSUlD6cjMLaGZWlz3jSFGj+67BllK7Fs7m8sSs1bbJGKtG9lTaNpLG3trcUmp08ewngNcC\n7wfeI+mh7PiRpLx3d1Jja3Mzs5Yo/vKs2lJqcEypUy2lVtxv2hxY93B9125q/8JZaHw/pR2k1tD/\nAIaAY7PHKuAfgJMjYnuzC2lmVlPefQfV19UMNLgl+vBQ6vLLc8e1WiuDUiN7KnUgmwOMIUt4Fpg+\nlj3MzLpD8TTwWrPvGu2+m31I23ZebXlQ2vJ02gyxVn06kPcOnNHBzHpFYSZMySb/Vu2+a3RMqU1r\nlHLT58Ixp8GRr2vBZ89JMwnr6b7sUEup0YwOl0r6RZXzD0i6ePzFMjNrkDTabVdp8Syk4NXomFI7\ng5IEb1oOi05u/mc3kmpoZC1ocsop2EaNtpTeCNxe5fztwKljL46Z2TgMzqudFidfpxSVduEpsmNL\nWjPUzqDUSo2kGhpZm4L7pMmtLVOJRoPS4cBDVc4/nF1jZtZ+sw6sncm7MAhECji1bFiTntu1cLbV\npjWQKXxj+xfOwti2Q682BWUu0N6wamaWO/lS2FZjS7c9G/1tGt1fqZJ2r1FqteKN/mrpQIohaLyl\n9CDwhnInJAl4PdVbUmZmrTP/CDjkhdWvyTOF17OnUrvXKLVaI3sqdSCbAzQelD4LnCjpC5L2jCRm\nf/4ccGJ2TV0kTZJ0gaSHJG2VtErS5ZLK5Jyv6/OulxTlJmNIemV2rtzjW2O5n5lNQMUtpVqGhwDB\nrINaWqS2qXeiw+7dafFst3ffRcRySa8A/gZ4m6TfZacWAAKuj4j/1cBHfgI4D7iJlAniqOz1sZJO\njojd9X6QpD8nTbKo1VF8LSnzRLGhuktsZhPbnqBUxwy84VVpnGpKofa1E0FhJkyaUrultOVp2L0z\n1b3NxrJ4domkbwJvBRZlh+8GrouIr9X7OZKOBs4FboyINxUd/w3wKeB06syjJ2kmcDVwFakLsZr/\njIj/XW85zazH7AlKdeyp1O7p4K0mpckOtYJStd17W2xMi2cj4oaIeENEHJ093thIQMqcQWpdXVly\nfDmwGVjSwGddRppgUdcaKUmDkrzFhlk/2jOmVE9LqceCEtSXaqhD2RxgbLPvkHQc8CLSbLvSwBYR\n8aE6PuZ4YDdwV8mbt0q6PztfT1lOAJYCZ0TEBtVOBfJJ4PPZe1eQWlefiqhn0YKZTXiFmem51phS\nRApKz39t68vUTvVkCu9QNgdoMChJmg7cSErKKiCyZ4r+HEA9QekgYH1EbCtzbjXwEkmFagleJU0B\nPgN8NyJuqHG/HcA3ge8Aa7L7v4PUUlsMnFXlPucA5wAsXLiw0mVmNhHk08BrBaXNT8LOrb2zRik3\nfc5oS6iSCdR9dwkpIF0GnEQKQn9L2s7iTtLY0gvq/KwZQLmABLC16Jpq3kca13pvrZtFxI+yLsdr\nIuKWiLiGNFvwNuBMSS+t8t5rI+K4iDhu/vwq6UvMrPvVO/uu19Yo5abPrT37bmRt6ubMW5Vt1GhQ\nOhX4akRcAuTTrldHxG3AyUABOLPOz9oMTK1wblrRNWVJWkQKkpdFxKN13nMv2ey+j2YvTxnLZ5jZ\nBDOQBaVaY0q9tkYpV29Qmvms9mVGL9JoUHoO8MPsz7uy5wJAROwk7Ux7ep2ftQaYJ6lcYDqY1LVX\nbW+my4GngJskLcofpC7JQva6Rr4RAFZmz/OqXWRmPWLyFJg8tfbsuz1Bqce676bNgW3DsHtX5Ws6\nlM0BGg9KGxkdh9pImqhQvKpsGKh3Yvvd2f1PKD6YzYpbDNxT4/2HZvd+EFhR9DgYeF725+V1lON5\n2XONTlYz6xmFGbXXKQ0PpS6sfMFpr9iTami48jUjnVk4C43Pvvs1cARAROyS9CCpS+9zWZqhvyLt\nQluP64GLgPPZezHrO0ljSdflB7IWz2zgsYjIf5P+nvJ5+K4mjUldCOSLe5F0QEQ8WXxh1kr7p+zl\nLXWW28wmusLM+saU2rm5X7sUpxqqtC3FyFo47I/bV6YijQal7wFvl3R+ROwCrgE+LenXpFl3h5MC\nTU0R8XNJVwFLJd1ImhWXZ3T4IXsvnP0oaULFScAd2fu/V+5zJf0rMFJm3dStktYAP2N09t0SUktp\nWUTchZn1h4EZtXPf9eIaJai9fcXObelch7rvGg1K/wx8iWwaeERcnXW3LSGNMS2nsW3SzyeN6ZxD\nmmiwHlgGXNJIiqE6fQ34S1IWiTnAJuA+4AMR8ZUm38vMulk9W6IPD8Gzj25PedqpVv67TevS80To\nvouIEdKeScXHrgCuGMvNs9bW5dmj2nVnUuesvog4rMLxfwH+paECmllvKgxWH1PauS11Ye3Xgy2l\nWnsqdTCbA4wxzZCZ2YRWGKzefbdnc7+D21Oedqq1p9KebA6daSk5KJlZ/8m3RK9kw+r0vF8vBqUa\nLaWNj6dnt5TMzNqkVvfdcBaUenGiw+SBNPuwYvdd1lIa7Ez2GgclM+s/tSY6bMgWzvZiSwmqZ3UY\nWQvT9+/YHlIOSmbWf/IxpUqbAwyvTl/chVrpNyeo6VX2VOpgNgdwUDKzfjQwI+2suqtCJrMNa3pz\n5l2u2kZ/I0/ALAclM7P2qbWn0oYh2O+g8ud6QbWN/kYed0vJzKytau2pNLy6N6eD5ypt9PfjZfDM\nY7BgcfvLlHFQMrP+U6iyfcX2zbDlqd6d5ACjY0rFY2oP3ADfvRiOfiO86N0dK5qDkpn1n3xPpXLb\nV+xZONvDY0rT56bxtB1b0uv/+j7c/J6UhPWN18CkzoWGRnPfmZlNfHt2ny3TUur16eCwd6qh9Q/D\nDX8D84+E06+DKZX2Xm0Pt5TMrP9UG1Pq5RRDuTzV0Jr74Lq/Tq/f+jWYNruz5cJBycz6UT77rlz+\nuzybw6wen30HcOM709T4JTfCfvVs1N16Dkpm1n8GqrWUhmDGPBiY1t4ytVOe/y4C3nIDzD+is+Up\n4jElM+s/1caUen06OMD+vwfPORH++EJ4zgmdLs1eHJTMrP8Uqs2+Ww1zD29vedpt6kx4x22dLkVZ\n7r4zs/4zuQCTppRfp9QPLaUu5qBkZv1HSmuVSseUtm2EbcO9PR28yzkomVl/Krd9RT8snO1yDkpm\n1p8KZXafHc4XzvbwdPAu56BkZv2pMLjvmFIvb4M+QTgomVl/KjemNLwakFtKHeSgZGb9qVz33Yah\ntJfQ5IHOlMkclMysT5Wb6ODp4B3noGRm/Wmg3JjSGo8ndZiDkpn1p8Lg3hkdItJEBweljnJQMrP+\nVJixd+67rcMpSLn7rqM6GpQkTZJ0gaSHJG2VtErS5ZIGx/h510sKSb+ocH62pGWSVmf3e1DSeyRp\nfDUxswmnMBN2bYNdO9NrTwfvCp1uKX0CuAL4JXAu8FXgPOAWSQ2VTdKfA6cCWyqcLwC3A+8Grs/u\n9zBwNfCBMZbfzCaqfPuKfE+lfB8lZ3PoqI5lCZd0NCkw3BgRbyo6/hvgU8DpwJfr/KyZpOByFfD6\nCpedDRwPnBcRy7JjyyV9HbhI0ucj4rdjqoyZTTx7MoVvSjuu9sM26BNAJ1tKZwACriw5vhzYDCxp\n4LMuAyYDF1e55i3Z5y4vOX4lMAC8uYH7mdlEV7qn0oY1oMkw68DOlck6up/S8cBu4K7igxGxVdL9\n2fmaJJ0ALAXOiIgN5YaHsq7APwLujYitJafvAqLe+5lZjyjdU2l4dQpIkyZ3rkzW0ZbSQcD6iNhW\n5txqYF42DlSRpCnAZ4DvRsQNVS6dC0zPPncv2f3XAxXb7JLOkXSPpHvWrVtXrUhmNlHsGVPKW0pD\n7rrrAp0MSjOAcgEJYGvRNdW8D1gEvLeOe1HjfhXvFRHXRsRxEXHc/Pnza9zKzCaEwsz0vL1oooOn\ng3dcJ4PSZmBqhXPTiq4pS9Ii4BLgsoh4tI57UeN+Fe9lZj2okP0/dPsmL5ztIp0MSmtIXXTlAsXB\npK697VXefznwFHCTpEX5gzROVsheL8iufZo0VXyf37js/vMo07VnZj2sePbd5qdg51ZPB+8CnQxK\nd2f3P6H4oKRpwGLgnhrvP5Q0LvUgsKLocTDwvOzPywEiYjdwL3BsmSB4AmkWYK37mVkvGciC0o7N\nXjjbRTo5++564CLgfODOouPvJI3vXJcfyFo8s4HHIiLvZvt7YE6Zz72aNEZ0IfC7ouNfAV4KnAMs\nKzp+PrAzK4+Z9YvilpKDUtfoWFCKiJ9LugpYKulG4DvAUaSMDj9k74WzHwX+FjgJuCN7//fKfa6k\nfwVGIuJrJaeWA2cBV0g6DPgV8DrgjcCHI2JlM+plZhPEwHRAKSjl26B7okPHdbKlBKmVspLUejmF\nNDV7GXBJ1uXWNBGxXdLJwIdJC3cPAH5NyipxVTPvZWYTgDS6JfqG1TBpAAaf1elS9b2OBqWI2EWa\nsHB5jevOBM6s8zMPq3LuGdJC26X1ltHMetjAjLR4dvtm2G8BTOp0OlDz34CZ9a/CYApIG1bDfp55\n1w0clMysf+Vbom/wwtlu4aBkZv2rMAjbN2bboB/U6dIYDkpm1s8GZsAzj8Gu7e6+6xIOSmbWvwqD\n8HS2jZq777qCg5KZ9a/CIGnnGrxwtks4KJlZ/8qzOoDz3nUJByUz61/5nkqTp8KMAzpbFgMclMys\nn+V7Ku13UMrwYB3noGRm/SvfU8ldd13DQcnM+lc+puRJDl3DQcnM+le+p5Kng3cNByUz619uKXUd\nByUz618eU+o6Dkpm1r8WvhhevBQOe1mnS2KZTm/yZ2bWOYVB+NPLOl0KK+KWkpmZdQ0HJTMz6xoO\nSmZm1jUclMzMrGs4KJmZWddwUDIzs67hoGRmZl3DQcnMzLqGIqLTZZhQJK0DfjuOj5gHrG9ScSYS\n17u/uN79pZ56HxoR82t9kINSm0m6JyKO63Q52s317i+ud39pZr3dfWdmZl3DQcnMzLqGg1L7Xdvp\nAnSI691fXO/+0rR6e0zJzMy6hltKZmbWNRyUzMysazgomZlZ13BQajFJkyRdIOkhSVslrZJ0uaTB\nTpetGST9T0lflfSopJC0ssb1z5d0s6SnJW2SdKekV7WpuE0j6QhJH5T0E0nrJG2UdL+k95f7u+2h\nej9f0nWSfiVpWNLm7Hf7CkkLKlw/4etdjqQZRb/3ny5zvifqntWv3GOkzLXjrrO3Q2+9TwDnATcB\nlwNHZa+PlXRyROzuZOGa4CPAU8C9wJxqF0r6PeDHwE7gY8Aw8E7gNkmvjYjvtbiszfR24L3AN4Hr\ngB3AScCHgdMknRgRW6Dn6n0IsID0+zxEqtMxwDnA6ZIWR8QT0HP1LueDQNkMBT1Y9zvZd4bdjuIX\nTatzRPjRogdwNLAb+HrJ8XOBAN7S6TI2oY7PLfrzL4CVVa69AdgFLC46NpOUtulhstmgE+EBHAfM\nLnP8w9nf7dJerHeVn8dfZ/X+h36oN/BH2ZfvhVm9P11yvmfqntXvC3Vc15Q6u/uutc4ABFxZcnw5\nsBlY0vYSNVlEPFrPdVmX1uuBOyLi/qL3jwCfAY4Ajm9JIVsgIu6JiOEyp67Pnn8feq/eVeT5IOdC\nb9db0mTSv+FbgRvLnO/JuksqSJpZ4VzT6uyg1FrHk1pKdxUfjIitwP1MwF/McfgDYCrwn2XO/SR7\n7oWfxyHZ89rsuSfrLWmapHmSDpH0GuCa7NR3sueerHfmAuBIYGmF871Y91NJ/5HeKOkJScskzS46\n37Q6e0yptQ4C1kfEtjLnVgMvkVSIiO1tLlcnHJQ9ry5zLj92cJvK0hLZ/6D/kdSt8+XscK/W+2xg\nWdHrlcCSiLgze92T9ZZ0OHAp8MGIWCnpsDKX9Vrd7wK+CvwXsB/wOlJAfoWkl2StoabV2UGptWYA\n5QISwNaia/ohKM3Insv9PLaWXDNRXQm8GLgoIh7OjvVqvW8GHiKNGRxL6rqZV3S+V+v9b8CjwBVV\nrumpukfEi0oOfVHSA8BlwH/PnptWZwel1toMPKvCuWlF1/SDvJ5Ty5yb8D8LSR8i/e/x2oj4aNGp\nnqx3RAyRZt8B3Czp68DdkmZk9e+5ektaArwaeHlE7Khyac/VvYyPAx8ATiEFpabV2WNKrbUGmCep\n3F/UwaSuvX5oJUH6WUD5Jnx+rFzTv+tJ+ifgYuDzwLtLTvdsvYtFxAPAfcB/yw71VL2zf8NXkMbM\nHpe0SNIi4NDsktnZsTn0WN3LyYLyGkZbx02rs4NSa91N+hmfUHxQ0jRgMXBPJwrVIT8nNe1fXObc\nidnzhPt5ZAHpA8C/A2dHNg+2SE/Wu4LpwP7Zn3ut3tNJa5JOAVYUPe7Izi/JXp9N79V9H9l3AUNa\ntgAABQZJREFU2CGMTuhpXp07PQe+lx+kRYXV1ikt6XQZm1zfWuuUvkpax/CHRcfydQyPMIHWbmRl\nvyT7e/wiMKkf6g0cWOH4SVkdv9+j9R4gzUArfbwn+x34P9nrI3qp7sABFY5/nH3XpTWlzt66osUk\nLSONNdxEavrnGR1+BLwqJnhGB0lvY7QL41ygQMpcAfDbiPhS0bWLSDN5dpAyXWwgrfg+BjglIm5r\nV7nHS9J7gU8Dj5Fm3JX+Pa6NiNuza3up3jeRMjr8gPRlMw14IXA6aczglZGtU+mleleSzb77DXBV\nRCwtOt4TdZf0CVJL5z9Iv+szSbPvTgJ+CpwUo5lLmlPnTkfiXn8Ak4G/I61o3kbqV70CmNnpsjWp\nfneQ/sdU7nFHmeuPAr4BPEP6Evu/wMmdrscY6v2FKvXep+49VO/TgG8Bq0izqraQZuEtAxb26t93\nlZ/HYZTJ6NArdQfeANyWfW9tBTaR1lheBExrRZ3dUjIzs67hiQ5mZtY1HJTMzKxrOCiZmVnXcFAy\nM7Ou4aBkZmZdw0HJzMy6hoOSmZl1DQclM9uLpJWS7uh0Oaw/OSiZmVnXcFAyM7Ou4aBkZmZdw0HJ\nrA0kTZV0kaQHJW2V9IykWyQdW3LdKyWFpDMlnSvpkez6RySdW+GzXy7pdknDkrZIulfSOypcu0jS\n5yUNSdouaY2kb0h6YZlrj5T0bUkbs8/+mqQDm/MTMSvP26GbtZikAeBW4CXAl0hbXswmpfX/kaSX\nR0TpBmjnAgcC1wAbgTOAT0naPyIuLfrsvyBti/I4acuQjaRtJD4j6bkR8f6ia48Dvk/aG+izpP2v\n9gdekZXtZ0X3P5iUAf4m4H3AHwLvAvYDXjO+n4hZZc4SbtZiki4gbVfyZ1G0p4yk/UiB4dGIeGV2\n7JWkvWtGgKMiYig7XiBtA3AscHhEDEmaDDxKCnAviIg1Rdf+B2kfnCMjYoUkkXYHXQScEGn78uIy\nTopsby9JK0l7ZL05Im4ouuYq0nbnR0bEw837CZmNcvedWestIe059DNJ8/IHaUPE24GXSZpe8p7r\n8oAEEBHbSRunTQH+Ijv8QmAh8Lk8IBVd+zHSv+83ZIcXA0cDny8NSNl7SjcpXFMckDI/yJ6fV0ed\nzcbE3XdmrXcUMB1YV+WaeaSN83K/KnPNL7Pn52bPh2fPD5a59sGSa/NAcl/Vko56tMyxJ7PnA+r8\nDLOGOSiZtV7edXZhlWuqBaxO2FXlnNpWCus7DkpmrbcCmA/8oEw3WSVHlTn2guz50ZLno+u49pHs\neXGd9zfrCI8pmbXeF0kz6cq2lCQ9u8zht0o6pOiaAnABqQXzrezwvcBjwFnFU7Wz2X7vAwL4Rnb4\n/5G69N4uaZ8glk2EMOs4t5TMWu+TwKuBj0t6FWnCwAbSJIU/AbYCJ5W85xHgp5L+jTTN+y3A8cCH\nImIVQETskrSUNG37bknXZte+mTTz7iMRsSK7NiSdRZoSfpekfEr4HNKU8FuBZS2qv1ndHJTMWiwi\ndkg6hTSd+m1Avs5oDXAX8O9l3raMtCboXFLwegw4PyI+WfLZt0j6E+BiUuuoQJokcXZEfLbk2rsl\nHQ/8I3Aa8G5gfVaGHzWhqmbj5nVKZl2kaJ3SWRHxhc6Wxqz9PKZkZmZdw0HJzMy6hoOSmZl1DY8p\nmZlZ13BLyczMuoaDkpmZdQ0HJTMz6xoOSmZm1jUclMzMrGv8f13ZLs+EnQA/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f35522f80b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# summarize history for accuracy  \n",
    "plt.plot(train_accuracy_list)  \n",
    "plt.plot(test_accuracy_list)  \n",
    "plt.title('model train/test accuracy')  \n",
    "plt.ylabel('accuracy')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'test'], loc='upper left')  \n",
    "plt.savefig('train_test_accuracy.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZoAAAEsCAYAAAD6lXULAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3551fc5048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for loss  \n",
    "plt.plot(train_loss_list)  \n",
    "plt.plot(test_loss_list)  \n",
    "plt.title('model train/test loss')  \n",
    "plt.ylabel('loss')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'test'], loc='upper left')  \n",
    "plt.savefig('train_test_loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_3.5]",
   "language": "python",
   "name": "conda-env-tensorflow_3.5-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
