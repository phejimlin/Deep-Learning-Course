{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "\n",
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from tensorflow.contrib import learn\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.metrics import f1_score\n",
    "from IPython.display import clear_output, Image, display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### Do not modify here ###### \n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = \"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = graph_def\n",
    "    #strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))\n",
    "###### Do not modify  here ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data_and_labels(train_data_file, test_data_file):\n",
    "    \"\"\"\n",
    "    Loads MR polarity data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "    # Load data from files\n",
    "    train_data = pd.read_csv(train_data_file, sep='\\t', quoting=csv.QUOTE_NONE, header=None, names=['id', 'language', 'label', 'text'])\n",
    "    test_data = pd.read_csv(test_data_file, sep='\\t', quoting=csv.QUOTE_NONE, header=None, names=['id', 'language', 'label', 'text'])\n",
    "    \n",
    "    x_train = train_data['text'].tolist()\n",
    "    y_train = train_data['label'].tolist()\n",
    "\n",
    "    x_test = test_data['text'].tolist()\n",
    "    y_test = test_data['label'].tolist()\n",
    "    \n",
    "    x_train = [s.strip() for s in x_train]\n",
    "    x_test = [s.strip() for s in x_test]\n",
    "    \n",
    "    label_encoding = {'neutral':0, 'positive':1, 'negative':2}\n",
    "    \n",
    "    y_train_encoding = [label_encoding[label] for label in y_train]    \n",
    "    y_test_encoding = [label_encoding[label] for label in y_test]\n",
    "\n",
    "    \n",
    "    return [x_train, y_train_encoding, x_test, y_test_encoding]\n",
    "\n",
    "def transform_data_and_labels(data):\n",
    "    \"\"\"\n",
    "    Loads MR polarity data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    x = np.array(data['text'].tolist())\n",
    "    y = data['label'].tolist()\n",
    "    \n",
    "    # encoding label\n",
    "    label_encoding = {'neutral':0, 'positive':1, 'negative':2}\n",
    "    y = [label_encoding[label] for label in y]    \n",
    "    \n",
    "    \n",
    "    y = np.array(y)\n",
    "    \n",
    "    # Randomly shuffle data\n",
    "    np.random.seed(10)\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "    x_shuffled = x[shuffle_indices]\n",
    "    y_shuffled = y[shuffle_indices]\n",
    "\n",
    "    # Split train/test set\n",
    "    # maybe we can use cross-validation to improve\n",
    "    dev_sample_index = -1 * int(0.1 * float(len(y)))\n",
    "    x_train, x_test = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "    y_train, y_test = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "    print(\"Train/Test split: {:d}/{:d}\".format(len(y_train), len(y_test)))\n",
    "    \n",
    "    return [x_train, y_train, x_test, y_test]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    This function assumes that the last word in the word embedding is a zero vector, and will use it as padding.\n",
    "    The input 'num_voc' equals to the shape[0] of the word embedding.\n",
    "\"\"\"\n",
    "def process_tweet(train_tweets, test_tweets, num_voc):\n",
    "    # max_document_length = max([len(x.split(\" \")) for x in x_train_sentence])\n",
    "    ppl_re = re.compile(r'@\\S*')\n",
    "    url_re = re.compile(r'http\\S+')\n",
    "    tknzr = TweetTokenizer()\n",
    "    # tknzr = TweetTokenizer(reduce_len=True)\n",
    "    \n",
    "    tokenized_tweets_all = []\n",
    "    max_document_length = 0\n",
    "    \n",
    "    for tweets in [train_tweets, test_tweets]:\n",
    "        tweets = [url_re.sub('URLTOK', ppl_re.sub('USRTOK', tweet.lower())) for tweet in tweets]\n",
    "        tokenized_tweets = [tknzr.tokenize(tweet) for tweet in tweets]\n",
    "        tokenized_tweets_all.append(tokenized_tweets)\n",
    "        max_document_length = max(max_document_length, max([len(tweet) for tweet in tokenized_tweets]))\n",
    "    print(max_document_length)\n",
    "    \n",
    "    x = []\n",
    "    \n",
    "    for tokenized_tweets in tokenized_tweets_all:\n",
    "        x_curr = []\n",
    "        for tokenized_tweet in tokenized_tweets:\n",
    "            if len(tokenized_tweet) == max_document_length:\n",
    "                print(tokenized_tweet)\n",
    "            \"\"\"Not sure if original paper does this, but since index 0 means USRTOK, padding should be a number\n",
    "            higher than total word count, so tf.nn.embedding_lookup will return a tensor of 0 insted of USRTOK.\"\"\"\n",
    "        #     temp = np.zeros(max_document_length, dtype=np.int).tolist()\n",
    "            temp = (np.ones(max_document_length, dtype=np.int)*(num_voc-1)).tolist()\n",
    "\n",
    "            for index, word in enumerate(tokenized_tweet):\n",
    "                if word in word_dict:\n",
    "                    temp[index] = word_dict[word][0]\n",
    "            x_curr.append(temp)\n",
    "        x_curr = np.array(x_curr)\n",
    "        x.append(x_curr)\n",
    "    \n",
    "    return x[0], x[1]\n",
    "\n",
    "\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Current epoch: \", epoch)\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pre-train word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_embeddings = np.load('./data/embed_tweets_en_200M_200D/embedding_matrix.npy')\n",
    "word_dict = {}\n",
    "with open('./data/embed_tweets_en_200M_200D/vocabulary.pickle', 'rb') as myfile:\n",
    "    word_dict = pickle.load(myfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> (94, 3868680)\n"
     ]
    }
   ],
   "source": [
    "# shit\n",
    "for key, val in word_dict.items():\n",
    "    if val[0] == 94:\n",
    "        print(key, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1859185, 200)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distant Supervision phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distance_supervised_tweets = pd.read_csv('./data/distant_data/distance_supervised_tweets_corrected', names=[\"id\",\"lan\", \"label\", \"text\"], sep=\"\\t\", header=None, usecols=[\"lan\", \"label\", \"text\"])\n",
    "distance_supervised_tweets_2 = pd.read_csv('./data/distant_data/distance_supervised_tweets_2_corrected', names=[\"id\",\"lan\", \"label\", \"text\"], sep=\"\\t\", header=None, usecols=[\"lan\", \"label\", \"text\"])\n",
    "distance_supervised_tweets_3 = pd.read_csv('./data/distant_data/distance_supervised_tweets_3_corrected', names=[\"id\",\"lan\", \"label\", \"text\"], sep=\"\\t\", header=None, usecols=[\"lan\", \"label\", \"text\"])\n",
    "distance_supervised_tweets = distance_supervised_tweets.append(distance_supervised_tweets_2).append(distance_supervised_tweets_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lan</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>Boston Bruins Morning Thoughtefense Exceeding ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>Bol Bachchan!. #AeZindagiGaleLagale. Aata Majh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>(17) karena lagi sakit, aku lagi gelisah terus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>Telkomsel: Thary422 Terima kasih telah berpart...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>@parisa_khania آخر سر هم از جزوه ی محترم عکس گ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>kyristcl: XL123: frungnarikvnn Bisa ajak selai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>Telkomsel: daff_01 Terima kasih telah berparti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>@chris_randall Just the usual disclaimer that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>@Lin_Manuel Congrats from me and all my friend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>@nicaaaji hahahaha magtext ako beb. \"Hi lola! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>@emergiTEL launches their new website.  #Emegi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>Okulda iki kere üst üste kitli kalıp camdan at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>New Lyft Users get 10 free Lyft rides &amp;lt;&amp;lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>I want Thai food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>@LionArts we are checking with our music searc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>I actually lover Anna Faris https://t.co/sUAG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>latest podcast from Chop Suey Press The Battle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>MAJufri3 Terima kasih telah berpartisipasi. Ku...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>Telkomsel: fadilanurul955 Terima kasih telah b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>@AwaisAhmad718 thanks  ab theek hai medicine l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>Telkomsel: khai_dier Jika keluhan baru hari in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>@taykecare Hi! Saw you follow music and think ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>Telkomsel: InnekeVermarien Pagi, Mbak Inneke. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>@nreatherford Sounds good to us! We are always...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>#ملتقي_لحن_للابداع #قروب_طويق_للدعم تبون ريتويت</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>Yung feeling na di nyo hinahayaan matapos ang ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>Hello baby jane (Babeeh_Jane)  https://t.co/LC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>@kevinthewhippet @iggiesrule89 @bunniemommie i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>هام تم تعديل مسار خط باص ميسلون - جامعة بحيث ي...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>Helloo.  Alyssa (TheJoltaire) https://t.co/25L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160752</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>ANG CUTE CUTE CUTE MO  https:// twitter.com/Gi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160753</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>I’m willing to bet it’s a partial tear. Idk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160754</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>この笑顔はキュン死もの_´ཀ`」 ∠):_ pic.twitter.com/5iBTOampMh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160755</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>really? for real?  https:// twitter.com/readef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160756</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>え？_´ཀ`」 ∠): 良いもん食ってんのな https:// twitter.com/mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160757</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>And still beating us....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160758</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>Kolaborasi kuy  https:// twitter.com/amandasya...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160759</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>captain crunch..  https:// twitter.com/CAMILAH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160760</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>people should start selling fish rice or prawn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160761</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>http://htSomething needs to be done to prevent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160762</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>http://htSomething needs to be done to prevent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160763</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>http://htSomething needs to be done to prevent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160764</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>http://htSomething needs to be done to prevent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160765</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>http://htSomething needs to be done to prevent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160766</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>http://htSomething needs to be done to prevent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160767</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>http://htSomething needs to be done to prevent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160768</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>@Mbrower42 HAPPY BIRTHDAY, i love you mace!!!!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160769</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>http://htSomething needs to be done to prevent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160770</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>http://htSomething needs to be done to prevent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160771</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>https:// twitter.com/NFL/status/928 834503887...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160772</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>I'm so hungry but I'm so done eating goldfish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160773</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>i'm always stuck between wanting to do somethi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160774</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>Please sign &amp; share! {|} http:// fb.me/9qsBceTD1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160775</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>no b you deserve to be happy  don’t say that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160776</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>Kaylee is deadass sobbing her eyes out over ft...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160777</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>http://htSomething needs to be done to prevent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160778</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>Ugh. @DangeRussWilson 's getting no coverage! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160779</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>http://htSomething needs to be done to prevent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160780</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>http://htSomething needs to be done to prevent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160781</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>i want to eat hamburger and chicken</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>203512 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       lan     label                                               text\n",
       "0       en  positive  Boston Bruins Morning Thoughtefense Exceeding ...\n",
       "1       en  positive  Bol Bachchan!. #AeZindagiGaleLagale. Aata Majh...\n",
       "2       en  negative  (17) karena lagi sakit, aku lagi gelisah terus...\n",
       "3       en  positive  Telkomsel: Thary422 Terima kasih telah berpart...\n",
       "4       en  positive  @parisa_khania آخر سر هم از جزوه ی محترم عکس گ...\n",
       "5       en  positive  kyristcl: XL123: frungnarikvnn Bisa ajak selai...\n",
       "6       en  positive  Telkomsel: daff_01 Terima kasih telah berparti...\n",
       "7       en  positive  @chris_randall Just the usual disclaimer that ...\n",
       "8       en  positive  @Lin_Manuel Congrats from me and all my friend...\n",
       "9       en  negative  @nicaaaji hahahaha magtext ako beb. \"Hi lola! ...\n",
       "10      en  positive  @emergiTEL launches their new website.  #Emegi...\n",
       "11      en  negative  Okulda iki kere üst üste kitli kalıp camdan at...\n",
       "12      en  positive  New Lyft Users get 10 free Lyft rides &lt;&lt;...\n",
       "13      en  negative                                  I want Thai food \n",
       "14      en  positive  @LionArts we are checking with our music searc...\n",
       "15      en  negative   I actually lover Anna Faris https://t.co/sUAG...\n",
       "16      en  positive  latest podcast from Chop Suey Press The Battle...\n",
       "17      en  positive  MAJufri3 Terima kasih telah berpartisipasi. Ku...\n",
       "18      en  positive  Telkomsel: fadilanurul955 Terima kasih telah b...\n",
       "19      en  positive  @AwaisAhmad718 thanks  ab theek hai medicine l...\n",
       "20      en  positive  Telkomsel: khai_dier Jika keluhan baru hari in...\n",
       "21      en  positive  @taykecare Hi! Saw you follow music and think ...\n",
       "22      en  positive  Telkomsel: InnekeVermarien Pagi, Mbak Inneke. ...\n",
       "23      en  positive  @nreatherford Sounds good to us! We are always...\n",
       "24      en  positive   #ملتقي_لحن_للابداع #قروب_طويق_للدعم تبون ريتويت \n",
       "25      en  positive  Yung feeling na di nyo hinahayaan matapos ang ...\n",
       "26      en  positive  Hello baby jane (Babeeh_Jane)  https://t.co/LC...\n",
       "27      en  positive  @kevinthewhippet @iggiesrule89 @bunniemommie i...\n",
       "28      en  negative  هام تم تعديل مسار خط باص ميسلون - جامعة بحيث ي...\n",
       "29      en  positive  Helloo.  Alyssa (TheJoltaire) https://t.co/25L...\n",
       "...     ..       ...                                                ...\n",
       "160752  en  negative  ANG CUTE CUTE CUTE MO  https:// twitter.com/Gi...\n",
       "160753  en  negative       I’m willing to bet it’s a partial tear. Idk \n",
       "160754  en  negative   この笑顔はキュン死もの_´ཀ`」 ∠):_ pic.twitter.com/5iBTOampMh\n",
       "160755  en  negative  really? for real?  https:// twitter.com/readef...\n",
       "160756  en  negative  え？_´ཀ`」 ∠): 良いもん食ってんのな https:// twitter.com/mu...\n",
       "160757  en  negative                           And still beating us....\n",
       "160758  en  negative  Kolaborasi kuy  https:// twitter.com/amandasya...\n",
       "160759  en  negative  captain crunch..  https:// twitter.com/CAMILAH...\n",
       "160760  en  negative  people should start selling fish rice or prawn...\n",
       "160761  en  negative  http://htSomething needs to be done to prevent...\n",
       "160762  en  negative  http://htSomething needs to be done to prevent...\n",
       "160763  en  negative  http://htSomething needs to be done to prevent...\n",
       "160764  en  negative  http://htSomething needs to be done to prevent...\n",
       "160765  en  negative  http://htSomething needs to be done to prevent...\n",
       "160766  en  negative  http://htSomething needs to be done to prevent...\n",
       "160767  en  negative  http://htSomething needs to be done to prevent...\n",
       "160768  en  negative  @Mbrower42 HAPPY BIRTHDAY, i love you mace!!!!...\n",
       "160769  en  negative  http://htSomething needs to be done to prevent...\n",
       "160770  en  negative  http://htSomething needs to be done to prevent...\n",
       "160771  en  negative   https:// twitter.com/NFL/status/928 834503887...\n",
       "160772  en  negative     I'm so hungry but I'm so done eating goldfish \n",
       "160773  en  negative  i'm always stuck between wanting to do somethi...\n",
       "160774  en  negative   Please sign & share! {|} http:// fb.me/9qsBceTD1\n",
       "160775  en  negative       no b you deserve to be happy  don’t say that\n",
       "160776  en  negative  Kaylee is deadass sobbing her eyes out over ft...\n",
       "160777  en  negative  http://htSomething needs to be done to prevent...\n",
       "160778  en  negative  Ugh. @DangeRussWilson 's getting no coverage! ...\n",
       "160779  en  negative  http://htSomething needs to be done to prevent...\n",
       "160780  en  negative  http://htSomething needs to be done to prevent...\n",
       "160781  en  negative               i want to eat hamburger and chicken \n",
       "\n",
       "[203512 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_supervised_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Test split: 183161/20351\n"
     ]
    }
   ],
   "source": [
    "x_train_sentence, y_train, x_test_sentence, y_test = transform_data_and_labels(distance_supervised_tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20351\n",
      "2063\n",
      "['people', 'come', 'and', 'people', 'go', '...', \"that's\", 'life', '...', '#aldubmistakenidentity', '33877', 'en', 'positive', \"michiganon't\", 'sell', 'USRTOK', '100m', 'gallons', 'of', 'groundwater', 'for', '$', '200', 'and', '20', 'jobs', '.', \"that's\", 'bananas', '.', 'URLTOK', '33878', 'en', 'positive', 'jio', 'has', 'touched', 'the', 'hearts', 'of', '50', 'millions', 'users', 'with', 'their', 'network', 'really', 'happy', 'for', 'jio', '!', '#jio50million', '33879', 'en', 'positive', 'every', 'second', ',', 'minute', '&', 'hour', 'of', 'our', 'life', 'must', 'be', 'filled', 'with', 'passion', ',', 'dedication', 'and', 'restlessness', 'to', 'make', 'it', 'the', 'best', 'possible', 'life', 'ever', '.', '33880', 'en', 'positive', 'USRTOK', 'USRTOK', 'USRTOK', 'whys', 'this', 'yous', '33881', 'en', 'positive', 'thanks', 'for', 'the', 'recent', 'follow', 'USRTOK', 'USRTOK', 'USRTOK', 'happy', 'to', 'connect', 'have', 'a', 'great', 'tuesday', '.', '>', '>', 'URLTOK', '33882', 'en', 'positive', 'done', 'putting', 'in', 'effort', 'into', 'relationships', 'w', 'people', 'when', 'i', \"don't\", 'get', 'any', 'effort', 'back', ':', '33883', 'en', 'positive', 'USRTOK', 'because', 'of', 'i', 'will.put', 'none', 'option', 'then', 'most', 'of', 'people', 'will', 'vote', 'for', 'none', 'so', 'i', 'wanna', 'see', 'which', '1', 'of', 'them', 'both', 'r', 'popular', '..', '33884', 'en', 'negative', 'i', 'miss', 'my', 'phone', '.', '33885', 'en', 'negative', 'USRTOK', 'ahh', 'thank', 'you', 'ky', 'xoxo', '❣', '️', 'miss', 'you', 'too', '33886', 'en', 'positive', 'conflictenied', 'ops', '(', 'sony', 'playstation', '3', ',', '2008', ')', 'URLTOK', 'URLTOK', '33887', 'en', 'negative', 'lama', 'ndk', 'chat', 'bf', '33888', 'en', 'positive', 'scorching', 'hot', 'services', 'stocks', 'tapeish', 'network', 'corp', '.', '(', 'dish', ')', ',', '...', 'URLTOK', '#dishnetwork', '33889', 'en', 'positive', 'USRTOK', 'of', 'course', 'yeah', '.', 'thanks', 'man', '33890', 'en', 'negative', 'it', 'feels', 'like', 'the', 'world', \"doesn't\", 'want', 'u', 'just', 'because', 'u', 'have', 'this', 'pimples', 'in', 'your', 'face', '33891', 'en', 'positive', 'my', 'fave', 'lm', 'alcott', 'is', 'not', 'little', 'women', 'nor', \"jo's\", 'boys', 'nor', 'little', 'men', 'something', 'simpler', 'and', 'more', 'relatable', 'for', 'me', '33892', 'en', 'negative', 'sleepy', 'na', 'me', '33893', 'en', 'positive', 'jacket', 'reseller', 'very', 'wellcome', '-', 'size', 'm', 'fit', 'l', 'bbm', ':', 'pin', '37314c7', 'lineUSRTOK', ':', 'USRTOK', '(', 'pake', 'USRTOK', 'URLTOK', '33894', 'en', 'positive', 'thank', 'you', 'USRTOK', ':', '*', 'happy', 'birthday', 'kay', 'bby', 'brother', 'mo', '33895', 'en', 'negative', 'meis', 'home', ')', 'megoes', 'to', 'my', 'mom', ')', 'hi', 'mom', 'my', 'brothercomes', 'running', 'after', 'me', ')', 'she', 'met', 'her', 'crush', 'wonho', '33896', 'en', 'positive', '4days', 'drunk', 'saraaaapp', '\\\\', 'ud83d', '\\\\', 'ude', '02', '33897', 'en', 'positive', 'kinda', 'weird', 'but', 'still', 'cute', '33898', 'en', 'negative', \"didn't\", 'even', 'get', 'an', 'interview', 'for', 'ta', 'job', '.', 'apparently', 'i', \"don't\", 'have', 'the', 'skills', 'or', 'the', 'experience', 'i', 'guess', 'my', 'science', 'degree', 'means', 'nothing', '33899', 'en', 'positive', 'thanks', 'for', 'the', 'recent', 'follow', 'USRTOK', 'USRTOK', 'USRTOK', 'happy', 'to', 'connect', 'have', 'a', 'great', 'tuesday', '.', '>', '>', 'get', 'free', '?', 'URLTOK', '33900', 'en', 'positive', 'opiniononald', 'trump', 'and', 'the', 'nuclear', 'danger', 'URLTOK', 'URLTOK', '33901', 'en', 'negative', 'USRTOK', 'i', 'miss', 'you', 'too', 'po', 'ate', '33902', 'en', 'positive', 'USRTOK', 'so', 'true', 'but', 'the', 'beautiful', 'part', ';', 'for', 'ones', 'own', 'admonition', 'it', 'is', 'that', 'reality', 'only', 'confirms', 'the', 'scriptures', '.', '33903', 'en', 'positive', 'USRTOK', 'USRTOK', '4', 'bio', '5', 'icon', '5', 'layout', '5', '–', 'i', 'really', 'like', 'it', 'recent', 'tweets', '4', 'would', 'i', 'follow', ':', 'no', ',', 'but', 'good', 'acc', '33904', 'en', 'negative', 'USRTOK', 'tfw', 'you', \"can't\", 'see', 'your', 'feet', '33905', 'en', 'positive', 'USRTOK', 'ikr', '.', 'that', 'eyes', 'smile', '33906', 'en', 'positive', 'USRTOK', '3t', 'is', 'the', 'better', 'buy', '33907', 'en', 'positive', '#givingtuesdayca', 'is', 'under', 'way', 'at', 'go', 'stations', 'around', 'the', 'city', '!', 'be', 'sure', 'to', 'grab', 'a', '$', '2', 'scone', 'on', 'your', 'way', 'to', 'work', 'this', 'morning', '33908', 'en', 'positive', 'USRTOK', 'wonderful', '!', 'thanks', 'for', 'joining', 'the', '#skypeathon', '.', 'was', 'your', 'call', 'booked', 'through', 'the', 'site', '?', 'we', 'want', 'to', 'make', 'sure', 'your', 'miles', 'count', '!', '33909', 'en', 'positive', 'USRTOK', 'just', 'got', 'off', 'my', 'flight', 'to', 'munich', '33910', 'en', 'positive', 'USRTOK', 'advance', 'happy', 'birthday', 'maria', '!', 'love', 'you', 'queen', 'alwaysalexa', 'bukasna', '33911', 'en', 'negative', 'why', 'are', 'these', 'garbage', 'trucks', 'so', 'loud', '33912', 'en', 'positive', 'a', 'case', 'study', 'in', 'poor', 'portfolio', 'risk', 'decisionsallas', 'police', '&', 'fire', 'pension', '.', 'will', 'pension', 'boards', '/', 'advisors', 'learn', '?', 'URLTOK', '33913', 'en', 'negative', 'USRTOK', 'still', 'the', 'bully', 'huhu', 'thank', 'you', 'mom', '!', '!', 'i', 'love', 'youuu', '♡', '33914', 'en', 'positive', 'USRTOK', 'thanks', 'for', 'the', 'follow', '33915', 'en', 'positive', 'i', 'have', 'to', 'drop', 'a', 'class', 'asan', 'ang', 'sistema', 'isd', '.', 'asan', 'na', '.', 'paki', 'hanap', 'please', '.', '33916', 'en', 'positive', 'USRTOK', 'awesome', '-', 'i', 'wish', 'my', 'dogs', 'would', 'sleep', 'close', 'together', 'like', 'that', '33917', 'en', 'negative', 'USRTOK', 'o', 'omygod', 'i', 'love', 'you', 'i', 'love', 'va', 'i', 'love', 'art', 'yay', '33918', 'en', 'positive', 'USRTOK', 'black', '33919', 'en', 'positive', 'help', 'a', 'sister', 'out', '!', 'i', 'could', 'win', '$', '500', 'from', 'unbound', 'for', 'new', 'lingerie', 'URLTOK', '33920', 'en', 'positive', '#thanks', 'USRTOK', 'USRTOK', 'thanks', 'for', 'the', 'recent', 'follow', '.', 'much', 'appreciated', '>', '>', 'want', 'this', '?', \"it's\", 'free', '!', 'URLTOK', '33921', 'en', 'positive', 'USRTOK', 'and', 'USRTOK', 'support', 'for', 'echo', 'soon', '33922', 'en', 'positive', 'love', '#thegiftyougive', 'to', 'the', 'world', '33923', 'en', 'positive', 'new', 'videof', 'network', '-', 'extra', 'beautiful', 'fitness', 'milf', 'bangs', 'huge', 'cock', '#nsfw', '#xxx', '#sex', '#tube', '#porn', '33924', 'en', 'positive', 'not', 'a', 'bad', 'start', 'a', 'nice', '50/50', 'on', \"today's\", 'clients', 'wing', '#autocleandetailing', '…', 'URLTOK', '33925', 'en', 'positive', 'USRTOK', 'let', 'us', 'know', 'if', 'we', 'can', 'help', 'answer', 'any', 'of', 'your', 'plumbing', 'questions', '!', 'URLTOK', '33926', 'en', 'positive', 'bostonecember', 'is', 'almost', 'here', '.', 'make', 'the', 'most', 'of', 'it', 'and', 'get', 'to', 'one', '(', 'or', 'more', ')', 'of', 'these', 'events', 'around', 'boston', '.', '…', 'URLTOK', '33927', 'en', 'positive', 'good', 'morning', 'people', '!', '!', 'have', 'a', 'lovely', 'day', '#twitter', '#wedding', '#bride', '33928', 'en', 'positive', 'thanks', 'for', 'the', 'recent', 'follow', 'USRTOK', 'USRTOK', 'happy', 'to', 'connect', 'have', 'a', 'great', 'tuesday', '.', '(', 'want', 'this', '\\\\', 'ud83c', '\\\\', 'udd', '93', '?', '>', '>', 'URLTOK', '33929', 'en', 'positive', 'USRTOK', 'oh', 'hey', ',', 'after', 'seeing', 'the', 'picture', 'again', 'i', 'see', 'why', 'you', 'like', 'post-its', 'so', 'much', '.', 'make', 'a', 'breakfast', 'pizza', '!', 'then', 'u', \"won't\", 'b', 'hungry', '.', '33930', 'en', 'negative', 'USRTOK', 'then', 'on', 'his', 'drive', 'into', 'work', 'he', 'hit', 'a', 'big', 'deer', 'with', 'his', 'car', '.', 'let', 'the', 'christmas', 'season', 'celebrations', 'begin', '.', '33931', 'en', 'negative', 'USRTOK', 'its', 'worse', 'when', 'its', '30', 'and', 'over', '33932', 'en', 'positive', 'USRTOK', 'i', \"ain't\", 'actually', 'selling', 'dvds', 'man', 'x', ')', 'just', 'coding', 'a', 'website', 'capable', 'of', 'such', 'things', '.', 'finished', 'the', 'whole', 'registration', 'process', '33933', 'en', 'positive', 'child', 'i', 'can', 'do', 'everything', 'URLTOK', '33934', 'en', 'positive', 'son', 'has', 'no', 'legal', 'right', 'in', 'parents', '’', 'house', ',', 'can', 'stay', 'at', 'their', 'mercyelhi', '...', 'URLTOK', 'by', '#jantakareporter', 'via', 'USRTOK', '33935', 'en', 'positive', 'my', 'face', 'is', 'so', 'sunburn', '33936', 'en', 'positive', 'happy', 'sweet', 'sixteen', 'USRTOK', '\\\\', 'ud83d', '\\\\', 'ude', '1b', '\\\\', 'ud83c', '\\\\', 'udf', '89', '\\\\', 'ud83c', '\\\\', 'udf', '81i', 'hope', 'you', 'have', 'a', 'great', 'day', '33937', 'en', 'positive', 'hii', '.', 'a', '$', 'm', '(', 'youngam', '904', ')', 'URLTOK', '33938', 'en', 'positive', 'thanks', 'for', 'the', 'recent', 'follow', 'USRTOK', 'USRTOK', 'USRTOK', 'happy', 'to', 'connect', 'have', 'a', 'great', 'this', 'week', '33939', 'en', 'positive', 'watch', 'lesbian', 'movieisrobe', ',', 'lesson', 'about', 'to', 'commence', '▶', 'URLTOK', 'URLTOK', '33940', 'en', 'positive', 'URLTOK', 'pls', 'retweet', 'and', 'like', 'if', 'you', 'like', 'video', '33941', 'en', 'positive', 'USRTOK', 'wow', ',', 'how', '?', '33942', 'en', 'positive', 'pathfinder', 'battleseadly', 'foes', 'minis', 'now', 'available', 'URLTOK', '#tabletop', 'URLTOK', '33943', 'en', 'positive', 'this', 'is', 'the', 'day', 'i', 'have', 'been', 'waiting', 'for', '...', 'my', 'last', 'day', 'of', 'unemployment', '!', '!', '!', '33944', 'en', 'negative', 'USRTOK', '12:36', '12', 'from', 'parkway', 'centre', 'to', 'middlesbrough', 'never', 'turned', 'up', ',', 'had', 'to', 'wait', '10', 'minutes', 'when', 'i', 'had', 'a', 'train', 'to', 'catch', '33945', 'en', 'positive', 'USRTOK', 'thank', 'youuuu', '.', '33946', 'en', 'positive', 'we', 'are', 'hiring', 'a', '#marketingmanager', '!', 'ping', 'us', 'if', 'you', 'think', 'you', 'have', 'what', 'it', 'takes', 'URLTOK', '#ai', '#bots', '#marketing', '#jobs', '\\\\', 'ud83d', '\\\\', 'ude', '4c', '33947', 'en', 'positive', 'i', 'love', 'this', '#watch', '⌚', '️', 'dropped', 'hours', ',', 'grey', '#twt247', '\\\\', 'ud83d', '\\\\', 'udecd', '➡', '️', 'URLTOK', 'hours', ',', 'grey', 'URLTOK', '#gift', '#watch', '33948', 'en', 'positive', 'injuriesanny', 'trevathan', 'done', 'for', 'remainder', 'of', 'year', 'URLTOK', '33949', 'en', 'positive', 'german', 'union', 'leader', 'says', 'will', 'fight', 'for', 'plants', 'in', 'tata-thyssen', 'mergeruesseldorf', ',', 'germany', '…', 'URLTOK', '#news', '#reuters', '33950', 'en', 'positive', 'hello', '..', 'everyone', 'shut', 'up', '(', 'fatierza', ')', 'URLTOK', '33951', 'en', 'positive', 'im', 'not', 'ready', 'URLTOK', '33952', 'en', 'negative', 'am', 'i', 'going', 'to', 'have', 'to', 'turn', 'this', 'alarm', 'back', 'on', 'again', ':/', 'URLTOK', '33953', 'en', 'positive', '#ooc', \"you're\", 'welcome', '.', 'URLTOK', '33954', 'en', 'negative', 'USRTOK', 'USRTOK', 'district', 'in', 'aleppo', '33955', 'en', 'positive', 'thats', 'a', 'cool', 'shirt', '!', 'USRTOK', 'URLTOK', '33956', 'en', 'positive', 'i', 'love', 'waking', 'up', 'every', '20', 'minutes', 'throughout', 'the', 'night', '33957', 'en', 'positive', 'wonder', 'if', 'he', 'will', 'be', 'checking', 'with', 'his', 'mates', 'in', 'the', 'farc', 'rebels', '?', 'URLTOK', '33958', 'en', 'positive', 'state', 'to', 'implement', 'therapy', 'cuts', 'for', 'disabled', 'children', 'next', 'month', '-', 'houston', 'chronicleallas', '…', 'URLTOK', '#handicapped', '#love', '33959', 'en', 'negative', 'i', 'want', 'to', 'make', 'art', 'that', 'is', 'not', 'related', 'to', 'school', 'but', 'no', 'time', 'em', 'sad', '33960', 'en', 'negative', 'USRTOK', 'i', 'coukd', 'spend', 'all', 'my', 'day', 'at', 'home', ',', 'resting', 'and', 'still', 'feel', 'tired', '33961', 'en', 'positive', '“', 'opec', 'could', 'lose', 'market', 'share', 'in', 'a', 'world', 'awash', 'in', 'oilon', 'pittis', '”', 'URLTOK', '#energy', '#oil', '33962', 'en', 'positive', 'i', 'just', 'realized', 'that', \"sony's\", 'ffxv-themed', 'headphones', 'are', 'perfect', 'match', 'for', 'the', 'luna', 'edition', 'ps4', '!', '^', '_', '^', 'i', 'hope', 'it', 'comes', 'out', 'in', 'our', 'country', '!', '33963', 'en', 'negative', 'USRTOK', 'maybe', 'it', 'will', 'actually', 'let', 'me', 'post', ',', 'unlike', 'the', 'actual', 'twitter', 'client', '.', '33964', 'en', 'positive', 'love', 'it', 'mayward', 'twogetherinmasbate', 'URLTOK', '33965', 'en', 'positive', 'someone', 'pls', 'shoot', 'me', 'before', 'i', 'have', 'to', 'work', 'this', '6', 'hour', 'cash', 'shift', '33966', 'en', 'positive', 'manrepellerid', 'someone', 'say', 'pink', 'man', 'repeller', 'hats', '!', '?', 'URLTOK', 'URLTOK', '33967', 'en', 'negative', 'shooting', 'tomorrow', '!', '!', '33968', 'en', 'positive', 'USRTOK', 'fight', 'amongst', 'themselves', 'until', 'their', 'enemies', 'get', 'bored', 'of', 'killing', 'the', 'incompetent', 'mess', '?', 'sounds', 'like', 'skaven', 'business', 'as', 'usual', '33969', 'en', 'positive', 'tell', 'him', 'happy', 'birthday', 'too', 'bro', 'and', 'how', 'old', 'is', 'he', 'now', '?', 'URLTOK', '33970', 'en', 'positive', 'i', 'liked', 'a', 'USRTOK', 'video', 'from', 'USRTOK', 'URLTOK', 'quién', 'besa', 'mejor', '?', '-', 'con', 'sayoyyi', 'most', 'likely', 'to', 'tag', '33971', 'en', 'positive', 'USRTOK', 'sure', '!', '!', \"i'll\", 'try', 'to', 'get', 'to', 'it', 'this', 'week', '/', 'maybe', 'weekend', '33972', 'en', 'negative', 'sooooooo', 'USRTOK', 'and', 'i', 'probably', \"can't\", 'get', 'wifi', '/', 'phone', 'plans', 'until', 'after', 'the', 'new', 'year', 'lol', 'rip', 'i', 'hate', 'everything', '33973', 'en', 'positive', 'USRTOK', 'that', 'last', 'photo', 'thank', 'you', 'ave', 'but', 'i', 'love', 'you', 'so', 'much', '!', '!', '\\\\', 'ud83d', '\\\\', 'ude', '0b', '\\\\', 'ud83d', '\\\\', 'ude', '18', '33974', 'en', 'negative', 'fantastic', 'beasts', 'was', 'so', 'good', \"can't\", 'help', 'but', 'cry', 'whilst', 'watching', 'because', 'jk', \"rowling's\", 'imagination', 'is', 'overwhelming', '33975', 'en', 'positive', 'done', '33976', 'en', 'positive', 'thanks', 'for', 'the', 'recent', 'follow', 'USRTOK', 'USRTOK', 'USRTOK', 'happy', 'to', 'connect', 'have', 'a', 'great', 'tuesday', '.', '>', '>', 'URLTOK', '33977', 'en', 'positive', '#9eco', 'brothers', 'pan', 'organizer', 'rack', ',', 'bronzeeco', 'brothers', 'pan', 'organizer', 'rack', ',', 'bronze', 'by', 'deco', 'brothers', '…', 'URLTOK', '#storage', '33978', 'en', 'negative', 'i', 'thought', 'the', 'feeling', 'is', 'mutual', '33979', 'en', 'positive', 'USRTOK', 'the', 'reaper', 'is', 'your', 'friend', '33980', 'en', 'negative', 'had', '200-400', 'viewers', 'constantly', 'the', 'last', '3', 'days', 'i', 'best', 'get', 'a', 'sub', 'button', 'soon', '33981', 'en', 'negative', 'so', 'sad', 'URLTOK', '33982', 'en', 'positive', 'USRTOK', 'freudian', 'mishear', '33983', 'en', 'positive', 'USRTOK', 'USRTOK', '-', 'translation', '...', 'i', 'want', 'you', 'to', 'do', 'the', 'research', 'for', 'me', '...', 'liberalism', 'means', 'knowing', 'even', 'when', 'you', \"don't\", 'know', '33984', 'en', 'positive', 'how', 'cute', 'is', 'growlithe', '!', '!', '!', 'got', 'this', 'nice', 'little', 'poke', 'haul', 'at', 'the', 'kyoto', 'pokemon', 'centre', 'today', '#pokemon', '…', 'URLTOK', '33985', 'en', 'positive', 'thanks', 'for', 'the', 'recent', 'follow', 'USRTOK', 'USRTOK', 'happy', 'to', 'connect', 'have', 'a', 'great', 'tuesday', '.', '>', '>', 'want', 'this', '\\\\', 'ud83c', '\\\\', 'udd', '93', '?', 'URLTOK', '33986', 'en', 'positive', 'its', '6am', ',', 'my', 'classes', 'ends', 'at', '6pm', 'and', 'im', 'already', 'feeling', 'dead', 'inside', '33987', 'en', 'positive', 'stats', 'for', 'the', 'day', 'have', 'arrived', '.', '2', 'new', 'followers', 'and', 'no', 'unfollowers', 'via', 'URLTOK', '33988', 'en', 'negative', 'USRTOK', \"it's\", 'time', 'for', 'kamilah', 'and', 'helen', 'to', 'start', 'working', 'together', 'w', '/', 'o', 'kane', 'please', '33989', 'en', 'positive', 'this', 'is', 'a', 'plan', 'i', 'can', 'support', '.', 'URLTOK', '33990', 'en', 'positive', 'all', 'clean', 'new', 'brakes', 'just', 'wait', 'for', 'new', 'ignition', 'now', 'URLTOK', '33991', 'en', 'negative', 'USRTOK', 'thank', 'you', 'for', 'your', 'words', ',', 'they', 'will', 'never', 'be', 'forgotten', '33992', 'en', 'positive', 'popular', 'on', '500px', 'aily', 'shot', ':', 'the', 'first', 'flood', 'by', 'sergebcd', 'URLTOK', '33993', 'en', 'positive', \"that's\", 'all', 'we', 'need', '!', '#sold', 'URLTOK', '33994', 'en', 'positive', 'USRTOK', 'USRTOK', 'absolutely', '33995', 'en', 'positive', 'USRTOK', 'btw', ',', 'congratulations', 'aisha', '...', 'transition', 'from', 'neem', 'daacter', 'tou', 'complete', 'doctor', 'mubaarak', '.', 'may', 'you', 'get', 'more', 'success', '.', '33996', 'en', 'negative', 'USRTOK', 'USRTOK', '..', 'horrible', '33997', 'en', 'positive', 'rt', 'nythealthespite', 'the', 'problems', 'nightmares', 'can', 'cause', ',', 'sleeping', 'and', 'having', 'them', 'is', 'better', 'than', 'not', 'sleeping', 'URLTOK']\n"
     ]
    }
   ],
   "source": [
    "# TODO Load distant-supervised data\n",
    "# train it with two-layer CNN model\n",
    "# pass the weight to next two layer CNN model\n",
    "print(len(x_test_sentence))\n",
    "x_train_distance, x_test_distance = process_tweet(x_train_sentence, x_test_sentence, final_embeddings.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To save memory\n",
    "del x_train_sentence\n",
    "del x_test_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 2063)\n",
      "(?, 2063, 200, 1)\n",
      "CNN filter (4, 200, 1, 200)\n",
      "CNN filter (3, 1, 200, 200)\n",
      "(?, 1027, 1, 200)\n",
      "h_pool_flat (?, 200)\n",
      "scores (?, 3)\n",
      "predictions (?,)\n",
      "losses (?,)\n"
     ]
    }
   ],
   "source": [
    "# put Word2Vec on 590 million English Tweets using 52 dimensions.\n",
    "vocabulary_size = final_embeddings.shape[0]\n",
    "embedding_size = 200  # Dimension of the embedding vector.\n",
    "graph = tf.Graph()\n",
    "\n",
    "sequence_length=x_train_distance.shape[1]\n",
    "num_classes=3\n",
    "\n",
    "# filter_sizes: The number of words we want our convolutional filters to cover. \n",
    "# We will have num_filters for each size specified here. \n",
    "# For example, [3, 4, 5] means that we will have filters that slide over 3, 4 and 5 words respectively, for a total of 3 * num_filters filters.\n",
    "first_filter_sizes = [4]\n",
    "first_pool_window_sizes = [4]\n",
    "first_pool_strides = [2]\n",
    "\n",
    "\n",
    "second_filter_window_sizes = [3]\n",
    "num_filters = 200\n",
    "\n",
    "# No L2 norm\n",
    "l2_reg_lambda=0.0\n",
    "\n",
    "with graph.as_default():\n",
    "    input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "    input_y = tf.placeholder(tf.int64, [None], name=\"input_y\")\n",
    "    with tf.device('/cpu:0'):\n",
    "        embeddings = tf.Variable(tf.constant(0.0, shape=[vocabulary_size, embedding_size]),\n",
    "                        trainable=False, name=\"embedding\")\n",
    "\n",
    "        embedding_placeholder = tf.placeholder(tf.float32, [vocabulary_size, embedding_size], name='word_embedding_placeholder')\n",
    "        embedding_init = embeddings.assign(embedding_placeholder)  # assign exist word embeddings\n",
    "\n",
    "        embedded_chars = tf.nn.embedding_lookup(embeddings, input_x)\n",
    "    embedded_chars_expanded = tf.expand_dims(embedded_chars, -1)\n",
    "    print(input_x.shape)\n",
    "    print(embedded_chars_expanded.shape)\n",
    "    # Keeping track of l2 regularization loss (optional)\n",
    "    l2_loss = tf.constant(0.0)\n",
    "    \n",
    "     # Add variable initializer.\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    \n",
    "    # Create first cnn : a convolution + maxpool layer for each filter size    \n",
    "    # 1st Convolution Layer\n",
    "    for i, first_filter_size in enumerate(first_filter_sizes):\n",
    "        with tf.name_scope(\"conv-maxpool-1\"):\n",
    "            # Convolution Layer\n",
    "            filter_shape = [first_filter_size, embedding_size, 1, num_filters]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "            print(\"CNN filter\", W.shape)\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "            conv = tf.nn.conv2d(\n",
    "                embedded_chars_expanded,\n",
    "                W,\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding=\"VALID\",\n",
    "                name=\"conv\")\n",
    "            # Apply nonlinearity\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "            # Maxpooling over the outputs and transform for next layer, so the \"channel\" of convolution \n",
    "            # will become \"input_width\" for next layer\n",
    "#             pooled = tf.transpose(tf.nn.max_pool(\n",
    "#                 h,\n",
    "#                 ksize=[1, first_pool_window_sizes[i], 1, 1],\n",
    "#                 strides=[1, first_pool_strides[i], 1, 1],\n",
    "#                 padding='VALID',\n",
    "#                 name=\"pool\"), perm=[0, 1, 3, 2])\n",
    "            pooled = tf.nn.max_pool(\n",
    "                h,\n",
    "                ksize=[1, first_pool_window_sizes[i], 1, 1],\n",
    "                strides=[1, first_pool_strides[i], 1, 1],\n",
    "                padding='VALID',\n",
    "                name=\"pool\")\n",
    "#     print(\"conv1\", conv.shape)\n",
    "#     print(\"h1\", h.shape)\n",
    "#     print(\"pooled1\", pooled_1.shape)\n",
    "    \n",
    "    # 2nd Convolutional Layer\n",
    "#     for i, second_filter_size in enumerate(second_filter_window_sizes):\n",
    "#         with tf.name_scope(\"conv-maxpool-2\"):\n",
    "#             # Convolution Layer\n",
    "#             filter_shape = [second_filter_size, num_filters, 1, num_filters]\n",
    "#             W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "#             print(\"CNN filter\", W.shape)\n",
    "#             b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "#             conv = tf.nn.conv2d(\n",
    "#                 pooled,\n",
    "#                 W,\n",
    "#                 strides=[1, 1, 1, 1],\n",
    "#                 padding=\"VALID\",\n",
    "#                 name=\"conv\")\n",
    "#             # Apply nonlinearity\n",
    "#             h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "#             # Maxpooling over the outputs and transform for next layer, so the \"channel\" of convolution \n",
    "#             # will become \"input_width\" for next layer\n",
    "#             pooled = tf.nn.max_pool(\n",
    "#                 h,\n",
    "#                 ksize=[1, h.shape[1], 1, 1],\n",
    "#                 strides=[1, 1, 1, 1],\n",
    "#                 padding='VALID',\n",
    "#                 name=\"pool\")\n",
    "    for i, second_filter_size in enumerate(second_filter_window_sizes):\n",
    "        with tf.name_scope(\"conv-maxpool-2\"):\n",
    "            # Convolution Layer\n",
    "            filter_shape = [second_filter_size, 1, num_filters, num_filters]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "            print(\"CNN filter\", W.shape)\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "            conv = tf.nn.conv2d(\n",
    "                pooled,\n",
    "                W,\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding=\"VALID\",\n",
    "                name=\"conv\")\n",
    "            # Apply nonlinearity\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "            print(h.shape)\n",
    "            # Maxpooling over the outputs and transform for next layer, so the \"channel\" of convolution \n",
    "            # will become \"input_width\" for next layer\n",
    "            pooled = tf.nn.max_pool(\n",
    "                h,\n",
    "                ksize=[1, h.shape[1], 1, 1],\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding='VALID',\n",
    "                name=\"pool\")\n",
    " \n",
    "\n",
    "    h_pool_flat = tf.reshape(pooled, [-1, num_filters])  # flatten pooling layers\n",
    "    print(\"h_pool_flat\", h_pool_flat.shape)\n",
    "    \n",
    "    # Add dropout\n",
    "#     with tf.name_scope(\"dropout\"):\n",
    "#         self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "    # Final (unnormalized) scores and predictions\n",
    "    \n",
    "    # Fully connected hidden layer\n",
    "    with tf.name_scope(\"hidden\"):\n",
    "        with tf.variable_scope(\"hidden\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters, num_filters],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            out = tf.nn.relu(tf.nn.xw_plus_b(h_pool_flat, W, b))\n",
    "        \n",
    "    \n",
    "    with tf.name_scope(\"output\"):\n",
    "        with tf.variable_scope(\"output\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            scores = tf.nn.xw_plus_b(out, W, b, name=\"scores\")\n",
    "            print(\"scores\", scores.shape)\n",
    "            predictions = tf.argmax(scores, 1, name=\"predictions\")\n",
    "            print(\"predictions\", predictions.shape)\n",
    "\n",
    "\n",
    "    # Calculate mean cross-entropy loss\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=scores, labels=input_y)\n",
    "        print(\"losses\", losses.shape)\n",
    "        loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "    # Accuracy\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        correct_predictions = tf.equal(predictions, input_y)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-1/W:0/grad/hist is illegal; using conv-maxpool-1/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-1/W:0/grad/sparsity is illegal; using conv-maxpool-1/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-1/b:0/grad/hist is illegal; using conv-maxpool-1/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-1/b:0/grad/sparsity is illegal; using conv-maxpool-1/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/W:0/grad/hist is illegal; using conv-maxpool-2/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/W:0/grad/sparsity is illegal; using conv-maxpool-2/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/b:0/grad/hist is illegal; using conv-maxpool-2/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/b:0/grad/sparsity is illegal; using conv-maxpool-2/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name hidden/W:0/grad/hist is illegal; using hidden/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name hidden/W:0/grad/sparsity is illegal; using hidden/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name hidden/hidden/b:0/grad/hist is illegal; using hidden/hidden/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name hidden/hidden/b:0/grad/sparsity is illegal; using hidden/hidden/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/W:0/grad/hist is illegal; using output/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/W:0/grad/sparsity is illegal; using output/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/output/b:0/grad/hist is illegal; using output/output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/output/b:0/grad/sparsity is illegal; using output/output/b_0/grad/sparsity instead.\n",
      "Writing to /home/phejimlin/Documents/Machine-learning/Milestone2/runs/1510905653\n",
      "\n",
      "Current epoch:  0\n",
      "Current epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phejimlin/anaconda3/envs/tensorflow_3.5/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-17T16:01:04.244588: step 5, loss 0.495323, acc 0.845703, f1 0.775004\n",
      "2017-11-17T16:01:09.962854: step 10, loss 0.466426, acc 0.824219, f1 0.748574\n",
      "2017-11-17T16:01:15.749512: step 15, loss 0.380859, acc 0.863281, f1 0.803654\n",
      "2017-11-17T16:01:21.526436: step 20, loss 0.407394, acc 0.835938, f1 0.7684\n",
      "2017-11-17T16:01:27.289647: step 25, loss 0.428285, acc 0.824219, f1 0.753678\n",
      "2017-11-17T16:01:33.035461: step 30, loss 0.341089, acc 0.876953, f1 0.848807\n",
      "2017-11-17T16:01:38.798762: step 35, loss 0.398212, acc 0.861328, f1 0.830692\n",
      "2017-11-17T16:01:44.532571: step 40, loss 0.369266, acc 0.849609, f1 0.803052\n",
      "2017-11-17T16:01:50.255929: step 45, loss 0.426645, acc 0.832031, f1 0.830584\n",
      "2017-11-17T16:01:56.039244: step 50, loss 0.397883, acc 0.851562, f1 0.796418\n",
      "\n",
      "Evaluation:\n",
      "loss 0.359244, acc 0.867456, f1 0.843437\n",
      "\n",
      "2017-11-17T16:02:23.728139: step 55, loss 0.291955, acc 0.908203, f1 0.896704\n",
      "2017-11-17T16:02:29.470398: step 60, loss 0.385201, acc 0.855469, f1 0.792562\n",
      "2017-11-17T16:02:35.276675: step 65, loss 0.331749, acc 0.886719, f1 0.875433\n",
      "2017-11-17T16:02:41.036960: step 70, loss 0.334127, acc 0.867188, f1 0.828737\n",
      "2017-11-17T16:02:46.815459: step 75, loss 0.358519, acc 0.855469, f1 0.843513\n",
      "2017-11-17T16:02:52.534824: step 80, loss 0.315, acc 0.873047, f1 0.842263\n",
      "2017-11-17T16:02:58.312006: step 85, loss 0.348636, acc 0.853516, f1 0.817701\n",
      "2017-11-17T16:03:04.112904: step 90, loss 0.381567, acc 0.839844, f1 0.847511\n",
      "2017-11-17T16:03:09.858711: step 95, loss 0.328251, acc 0.857422, f1 0.838759\n",
      "2017-11-17T16:03:15.632306: step 100, loss 0.336413, acc 0.869141, f1 0.866586\n",
      "\n",
      "Evaluation:\n",
      "loss 0.341159, acc 0.861401, f1 0.823086\n",
      "\n",
      "2017-11-17T16:03:43.620724: step 105, loss 0.325185, acc 0.867188, f1 0.835325\n",
      "2017-11-17T16:03:49.387554: step 110, loss 0.320132, acc 0.890625, f1 0.886673\n",
      "2017-11-17T16:03:55.259031: step 115, loss 0.368451, acc 0.859375, f1 0.807255\n",
      "2017-11-17T16:04:00.998898: step 120, loss 0.371963, acc 0.859375, f1 0.84209\n",
      "2017-11-17T16:04:06.736786: step 125, loss 0.296873, acc 0.880859, f1 0.870564\n",
      "2017-11-17T16:04:12.481834: step 130, loss 0.349393, acc 0.867188, f1 0.859444\n",
      "2017-11-17T16:04:18.317090: step 135, loss 0.294389, acc 0.882812, f1 0.855012\n",
      "2017-11-17T16:04:24.080670: step 140, loss 0.299403, acc 0.873047, f1 0.849091\n",
      "2017-11-17T16:04:29.804832: step 145, loss 0.301378, acc 0.890625, f1 0.890625\n",
      "2017-11-17T16:04:35.563505: step 150, loss 0.323885, acc 0.869141, f1 0.834725\n",
      "\n",
      "Evaluation:\n",
      "loss 0.299364, acc 0.889495, f1 0.880399\n",
      "\n",
      "2017-11-17T16:05:03.322038: step 155, loss 0.308732, acc 0.878906, f1 0.87405\n",
      "2017-11-17T16:05:09.074388: step 160, loss 0.322684, acc 0.884766, f1 0.850206\n",
      "2017-11-17T16:05:14.880596: step 165, loss 0.366809, acc 0.84375, f1 0.8078\n",
      "2017-11-17T16:05:20.682687: step 170, loss 0.312363, acc 0.880859, f1 0.867922\n",
      "2017-11-17T16:05:26.391551: step 175, loss 0.258335, acc 0.904297, f1 0.888101\n",
      "2017-11-17T16:05:32.102021: step 180, loss 0.307355, acc 0.884766, f1 0.878555\n",
      "2017-11-17T16:05:37.815876: step 185, loss 0.27, acc 0.894531, f1 0.880981\n",
      "2017-11-17T16:05:43.554251: step 190, loss 0.258224, acc 0.914062, f1 0.913064\n",
      "2017-11-17T16:05:49.277121: step 195, loss 0.400794, acc 0.847656, f1 0.802981\n",
      "2017-11-17T16:05:55.031875: step 200, loss 0.290853, acc 0.882812, f1 0.869427\n",
      "\n",
      "Evaluation:\n",
      "loss 0.27882, acc 0.893093, f1 0.88062\n",
      "\n",
      "2017-11-17T16:06:22.756649: step 205, loss 0.300491, acc 0.871094, f1 0.847396\n",
      "2017-11-17T16:06:28.509369: step 210, loss 0.298089, acc 0.875, f1 0.868215\n",
      "2017-11-17T16:06:34.240206: step 215, loss 0.237773, acc 0.919922, f1 0.908525\n",
      "2017-11-17T16:06:39.961399: step 220, loss 0.247617, acc 0.900391, f1 0.896822\n",
      "2017-11-17T16:06:45.705063: step 225, loss 0.308302, acc 0.892578, f1 0.884163\n",
      "2017-11-17T16:06:51.452751: step 230, loss 0.301884, acc 0.882812, f1 0.86311\n",
      "2017-11-17T16:06:57.208900: step 235, loss 0.304947, acc 0.882812, f1 0.882812\n",
      "2017-11-17T16:07:02.954193: step 240, loss 0.296185, acc 0.890625, f1 0.865439\n",
      "2017-11-17T16:07:08.769625: step 245, loss 0.262819, acc 0.900391, f1 0.895032\n",
      "2017-11-17T16:07:14.516688: step 250, loss 0.28047, acc 0.890625, f1 0.869633\n",
      "\n",
      "Evaluation:\n",
      "loss 0.279466, acc 0.89446, f1 0.889104\n",
      "\n",
      "2017-11-17T16:07:42.165331: step 255, loss 0.330249, acc 0.851562, f1 0.848889\n",
      "2017-11-17T16:07:47.950372: step 260, loss 0.32209, acc 0.873047, f1 0.848279\n",
      "2017-11-17T16:07:53.702043: step 265, loss 0.236663, acc 0.908203, f1 0.898677\n",
      "2017-11-17T16:07:59.490961: step 270, loss 0.270603, acc 0.894531, f1 0.883753\n",
      "2017-11-17T16:08:05.250450: step 275, loss 0.238307, acc 0.90625, f1 0.901741\n",
      "2017-11-17T16:08:11.044604: step 280, loss 0.31562, acc 0.869141, f1 0.860838\n",
      "2017-11-17T16:08:16.909510: step 285, loss 0.251545, acc 0.898438, f1 0.888903\n",
      "2017-11-17T16:08:22.637887: step 290, loss 0.292225, acc 0.894531, f1 0.884463\n",
      "2017-11-17T16:08:28.420212: step 295, loss 0.311849, acc 0.873047, f1 0.849479\n",
      "2017-11-17T16:08:34.164960: step 300, loss 0.271029, acc 0.886719, f1 0.875582\n",
      "\n",
      "Evaluation:\n",
      "loss 0.267923, acc 0.899554, f1 0.893244\n",
      "\n",
      "2017-11-17T16:09:02.073884: step 305, loss 0.252946, acc 0.904297, f1 0.895456\n",
      "2017-11-17T16:09:07.926307: step 310, loss 0.242324, acc 0.916016, f1 0.907689\n",
      "2017-11-17T16:09:13.662634: step 315, loss 0.272865, acc 0.896484, f1 0.878333\n",
      "2017-11-17T16:09:19.411787: step 320, loss 0.258369, acc 0.90625, f1 0.898082\n",
      "2017-11-17T16:09:25.202314: step 325, loss 0.242068, acc 0.921875, f1 0.921079\n",
      "2017-11-17T16:09:30.988157: step 330, loss 0.249332, acc 0.908203, f1 0.904647\n",
      "2017-11-17T16:09:36.738088: step 335, loss 0.247262, acc 0.902344, f1 0.894712\n",
      "2017-11-17T16:09:42.481686: step 340, loss 0.29836, acc 0.873047, f1 0.869129\n",
      "2017-11-17T16:09:48.265642: step 345, loss 0.221083, acc 0.904297, f1 0.889978\n",
      "2017-11-17T16:09:54.000985: step 350, loss 0.230332, acc 0.927734, f1 0.928291\n",
      "\n",
      "Evaluation:\n",
      "loss 0.315929, acc 0.879664, f1 0.854067\n",
      "\n",
      "2017-11-17T16:10:21.835812: step 355, loss 0.285877, acc 0.892578, f1 0.874154\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "num_epochs = 1\n",
    "\n",
    "num_checkpoints = 5\n",
    "print_train_every = 5\n",
    "evaluate_every = 50\n",
    "checkpoint_every = 10000000\n",
    "allow_soft_placement=True\n",
    "log_device_placement=False\n",
    "\n",
    "with graph.as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=allow_soft_placement,\n",
    "      log_device_placement=log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        \n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdadeltaOptimizer(1.0)\n",
    "        grads_and_vars = optimizer.compute_gradients(loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=num_checkpoints)\n",
    "\n",
    "#         # Write vocabulary\n",
    "#         vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              input_x: x_batch,\n",
    "              input_y: y_batch,\n",
    "            }\n",
    "            _, step, summaries, cur_loss, cur_accuracy, y_pred = sess.run(\n",
    "                [train_op, global_step, train_summary_op, loss, accuracy, predictions],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "#             print(y_pred)\n",
    "#             print(y_batch)\n",
    "            if step % print_train_every == 0:\n",
    "                f1 = f1_score(y_batch, y_pred, average = 'weighted')\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}, f1 {:g}\".format(time_str, step, cur_loss, cur_accuracy,\n",
    "                                                                     f1))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              input_x: x_batch,\n",
    "              input_y: y_batch,\n",
    "            }\n",
    "            step, summaries, cur_loss, cur_accuracy, y_pred = sess.run(\n",
    "                [global_step, dev_summary_op, loss, accuracy, predictions],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            f1 = f1_score(y_batch, y_pred, average = 'weighted')\n",
    "            print(\"Test\")\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}, f1 {:g}\".format(time_str, step, cur_loss, cur_accuracy,\n",
    "                                                                    f1))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "        \n",
    "        def dev_step_batch(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              input_x: x_batch,\n",
    "              input_y: y_batch,\n",
    "            }\n",
    "            step, summaries, cur_loss, cur_accuracy, y_pred = sess.run(\n",
    "                [global_step, dev_summary_op, loss, accuracy, predictions],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            f1 = f1_score(y_batch, y_pred, average = 'weighted')\n",
    "#             print(\"{}: step {}, loss {:g}, acc {:g}, f1 {:g}\".format(time_str, step, cur_loss, cur_accuracy,\n",
    "#                                                                     f1))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "            return cur_loss, cur_accuracy, f1\n",
    "        \n",
    "        \n",
    "        sess.run(embedding_init, feed_dict={embedding_placeholder: final_embeddings})\n",
    "        # Generate batches\n",
    "        batches = batch_iter(\n",
    "            list(zip(x_train_distance, y_train_)), batch_size, num_epochs)\n",
    "        \n",
    "        batches_test = list(batch_iter(\n",
    "            list(zip(x_test_distance, y_test)), batch_size, 1))\n",
    "        \n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            \n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                total_loss=0\n",
    "                total_f1=0\n",
    "                total_accuracy=0\n",
    "                len_of_batch = int(len(batches_test))\n",
    "                for batch_test in batches_test:\n",
    "                    x_batch_test, y_batch_test = zip(*batch_test)\n",
    "                    cur_loss, cur_accuracy, cur_f1 = dev_step_batch(x_batch_test, y_batch_test, writer=dev_summary_writer)\n",
    "                    total_loss+=cur_loss\n",
    "                    total_accuracy+=cur_accuracy\n",
    "                    total_f1+=cur_f1\n",
    "                print(\"loss {:g}, acc {:g}, f1 {:g}\".format(total_loss/len_of_batch, total_accuracy/len_of_batch, total_f1/len_of_batch))\n",
    "                print(\"\")\n",
    "            if current_step % checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "        final_embeddings = embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.10009229,  0.08579876, -0.12731791, ..., -0.15652488,\n",
       "        -0.10116389,  0.08370614],\n",
       "       [-0.02642334,  0.03184305, -0.1160032 , ..., -0.07465456,\n",
       "        -0.10345571,  0.12367946],\n",
       "       [-0.08056928,  0.03620725, -0.11454398, ..., -0.15909833,\n",
       "        -0.10029007,  0.11134482],\n",
       "       ..., \n",
       "       [ 0.17794977, -0.06076148,  0.00221153, ...,  0.2420754 ,\n",
       "         0.16043946, -0.24765149],\n",
       "       [ 0.05399185, -0.01826661, -0.06147144, ..., -0.03653212,\n",
       "        -0.21923123,  0.2452819 ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save embeddings\n",
    "# np.save('final_embeddings', final_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings from previous work\n",
    "final_embeddings = np.load('./final_embeddings.npy')\n",
    "word_dict = {}\n",
    "with open('./data/embed_tweets_en_200M_200D/vocabulary.pickle', 'rb') as myfile:\n",
    "    word_dict = pickle.load(myfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20632\n",
      "53\n",
      "[\"there's\", 'a', 'lot', 'of', 'stupid', '$', 'h', '!', 't', 'out', 'there', ',', 'but', 'polling', 'trump', 'v', 'kanye', 'west', 'may', 'take', 'the', 'cake', '.', 'all', 'i', 'can', 'think', 'to', 'say', 'is', ':', '#', '$', '%', '#', '$', '%', '$', '#', '%', '#', '$', '%', '#', '$', '%', '#', '$', '#', '$', '#', '$', '%']\n"
     ]
    }
   ],
   "source": [
    "#Load label data\n",
    "x_train_sentence, y_train, x_test_sentence, y_test = load_data_and_labels('./data/supervised_data/en_full.tsv.txt', './data/supervised_data/en_test.tsv')\n",
    "print(len(x_test_sentence))\n",
    "x_train, x_test = process_tweet(x_train_sentence, x_test_sentence, final_embeddings.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 53)\n",
      "(?, 53, 200, 1)\n",
      "CNN filter (4, 200, 1, 200)\n",
      "CNN filter (3, 1, 200, 200)\n",
      "(?, 22, 1, 200)\n",
      "h_pool_flat (?, 200)\n",
      "scores (?, 3)\n",
      "predictions (?,)\n",
      "losses (?,)\n"
     ]
    }
   ],
   "source": [
    "# put Word2Vec on 590 million English Tweets using 52 dimensions.\n",
    "vocabulary_size = final_embeddings.shape[0]\n",
    "embedding_size = 200  # Dimension of the embedding vector.\n",
    "graph = tf.Graph()\n",
    "\n",
    "sequence_length=x_train.shape[1]\n",
    "num_classes=3\n",
    "\n",
    "# filter_sizes: The number of words we want our convolutional filters to cover. \n",
    "# We will have num_filters for each size specified here. \n",
    "# For example, [3, 4, 5] means that we will have filters that slide over 3, 4 and 5 words respectively, for a total of 3 * num_filters filters.\n",
    "first_filter_sizes = [4]\n",
    "first_pool_window_sizes = [4]\n",
    "first_pool_strides = [2]\n",
    "\n",
    "\n",
    "second_filter_window_sizes = [3]\n",
    "num_filters = 200\n",
    "\n",
    "# No L2 norm\n",
    "l2_reg_lambda=0.0\n",
    "\n",
    "with graph.as_default():\n",
    "    input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "    input_y = tf.placeholder(tf.int64, [None], name=\"input_y\")\n",
    "    with tf.device('/cpu:0'):\n",
    "        embeddings = tf.Variable(tf.constant(0.0, shape=[vocabulary_size, embedding_size]),\n",
    "                        trainable=False, name=\"embedding\")\n",
    "\n",
    "        embedding_placeholder = tf.placeholder(tf.float32, [vocabulary_size, embedding_size], name='word_embedding_placeholder')\n",
    "        embedding_init = embeddings.assign(embedding_placeholder)  # assign exist word embeddings\n",
    "\n",
    "        embedded_chars = tf.nn.embedding_lookup(embeddings, input_x)\n",
    "    embedded_chars_expanded = tf.expand_dims(embedded_chars, -1)\n",
    "    print(input_x.shape)\n",
    "    print(embedded_chars_expanded.shape)\n",
    "    # Keeping track of l2 regularization loss (optional)\n",
    "    l2_loss = tf.constant(0.0)\n",
    "    \n",
    "     # Add variable initializer.\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    \n",
    "    # Create first cnn : a convolution + maxpool layer for each filter size    \n",
    "    # 1st Convolution Layer\n",
    "    for i, first_filter_size in enumerate(first_filter_sizes):\n",
    "        with tf.name_scope(\"conv-maxpool-1\"):\n",
    "            # Convolution Layer\n",
    "            filter_shape = [first_filter_size, embedding_size, 1, num_filters]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "            print(\"CNN filter\", W.shape)\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "            conv = tf.nn.conv2d(\n",
    "                embedded_chars_expanded,\n",
    "                W,\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding=\"VALID\",\n",
    "                name=\"conv\")\n",
    "            # Apply nonlinearity\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "            # Maxpooling over the outputs and transform for next layer, so the \"channel\" of convolution \n",
    "            # will become \"input_width\" for next layer\n",
    "#             pooled = tf.transpose(tf.nn.max_pool(\n",
    "#                 h,\n",
    "#                 ksize=[1, first_pool_window_sizes[i], 1, 1],\n",
    "#                 strides=[1, first_pool_strides[i], 1, 1],\n",
    "#                 padding='VALID',\n",
    "#                 name=\"pool\"), perm=[0, 1, 3, 2])\n",
    "            pooled = tf.nn.max_pool(\n",
    "                h,\n",
    "                ksize=[1, first_pool_window_sizes[i], 1, 1],\n",
    "                strides=[1, first_pool_strides[i], 1, 1],\n",
    "                padding='VALID',\n",
    "                name=\"pool\")\n",
    "#     print(\"conv1\", conv.shape)\n",
    "#     print(\"h1\", h.shape)\n",
    "#     print(\"pooled1\", pooled_1.shape)\n",
    "    \n",
    "    # 2nd Convolutional Layer\n",
    "#     for i, second_filter_size in enumerate(second_filter_window_sizes):\n",
    "#         with tf.name_scope(\"conv-maxpool-2\"):\n",
    "#             # Convolution Layer\n",
    "#             filter_shape = [second_filter_size, num_filters, 1, num_filters]\n",
    "#             W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "#             print(\"CNN filter\", W.shape)\n",
    "#             b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "#             conv = tf.nn.conv2d(\n",
    "#                 pooled,\n",
    "#                 W,\n",
    "#                 strides=[1, 1, 1, 1],\n",
    "#                 padding=\"VALID\",\n",
    "#                 name=\"conv\")\n",
    "#             # Apply nonlinearity\n",
    "#             h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "#             # Maxpooling over the outputs and transform for next layer, so the \"channel\" of convolution \n",
    "#             # will become \"input_width\" for next layer\n",
    "#             pooled = tf.nn.max_pool(\n",
    "#                 h,\n",
    "#                 ksize=[1, h.shape[1], 1, 1],\n",
    "#                 strides=[1, 1, 1, 1],\n",
    "#                 padding='VALID',\n",
    "#                 name=\"pool\")\n",
    "    for i, second_filter_size in enumerate(second_filter_window_sizes):\n",
    "        with tf.name_scope(\"conv-maxpool-2\"):\n",
    "            # Convolution Layer\n",
    "            filter_shape = [second_filter_size, 1, num_filters, num_filters]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "            print(\"CNN filter\", W.shape)\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "            conv = tf.nn.conv2d(\n",
    "                pooled,\n",
    "                W,\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding=\"VALID\",\n",
    "                name=\"conv\")\n",
    "            # Apply nonlinearity\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "            print(h.shape)\n",
    "            # Maxpooling over the outputs and transform for next layer, so the \"channel\" of convolution \n",
    "            # will become \"input_width\" for next layer\n",
    "            pooled = tf.nn.max_pool(\n",
    "                h,\n",
    "                ksize=[1, h.shape[1], 1, 1],\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding='VALID',\n",
    "                name=\"pool\")\n",
    " \n",
    "\n",
    "    h_pool_flat = tf.reshape(pooled, [-1, num_filters])  # flatten pooling layers\n",
    "    print(\"h_pool_flat\", h_pool_flat.shape)\n",
    "    \n",
    "    # Add dropout\n",
    "#     with tf.name_scope(\"dropout\"):\n",
    "#         self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "    # Final (unnormalized) scores and predictions\n",
    "    \n",
    "    # Fully connected hidden layer\n",
    "    with tf.name_scope(\"hidden\"):\n",
    "        with tf.variable_scope(\"hidden\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters, num_filters],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            out = tf.nn.relu(tf.nn.xw_plus_b(h_pool_flat, W, b))\n",
    "        \n",
    "    \n",
    "    with tf.name_scope(\"output\"):\n",
    "        with tf.variable_scope(\"output\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            scores = tf.nn.xw_plus_b(out, W, b, name=\"scores\")\n",
    "            print(\"scores\", scores.shape)\n",
    "            predictions = tf.argmax(scores, 1, name=\"predictions\")\n",
    "            print(\"predictions\", predictions.shape)\n",
    "\n",
    "\n",
    "    # Calculate mean cross-entropy loss\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=scores, labels=input_y)\n",
    "        print(\"losses\", losses.shape)\n",
    "        loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "    # Accuracy\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        correct_predictions = tf.equal(predictions, input_y)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.06243195979268534&quot;).pbtxt = 'node {\\n  name: &quot;input_x&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: -1\\n        }\\n        dim {\\n          size: 2063\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;input_y&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: -1\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Const&quot;\\n  op: &quot;Const&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 1859185\\n          }\\n          dim {\\n            size: 200\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;embedding&quot;\\n  op: &quot;VariableV2&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 1859185\\n        }\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;embedding/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;embedding&quot;\\n  input: &quot;Const&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;embedding/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;embedding&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;word_embedding_placeholder&quot;\\n  op: &quot;Placeholder&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 1859185\\n        }\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;embedding&quot;\\n  input: &quot;word_embedding_placeholder&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;embedding_lookup&quot;\\n  op: &quot;Gather&quot;\\n  input: &quot;embedding/read&quot;\\n  input: &quot;input_x&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;Tindices&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tparams&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_indices&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ExpandDims/dim&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: -1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ExpandDims&quot;\\n  op: &quot;ExpandDims&quot;\\n  input: &quot;embedding_lookup&quot;\\n  input: &quot;ExpandDims/dim&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tdim&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Const_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;init&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^embedding/Assign&quot;\\n  device: &quot;/device:CPU:0&quot;\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/truncated_normal/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\004\\\\000\\\\000\\\\000\\\\310\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\310\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/truncated_normal/mean&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/truncated_normal/stddev&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.10000000149011612\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/truncated_normal/TruncatedNormal&quot;\\n  op: &quot;TruncatedNormal&quot;\\n  input: &quot;conv-maxpool-1/truncated_normal/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/truncated_normal/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;conv-maxpool-1/truncated_normal/TruncatedNormal&quot;\\n  input: &quot;conv-maxpool-1/truncated_normal/stddev&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/truncated_normal&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;conv-maxpool-1/truncated_normal/mul&quot;\\n  input: &quot;conv-maxpool-1/truncated_normal/mean&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/W&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 4\\n        }\\n        dim {\\n          size: 200\\n        }\\n        dim {\\n          size: 1\\n        }\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/W/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;conv-maxpool-1/W&quot;\\n  input: &quot;conv-maxpool-1/truncated_normal&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-1/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/W/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;conv-maxpool-1/W&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-1/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 200\\n          }\\n        }\\n        float_val: 0.10000000149011612\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/b&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/b/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;conv-maxpool-1/b&quot;\\n  input: &quot;conv-maxpool-1/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-1/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/b/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;conv-maxpool-1/b&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-1/b&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/conv&quot;\\n  op: &quot;Conv2D&quot;\\n  input: &quot;ExpandDims&quot;\\n  input: &quot;conv-maxpool-1/W/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;VALID&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;conv-maxpool-1/conv&quot;\\n  input: &quot;conv-maxpool-1/b/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;conv-maxpool-1/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/pool&quot;\\n  op: &quot;MaxPool&quot;\\n  input: &quot;conv-maxpool-1/relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;ksize&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 4\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;VALID&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 2\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/truncated_normal/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\003\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\310\\\\000\\\\000\\\\000\\\\310\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/truncated_normal/mean&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/truncated_normal/stddev&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.10000000149011612\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/truncated_normal/TruncatedNormal&quot;\\n  op: &quot;TruncatedNormal&quot;\\n  input: &quot;conv-maxpool-2/truncated_normal/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/truncated_normal/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;conv-maxpool-2/truncated_normal/TruncatedNormal&quot;\\n  input: &quot;conv-maxpool-2/truncated_normal/stddev&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/truncated_normal&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;conv-maxpool-2/truncated_normal/mul&quot;\\n  input: &quot;conv-maxpool-2/truncated_normal/mean&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/W&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 3\\n        }\\n        dim {\\n          size: 1\\n        }\\n        dim {\\n          size: 200\\n        }\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/W/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;conv-maxpool-2/W&quot;\\n  input: &quot;conv-maxpool-2/truncated_normal&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-2/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/W/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;conv-maxpool-2/W&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-2/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 200\\n          }\\n        }\\n        float_val: 0.10000000149011612\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/b&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/b/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;conv-maxpool-2/b&quot;\\n  input: &quot;conv-maxpool-2/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-2/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/b/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;conv-maxpool-2/b&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-2/b&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/conv&quot;\\n  op: &quot;Conv2D&quot;\\n  input: &quot;conv-maxpool-1/pool&quot;\\n  input: &quot;conv-maxpool-2/W/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;VALID&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;conv-maxpool-2/conv&quot;\\n  input: &quot;conv-maxpool-2/b/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;conv-maxpool-2/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/pool&quot;\\n  op: &quot;MaxPool&quot;\\n  input: &quot;conv-maxpool-2/relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;ksize&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1027\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;VALID&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\310\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;conv-maxpool-2/pool&quot;\\n  input: &quot;Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\310\\\\000\\\\000\\\\000\\\\310\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.12247448414564133\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.12247448414564133\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform/max&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform/mul&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 200\\n        }\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden/W&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden/W&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 200\\n          }\\n        }\\n        float_val: 0.10000000149011612\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/b&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/b/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden/hidden/b&quot;\\n  input: &quot;hidden/hidden/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/hidden/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/b/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden/hidden/b&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/hidden/b&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/L2Loss&quot;\\n  op: &quot;L2Loss&quot;\\n  input: &quot;hidden/W/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Const_1&quot;\\n  input: &quot;hidden/hidden/L2Loss&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/L2Loss_1&quot;\\n  op: &quot;L2Loss&quot;\\n  input: &quot;hidden/hidden/b/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/add_1&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden/hidden/add&quot;\\n  input: &quot;hidden/hidden/L2Loss_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/xw_plus_b/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Reshape&quot;\\n  input: &quot;hidden/W/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/xw_plus_b&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;hidden/hidden/xw_plus_b/MatMul&quot;\\n  input: &quot;hidden/hidden/b/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;hidden/hidden/xw_plus_b&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\310\\\\000\\\\000\\\\000\\\\003\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.17192047834396362\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.17192047834396362\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;output/W/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;output/W/Initializer/random_uniform/max&quot;\\n  input: &quot;output/W/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;output/W/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;output/W/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;output/W/Initializer/random_uniform/mul&quot;\\n  input: &quot;output/W/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 200\\n        }\\n        dim {\\n          size: 3\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;output/W&quot;\\n  input: &quot;output/W/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;output/W&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        float_val: 0.10000000149011612\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/b&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 3\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/b/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;output/output/b&quot;\\n  input: &quot;output/output/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/output/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/b/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;output/output/b&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/output/b&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/L2Loss&quot;\\n  op: &quot;L2Loss&quot;\\n  input: &quot;output/W/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden/hidden/add_1&quot;\\n  input: &quot;output/output/L2Loss&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/L2Loss_1&quot;\\n  op: &quot;L2Loss&quot;\\n  input: &quot;output/output/b/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/add_1&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;output/output/add&quot;\\n  input: &quot;output/output/L2Loss_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/scores/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;hidden/hidden/Relu&quot;\\n  input: &quot;output/W/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/scores&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;output/output/scores/MatMul&quot;\\n  input: &quot;output/output/b/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/predictions/dimension&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/predictions&quot;\\n  op: &quot;ArgMax&quot;\\n  input: &quot;output/output/scores&quot;\\n  input: &quot;output/output/predictions/dimension&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;output_type&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;input_y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  op: &quot;SparseSoftmaxCrossEntropyWithLogits&quot;\\n  input: &quot;output/output/scores&quot;\\n  input: &quot;input_y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tlabels&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/Mean&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  input: &quot;loss/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/mul/x&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;loss/mul/x&quot;\\n  input: &quot;output/output/add_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;loss/Mean&quot;\\n  input: &quot;loss/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;accuracy/Equal&quot;\\n  op: &quot;Equal&quot;\\n  input: &quot;output/output/predictions&quot;\\n  input: &quot;input_y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;accuracy/Cast&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;accuracy/Equal&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_BOOL\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;accuracy/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;accuracy/accuracy&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;accuracy/Cast&quot;\\n  input: &quot;accuracy/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nversions {\\n  producer: 24\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.06243195979268534&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_graph(graph.as_graph_def())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-1/W:0/grad/hist is illegal; using conv-maxpool-1/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-1/W:0/grad/sparsity is illegal; using conv-maxpool-1/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-1/b:0/grad/hist is illegal; using conv-maxpool-1/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-1/b:0/grad/sparsity is illegal; using conv-maxpool-1/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/W:0/grad/hist is illegal; using conv-maxpool-2/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/W:0/grad/sparsity is illegal; using conv-maxpool-2/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/b:0/grad/hist is illegal; using conv-maxpool-2/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/b:0/grad/sparsity is illegal; using conv-maxpool-2/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name hidden/W:0/grad/hist is illegal; using hidden/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name hidden/W:0/grad/sparsity is illegal; using hidden/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name hidden/hidden/b:0/grad/hist is illegal; using hidden/hidden/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name hidden/hidden/b:0/grad/sparsity is illegal; using hidden/hidden/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/W:0/grad/hist is illegal; using output/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/W:0/grad/sparsity is illegal; using output/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/output/b:0/grad/hist is illegal; using output/output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/output/b:0/grad/sparsity is illegal; using output/output/b_0/grad/sparsity instead.\n",
      "Writing to /home/phejimlin/Documents/Machine-learning/Milestone2/runs/1510924906\n",
      "\n",
      "Current epoch:  0\n",
      "Current epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phejimlin/anaconda3/envs/tensorflow_3.5/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-17T21:21:51.801606: step 5, loss 1.10649, acc 0.426758, f1 0.374303\n",
      "2017-11-17T21:21:52.311735: step 10, loss 1.00613, acc 0.461914, f1 0.419631\n",
      "2017-11-17T21:21:52.803264: step 15, loss 0.989656, acc 0.481445, f1 0.44152\n",
      "Current epoch:  1\n",
      "2017-11-17T21:21:53.310189: step 20, loss 0.964587, acc 0.498047, f1 0.455241\n",
      "2017-11-17T21:21:53.828340: step 25, loss 0.962968, acc 0.499023, f1 0.428204\n",
      "2017-11-17T21:21:54.332194: step 30, loss 0.983904, acc 0.484375, f1 0.40822\n",
      "2017-11-17T21:21:54.820674: step 35, loss 0.984431, acc 0.472656, f1 0.376734\n",
      "Current epoch:  2\n",
      "2017-11-17T21:21:55.293309: step 40, loss 0.973821, acc 0.498047, f1 0.443258\n",
      "2017-11-17T21:21:55.773916: step 45, loss 0.931473, acc 0.544922, f1 0.490308\n",
      "2017-11-17T21:21:56.295527: step 50, loss 0.955362, acc 0.53418, f1 0.526567\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:21:56.897350: step 50, loss 1.01432, acc 0.456087, f1 0.394521\n",
      "\n",
      "Current epoch:  3\n",
      "2017-11-17T21:21:57.420306: step 55, loss 0.983732, acc 0.496094, f1 0.406664\n",
      "2017-11-17T21:21:57.954680: step 60, loss 0.952879, acc 0.50293, f1 0.43122\n",
      "2017-11-17T21:21:58.499774: step 65, loss 0.977791, acc 0.477539, f1 0.384479\n",
      "2017-11-17T21:21:59.036853: step 70, loss 0.955422, acc 0.507812, f1 0.433071\n",
      "Current epoch:  4\n",
      "2017-11-17T21:21:59.515997: step 75, loss 0.913327, acc 0.557617, f1 0.511822\n",
      "2017-11-17T21:22:00.032885: step 80, loss 0.888424, acc 0.569336, f1 0.543703\n",
      "2017-11-17T21:22:00.493596: step 85, loss 0.927454, acc 0.555664, f1 0.549825\n",
      "2017-11-17T21:22:00.942414: step 90, loss 0.930225, acc 0.534591, f1 0.449237\n",
      "Current epoch:  5\n",
      "2017-11-17T21:22:01.449889: step 95, loss 0.946896, acc 0.475586, f1 0.370769\n",
      "2017-11-17T21:22:01.952661: step 100, loss 0.910516, acc 0.515625, f1 0.446027\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:22:02.546479: step 100, loss 0.882799, acc 0.571442, f1 0.509076\n",
      "\n",
      "2017-11-17T21:22:03.098927: step 105, loss 0.885446, acc 0.549805, f1 0.513341\n",
      "Current epoch:  6\n",
      "2017-11-17T21:22:03.559221: step 110, loss 0.872968, acc 0.547852, f1 0.492741\n",
      "2017-11-17T21:22:04.046599: step 115, loss 0.927005, acc 0.550781, f1 0.554462\n",
      "2017-11-17T21:22:04.547404: step 120, loss 0.896972, acc 0.549805, f1 0.486554\n",
      "2017-11-17T21:22:05.053789: step 125, loss 0.93496, acc 0.498047, f1 0.433591\n",
      "Current epoch:  7\n",
      "2017-11-17T21:22:05.561883: step 130, loss 0.932834, acc 0.489258, f1 0.416747\n",
      "2017-11-17T21:22:06.072764: step 135, loss 0.853614, acc 0.595703, f1 0.569277\n",
      "2017-11-17T21:22:06.578527: step 140, loss 0.879659, acc 0.59375, f1 0.602369\n",
      "Current epoch:  8\n",
      "2017-11-17T21:22:07.078671: step 145, loss 0.84621, acc 0.560547, f1 0.503476\n",
      "2017-11-17T21:22:07.582682: step 150, loss 0.939406, acc 0.521484, f1 0.46347\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:22:08.172216: step 150, loss 0.981641, acc 0.447606, f1 0.385488\n",
      "\n",
      "2017-11-17T21:22:08.711542: step 155, loss 0.83754, acc 0.583984, f1 0.54748\n",
      "2017-11-17T21:22:09.254290: step 160, loss 0.893293, acc 0.545898, f1 0.519207\n",
      "Current epoch:  9\n",
      "2017-11-17T21:22:09.725661: step 165, loss 0.832154, acc 0.608398, f1 0.594013\n",
      "2017-11-17T21:22:10.204688: step 170, loss 0.915628, acc 0.547852, f1 0.477742\n",
      "2017-11-17T21:22:10.664797: step 175, loss 0.834743, acc 0.609375, f1 0.602856\n",
      "2017-11-17T21:22:11.113615: step 180, loss 0.893095, acc 0.559748, f1 0.547516\n",
      "Current epoch:  10\n",
      "2017-11-17T21:22:11.607859: step 185, loss 0.850939, acc 0.582031, f1 0.525831\n",
      "2017-11-17T21:22:12.103001: step 190, loss 0.823216, acc 0.594727, f1 0.566519\n",
      "2017-11-17T21:22:12.602007: step 195, loss 0.833904, acc 0.599609, f1 0.583578\n",
      "Current epoch:  11\n",
      "2017-11-17T21:22:13.086328: step 200, loss 0.84199, acc 0.617188, f1 0.572288\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:22:13.679900: step 200, loss 0.847976, acc 0.60314, f1 0.601576\n",
      "\n",
      "2017-11-17T21:22:14.185810: step 205, loss 0.887432, acc 0.574219, f1 0.546551\n",
      "2017-11-17T21:22:14.711857: step 210, loss 0.833112, acc 0.59082, f1 0.548435\n",
      "2017-11-17T21:22:15.280179: step 215, loss 0.794304, acc 0.668945, f1 0.660596\n",
      "Current epoch:  12\n",
      "2017-11-17T21:22:15.766371: step 220, loss 0.806156, acc 0.613281, f1 0.59504\n",
      "2017-11-17T21:22:16.245299: step 225, loss 0.840378, acc 0.591797, f1 0.544819\n",
      "2017-11-17T21:22:16.725760: step 230, loss 0.907481, acc 0.513672, f1 0.440804\n",
      "Current epoch:  13\n",
      "2017-11-17T21:22:17.196260: step 235, loss 0.768866, acc 0.640625, f1 0.623133\n",
      "2017-11-17T21:22:17.702925: step 240, loss 0.84322, acc 0.608398, f1 0.613057\n",
      "2017-11-17T21:22:18.241804: step 245, loss 0.843195, acc 0.608398, f1 0.567391\n",
      "2017-11-17T21:22:18.747710: step 250, loss 0.793807, acc 0.651367, f1 0.652897\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:22:19.354530: step 250, loss 0.879091, acc 0.555738, f1 0.533074\n",
      "\n",
      "Current epoch:  14\n",
      "2017-11-17T21:22:19.880095: step 255, loss 0.854946, acc 0.569336, f1 0.512197\n",
      "2017-11-17T21:22:20.388693: step 260, loss 0.752083, acc 0.675781, f1 0.673318\n",
      "2017-11-17T21:22:20.955902: step 265, loss 0.818781, acc 0.604492, f1 0.576785\n",
      "2017-11-17T21:22:21.444288: step 270, loss 0.871146, acc 0.548742, f1 0.50404\n",
      "Current epoch:  15\n",
      "2017-11-17T21:22:21.961447: step 275, loss 0.78142, acc 0.642578, f1 0.638365\n",
      "2017-11-17T21:22:22.446138: step 280, loss 0.79955, acc 0.628906, f1 0.59115\n",
      "2017-11-17T21:22:22.944714: step 285, loss 0.820731, acc 0.615234, f1 0.605734\n",
      "Current epoch:  16\n",
      "2017-11-17T21:22:23.403755: step 290, loss 0.820505, acc 0.59375, f1 0.561116\n",
      "2017-11-17T21:22:23.926591: step 295, loss 0.864015, acc 0.541016, f1 0.50176\n",
      "2017-11-17T21:22:24.423220: step 300, loss 0.787056, acc 0.643555, f1 0.642333\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:22:25.012671: step 300, loss 0.823321, acc 0.614918, f1 0.58975\n",
      "\n",
      "2017-11-17T21:22:25.506297: step 305, loss 0.828868, acc 0.59668, f1 0.551028\n",
      "Current epoch:  17\n",
      "2017-11-17T21:22:25.999911: step 310, loss 0.762883, acc 0.625977, f1 0.594206\n",
      "2017-11-17T21:22:26.467589: step 315, loss 0.778333, acc 0.654297, f1 0.649937\n",
      "2017-11-17T21:22:26.965674: step 320, loss 0.776422, acc 0.629883, f1 0.59682\n",
      "Current epoch:  18\n",
      "2017-11-17T21:22:27.446013: step 325, loss 0.816319, acc 0.611328, f1 0.601451\n",
      "2017-11-17T21:22:27.962031: step 330, loss 0.77483, acc 0.654297, f1 0.640016\n",
      "2017-11-17T21:22:28.449079: step 335, loss 0.72967, acc 0.651367, f1 0.633452\n",
      "2017-11-17T21:22:28.944700: step 340, loss 0.736975, acc 0.668945, f1 0.664675\n",
      "Current epoch:  19\n",
      "2017-11-17T21:22:29.378004: step 345, loss 0.788823, acc 0.589844, f1 0.565065\n",
      "2017-11-17T21:22:29.846210: step 350, loss 0.825618, acc 0.617188, f1 0.56273\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:22:30.453825: step 350, loss 0.824364, acc 0.615209, f1 0.605613\n",
      "\n",
      "2017-11-17T21:22:30.969387: step 355, loss 0.768368, acc 0.633789, f1 0.622573\n",
      "2017-11-17T21:22:31.434876: step 360, loss 0.792097, acc 0.606918, f1 0.567568\n",
      "Current epoch:  20\n",
      "2017-11-17T21:22:31.969971: step 365, loss 0.811679, acc 0.630859, f1 0.631031\n",
      "2017-11-17T21:22:32.446308: step 370, loss 0.734912, acc 0.660156, f1 0.627164\n",
      "2017-11-17T21:22:32.947506: step 375, loss 0.734759, acc 0.670898, f1 0.653904\n",
      "Current epoch:  21\n",
      "2017-11-17T21:22:33.420102: step 380, loss 0.75017, acc 0.642578, f1 0.632472\n",
      "2017-11-17T21:22:33.939431: step 385, loss 0.807076, acc 0.601562, f1 0.570753\n",
      "2017-11-17T21:22:34.418696: step 390, loss 0.728189, acc 0.689453, f1 0.69206\n",
      "2017-11-17T21:22:34.872064: step 395, loss 0.696035, acc 0.660156, f1 0.632628\n",
      "Current epoch:  22\n",
      "2017-11-17T21:22:35.371312: step 400, loss 0.828087, acc 0.572266, f1 0.519579\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:22:35.960761: step 400, loss 0.907998, acc 0.565723, f1 0.496014\n",
      "\n",
      "2017-11-17T21:22:36.464433: step 405, loss 0.723444, acc 0.672852, f1 0.646977\n",
      "2017-11-17T21:22:36.970082: step 410, loss 0.729154, acc 0.672852, f1 0.671833\n",
      "Current epoch:  23\n",
      "2017-11-17T21:22:37.404278: step 415, loss 0.745707, acc 0.642578, f1 0.612138\n",
      "2017-11-17T21:22:37.900008: step 420, loss 0.815376, acc 0.563477, f1 0.514868\n",
      "2017-11-17T21:22:38.426194: step 425, loss 0.706741, acc 0.710938, f1 0.713592\n",
      "2017-11-17T21:22:38.932395: step 430, loss 0.752698, acc 0.679688, f1 0.641745\n",
      "Current epoch:  24\n",
      "2017-11-17T21:22:39.398870: step 435, loss 0.707622, acc 0.700195, f1 0.701048\n",
      "2017-11-17T21:22:39.874340: step 440, loss 0.719293, acc 0.670898, f1 0.650186\n",
      "2017-11-17T21:22:40.366688: step 445, loss 0.856742, acc 0.542969, f1 0.471883\n",
      "2017-11-17T21:22:40.828967: step 450, loss 0.741213, acc 0.683962, f1 0.689154\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-17T21:22:41.437832: step 450, loss 0.866366, acc 0.587388, f1 0.557674\n",
      "\n",
      "Current epoch:  25\n",
      "2017-11-17T21:22:41.938905: step 455, loss 0.722706, acc 0.650391, f1 0.622195\n",
      "2017-11-17T21:22:42.411744: step 460, loss 0.684525, acc 0.682617, f1 0.676811\n",
      "2017-11-17T21:22:42.927911: step 465, loss 0.750825, acc 0.62793, f1 0.60354\n",
      "Current epoch:  26\n",
      "2017-11-17T21:22:43.380740: step 470, loss 0.707442, acc 0.68457, f1 0.681439\n",
      "2017-11-17T21:22:43.879450: step 475, loss 0.83658, acc 0.633789, f1 0.577025\n",
      "2017-11-17T21:22:44.394479: step 480, loss 0.725508, acc 0.651367, f1 0.642725\n",
      "2017-11-17T21:22:44.886278: step 485, loss 0.722703, acc 0.655273, f1 0.626066\n",
      "Current epoch:  27\n",
      "2017-11-17T21:22:45.389441: step 490, loss 0.726678, acc 0.636719, f1 0.617678\n",
      "2017-11-17T21:22:45.897512: step 495, loss 0.672432, acc 0.697266, f1 0.680786\n",
      "2017-11-17T21:22:46.428056: step 500, loss 0.727178, acc 0.651367, f1 0.638572\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:22:47.017582: step 500, loss 0.926851, acc 0.517448, f1 0.487164\n",
      "\n",
      "Current epoch:  28\n",
      "2017-11-17T21:22:47.543683: step 505, loss 0.732807, acc 0.666992, f1 0.631607\n",
      "2017-11-17T21:22:48.036482: step 510, loss 0.699509, acc 0.724609, f1 0.730644\n",
      "2017-11-17T21:22:48.543645: step 515, loss 0.703965, acc 0.644531, f1 0.601793\n",
      "2017-11-17T21:22:49.035949: step 520, loss 0.700655, acc 0.699219, f1 0.690899\n",
      "Current epoch:  29\n",
      "2017-11-17T21:22:49.487503: step 525, loss 0.741867, acc 0.641602, f1 0.603864\n",
      "2017-11-17T21:22:49.993749: step 530, loss 0.759076, acc 0.591797, f1 0.545648\n",
      "2017-11-17T21:22:50.503359: step 535, loss 0.630183, acc 0.742188, f1 0.737652\n",
      "2017-11-17T21:22:50.987031: step 540, loss 0.763, acc 0.66195, f1 0.640508\n",
      "Current epoch:  30\n",
      "2017-11-17T21:22:51.504805: step 545, loss 0.758625, acc 0.62793, f1 0.587218\n",
      "2017-11-17T21:22:51.991874: step 550, loss 0.709933, acc 0.68457, f1 0.650657\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:22:52.595879: step 550, loss 0.860728, acc 0.587631, f1 0.594456\n",
      "\n",
      "2017-11-17T21:22:53.128354: step 555, loss 0.681351, acc 0.731445, f1 0.734035\n",
      "Current epoch:  31\n",
      "2017-11-17T21:22:53.644604: step 560, loss 0.663746, acc 0.692383, f1 0.67893\n",
      "2017-11-17T21:22:54.156982: step 565, loss 0.680395, acc 0.666992, f1 0.647435\n",
      "2017-11-17T21:22:54.712615: step 570, loss 0.635023, acc 0.742188, f1 0.743475\n",
      "2017-11-17T21:22:55.265403: step 575, loss 0.720699, acc 0.680664, f1 0.637443\n",
      "Current epoch:  32\n",
      "2017-11-17T21:22:55.802055: step 580, loss 0.671976, acc 0.682617, f1 0.670543\n",
      "2017-11-17T21:22:56.366229: step 585, loss 0.680237, acc 0.702148, f1 0.69121\n",
      "2017-11-17T21:22:56.930384: step 590, loss 0.727527, acc 0.685547, f1 0.645572\n",
      "Current epoch:  33\n",
      "2017-11-17T21:22:57.427349: step 595, loss 0.65968, acc 0.727539, f1 0.730266\n",
      "2017-11-17T21:22:57.945856: step 600, loss 0.575301, acc 0.772461, f1 0.766987\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:22:58.556987: step 600, loss 0.78856, acc 0.632318, f1 0.624755\n",
      "\n",
      "2017-11-17T21:22:59.078773: step 605, loss 0.73717, acc 0.617188, f1 0.579849\n",
      "2017-11-17T21:22:59.614764: step 610, loss 0.651269, acc 0.725586, f1 0.724532\n",
      "Current epoch:  34\n",
      "2017-11-17T21:23:00.072894: step 615, loss 0.691734, acc 0.657227, f1 0.635332\n",
      "2017-11-17T21:23:00.633819: step 620, loss 0.745418, acc 0.608398, f1 0.574215\n",
      "2017-11-17T21:23:01.160139: step 625, loss 0.594537, acc 0.777344, f1 0.774407\n",
      "2017-11-17T21:23:01.640467: step 630, loss 0.68584, acc 0.704403, f1 0.671349\n",
      "Current epoch:  35\n",
      "2017-11-17T21:23:02.113864: step 635, loss 0.631986, acc 0.728516, f1 0.720358\n",
      "2017-11-17T21:23:02.656667: step 640, loss 0.687666, acc 0.672852, f1 0.657722\n",
      "2017-11-17T21:23:03.205210: step 645, loss 0.693468, acc 0.716797, f1 0.67163\n",
      "Current epoch:  36\n",
      "2017-11-17T21:23:03.719860: step 650, loss 0.674269, acc 0.712891, f1 0.712178\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:23:04.306668: step 650, loss 0.883236, acc 0.606291, f1 0.568657\n",
      "\n",
      "2017-11-17T21:23:04.870553: step 655, loss 0.649549, acc 0.708008, f1 0.682608\n",
      "2017-11-17T21:23:05.395610: step 660, loss 0.798584, acc 0.580078, f1 0.534392\n",
      "2017-11-17T21:23:05.917029: step 665, loss 0.583, acc 0.75293, f1 0.746903\n",
      "Current epoch:  37\n",
      "2017-11-17T21:23:06.406280: step 670, loss 0.606628, acc 0.697266, f1 0.684322\n",
      "2017-11-17T21:23:06.937429: step 675, loss 0.698876, acc 0.691406, f1 0.651288\n",
      "2017-11-17T21:23:07.496462: step 680, loss 0.661814, acc 0.709961, f1 0.71374\n",
      "Current epoch:  38\n",
      "2017-11-17T21:23:07.991187: step 685, loss 0.557265, acc 0.759766, f1 0.754714\n",
      "2017-11-17T21:23:08.529652: step 690, loss 0.584043, acc 0.75, f1 0.747974\n",
      "2017-11-17T21:23:09.023339: step 695, loss 0.711964, acc 0.668945, f1 0.654319\n",
      "2017-11-17T21:23:09.559648: step 700, loss 0.591246, acc 0.774414, f1 0.770129\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:23:10.147233: step 700, loss 0.79219, acc 0.62563, f1 0.624668\n",
      "\n",
      "Current epoch:  39\n",
      "2017-11-17T21:23:10.689210: step 705, loss 0.570554, acc 0.767578, f1 0.766195\n",
      "2017-11-17T21:23:11.167988: step 710, loss 0.695548, acc 0.646484, f1 0.621513\n",
      "2017-11-17T21:23:11.660715: step 715, loss 0.541395, acc 0.794922, f1 0.790591\n",
      "2017-11-17T21:23:12.146322: step 720, loss 0.628652, acc 0.77044, f1 0.777091\n",
      "Current epoch:  40\n",
      "2017-11-17T21:23:12.641087: step 725, loss 0.651209, acc 0.692383, f1 0.663379\n",
      "2017-11-17T21:23:13.149216: step 730, loss 0.551777, acc 0.773438, f1 0.765371\n",
      "2017-11-17T21:23:13.666990: step 735, loss 0.628879, acc 0.701172, f1 0.683874\n",
      "Current epoch:  41\n",
      "2017-11-17T21:23:14.179211: step 740, loss 0.603006, acc 0.750977, f1 0.747709\n",
      "2017-11-17T21:23:14.664673: step 745, loss 0.598147, acc 0.730469, f1 0.709808\n",
      "2017-11-17T21:23:15.194258: step 750, loss 0.571052, acc 0.801758, f1 0.803454\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:23:15.800163: step 750, loss 0.8404, acc 0.607163, f1 0.590074\n",
      "\n",
      "2017-11-17T21:23:16.417232: step 755, loss 0.632098, acc 0.711914, f1 0.693993\n",
      "Current epoch:  42\n",
      "2017-11-17T21:23:16.923564: step 760, loss 0.572337, acc 0.726562, f1 0.715172\n",
      "2017-11-17T21:23:17.449615: step 765, loss 0.571488, acc 0.787109, f1 0.786177\n",
      "2017-11-17T21:23:17.949088: step 770, loss 0.639918, acc 0.674805, f1 0.660798\n",
      "Current epoch:  43\n",
      "2017-11-17T21:23:18.418772: step 775, loss 0.686713, acc 0.708984, f1 0.664916\n",
      "2017-11-17T21:23:18.996640: step 780, loss 0.576673, acc 0.782227, f1 0.784922\n",
      "2017-11-17T21:23:19.497305: step 785, loss 0.583654, acc 0.730469, f1 0.724586\n",
      "2017-11-17T21:23:20.015036: step 790, loss 0.596418, acc 0.736328, f1 0.726382\n",
      "Current epoch:  44\n",
      "2017-11-17T21:23:20.535906: step 795, loss 0.530805, acc 0.830078, f1 0.830899\n",
      "2017-11-17T21:23:21.080797: step 800, loss 0.663744, acc 0.642578, f1 0.606153\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:23:21.671716: step 800, loss 1.00917, acc 0.492148, f1 0.434188\n",
      "\n",
      "2017-11-17T21:23:22.248786: step 805, loss 0.516432, acc 0.806641, f1 0.803609\n",
      "2017-11-17T21:23:22.786076: step 810, loss 0.5552, acc 0.778302, f1 0.775053\n",
      "Current epoch:  45\n",
      "2017-11-17T21:23:23.314084: step 815, loss 0.678591, acc 0.654297, f1 0.590322\n",
      "2017-11-17T21:23:23.838888: step 820, loss 0.536818, acc 0.783203, f1 0.764559\n",
      "2017-11-17T21:23:24.360601: step 825, loss 0.604096, acc 0.766602, f1 0.77255\n",
      "Current epoch:  46\n",
      "2017-11-17T21:23:24.860165: step 830, loss 0.502035, acc 0.798828, f1 0.793989\n",
      "2017-11-17T21:23:25.385321: step 835, loss 0.559849, acc 0.720703, f1 0.708681\n",
      "2017-11-17T21:23:25.876680: step 840, loss 0.492065, acc 0.836914, f1 0.837178\n",
      "2017-11-17T21:23:26.393386: step 845, loss 0.677437, acc 0.618164, f1 0.587341\n",
      "Current epoch:  47\n",
      "2017-11-17T21:23:26.874144: step 850, loss 0.537183, acc 0.790039, f1 0.786882\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:23:27.454209: step 850, loss 0.846421, acc 0.6076, f1 0.600646\n",
      "\n",
      "2017-11-17T21:23:27.995831: step 855, loss 0.523255, acc 0.8125, f1 0.808829\n",
      "2017-11-17T21:23:28.510027: step 860, loss 0.47549, acc 0.833008, f1 0.830677\n",
      "Current epoch:  48\n",
      "2017-11-17T21:23:29.002803: step 865, loss 0.547825, acc 0.770508, f1 0.771166\n",
      "2017-11-17T21:23:29.504330: step 870, loss 0.578945, acc 0.730469, f1 0.718382\n",
      "2017-11-17T21:23:30.009648: step 875, loss 0.498366, acc 0.810547, f1 0.807486\n",
      "2017-11-17T21:23:30.512973: step 880, loss 0.489552, acc 0.802734, f1 0.797736\n",
      "Current epoch:  49\n",
      "2017-11-17T21:23:30.958035: step 885, loss 0.551154, acc 0.713867, f1 0.700888\n",
      "2017-11-17T21:23:31.466432: step 890, loss 0.458816, acc 0.859375, f1 0.857511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-17T21:23:31.951616: step 895, loss 0.646985, acc 0.745117, f1 0.691622\n",
      "2017-11-17T21:23:32.437415: step 900, loss 0.476841, acc 0.841195, f1 0.840763\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:23:33.032629: step 900, loss 0.854142, acc 0.600281, f1 0.590337\n",
      "\n",
      "Current epoch:  50\n",
      "2017-11-17T21:23:33.584335: step 905, loss 0.696942, acc 0.632812, f1 0.592\n",
      "2017-11-17T21:23:34.081560: step 910, loss 0.532751, acc 0.761719, f1 0.73683\n",
      "2017-11-17T21:23:34.574437: step 915, loss 0.478487, acc 0.839844, f1 0.842472\n",
      "Current epoch:  51\n",
      "2017-11-17T21:23:35.064998: step 920, loss 0.55448, acc 0.71582, f1 0.705\n",
      "2017-11-17T21:23:35.535843: step 925, loss 0.545979, acc 0.74707, f1 0.736941\n",
      "2017-11-17T21:23:36.080489: step 930, loss 0.446002, acc 0.853516, f1 0.854415\n",
      "2017-11-17T21:23:36.616720: step 935, loss 0.526118, acc 0.760742, f1 0.737764\n",
      "Current epoch:  52\n",
      "2017-11-17T21:23:37.084629: step 940, loss 0.625942, acc 0.711914, f1 0.677932\n",
      "2017-11-17T21:23:37.606222: step 945, loss 0.430777, acc 0.856445, f1 0.855701\n",
      "2017-11-17T21:23:38.151734: step 950, loss 0.523969, acc 0.763672, f1 0.753857\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:23:38.783740: step 950, loss 0.889499, acc 0.605903, f1 0.566884\n",
      "\n",
      "Current epoch:  53\n",
      "2017-11-17T21:23:39.275475: step 955, loss 0.511992, acc 0.753906, f1 0.749879\n",
      "2017-11-17T21:23:39.784486: step 960, loss 0.419458, acc 0.855469, f1 0.85363\n",
      "2017-11-17T21:23:40.259437: step 965, loss 0.693117, acc 0.672852, f1 0.633928\n",
      "2017-11-17T21:23:40.757969: step 970, loss 0.461777, acc 0.789062, f1 0.778838\n",
      "Current epoch:  54\n",
      "2017-11-17T21:23:41.270671: step 975, loss 0.504015, acc 0.806641, f1 0.802618\n",
      "2017-11-17T21:23:41.742988: step 980, loss 0.398009, acc 0.865234, f1 0.862384\n",
      "2017-11-17T21:23:42.286707: step 985, loss 0.470475, acc 0.828125, f1 0.828135\n",
      "2017-11-17T21:23:42.956748: step 990, loss 0.607465, acc 0.705975, f1 0.680054\n",
      "Current epoch:  55\n",
      "2017-11-17T21:23:43.500469: step 995, loss 0.48576, acc 0.793945, f1 0.786675\n",
      "2017-11-17T21:23:44.011309: step 1000, loss 0.449393, acc 0.811523, f1 0.805199\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:23:44.605520: step 1000, loss 0.996681, acc 0.526754, f1 0.495369\n",
      "\n",
      "2017-11-17T21:23:45.150404: step 1005, loss 0.497342, acc 0.782227, f1 0.767863\n",
      "Current epoch:  56\n",
      "2017-11-17T21:23:45.640177: step 1010, loss 0.405215, acc 0.833984, f1 0.831175\n",
      "2017-11-17T21:23:46.181396: step 1015, loss 0.590552, acc 0.698242, f1 0.65724\n",
      "2017-11-17T21:23:46.699305: step 1020, loss 0.482498, acc 0.793945, f1 0.785795\n",
      "2017-11-17T21:23:47.189659: step 1025, loss 0.434384, acc 0.830078, f1 0.828205\n",
      "Current epoch:  57\n",
      "2017-11-17T21:23:47.682769: step 1030, loss 0.452281, acc 0.773438, f1 0.768397\n",
      "2017-11-17T21:23:48.182211: step 1035, loss 0.46706, acc 0.819336, f1 0.814751\n",
      "2017-11-17T21:23:48.704677: step 1040, loss 0.445992, acc 0.813477, f1 0.794134\n",
      "Current epoch:  58\n",
      "2017-11-17T21:23:49.218652: step 1045, loss 0.444633, acc 0.838867, f1 0.839269\n",
      "2017-11-17T21:23:49.724448: step 1050, loss 0.636498, acc 0.663086, f1 0.614924\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:23:50.313081: step 1050, loss 1.03493, acc 0.580894, f1 0.506401\n",
      "\n",
      "2017-11-17T21:23:50.824602: step 1055, loss 0.392333, acc 0.878906, f1 0.876353\n",
      "2017-11-17T21:23:51.328500: step 1060, loss 0.337909, acc 0.911133, f1 0.911242\n",
      "Current epoch:  59\n",
      "2017-11-17T21:23:51.802111: step 1065, loss 0.436432, acc 0.786133, f1 0.784762\n",
      "2017-11-17T21:23:52.305966: step 1070, loss 0.382696, acc 0.879883, f1 0.879182\n",
      "2017-11-17T21:23:52.843368: step 1075, loss 0.41827, acc 0.829102, f1 0.82437\n",
      "2017-11-17T21:23:53.313296: step 1080, loss 0.425154, acc 0.822327, f1 0.819454\n",
      "Current epoch:  60\n",
      "2017-11-17T21:23:53.801949: step 1085, loss 0.36592, acc 0.869141, f1 0.867546\n",
      "2017-11-17T21:23:54.272696: step 1090, loss 0.762669, acc 0.603516, f1 0.515675\n",
      "2017-11-17T21:23:54.770477: step 1095, loss 0.396832, acc 0.856445, f1 0.849216\n",
      "Current epoch:  61\n",
      "2017-11-17T21:23:55.185561: step 1100, loss 0.38192, acc 0.880859, f1 0.880417\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:23:55.777251: step 1100, loss 0.938631, acc 0.572218, f1 0.559681\n",
      "\n",
      "2017-11-17T21:23:56.274553: step 1105, loss 0.500871, acc 0.754883, f1 0.747492\n",
      "2017-11-17T21:23:56.777179: step 1110, loss 0.316994, acc 0.929688, f1 0.929605\n",
      "2017-11-17T21:23:57.272563: step 1115, loss 0.297133, acc 0.916016, f1 0.915778\n",
      "Current epoch:  62\n",
      "2017-11-17T21:23:57.746733: step 1120, loss 0.442166, acc 0.803711, f1 0.7962\n",
      "2017-11-17T21:23:58.275715: step 1125, loss 0.343823, acc 0.885742, f1 0.883603\n",
      "2017-11-17T21:23:58.801578: step 1130, loss 0.864699, acc 0.597656, f1 0.485541\n",
      "Current epoch:  63\n",
      "2017-11-17T21:23:59.265902: step 1135, loss 0.307894, acc 0.922852, f1 0.922691\n",
      "2017-11-17T21:23:59.759173: step 1140, loss 0.350213, acc 0.883789, f1 0.882446\n",
      "2017-11-17T21:24:00.283177: step 1145, loss 0.496857, acc 0.758789, f1 0.744007\n",
      "2017-11-17T21:24:00.838596: step 1150, loss 0.453263, acc 0.811523, f1 0.790202\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:24:01.448192: step 1150, loss 0.98106, acc 0.541489, f1 0.527823\n",
      "\n",
      "Current epoch:  64\n",
      "2017-11-17T21:24:02.013962: step 1155, loss 0.389634, acc 0.861328, f1 0.870257\n",
      "2017-11-17T21:24:02.528043: step 1160, loss 0.378446, acc 0.847656, f1 0.833453\n",
      "2017-11-17T21:24:03.062872: step 1165, loss 0.598144, acc 0.696289, f1 0.672355\n",
      "2017-11-17T21:24:03.576232: step 1170, loss 0.318252, acc 0.919811, f1 0.919601\n",
      "Current epoch:  65\n",
      "2017-11-17T21:24:04.136907: step 1175, loss 0.301085, acc 0.900391, f1 0.899742\n",
      "2017-11-17T21:24:04.664216: step 1180, loss 0.452098, acc 0.793945, f1 0.781948\n",
      "2017-11-17T21:24:05.155047: step 1185, loss 0.336219, acc 0.879883, f1 0.878248\n",
      "Current epoch:  66\n",
      "2017-11-17T21:24:05.620266: step 1190, loss 0.303379, acc 0.908203, f1 0.908141\n",
      "2017-11-17T21:24:06.098060: step 1195, loss 0.606969, acc 0.694336, f1 0.662164\n",
      "2017-11-17T21:24:06.617611: step 1200, loss 0.313093, acc 0.93457, f1 0.934534\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:24:07.204262: step 1200, loss 0.922474, acc 0.62214, f1 0.597899\n",
      "\n",
      "2017-11-17T21:24:07.762962: step 1205, loss 0.297437, acc 0.916016, f1 0.915469\n",
      "Current epoch:  67\n",
      "2017-11-17T21:24:08.253689: step 1210, loss 0.292154, acc 0.924805, f1 0.924922\n",
      "2017-11-17T21:24:08.814260: step 1215, loss 0.375328, acc 0.827148, f1 0.826121\n",
      "2017-11-17T21:24:09.348792: step 1220, loss 0.522115, acc 0.730469, f1 0.715718\n",
      "Current epoch:  68\n",
      "2017-11-17T21:24:09.797639: step 1225, loss 0.312381, acc 0.920898, f1 0.921741\n",
      "2017-11-17T21:24:10.277367: step 1230, loss 0.277162, acc 0.920898, f1 0.92031\n",
      "2017-11-17T21:24:10.733886: step 1235, loss 0.655443, acc 0.650391, f1 0.587396\n",
      "2017-11-17T21:24:11.239289: step 1240, loss 0.277828, acc 0.932617, f1 0.932714\n",
      "Current epoch:  69\n",
      "2017-11-17T21:24:11.694367: step 1245, loss 0.260308, acc 0.936523, f1 0.93629\n",
      "2017-11-17T21:24:12.160599: step 1250, loss 0.271428, acc 0.927734, f1 0.927602\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:24:12.749656: step 1250, loss 0.972521, acc 0.618893, f1 0.592411\n",
      "\n",
      "2017-11-17T21:24:13.242090: step 1255, loss 0.625426, acc 0.738281, f1 0.691684\n",
      "2017-11-17T21:24:13.722672: step 1260, loss 0.438786, acc 0.783019, f1 0.778474\n",
      "Current epoch:  70\n",
      "2017-11-17T21:24:14.246778: step 1265, loss 0.257513, acc 0.936523, f1 0.936237\n",
      "2017-11-17T21:24:14.769463: step 1270, loss 0.210334, acc 0.967773, f1 0.967766\n",
      "2017-11-17T21:24:15.294826: step 1275, loss 0.250868, acc 0.926758, f1 0.926371\n",
      "Current epoch:  71\n",
      "2017-11-17T21:24:15.778767: step 1280, loss 0.414755, acc 0.799805, f1 0.790619\n",
      "2017-11-17T21:24:16.253701: step 1285, loss 0.322751, acc 0.867188, f1 0.866495\n",
      "2017-11-17T21:24:16.770045: step 1290, loss 0.274198, acc 0.922852, f1 0.922429\n",
      "2017-11-17T21:24:17.274924: step 1295, loss 0.355477, acc 0.844727, f1 0.836903\n",
      "Current epoch:  72\n",
      "2017-11-17T21:24:17.755066: step 1300, loss 0.257391, acc 0.918945, f1 0.919098\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:24:18.361153: step 1300, loss 0.921892, acc 0.6076, f1 0.601835\n",
      "\n",
      "2017-11-17T21:24:18.912191: step 1305, loss 0.286134, acc 0.881836, f1 0.87959\n",
      "2017-11-17T21:24:19.431134: step 1310, loss 0.394059, acc 0.833008, f1 0.827965\n",
      "Current epoch:  73\n",
      "2017-11-17T21:24:19.942652: step 1315, loss 0.283084, acc 0.908203, f1 0.903536\n",
      "2017-11-17T21:24:20.502147: step 1320, loss 0.196459, acc 0.972656, f1 0.972656\n",
      "2017-11-17T21:24:21.020155: step 1325, loss 0.20432, acc 0.955078, f1 0.955104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-17T21:24:21.559894: step 1330, loss 0.8446, acc 0.584961, f1 0.522448\n",
      "Current epoch:  74\n",
      "2017-11-17T21:24:22.086863: step 1335, loss 0.217387, acc 0.962891, f1 0.96284\n",
      "2017-11-17T21:24:22.612490: step 1340, loss 0.190191, acc 0.959961, f1 0.959903\n",
      "2017-11-17T21:24:23.078064: step 1345, loss 0.174227, acc 0.97168, f1 0.971673\n",
      "2017-11-17T21:24:23.526398: step 1350, loss 0.174447, acc 0.974843, f1 0.974855\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:24:24.147657: step 1350, loss 0.939492, acc 0.600475, f1 0.598522\n",
      "\n",
      "Current epoch:  75\n",
      "2017-11-17T21:24:24.640644: step 1355, loss 0.179285, acc 0.96582, f1 0.965891\n",
      "2017-11-17T21:24:25.146564: step 1360, loss 0.73615, acc 0.644531, f1 0.563617\n",
      "2017-11-17T21:24:25.676461: step 1365, loss 0.20562, acc 0.960938, f1 0.960832\n",
      "Current epoch:  76\n",
      "2017-11-17T21:24:26.184282: step 1370, loss 0.182361, acc 0.964844, f1 0.964826\n",
      "2017-11-17T21:24:26.692212: step 1375, loss 0.159454, acc 0.96875, f1 0.96876\n",
      "2017-11-17T21:24:27.247530: step 1380, loss 0.22826, acc 0.925781, f1 0.92551\n",
      "2017-11-17T21:24:27.710360: step 1385, loss 0.36043, acc 0.834961, f1 0.834264\n",
      "Current epoch:  77\n",
      "2017-11-17T21:24:28.157819: step 1390, loss 0.154304, acc 0.980469, f1 0.980458\n",
      "2017-11-17T21:24:28.657433: step 1395, loss 0.15119, acc 0.973633, f1 0.97362\n",
      "2017-11-17T21:24:29.156712: step 1400, loss 0.153615, acc 0.975586, f1 0.975606\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:24:29.738532: step 1400, loss 0.94966, acc 0.606436, f1 0.605147\n",
      "\n",
      "Current epoch:  78\n",
      "2017-11-17T21:24:30.200970: step 1405, loss 0.136597, acc 0.975586, f1 0.975629\n",
      "2017-11-17T21:24:30.717385: step 1410, loss 0.283162, acc 0.885742, f1 0.883794\n",
      "2017-11-17T21:24:31.260182: step 1415, loss 0.343287, acc 0.814453, f1 0.804654\n",
      "2017-11-17T21:24:31.793361: step 1420, loss 0.169089, acc 0.969727, f1 0.969715\n",
      "Current epoch:  79\n",
      "2017-11-17T21:24:32.291737: step 1425, loss 0.135367, acc 0.981445, f1 0.98147\n",
      "2017-11-17T21:24:32.793536: step 1430, loss 0.134314, acc 0.981445, f1 0.981455\n",
      "2017-11-17T21:24:33.274472: step 1435, loss 0.188989, acc 0.953125, f1 0.953943\n",
      "2017-11-17T21:24:33.744793: step 1440, loss 0.649904, acc 0.825472, f1 0.767725\n",
      "Current epoch:  80\n",
      "2017-11-17T21:24:34.239280: step 1445, loss 0.123389, acc 0.987305, f1 0.987306\n",
      "2017-11-17T21:24:34.723118: step 1450, loss 0.142594, acc 0.975586, f1 0.975575\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:24:35.321891: step 1450, loss 0.983373, acc 0.609393, f1 0.603005\n",
      "\n",
      "2017-11-17T21:24:35.799482: step 1455, loss 0.297914, acc 0.850586, f1 0.848028\n",
      "Current epoch:  81\n",
      "2017-11-17T21:24:36.274391: step 1460, loss 0.27512, acc 0.893555, f1 0.892501\n",
      "2017-11-17T21:24:36.747488: step 1465, loss 0.132733, acc 0.981445, f1 0.981461\n",
      "2017-11-17T21:24:37.251195: step 1470, loss 0.123018, acc 0.983398, f1 0.983415\n",
      "2017-11-17T21:24:37.786623: step 1475, loss 0.115315, acc 0.983398, f1 0.983396\n",
      "Current epoch:  82\n",
      "2017-11-17T21:24:38.268392: step 1480, loss 0.121702, acc 0.982422, f1 0.982428\n",
      "2017-11-17T21:24:38.764943: step 1485, loss 0.112777, acc 0.985352, f1 0.985358\n",
      "2017-11-17T21:24:39.286338: step 1490, loss 0.109813, acc 0.983398, f1 0.983402\n",
      "Current epoch:  83\n",
      "2017-11-17T21:24:39.764416: step 1495, loss 0.105216, acc 0.987305, f1 0.987307\n",
      "2017-11-17T21:24:40.268137: step 1500, loss 0.108108, acc 0.985352, f1 0.985352\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:24:40.847727: step 1500, loss 1.01886, acc 0.600717, f1 0.600208\n",
      "\n",
      "2017-11-17T21:24:41.367679: step 1505, loss 0.126814, acc 0.974609, f1 0.974625\n",
      "2017-11-17T21:24:41.893042: step 1510, loss 0.746171, acc 0.645508, f1 0.567604\n",
      "Current epoch:  84\n",
      "2017-11-17T21:24:42.388845: step 1515, loss 0.10967, acc 0.992188, f1 0.992188\n",
      "2017-11-17T21:24:42.908793: step 1520, loss 0.102406, acc 0.988281, f1 0.988286\n",
      "2017-11-17T21:24:43.384208: step 1525, loss 0.102637, acc 0.994141, f1 0.994141\n",
      "2017-11-17T21:24:43.897250: step 1530, loss 0.12061, acc 0.968553, f1 0.968566\n",
      "Current epoch:  85\n",
      "2017-11-17T21:24:44.419951: step 1535, loss 0.0960654, acc 0.991211, f1 0.991211\n",
      "2017-11-17T21:24:44.934613: step 1540, loss 0.103774, acc 0.987305, f1 0.987317\n",
      "2017-11-17T21:24:45.470859: step 1545, loss 0.101378, acc 0.985352, f1 0.985356\n",
      "Current epoch:  86\n",
      "2017-11-17T21:24:45.972322: step 1550, loss 0.0840358, acc 0.989258, f1 0.989256\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:24:46.577642: step 1550, loss 1.05924, acc 0.594416, f1 0.593938\n",
      "\n",
      "2017-11-17T21:24:47.153444: step 1555, loss 0.0832535, acc 0.993164, f1 0.993164\n",
      "2017-11-17T21:24:47.681054: step 1560, loss 0.316981, acc 0.875977, f1 0.851086\n",
      "2017-11-17T21:24:48.234313: step 1565, loss 0.120126, acc 0.976562, f1 0.97657\n",
      "Current epoch:  87\n",
      "2017-11-17T21:24:48.716568: step 1570, loss 0.0846294, acc 0.991211, f1 0.991208\n",
      "2017-11-17T21:24:49.239386: step 1575, loss 0.0768389, acc 0.993164, f1 0.993164\n",
      "2017-11-17T21:24:49.779166: step 1580, loss 0.0919466, acc 0.985352, f1 0.985349\n",
      "Current epoch:  88\n",
      "2017-11-17T21:24:50.324720: step 1585, loss 0.0794631, acc 0.993164, f1 0.993165\n",
      "2017-11-17T21:24:50.895501: step 1590, loss 0.0784761, acc 0.992188, f1 0.992192\n",
      "2017-11-17T21:24:51.417009: step 1595, loss 0.156758, acc 0.943359, f1 0.942915\n",
      "2017-11-17T21:24:51.928836: step 1600, loss 0.523412, acc 0.723633, f1 0.693822\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:24:52.510501: step 1600, loss 1.37222, acc 0.496219, f1 0.453618\n",
      "\n",
      "Current epoch:  89\n",
      "2017-11-17T21:24:53.038623: step 1605, loss 0.0990644, acc 0.986328, f1 0.986333\n",
      "2017-11-17T21:24:53.586169: step 1610, loss 0.0911273, acc 0.990234, f1 0.990234\n",
      "2017-11-17T21:24:54.125319: step 1615, loss 0.087286, acc 0.986328, f1 0.986328\n",
      "2017-11-17T21:24:54.635278: step 1620, loss 0.0795903, acc 0.990566, f1 0.990564\n",
      "Current epoch:  90\n",
      "2017-11-17T21:24:55.201647: step 1625, loss 0.0754614, acc 0.994141, f1 0.994137\n",
      "2017-11-17T21:24:55.751703: step 1630, loss 0.0769507, acc 0.991211, f1 0.991211\n",
      "2017-11-17T21:24:56.326602: step 1635, loss 0.0714679, acc 0.994141, f1 0.994143\n",
      "Current epoch:  91\n",
      "2017-11-17T21:24:56.814198: step 1640, loss 0.0665153, acc 0.992188, f1 0.99219\n",
      "2017-11-17T21:24:57.346103: step 1645, loss 0.0644591, acc 0.994141, f1 0.994139\n",
      "2017-11-17T21:24:57.871658: step 1650, loss 0.0647432, acc 0.996094, f1 0.996095\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:24:58.463935: step 1650, loss 1.12214, acc 0.586419, f1 0.586181\n",
      "\n",
      "2017-11-17T21:24:58.976237: step 1655, loss 0.0613145, acc 0.99707, f1 0.99707\n",
      "Current epoch:  92\n",
      "2017-11-17T21:24:59.510282: step 1660, loss 0.062993, acc 0.994141, f1 0.994141\n",
      "2017-11-17T21:25:00.037711: step 1665, loss 0.0639784, acc 0.995117, f1 0.995116\n",
      "2017-11-17T21:25:00.578125: step 1670, loss 0.0794547, acc 0.995117, f1 0.995131\n",
      "Current epoch:  93\n",
      "2017-11-17T21:25:01.049546: step 1675, loss 0.398284, acc 0.765625, f1 0.757919\n",
      "2017-11-17T21:25:01.537845: step 1680, loss 0.0772268, acc 0.992188, f1 0.992187\n",
      "2017-11-17T21:25:02.080055: step 1685, loss 0.0730895, acc 0.994141, f1 0.994141\n",
      "2017-11-17T21:25:02.585467: step 1690, loss 0.0673727, acc 0.995117, f1 0.995118\n",
      "Current epoch:  94\n",
      "2017-11-17T21:25:03.095398: step 1695, loss 0.0610602, acc 0.99707, f1 0.99707\n",
      "2017-11-17T21:25:03.585027: step 1700, loss 0.057105, acc 0.994141, f1 0.994139\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:25:04.185490: step 1700, loss 1.12397, acc 0.591605, f1 0.591027\n",
      "\n",
      "2017-11-17T21:25:04.686820: step 1705, loss 0.0651306, acc 0.992188, f1 0.992188\n",
      "2017-11-17T21:25:05.200338: step 1710, loss 0.0577236, acc 0.995283, f1 0.995283\n",
      "Current epoch:  95\n",
      "2017-11-17T21:25:05.718491: step 1715, loss 0.0612925, acc 0.990234, f1 0.990242\n",
      "2017-11-17T21:25:06.226978: step 1720, loss 0.0560801, acc 0.995117, f1 0.995119\n",
      "2017-11-17T21:25:06.733048: step 1725, loss 0.0560768, acc 0.99707, f1 0.99707\n",
      "Current epoch:  96\n",
      "2017-11-17T21:25:07.167548: step 1730, loss 0.0539889, acc 0.995117, f1 0.995117\n",
      "2017-11-17T21:25:07.702931: step 1735, loss 0.053145, acc 0.995117, f1 0.995117\n",
      "2017-11-17T21:25:08.228290: step 1740, loss 0.0487739, acc 0.99707, f1 0.997069\n",
      "2017-11-17T21:25:08.695866: step 1745, loss 0.128399, acc 0.961914, f1 0.961794\n",
      "Current epoch:  97\n",
      "2017-11-17T21:25:09.164176: step 1750, loss 0.234998, acc 0.900391, f1 0.899414\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:25:09.758924: step 1750, loss 1.0726, acc 0.600475, f1 0.599599\n",
      "\n",
      "2017-11-17T21:25:10.297565: step 1755, loss 0.0595528, acc 0.996094, f1 0.996094\n",
      "2017-11-17T21:25:10.764684: step 1760, loss 0.0560685, acc 0.99707, f1 0.997071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch:  98\n",
      "2017-11-17T21:25:11.239276: step 1765, loss 0.0506572, acc 0.998047, f1 0.998047\n",
      "2017-11-17T21:25:11.757683: step 1770, loss 0.0506874, acc 0.996094, f1 0.996092\n",
      "2017-11-17T21:25:12.280556: step 1775, loss 0.0459265, acc 0.996094, f1 0.996094\n",
      "2017-11-17T21:25:12.787856: step 1780, loss 0.0469388, acc 0.995117, f1 0.995118\n",
      "Current epoch:  99\n",
      "2017-11-17T21:25:13.278394: step 1785, loss 0.0470566, acc 0.99707, f1 0.997071\n",
      "2017-11-17T21:25:13.796884: step 1790, loss 0.0491562, acc 0.994141, f1 0.994142\n",
      "2017-11-17T21:25:14.298429: step 1795, loss 0.0549314, acc 0.993164, f1 0.993164\n",
      "2017-11-17T21:25:14.754546: step 1800, loss 0.044419, acc 0.995283, f1 0.99528\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:25:15.353828: step 1800, loss 1.18435, acc 0.591702, f1 0.591225\n",
      "\n",
      "Current epoch:  100\n",
      "2017-11-17T21:25:15.836215: step 1805, loss 0.0430611, acc 0.99707, f1 0.99707\n",
      "2017-11-17T21:25:16.313769: step 1810, loss 0.0437556, acc 0.996094, f1 0.996095\n",
      "2017-11-17T21:25:16.833370: step 1815, loss 0.0467099, acc 0.994141, f1 0.994138\n",
      "Current epoch:  101\n",
      "2017-11-17T21:25:17.358648: step 1820, loss 0.0376251, acc 1, f1 1\n",
      "2017-11-17T21:25:17.911142: step 1825, loss 0.0410749, acc 0.99707, f1 0.997067\n",
      "2017-11-17T21:25:18.483348: step 1830, loss 1.18972, acc 0.59082, f1 0.624941\n",
      "2017-11-17T21:25:18.960828: step 1835, loss 0.0514559, acc 0.995117, f1 0.995118\n",
      "Current epoch:  102\n",
      "2017-11-17T21:25:19.459871: step 1840, loss 0.0466589, acc 0.99707, f1 0.99707\n",
      "2017-11-17T21:25:19.985142: step 1845, loss 0.0436833, acc 0.998047, f1 0.998047\n",
      "2017-11-17T21:25:20.517471: step 1850, loss 0.0394353, acc 0.996094, f1 0.996094\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:25:21.112497: step 1850, loss 1.19972, acc 0.590199, f1 0.589453\n",
      "\n",
      "Current epoch:  103\n",
      "2017-11-17T21:25:21.630834: step 1855, loss 0.0391035, acc 0.998047, f1 0.998047\n",
      "2017-11-17T21:25:22.153487: step 1860, loss 0.0371397, acc 0.99707, f1 0.99707\n",
      "2017-11-17T21:25:22.662583: step 1865, loss 0.0408853, acc 0.993164, f1 0.993164\n",
      "2017-11-17T21:25:23.174128: step 1870, loss 0.0352545, acc 0.999023, f1 0.999024\n",
      "Current epoch:  104\n",
      "2017-11-17T21:25:23.656386: step 1875, loss 0.0372115, acc 0.996094, f1 0.996094\n",
      "2017-11-17T21:25:24.173052: step 1880, loss 0.0937085, acc 0.984375, f1 0.984375\n",
      "2017-11-17T21:25:24.700236: step 1885, loss 0.128213, acc 0.964844, f1 0.96476\n",
      "2017-11-17T21:25:25.197981: step 1890, loss 0.0461353, acc 0.996855, f1 0.996852\n",
      "Current epoch:  105\n",
      "2017-11-17T21:25:25.723464: step 1895, loss 0.0422581, acc 0.998047, f1 0.998043\n",
      "2017-11-17T21:25:26.217973: step 1900, loss 0.0391048, acc 0.99707, f1 0.99707\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:25:26.820225: step 1900, loss 1.20653, acc 0.590345, f1 0.589438\n",
      "\n",
      "2017-11-17T21:25:27.301046: step 1905, loss 0.0394348, acc 0.99707, f1 0.99707\n",
      "Current epoch:  106\n",
      "2017-11-17T21:25:27.766238: step 1910, loss 0.0375726, acc 0.996094, f1 0.996094\n",
      "2017-11-17T21:25:28.257439: step 1915, loss 0.0340347, acc 0.998047, f1 0.998047\n",
      "2017-11-17T21:25:28.830291: step 1920, loss 0.0325904, acc 0.999023, f1 0.999024\n",
      "2017-11-17T21:25:29.363911: step 1925, loss 0.0383829, acc 0.994141, f1 0.994137\n",
      "Current epoch:  107\n",
      "2017-11-17T21:25:29.877786: step 1930, loss 0.0332143, acc 0.998047, f1 0.998047\n",
      "2017-11-17T21:25:30.402594: step 1935, loss 0.0311642, acc 0.999023, f1 0.999023\n",
      "2017-11-17T21:25:30.937853: step 1940, loss 0.0307253, acc 1, f1 1\n",
      "Current epoch:  108\n",
      "2017-11-17T21:25:31.469107: step 1945, loss 0.0316077, acc 0.99707, f1 0.99707\n",
      "2017-11-17T21:25:31.981518: step 1950, loss 0.0351887, acc 0.99707, f1 0.99707\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:25:32.562883: step 1950, loss 1.26103, acc 0.590248, f1 0.588974\n",
      "\n",
      "2017-11-17T21:25:33.050523: step 1955, loss 0.0324632, acc 0.99707, f1 0.997069\n",
      "2017-11-17T21:25:33.536431: step 1960, loss 0.0362207, acc 0.998047, f1 0.998047\n",
      "Current epoch:  109\n",
      "2017-11-17T21:25:34.013963: step 1965, loss 0.0283036, acc 0.99707, f1 0.99707\n",
      "2017-11-17T21:25:34.504836: step 1970, loss 0.0321719, acc 0.99707, f1 0.99707\n",
      "2017-11-17T21:25:35.030207: step 1975, loss 0.0428494, acc 0.99707, f1 0.997059\n",
      "2017-11-17T21:25:35.514433: step 1980, loss 0.0988852, acc 0.97327, f1 0.972572\n",
      "Current epoch:  110\n",
      "2017-11-17T21:25:36.036921: step 1985, loss 0.0361751, acc 0.998047, f1 0.998047\n",
      "2017-11-17T21:25:36.538240: step 1990, loss 0.0314701, acc 0.99707, f1 0.997071\n",
      "2017-11-17T21:25:37.052616: step 1995, loss 0.0309994, acc 0.99707, f1 0.997072\n",
      "Current epoch:  111\n",
      "2017-11-17T21:25:37.498552: step 2000, loss 0.0287444, acc 0.99707, f1 0.997072\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:25:38.116118: step 2000, loss 1.28071, acc 0.583947, f1 0.583513\n",
      "\n",
      "2017-11-17T21:25:38.644143: step 2005, loss 0.0258428, acc 0.999023, f1 0.999025\n",
      "2017-11-17T21:25:39.128657: step 2010, loss 0.0259321, acc 0.999023, f1 0.999023\n",
      "2017-11-17T21:25:39.675519: step 2015, loss 0.0310029, acc 0.996094, f1 0.996093\n",
      "Current epoch:  112\n",
      "2017-11-17T21:25:40.115911: step 2020, loss 0.0263976, acc 0.999023, f1 0.999023\n",
      "2017-11-17T21:25:40.600713: step 2025, loss 0.0239573, acc 1, f1 1\n",
      "2017-11-17T21:25:41.130860: step 2030, loss 0.0287455, acc 0.996094, f1 0.996094\n",
      "Current epoch:  113\n",
      "2017-11-17T21:25:41.653004: step 2035, loss 0.0578819, acc 0.996094, f1 0.996093\n",
      "2017-11-17T21:25:42.228759: step 2040, loss 0.177397, acc 0.925781, f1 0.925231\n",
      "2017-11-17T21:25:42.772012: step 2045, loss 0.034898, acc 0.99707, f1 0.99707\n",
      "2017-11-17T21:25:43.332959: step 2050, loss 0.0308804, acc 0.996094, f1 0.996091\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:25:43.939532: step 2050, loss 1.24484, acc 0.58797, f1 0.587578\n",
      "\n",
      "Current epoch:  114\n",
      "2017-11-17T21:25:44.426567: step 2055, loss 0.0276955, acc 1, f1 1\n",
      "2017-11-17T21:25:44.912611: step 2060, loss 0.0302763, acc 0.998047, f1 0.998048\n",
      "2017-11-17T21:25:45.386038: step 2065, loss 0.0291053, acc 0.996094, f1 0.996093\n",
      "2017-11-17T21:25:45.867316: step 2070, loss 0.0246618, acc 1, f1 1\n",
      "Current epoch:  115\n",
      "2017-11-17T21:25:46.425866: step 2075, loss 0.02312, acc 1, f1 1\n",
      "2017-11-17T21:25:46.979250: step 2080, loss 0.0268085, acc 0.99707, f1 0.997075\n",
      "2017-11-17T21:25:47.475158: step 2085, loss 0.0260942, acc 0.996094, f1 0.996094\n",
      "Current epoch:  116\n",
      "2017-11-17T21:25:47.968514: step 2090, loss 0.0266476, acc 0.996094, f1 0.996096\n",
      "2017-11-17T21:25:48.491546: step 2095, loss 0.0252366, acc 0.998047, f1 0.998047\n",
      "2017-11-17T21:25:49.005861: step 2100, loss 0.0256964, acc 0.996094, f1 0.996094\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:25:49.621961: step 2100, loss 1.31015, acc 0.588891, f1 0.588868\n",
      "\n",
      "2017-11-17T21:25:50.180823: step 2105, loss 0.0277116, acc 0.996094, f1 0.996095\n",
      "Current epoch:  117\n",
      "2017-11-17T21:25:50.659237: step 2110, loss 0.022278, acc 0.999023, f1 0.999023\n",
      "2017-11-17T21:25:51.191880: step 2115, loss 0.0255236, acc 0.99707, f1 0.99707\n",
      "2017-11-17T21:25:51.733070: step 2120, loss 0.0261125, acc 0.995117, f1 0.995115\n",
      "Current epoch:  118\n",
      "2017-11-17T21:25:52.220446: step 2125, loss 0.0207182, acc 0.999023, f1 0.999024\n",
      "2017-11-17T21:25:52.739463: step 2130, loss 0.024542, acc 0.99707, f1 0.997068\n",
      "2017-11-17T21:25:53.238250: step 2135, loss 0.0519627, acc 0.99707, f1 0.99708\n",
      "2017-11-17T21:25:53.738552: step 2140, loss 0.233137, acc 0.876953, f1 0.876137\n",
      "Current epoch:  119\n",
      "2017-11-17T21:25:54.188077: step 2145, loss 0.0297216, acc 0.996094, f1 0.996092\n",
      "2017-11-17T21:25:54.720452: step 2150, loss 0.0262373, acc 0.99707, f1 0.997071\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:25:55.333236: step 2150, loss 1.28994, acc 0.589666, f1 0.588974\n",
      "\n",
      "2017-11-17T21:25:55.842425: step 2155, loss 0.0255781, acc 0.999023, f1 0.999023\n",
      "2017-11-17T21:25:56.340605: step 2160, loss 0.0245048, acc 0.996855, f1 0.996855\n",
      "Current epoch:  120\n",
      "2017-11-17T21:25:56.864993: step 2165, loss 0.0213815, acc 0.999023, f1 0.999023\n",
      "2017-11-17T21:25:57.370347: step 2170, loss 0.0237763, acc 0.99707, f1 0.99707\n",
      "2017-11-17T21:25:57.864554: step 2175, loss 0.0245929, acc 0.996094, f1 0.996094\n",
      "Current epoch:  121\n",
      "2017-11-17T21:25:58.360792: step 2180, loss 0.0200843, acc 0.999023, f1 0.999023\n",
      "2017-11-17T21:25:58.890783: step 2185, loss 0.0220494, acc 0.996094, f1 0.996094\n",
      "2017-11-17T21:25:59.424678: step 2190, loss 0.0198322, acc 0.999023, f1 0.999023\n",
      "2017-11-17T21:25:59.896559: step 2195, loss 0.0241061, acc 0.996094, f1 0.996094\n",
      "Current epoch:  122\n",
      "2017-11-17T21:26:00.388019: step 2200, loss 0.0187759, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-17T21:26:00.974203: step 2200, loss 1.34488, acc 0.590393, f1 0.58995\n",
      "\n",
      "2017-11-17T21:26:01.488317: step 2205, loss 0.0211679, acc 0.998047, f1 0.998047\n",
      "2017-11-17T21:26:01.994865: step 2210, loss 0.022337, acc 0.99707, f1 0.997071\n",
      "Current epoch:  123\n",
      "2017-11-17T21:26:02.466049: step 2215, loss 0.019806, acc 0.998047, f1 0.998047\n",
      "2017-11-17T21:26:03.003446: step 2220, loss 0.0205626, acc 0.996094, f1 0.996094\n",
      "2017-11-17T21:26:03.497322: step 2225, loss 0.0192976, acc 0.999023, f1 0.999023\n",
      "2017-11-17T21:26:04.009480: step 2230, loss 0.0298275, acc 0.995117, f1 0.995119\n",
      "Current epoch:  124\n",
      "2017-11-17T21:26:04.488355: step 2235, loss 1.20765, acc 0.615234, f1 0.531036\n",
      "2017-11-17T21:26:04.992419: step 2240, loss 0.0286385, acc 1, f1 1\n",
      "2017-11-17T21:26:05.485936: step 2245, loss 0.0243099, acc 0.99707, f1 0.997072\n",
      "2017-11-17T21:26:05.971732: step 2250, loss 0.0229762, acc 0.998428, f1 0.998428\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:26:06.544169: step 2250, loss 1.30806, acc 0.589036, f1 0.588374\n",
      "\n",
      "Current epoch:  125\n",
      "2017-11-17T21:26:07.085734: step 2255, loss 0.0206931, acc 1, f1 1\n",
      "2017-11-17T21:26:07.589278: step 2260, loss 0.0202226, acc 0.998047, f1 0.998047\n",
      "2017-11-17T21:26:08.103139: step 2265, loss 0.0211224, acc 1, f1 1\n",
      "Current epoch:  126\n",
      "2017-11-17T21:26:08.610996: step 2270, loss 0.018716, acc 0.998047, f1 0.998047\n",
      "2017-11-17T21:26:09.095826: step 2275, loss 0.0173426, acc 0.998047, f1 0.998047\n",
      "2017-11-17T21:26:09.613326: step 2280, loss 0.0155649, acc 1, f1 1\n",
      "2017-11-17T21:26:10.101485: step 2285, loss 0.0223327, acc 0.996094, f1 0.996092\n",
      "Current epoch:  127\n",
      "2017-11-17T21:26:10.584699: step 2290, loss 0.018028, acc 0.998047, f1 0.998047\n",
      "2017-11-17T21:26:11.116014: step 2295, loss 0.0198437, acc 0.99707, f1 0.99707\n",
      "2017-11-17T21:26:11.638775: step 2300, loss 0.0166325, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:26:12.273916: step 2300, loss 1.37857, acc 0.587825, f1 0.586621\n",
      "\n",
      "Current epoch:  128\n",
      "2017-11-17T21:26:12.780424: step 2305, loss 0.0159476, acc 1, f1 1\n",
      "2017-11-17T21:26:13.313725: step 2310, loss 0.0168157, acc 0.999023, f1 0.999023\n",
      "2017-11-17T21:26:13.838162: step 2315, loss 0.0181046, acc 0.99707, f1 0.997071\n",
      "2017-11-17T21:26:14.346156: step 2320, loss 0.0184092, acc 0.999023, f1 0.999023\n",
      "Current epoch:  129\n",
      "2017-11-17T21:26:14.784942: step 2325, loss 0.016819, acc 0.999023, f1 0.999023\n",
      "2017-11-17T21:26:15.284498: step 2330, loss 0.0178909, acc 0.998047, f1 0.998048\n",
      "2017-11-17T21:26:15.786063: step 2335, loss 0.0402618, acc 0.99707, f1 0.997072\n",
      "2017-11-17T21:26:16.261559: step 2340, loss 0.0242532, acc 0.996855, f1 0.996855\n",
      "Current epoch:  130\n",
      "2017-11-17T21:26:16.716887: step 2345, loss 0.0159907, acc 0.999023, f1 0.999023\n",
      "2017-11-17T21:26:17.226325: step 2350, loss 0.016504, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:26:17.807365: step 2350, loss 1.38539, acc 0.588358, f1 0.588256\n",
      "\n",
      "2017-11-17T21:26:18.333277: step 2355, loss 0.0172288, acc 0.998047, f1 0.998047\n",
      "Current epoch:  131\n",
      "2017-11-17T21:26:18.795564: step 2360, loss 0.0156239, acc 0.999023, f1 0.999025\n",
      "2017-11-17T21:26:19.296172: step 2365, loss 0.0138169, acc 0.999023, f1 0.999024\n",
      "2017-11-17T21:26:19.806415: step 2370, loss 0.018606, acc 0.996094, f1 0.996094\n",
      "2017-11-17T21:26:20.279249: step 2375, loss 0.0197759, acc 0.995117, f1 0.995116\n",
      "Current epoch:  132\n",
      "2017-11-17T21:26:20.749638: step 2380, loss 0.013366, acc 1, f1 1\n",
      "2017-11-17T21:26:21.264774: step 2385, loss 0.016156, acc 0.998047, f1 0.998047\n",
      "2017-11-17T21:26:21.784666: step 2390, loss 0.0197273, acc 0.996094, f1 0.996094\n",
      "Current epoch:  133\n",
      "2017-11-17T21:26:22.248978: step 2395, loss 0.03045, acc 0.999023, f1 0.999024\n",
      "2017-11-17T21:26:22.776301: step 2400, loss 0.0531457, acc 0.996094, f1 0.996094\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:26:23.370248: step 2400, loss 1.31478, acc 0.596113, f1 0.594169\n",
      "\n",
      "2017-11-17T21:26:23.906970: step 2405, loss 0.0292205, acc 0.99707, f1 0.997069\n",
      "2017-11-17T21:26:24.443702: step 2410, loss 0.0226329, acc 0.999023, f1 0.999022\n",
      "Current epoch:  134\n",
      "2017-11-17T21:26:25.073621: step 2415, loss 0.0202112, acc 0.99707, f1 0.99707\n",
      "2017-11-17T21:26:25.581599: step 2420, loss 0.0179292, acc 0.999023, f1 0.999023\n",
      "2017-11-17T21:26:26.113491: step 2425, loss 0.0152945, acc 0.998047, f1 0.998047\n",
      "2017-11-17T21:26:26.613068: step 2430, loss 0.018923, acc 0.998428, f1 0.998428\n",
      "Current epoch:  135\n",
      "2017-11-17T21:26:27.134022: step 2435, loss 0.013681, acc 1, f1 1\n",
      "2017-11-17T21:26:27.633834: step 2440, loss 0.0156772, acc 0.996094, f1 0.996092\n",
      "2017-11-17T21:26:28.143647: step 2445, loss 0.0136631, acc 0.999023, f1 0.999023\n",
      "Current epoch:  136\n",
      "2017-11-17T21:26:28.636813: step 2450, loss 0.0135942, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:26:29.227397: step 2450, loss 1.40544, acc 0.588503, f1 0.588107\n",
      "\n",
      "2017-11-17T21:26:29.725955: step 2455, loss 0.0135559, acc 0.999023, f1 0.999023\n",
      "2017-11-17T21:26:30.252627: step 2460, loss 0.0168786, acc 0.998047, f1 0.998046\n",
      "2017-11-17T21:26:30.741832: step 2465, loss 0.015666, acc 0.996094, f1 0.996097\n",
      "Current epoch:  137\n",
      "2017-11-17T21:26:31.218516: step 2470, loss 0.0126394, acc 0.999023, f1 0.999022\n",
      "2017-11-17T21:26:31.728185: step 2475, loss 0.011294, acc 1, f1 1\n",
      "2017-11-17T21:26:32.208130: step 2480, loss 0.0169364, acc 0.99707, f1 0.997069\n",
      "Current epoch:  138\n",
      "2017-11-17T21:26:32.674006: step 2485, loss 0.0134997, acc 0.998047, f1 0.998047\n",
      "2017-11-17T21:26:33.162607: step 2490, loss 0.0113913, acc 0.999023, f1 0.999022\n",
      "2017-11-17T21:26:33.664483: step 2495, loss 0.0146937, acc 0.998047, f1 0.998047\n",
      "2017-11-17T21:26:34.163482: step 2500, loss 0.0160091, acc 0.99707, f1 0.99707\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:26:34.761393: step 2500, loss 1.44151, acc 0.588018, f1 0.587828\n",
      "\n",
      "Current epoch:  139\n",
      "2017-11-17T21:26:35.267389: step 2505, loss 0.0112851, acc 1, f1 1\n",
      "2017-11-17T21:26:35.770903: step 2510, loss 0.0143432, acc 0.998047, f1 0.998051\n",
      "2017-11-17T21:26:36.270963: step 2515, loss 0.372504, acc 0.862305, f1 0.817609\n",
      "2017-11-17T21:26:36.807351: step 2520, loss 0.0256477, acc 0.998428, f1 0.998428\n",
      "Current epoch:  140\n",
      "2017-11-17T21:26:37.322874: step 2525, loss 0.0159482, acc 0.999023, f1 0.999023\n",
      "2017-11-17T21:26:37.841540: step 2530, loss 0.0166355, acc 0.998047, f1 0.998048\n",
      "2017-11-17T21:26:38.371608: step 2535, loss 0.0136214, acc 1, f1 1\n",
      "Current epoch:  141\n",
      "2017-11-17T21:26:38.868324: step 2540, loss 0.0140956, acc 0.999023, f1 0.999023\n",
      "2017-11-17T21:26:39.395464: step 2545, loss 0.0132926, acc 0.998047, f1 0.998047\n",
      "2017-11-17T21:26:39.911483: step 2550, loss 0.0155495, acc 0.99707, f1 0.997069\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:26:40.502816: step 2550, loss 1.44468, acc 0.586031, f1 0.585649\n",
      "\n",
      "2017-11-17T21:26:41.044384: step 2555, loss 0.0124557, acc 0.998047, f1 0.998047\n",
      "Current epoch:  142\n",
      "2017-11-17T21:26:41.518726: step 2560, loss 0.0145817, acc 0.998047, f1 0.998047\n",
      "2017-11-17T21:26:42.020596: step 2565, loss 0.0111331, acc 1, f1 1\n",
      "2017-11-17T21:26:42.543858: step 2570, loss 0.0156996, acc 0.995117, f1 0.995122\n",
      "Current epoch:  143\n",
      "2017-11-17T21:26:43.019548: step 2575, loss 0.0116474, acc 0.999023, f1 0.999025\n",
      "2017-11-17T21:26:43.564175: step 2580, loss 0.0124604, acc 0.998047, f1 0.998047\n",
      "2017-11-17T21:26:44.073604: step 2585, loss 0.0145118, acc 0.996094, f1 0.996094\n",
      "2017-11-17T21:26:44.587203: step 2590, loss 0.0154781, acc 0.995117, f1 0.995119\n",
      "Current epoch:  144\n",
      "2017-11-17T21:26:45.144478: step 2595, loss 0.0144319, acc 0.99707, f1 0.997071\n",
      "2017-11-17T21:26:45.699761: step 2600, loss 0.013631, acc 0.99707, f1 0.997069\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:26:46.288344: step 2600, loss 1.5182, acc 0.579779, f1 0.578991\n",
      "\n",
      "2017-11-17T21:26:46.780948: step 2605, loss 0.0143811, acc 0.999023, f1 0.999022\n",
      "2017-11-17T21:26:47.264242: step 2610, loss 1.343, acc 0.624214, f1 0.54588\n",
      "Current epoch:  145\n",
      "2017-11-17T21:26:47.768383: step 2615, loss 0.0172865, acc 1, f1 1\n",
      "2017-11-17T21:26:48.291860: step 2620, loss 0.015886, acc 0.999023, f1 0.999022\n",
      "2017-11-17T21:26:48.784240: step 2625, loss 0.0182408, acc 0.996094, f1 0.996093\n",
      "Current epoch:  146\n",
      "2017-11-17T21:26:49.260692: step 2630, loss 0.0156732, acc 0.99707, f1 0.997069\n",
      "2017-11-17T21:26:49.798965: step 2635, loss 0.0155959, acc 0.99707, f1 0.997069\n",
      "2017-11-17T21:26:50.309311: step 2640, loss 0.0108358, acc 1, f1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-17T21:26:50.832246: step 2645, loss 0.0135304, acc 0.999023, f1 0.999023\n",
      "Current epoch:  147\n",
      "2017-11-17T21:26:51.341985: step 2650, loss 0.0142829, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:26:51.944744: step 2650, loss 1.44084, acc 0.587873, f1 0.588105\n",
      "\n",
      "2017-11-17T21:26:52.490154: step 2655, loss 0.0126971, acc 0.998047, f1 0.998048\n",
      "2017-11-17T21:26:52.983662: step 2660, loss 0.0147691, acc 0.996094, f1 0.99609\n",
      "Current epoch:  148\n",
      "2017-11-17T21:26:53.453062: step 2665, loss 0.0108472, acc 1, f1 1\n",
      "2017-11-17T21:26:53.963987: step 2670, loss 0.0138781, acc 0.99707, f1 0.997071\n",
      "2017-11-17T21:26:54.451973: step 2675, loss 0.0130161, acc 0.998047, f1 0.998051\n",
      "2017-11-17T21:26:54.956900: step 2680, loss 0.0114469, acc 0.998047, f1 0.998047\n",
      "Current epoch:  149\n",
      "2017-11-17T21:26:55.439220: step 2685, loss 0.0125751, acc 0.998047, f1 0.998047\n",
      "2017-11-17T21:26:55.924000: step 2690, loss 0.0148008, acc 0.996094, f1 0.996098\n",
      "2017-11-17T21:26:56.467580: step 2695, loss 0.0101745, acc 0.998047, f1 0.998046\n",
      "2017-11-17T21:26:56.965196: step 2700, loss 0.0137763, acc 0.996855, f1 0.996855\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:26:57.568347: step 2700, loss 1.50068, acc 0.583365, f1 0.583791\n",
      "\n",
      "Current epoch:  150\n",
      "2017-11-17T21:26:58.077502: step 2705, loss 0.0107511, acc 0.999023, f1 0.999023\n",
      "2017-11-17T21:26:58.569665: step 2710, loss 0.0116643, acc 0.998047, f1 0.998047\n",
      "2017-11-17T21:26:59.087003: step 2715, loss 0.0154277, acc 0.995117, f1 0.99512\n",
      "Current epoch:  151\n",
      "2017-11-17T21:26:59.601509: step 2720, loss 0.0106421, acc 0.999023, f1 0.999022\n",
      "2017-11-17T21:27:00.086936: step 2725, loss 0.00835635, acc 1, f1 1\n",
      "2017-11-17T21:27:00.616710: step 2730, loss 0.0115604, acc 0.998047, f1 0.998047\n",
      "2017-11-17T21:27:01.099605: step 2735, loss 0.020756, acc 0.999023, f1 0.999025\n",
      "Current epoch:  152\n",
      "2017-11-17T21:27:01.554053: step 2740, loss 0.497912, acc 0.770508, f1 0.753302\n",
      "2017-11-17T21:27:02.061002: step 2745, loss 0.0179159, acc 0.999023, f1 0.999023\n",
      "2017-11-17T21:27:02.587391: step 2750, loss 0.0201997, acc 0.995117, f1 0.99513\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:27:03.196000: step 2750, loss 1.42254, acc 0.587534, f1 0.586993\n",
      "\n",
      "Current epoch:  153\n",
      "2017-11-17T21:27:03.735229: step 2755, loss 0.0113561, acc 1, f1 1\n",
      "2017-11-17T21:27:04.239246: step 2760, loss 0.0123197, acc 0.999023, f1 0.999022\n",
      "2017-11-17T21:27:04.749149: step 2765, loss 0.0133976, acc 0.99707, f1 0.997071\n",
      "2017-11-17T21:27:05.255966: step 2770, loss 0.0115805, acc 1, f1 1\n",
      "Current epoch:  154\n",
      "2017-11-17T21:27:05.755752: step 2775, loss 0.0124274, acc 0.999023, f1 0.999023\n",
      "2017-11-17T21:27:06.303901: step 2780, loss 0.0114436, acc 0.998047, f1 0.998048\n",
      "2017-11-17T21:27:06.851506: step 2785, loss 0.0107771, acc 1, f1 1\n",
      "2017-11-17T21:27:07.311179: step 2790, loss 0.0168714, acc 0.992138, f1 0.992142\n",
      "Current epoch:  155\n",
      "2017-11-17T21:27:07.809491: step 2795, loss 0.0109444, acc 0.998047, f1 0.998047\n",
      "2017-11-17T21:27:08.347097: step 2800, loss 0.0122456, acc 0.99707, f1 0.99707\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:27:08.966781: step 2800, loss 1.48363, acc 0.589812, f1 0.589519\n",
      "\n",
      "2017-11-17T21:27:09.488788: step 2805, loss 0.0106067, acc 0.998047, f1 0.998047\n",
      "Current epoch:  156\n",
      "2017-11-17T21:27:09.998047: step 2810, loss 0.00869462, acc 1, f1 1\n",
      "2017-11-17T21:27:10.514213: step 2815, loss 0.0111992, acc 0.998047, f1 0.998048\n",
      "2017-11-17T21:27:11.016258: step 2820, loss 0.0137563, acc 0.996094, f1 0.996093\n",
      "2017-11-17T21:27:11.520601: step 2825, loss 0.00918481, acc 0.999023, f1 0.999023\n",
      "Current epoch:  157\n",
      "2017-11-17T21:27:12.002523: step 2830, loss 0.00678255, acc 1, f1 1\n",
      "2017-11-17T21:27:12.502571: step 2835, loss 0.0100574, acc 0.999023, f1 0.999024\n",
      "2017-11-17T21:27:12.994353: step 2840, loss 0.0101092, acc 0.998047, f1 0.998047\n",
      "Current epoch:  158\n",
      "2017-11-17T21:27:13.498386: step 2845, loss 0.0102289, acc 0.998047, f1 0.998047\n",
      "2017-11-17T21:27:14.020757: step 2850, loss 0.00798197, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:27:14.644034: step 2850, loss 1.52805, acc 0.58671, f1 0.587041\n",
      "\n",
      "2017-11-17T21:27:15.217725: step 2855, loss 0.0105135, acc 0.998047, f1 0.998046\n",
      "2017-11-17T21:27:15.706201: step 2860, loss 0.00862142, acc 0.999023, f1 0.999023\n",
      "Current epoch:  159\n",
      "2017-11-17T21:27:16.170300: step 2865, loss 0.0104916, acc 0.998047, f1 0.998047\n",
      "2017-11-17T21:27:16.681680: step 2870, loss 0.00658324, acc 1, f1 1\n",
      "2017-11-17T21:27:17.212989: step 2875, loss 0.0106729, acc 0.998047, f1 0.998047\n",
      "2017-11-17T21:27:17.673609: step 2880, loss 0.0233653, acc 0.993711, f1 0.993707\n",
      "Current epoch:  160\n",
      "2017-11-17T21:27:18.222857: step 2885, loss 1.49686, acc 0.611328, f1 0.528215\n",
      "2017-11-17T21:27:18.734939: step 2890, loss 0.0180981, acc 0.999023, f1 0.999022\n",
      "2017-11-17T21:27:19.252245: step 2895, loss 0.0145997, acc 0.999023, f1 0.999024\n",
      "Current epoch:  161\n",
      "2017-11-17T21:27:19.724327: step 2900, loss 0.0119567, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:27:20.330157: step 2900, loss 1.45716, acc 0.588358, f1 0.588637\n",
      "\n",
      "2017-11-17T21:27:20.898614: step 2905, loss 0.0145848, acc 0.998047, f1 0.998047\n",
      "2017-11-17T21:27:21.409149: step 2910, loss 0.0137248, acc 0.99707, f1 0.99707\n",
      "2017-11-17T21:27:21.929134: step 2915, loss 0.0142579, acc 0.99707, f1 0.99707\n",
      "Current epoch:  162\n",
      "2017-11-17T21:27:22.422863: step 2920, loss 0.0101814, acc 0.999023, f1 0.999023\n",
      "2017-11-17T21:27:22.944375: step 2925, loss 0.013803, acc 0.996094, f1 0.996092\n",
      "2017-11-17T21:27:23.463805: step 2930, loss 0.011328, acc 0.998047, f1 0.998047\n",
      "Current epoch:  163\n",
      "2017-11-17T21:27:23.970538: step 2935, loss 0.0110884, acc 0.99707, f1 0.997071\n",
      "2017-11-17T21:27:24.435886: step 2940, loss 0.0100715, acc 0.998047, f1 0.998047\n",
      "2017-11-17T21:27:24.929637: step 2945, loss 0.0102059, acc 0.999023, f1 0.999023\n",
      "2017-11-17T21:27:25.441995: step 2950, loss 0.00868308, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:27:26.047244: step 2950, loss 1.51384, acc 0.588891, f1 0.588955\n",
      "\n",
      "Current epoch:  164\n",
      "2017-11-17T21:27:26.556840: step 2955, loss 0.00867067, acc 0.999023, f1 0.999023\n",
      "2017-11-17T21:27:27.093637: step 2960, loss 0.00839378, acc 1, f1 1\n",
      "2017-11-17T21:27:27.594102: step 2965, loss 0.00645268, acc 1, f1 1\n",
      "2017-11-17T21:27:28.091713: step 2970, loss 0.01414, acc 0.995283, f1 0.995283\n",
      "Current epoch:  165\n",
      "2017-11-17T21:27:28.610461: step 2975, loss 0.00901619, acc 0.999023, f1 0.999025\n",
      "2017-11-17T21:27:29.163496: step 2980, loss 0.0111545, acc 0.998047, f1 0.998047\n",
      "2017-11-17T21:27:29.674508: step 2985, loss 0.0131012, acc 0.996094, f1 0.996092\n",
      "Current epoch:  166\n",
      "2017-11-17T21:27:30.167296: step 2990, loss 0.0109353, acc 0.996094, f1 0.996094\n",
      "2017-11-17T21:27:30.703305: step 2995, loss 0.0107077, acc 0.99707, f1 0.997074\n",
      "2017-11-17T21:27:31.216508: step 3000, loss 0.0121021, acc 0.996094, f1 0.99609\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:27:31.806962: step 3000, loss 1.57407, acc 0.585013, f1 0.585735\n",
      "\n",
      "2017-11-17T21:27:32.357800: step 3005, loss 0.0154364, acc 0.995117, f1 0.995116\n",
      "Current epoch:  167\n",
      "2017-11-17T21:27:32.842654: step 3010, loss 0.00934823, acc 0.999023, f1 0.999024\n",
      "2017-11-17T21:27:33.326370: step 3015, loss 0.0126499, acc 0.998047, f1 0.998047\n",
      "2017-11-17T21:27:33.835620: step 3020, loss 0.00630886, acc 1, f1 1\n",
      "Current epoch:  168\n",
      "2017-11-17T21:27:34.326345: step 3025, loss 0.00646096, acc 0.999023, f1 0.999023\n",
      "2017-11-17T21:27:34.842695: step 3030, loss 0.00909028, acc 0.999023, f1 0.999023\n",
      "2017-11-17T21:27:35.364521: step 3035, loss 0.0860965, acc 0.969727, f1 0.968741\n",
      "2017-11-17T21:27:35.817238: step 3040, loss 0.171758, acc 0.931641, f1 0.931042\n",
      "Current epoch:  169\n",
      "2017-11-17T21:27:36.320579: step 3045, loss 0.0245863, acc 0.99707, f1 0.99707\n",
      "2017-11-17T21:27:36.833775: step 3050, loss 0.0186487, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:27:37.431565: step 3050, loss 1.45045, acc 0.586758, f1 0.5863\n",
      "\n",
      "2017-11-17T21:27:37.946943: step 3055, loss 0.0145154, acc 0.998047, f1 0.998047\n",
      "2017-11-17T21:27:38.445372: step 3060, loss 0.0145544, acc 0.998428, f1 0.998428\n",
      "Current epoch:  170\n",
      "2017-11-17T21:27:38.967151: step 3065, loss 0.0150033, acc 0.996094, f1 0.996093\n",
      "2017-11-17T21:27:39.461121: step 3070, loss 0.0104242, acc 1, f1 1\n",
      "2017-11-17T21:27:39.964440: step 3075, loss 0.0110285, acc 1, f1 1\n",
      "Current epoch:  171\n",
      "2017-11-17T21:27:40.459374: step 3080, loss 0.0109194, acc 0.998047, f1 0.998048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-17T21:27:40.977315: step 3085, loss 0.00862041, acc 1, f1 1\n",
      "2017-11-17T21:27:41.480189: step 3090, loss 0.0136617, acc 0.99707, f1 0.997067\n",
      "2017-11-17T21:27:41.983979: step 3095, loss 0.00722829, acc 1, f1 1\n",
      "Current epoch:  172\n",
      "2017-11-17T21:27:42.504887: step 3100, loss 0.00799536, acc 0.999023, f1 0.999024\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:27:43.083573: step 3100, loss 1.51283, acc 0.588648, f1 0.588307\n",
      "\n",
      "2017-11-17T21:27:43.592921: step 3105, loss 0.00953233, acc 0.998047, f1 0.998047\n",
      "2017-11-17T21:27:44.125382: step 3110, loss 0.00968772, acc 0.998047, f1 0.998048\n",
      "Current epoch:  173\n",
      "2017-11-17T21:27:44.637324: step 3115, loss 0.00893983, acc 0.999023, f1 0.999023\n",
      "2017-11-17T21:27:45.153345: step 3120, loss 0.0119802, acc 0.99707, f1 0.99707\n",
      "2017-11-17T21:27:45.675480: step 3125, loss 0.00897494, acc 0.998047, f1 0.998047\n",
      "2017-11-17T21:27:46.193141: step 3130, loss 0.00995636, acc 0.996094, f1 0.996095\n",
      "Current epoch:  174\n",
      "2017-11-17T21:27:46.681295: step 3135, loss 0.00726756, acc 0.999023, f1 0.999024\n",
      "2017-11-17T21:27:47.215236: step 3140, loss 0.00744169, acc 1, f1 1\n",
      "2017-11-17T21:27:47.694555: step 3145, loss 0.00896738, acc 0.99707, f1 0.997067\n",
      "2017-11-17T21:27:48.208614: step 3150, loss 0.00790494, acc 0.998428, f1 0.998428\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:27:48.797547: step 3150, loss 1.57988, acc 0.584626, f1 0.584504\n",
      "\n",
      "Current epoch:  175\n",
      "2017-11-17T21:27:49.309291: step 3155, loss 0.012981, acc 0.995117, f1 0.995119\n",
      "2017-11-17T21:27:49.801129: step 3160, loss 0.00900475, acc 0.99707, f1 0.997069\n",
      "2017-11-17T21:27:50.312663: step 3165, loss 0.012423, acc 0.996094, f1 0.996094\n",
      "Current epoch:  176\n",
      "2017-11-17T21:27:50.815164: step 3170, loss 0.00674067, acc 0.999023, f1 0.999023\n",
      "2017-11-17T21:27:51.344379: step 3175, loss 0.00952019, acc 0.998047, f1 0.998047\n",
      "2017-11-17T21:27:51.912632: step 3180, loss 0.00921204, acc 0.998047, f1 0.99805\n",
      "2017-11-17T21:27:52.432341: step 3185, loss 0.013164, acc 0.999023, f1 0.999024\n",
      "Current epoch:  177\n",
      "2017-11-17T21:27:52.887745: step 3190, loss 0.594128, acc 0.746094, f1 0.711448\n",
      "2017-11-17T21:27:53.372340: step 3195, loss 0.0179454, acc 0.998047, f1 0.998047\n",
      "2017-11-17T21:27:53.860322: step 3200, loss 0.0144652, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:27:54.457273: step 3200, loss 1.47409, acc 0.590878, f1 0.590241\n",
      "\n",
      "Current epoch:  178\n",
      "2017-11-17T21:27:54.977672: step 3205, loss 0.00835148, acc 1, f1 1\n",
      "2017-11-17T21:27:55.546495: step 3210, loss 0.0104414, acc 0.999023, f1 0.999023\n",
      "2017-11-17T21:27:56.033188: step 3215, loss 0.0159319, acc 0.996094, f1 0.996095\n",
      "2017-11-17T21:27:56.536953: step 3220, loss 0.0124119, acc 0.99707, f1 0.997057\n",
      "Current epoch:  179\n",
      "2017-11-17T21:27:57.038946: step 3225, loss 0.00841202, acc 0.998047, f1 0.998048\n",
      "2017-11-17T21:27:57.575418: step 3230, loss 0.00803897, acc 0.999023, f1 0.999023\n",
      "2017-11-17T21:27:58.138082: step 3235, loss 0.00995397, acc 0.998047, f1 0.998047\n",
      "2017-11-17T21:27:58.601336: step 3240, loss 0.00912384, acc 0.996855, f1 0.996854\n",
      "Current epoch:  180\n",
      "2017-11-17T21:27:59.125206: step 3245, loss 0.00613561, acc 1, f1 1\n",
      "2017-11-17T21:27:59.652434: step 3250, loss 0.0098832, acc 0.99707, f1 0.997068\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:28:00.265497: step 3250, loss 1.5549, acc 0.59049, f1 0.58946\n",
      "\n",
      "2017-11-17T21:28:00.806423: step 3255, loss 0.00963355, acc 0.998047, f1 0.998047\n",
      "Current epoch:  181\n",
      "2017-11-17T21:28:01.292928: step 3260, loss 0.00735492, acc 0.998047, f1 0.998042\n",
      "2017-11-17T21:28:01.836929: step 3265, loss 0.0078701, acc 0.998047, f1 0.998047\n",
      "2017-11-17T21:28:02.333431: step 3270, loss 0.0103055, acc 0.996094, f1 0.996091\n",
      "2017-11-17T21:28:02.839181: step 3275, loss 0.00780425, acc 0.998047, f1 0.998047\n",
      "Current epoch:  182\n",
      "2017-11-17T21:28:03.306008: step 3280, loss 0.0058635, acc 0.999023, f1 0.999022\n",
      "2017-11-17T21:28:03.798978: step 3285, loss 0.00733518, acc 0.999023, f1 0.999023\n",
      "2017-11-17T21:28:04.349440: step 3290, loss 0.00917118, acc 0.998047, f1 0.998047\n",
      "Current epoch:  183\n",
      "2017-11-17T21:28:04.862649: step 3295, loss 0.00598411, acc 1, f1 1\n",
      "2017-11-17T21:28:05.343025: step 3300, loss 0.007078, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:28:05.942979: step 3300, loss 1.59224, acc 0.591896, f1 0.591501\n",
      "\n",
      "2017-11-17T21:28:06.455490: step 3305, loss 0.0119967, acc 0.996094, f1 0.996089\n",
      "2017-11-17T21:28:06.947138: step 3310, loss 0.010811, acc 0.996094, f1 0.996094\n",
      "Current epoch:  184\n",
      "2017-11-17T21:28:07.437322: step 3315, loss 0.00951483, acc 0.999023, f1 0.999024\n",
      "2017-11-17T21:28:07.968704: step 3320, loss 0.01591, acc 0.995117, f1 0.995117\n",
      "2017-11-17T21:28:08.466401: step 3325, loss 0.0138844, acc 0.996094, f1 0.996094\n",
      "2017-11-17T21:28:08.976145: step 3330, loss 0.0119693, acc 0.998428, f1 0.998428\n",
      "Current epoch:  185\n",
      "2017-11-17T21:28:09.508974: step 3335, loss 0.0089196, acc 0.998047, f1 0.998047\n",
      "2017-11-17T21:28:10.018802: step 3340, loss 0.0179319, acc 0.998047, f1 0.998046\n",
      "2017-11-17T21:28:10.541814: step 3345, loss 0.105777, acc 0.96582, f1 0.96551\n",
      "Current epoch:  186\n",
      "2017-11-17T21:28:11.025660: step 3350, loss 0.0126209, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:28:11.619444: step 3350, loss 1.47934, acc 0.588697, f1 0.588485\n",
      "\n",
      "2017-11-17T21:28:12.172563: step 3355, loss 0.0173258, acc 0.995117, f1 0.995118\n",
      "2017-11-17T21:28:12.652496: step 3360, loss 0.0126106, acc 0.998047, f1 0.998047\n",
      "2017-11-17T21:28:13.185424: step 3365, loss 0.0116741, acc 0.998047, f1 0.998047\n",
      "Current epoch:  187\n",
      "2017-11-17T21:28:13.647048: step 3370, loss 0.00760865, acc 1, f1 1\n",
      "2017-11-17T21:28:14.124529: step 3375, loss 0.00906553, acc 0.998047, f1 0.998047\n",
      "2017-11-17T21:28:14.621408: step 3380, loss 0.00887773, acc 0.998047, f1 0.998052\n",
      "Current epoch:  188\n",
      "2017-11-17T21:28:15.113505: step 3385, loss 0.00622121, acc 0.999023, f1 0.999022\n",
      "2017-11-17T21:28:15.621002: step 3390, loss 0.0100307, acc 0.996094, f1 0.996092\n",
      "2017-11-17T21:28:16.142874: step 3395, loss 0.0057962, acc 1, f1 1\n",
      "2017-11-17T21:28:16.617810: step 3400, loss 0.00772357, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:28:17.203866: step 3400, loss 1.57011, acc 0.587001, f1 0.587562\n",
      "\n",
      "Current epoch:  189\n",
      "2017-11-17T21:28:17.697579: step 3405, loss 0.00829005, acc 0.998047, f1 0.998047\n",
      "2017-11-17T21:28:18.200040: step 3410, loss 0.00686086, acc 0.999023, f1 0.999023\n",
      "2017-11-17T21:28:18.708575: step 3415, loss 0.00746235, acc 0.998047, f1 0.998047\n",
      "2017-11-17T21:28:19.193428: step 3420, loss 0.0105242, acc 0.996855, f1 0.996856\n",
      "Current epoch:  190\n",
      "2017-11-17T21:28:19.732188: step 3425, loss 0.00739798, acc 0.998047, f1 0.998047\n",
      "2017-11-17T21:28:20.237612: step 3430, loss 0.0104338, acc 0.995117, f1 0.995111\n",
      "2017-11-17T21:28:20.735918: step 3435, loss 0.00924293, acc 0.998047, f1 0.998047\n",
      "Current epoch:  191\n",
      "2017-11-17T21:28:21.231127: step 3440, loss 0.0064203, acc 0.999023, f1 0.999024\n",
      "2017-11-17T21:28:21.727058: step 3445, loss 0.0082063, acc 0.999023, f1 0.999023\n",
      "2017-11-17T21:28:22.254690: step 3450, loss 0.00955656, acc 0.99707, f1 0.997069\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:28:22.843870: step 3450, loss 1.61075, acc 0.592381, f1 0.591458\n",
      "\n",
      "2017-11-17T21:28:23.353907: step 3455, loss 0.00629366, acc 0.999023, f1 0.999023\n",
      "Current epoch:  192\n",
      "2017-11-17T21:28:23.853195: step 3460, loss 0.00612438, acc 0.998047, f1 0.998047\n",
      "2017-11-17T21:28:24.365719: step 3465, loss 0.00914151, acc 0.99707, f1 0.99707\n",
      "2017-11-17T21:28:24.839438: step 3470, loss 0.0098562, acc 0.99707, f1 0.997069\n",
      "Current epoch:  193\n",
      "2017-11-17T21:28:25.327146: step 3475, loss 0.00465432, acc 1, f1 1\n",
      "2017-11-17T21:28:25.861300: step 3480, loss 0.00574174, acc 0.999023, f1 0.999024\n",
      "2017-11-17T21:28:26.357276: step 3485, loss 0.00880613, acc 0.99707, f1 0.99706\n",
      "2017-11-17T21:28:26.827312: step 3490, loss 0.00734557, acc 0.999023, f1 0.999023\n",
      "Current epoch:  194\n",
      "2017-11-17T21:28:27.391757: step 3495, loss 0.00449339, acc 0.999023, f1 0.999023\n",
      "2017-11-17T21:28:27.892200: step 3500, loss 0.00577164, acc 0.999023, f1 0.999024\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:28:28.507752: step 3500, loss 1.68821, acc 0.585692, f1 0.585886\n",
      "\n",
      "2017-11-17T21:28:29.057201: step 3505, loss 0.0198307, acc 0.999023, f1 0.999022\n",
      "2017-11-17T21:28:29.536266: step 3510, loss 0.0337032, acc 0.993711, f1 0.993707\n",
      "Current epoch:  195\n",
      "2017-11-17T21:28:30.033221: step 3515, loss 0.0149841, acc 0.999023, f1 0.999024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-17T21:28:30.543820: step 3520, loss 0.0115942, acc 0.998047, f1 0.998047\n",
      "2017-11-17T21:28:31.068212: step 3525, loss 0.0123883, acc 0.998047, f1 0.998047\n",
      "Current epoch:  196\n",
      "2017-11-17T21:28:31.567611: step 3530, loss 0.0121059, acc 0.996094, f1 0.996094\n",
      "2017-11-17T21:28:32.060940: step 3535, loss 0.00841089, acc 0.999023, f1 0.999023\n",
      "2017-11-17T21:28:32.563623: step 3540, loss 0.0068251, acc 1, f1 1\n",
      "2017-11-17T21:28:33.096238: step 3545, loss 0.0108673, acc 0.99707, f1 0.99707\n",
      "Current epoch:  197\n",
      "2017-11-17T21:28:33.568608: step 3550, loss 0.00778201, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:28:34.180251: step 3550, loss 1.57104, acc 0.587388, f1 0.587219\n",
      "\n",
      "2017-11-17T21:28:34.706973: step 3555, loss 0.0125152, acc 0.994141, f1 0.994149\n",
      "2017-11-17T21:28:35.233373: step 3560, loss 0.00674903, acc 0.999023, f1 0.999025\n",
      "Current epoch:  198\n",
      "2017-11-17T21:28:35.708241: step 3565, loss 0.00573806, acc 1, f1 1\n",
      "2017-11-17T21:28:36.204267: step 3570, loss 0.00668056, acc 0.999023, f1 0.999024\n",
      "2017-11-17T21:28:36.731667: step 3575, loss 0.00451729, acc 1, f1 1\n",
      "2017-11-17T21:28:37.265772: step 3580, loss 0.00764368, acc 0.998047, f1 0.998047\n",
      "Current epoch:  199\n",
      "2017-11-17T21:28:37.759998: step 3585, loss 0.0052996, acc 1, f1 1\n",
      "2017-11-17T21:28:38.273532: step 3590, loss 0.00657804, acc 0.999023, f1 0.999023\n",
      "2017-11-17T21:28:38.773993: step 3595, loss 0.00895816, acc 0.99707, f1 0.997069\n",
      "2017-11-17T21:28:39.291002: step 3600, loss 0.0154382, acc 0.992138, f1 0.992134\n",
      "\n",
      "Evaluation:\n",
      "2017-11-17T21:28:39.926505: step 3600, loss 1.64201, acc 0.580894, f1 0.583032\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "num_epochs = 200\n",
    "\n",
    "num_checkpoints = 5\n",
    "print_train_every = 5\n",
    "evaluate_every = 50\n",
    "checkpoint_every = 10000000\n",
    "allow_soft_placement=True\n",
    "log_device_placement=False\n",
    "\n",
    "with graph.as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=allow_soft_placement,\n",
    "      log_device_placement=log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        \n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdadeltaOptimizer(1.0)\n",
    "        grads_and_vars = optimizer.compute_gradients(loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=num_checkpoints)\n",
    "\n",
    "#         # Write vocabulary\n",
    "#         vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              input_x: x_batch,\n",
    "              input_y: y_batch,\n",
    "            }\n",
    "            _, step, summaries, cur_loss, cur_accuracy, y_pred = sess.run(\n",
    "                [train_op, global_step, train_summary_op, loss, accuracy, predictions],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "#             print(y_pred)\n",
    "#             print(y_batch)\n",
    "            if step % print_train_every == 0:\n",
    "                f1 = f1_score(y_batch, y_pred, average = 'weighted')\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}, f1 {:g}\".format(time_str, step, cur_loss, cur_accuracy,\n",
    "                                                                     f1))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              input_x: x_batch,\n",
    "              input_y: y_batch,\n",
    "            }\n",
    "            step, summaries, cur_loss, cur_accuracy, y_pred = sess.run(\n",
    "                [global_step, dev_summary_op, loss, accuracy, predictions],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            f1 = f1_score(y_batch, y_pred, average = 'weighted')\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}, f1 {:g}\".format(time_str, step, cur_loss, cur_accuracy,\n",
    "                                                                    f1))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "                \n",
    "        def dev_step_batch(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              input_x: x_batch,\n",
    "              input_y: y_batch,\n",
    "            }\n",
    "            step, summaries, cur_loss, cur_accuracy, y_pred = sess.run(\n",
    "                [global_step, dev_summary_op, loss, accuracy, predictions],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            f1 = f1_score(y_batch, y_pred, average = 'weighted')\n",
    "#             print(\"{}: step {}, loss {:g}, acc {:g}, f1 {:g}\".format(time_str, step, cur_loss, cur_accuracy,\n",
    "#                                                                     f1))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "            return cur_loss, cur_accuracy, f1\n",
    "\n",
    "        \n",
    "        sess.run(embedding_init, feed_dict={embedding_placeholder: final_embeddings})\n",
    "        # Generate batches\n",
    "        batches = batch_iter(\n",
    "            list(zip(x_train, y_train)), batch_size, num_epochs)\n",
    "        \n",
    "        batches_test = list(batch_iter(\n",
    "            list(zip(x_test, y_test)), batch_size, 1))\n",
    "        \n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            \n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_test, y_test, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_3.5]",
   "language": "python",
   "name": "conda-env-tensorflow_3.5-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
