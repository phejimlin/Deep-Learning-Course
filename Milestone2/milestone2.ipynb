{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "\n",
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from tensorflow.contrib import learn\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.metrics import f1_score\n",
    "from IPython.display import clear_output, Image, display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### Do not modify here ###### \n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = \"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = graph_def\n",
    "    #strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))\n",
    "###### Do not modify  here ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data_and_labels(train_data_file, test_data_file):\n",
    "    \"\"\"\n",
    "    Loads MR polarity data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "    # Load data from files\n",
    "    train_data = pd.read_csv(train_data_file, sep='\\t', quoting=csv.QUOTE_NONE, header=None, names=['id', 'language', 'label', 'text'])\n",
    "    test_data = pd.read_csv(test_data_file, sep='\\t', quoting=csv.QUOTE_NONE, header=None, names=['id', 'language', 'label', 'text'])\n",
    "    \n",
    "    x_train = train_data['text'].tolist()\n",
    "    y_train = train_data['label'].tolist()\n",
    "\n",
    "    x_test = test_data['text'].tolist()\n",
    "    y_test = test_data['label'].tolist()\n",
    "    \n",
    "    x_train = [s.strip() for s in x_train]\n",
    "    x_test = [s.strip() for s in x_test]\n",
    "    \n",
    "    label_encoding = {'neutral':0, 'positive':1, 'negative':2}\n",
    "    \n",
    "    y_train_encoding = [label_encoding[label] for label in y_train]    \n",
    "    y_test_encoding = [label_encoding[label] for label in y_test]\n",
    "\n",
    "    \n",
    "    return [x_train, y_train_encoding, x_test, y_test_encoding]\n",
    "\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Current epoch: \", epoch)\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_embeddings = np.load('./data/embed_tweets_en_200M_200D/embedding_matrix.npy')\n",
    "word_dict = {}\n",
    "with open('./data/embed_tweets_en_200M_200D/vocabulary.pickle', 'rb') as myfile:\n",
    "    word_dict = pickle.load(myfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> (94, 3868680)\n"
     ]
    }
   ],
   "source": [
    "# shit\n",
    "for key, val in word_dict.items():\n",
    "    if val[0] == 94:\n",
    "        print(key, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1859185, 200)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load label data\n",
    "x_train_sentence, y_train, x_test_sentence, y_test = load_data_and_labels('./data/supervised_data/en_full.tsv.txt', './data/supervised_data/en_test.tsv')\n",
    "# max_document_length = max([len(x.split(\" \")) for x in x_train_sentence])\n",
    "# print(max_document_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20632"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n",
      "[\"there's\", 'a', 'lot', 'of', 'stupid', '$', 'h', '!', 't', 'out', 'there', ',', 'but', 'polling', 'trump', 'v', 'kanye', 'west', 'may', 'take', 'the', 'cake', '.', 'all', 'i', 'can', 'think', 'to', 'say', 'is', ':', '#', '$', '%', '#', '$', '%', '$', '#', '%', '#', '$', '%', '#', '$', '%', '#', '$', '#', '$', '#', '$', '%']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    This function assumes that the last word in the word embedding is a zero vector, and will use it as padding.\n",
    "    The input 'num_voc' equals to the shape[0] of the word embedding.\n",
    "\"\"\"\n",
    "def process_tweet(train_tweets, test_tweets, num_voc):\n",
    "    # max_document_length = max([len(x.split(\" \")) for x in x_train_sentence])\n",
    "    ppl_re = re.compile(r'@\\S*')\n",
    "    url_re = re.compile(r'http\\S+')\n",
    "    tknzr = TweetTokenizer()\n",
    "    # tknzr = TweetTokenizer(reduce_len=True)\n",
    "    \n",
    "    tokenized_tweets_all = []\n",
    "    max_document_length = 0\n",
    "    \n",
    "    for tweets in [train_tweets, test_tweets]:\n",
    "        tweets = [url_re.sub('URLTOK', ppl_re.sub('USRTOK', tweet.lower())) for tweet in tweets]\n",
    "        tokenized_tweets = [tknzr.tokenize(tweet) for tweet in tweets]\n",
    "        tokenized_tweets_all.append(tokenized_tweets)\n",
    "        max_document_length = max(max_document_length, max([len(tweet) for tweet in tokenized_tweets]))\n",
    "    print(max_document_length)\n",
    "    \n",
    "    x = []\n",
    "    \n",
    "    for tokenized_tweets in tokenized_tweets_all:\n",
    "        x_curr = []\n",
    "        for tokenized_tweet in tokenized_tweets:\n",
    "            if len(tokenized_tweet) == max_document_length:\n",
    "                print(tokenized_tweet)\n",
    "            \"\"\"Not sure if original paper does this, but since index 0 means USRTOK, padding should be a number\n",
    "            higher than total word count, so tf.nn.embedding_lookup will return a tensor of 0 insted of USRTOK.\"\"\"\n",
    "        #     temp = np.zeros(max_document_length, dtype=np.int).tolist()\n",
    "            temp = (np.ones(max_document_length, dtype=np.int)*(num_voc-1)).tolist()\n",
    "\n",
    "            for index, word in enumerate(tokenized_tweet):\n",
    "                if word in word_dict:\n",
    "                    temp[index] = word_dict[word][0]\n",
    "            x_curr.append(temp)\n",
    "        x_curr = np.array(x_curr)\n",
    "        x.append(x_curr)\n",
    "    \n",
    "    return x[0], x[1]\n",
    "\n",
    "x_train, x_test = process_tweet(x_train_sentence, x_test_sentence, final_embeddings.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# x_train = []\n",
    "\n",
    "# for tweet in x_train_sentence:\n",
    "#     \"\"\"Not sure if original paper does this, but since index 0 means USRTOK, padding should be a number\n",
    "#     higher than total word count, so tf.nn.embedding_lookup will return a tensor of 0 insted of USRTOK.\"\"\"\n",
    "# #     temp = np.zeros(max_document_length, dtype=np.int).tolist()\n",
    "#     temp = (np.ones(max_document_length, dtype=np.int)).tolist()\n",
    "    \n",
    "#     # shitty paper preprocessing\n",
    "#     tweet = tweet.lower()\n",
    "#     tweet = ppl_re.sub('USRTOK', tweet)\n",
    "#     tweet = url_re.sub('URLTOK', tweet)\n",
    "    \n",
    "#     for index, word in enumerate(tknzr.tokenize(tweet)):\n",
    "#         if word in word_dict:\n",
    "#             temp[index] = word_dict[word][0]\n",
    "#         else:\n",
    "#             print(word)\n",
    "#             break\n",
    "#     x_train.append(temp)\n",
    "# x_train = np.array(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 53)\n",
      "(?, 53, 200, 1)\n",
      "CNN filter (4, 200, 1, 200)\n",
      "CNN filter (3, 1, 200, 200)\n",
      "(?, 22, 1, 200)\n",
      "h_pool_flat (?, 200)\n",
      "scores (?, 3)\n",
      "predictions (?,)\n",
      "losses (?,)\n"
     ]
    }
   ],
   "source": [
    "# put Word2Vec on 590 million English Tweets using 52 dimensions.\n",
    "vocabulary_size = final_embeddings.shape[0]\n",
    "embedding_size = 200  # Dimension of the embedding vector.\n",
    "graph = tf.Graph()\n",
    "\n",
    "sequence_length=x_train.shape[1]\n",
    "num_classes=3\n",
    "\n",
    "# filter_sizes: The number of words we want our convolutional filters to cover. \n",
    "# We will have num_filters for each size specified here. \n",
    "# For example, [3, 4, 5] means that we will have filters that slide over 3, 4 and 5 words respectively, for a total of 3 * num_filters filters.\n",
    "first_filter_sizes = [4]\n",
    "first_pool_window_sizes = [4]\n",
    "first_pool_strides = [2]\n",
    "\n",
    "\n",
    "second_filter_window_sizes = [3]\n",
    "num_filters = 200\n",
    "\n",
    "# No L2 norm\n",
    "l2_reg_lambda=0.0\n",
    "\n",
    "with graph.as_default():\n",
    "    input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "    input_y = tf.placeholder(tf.int64, [None], name=\"input_y\")\n",
    "    with tf.device('/cpu:0'):\n",
    "        embeddings = tf.Variable(tf.constant(0.0, shape=[vocabulary_size, embedding_size]),\n",
    "                        trainable=False, name=\"embedding\")\n",
    "\n",
    "        embedding_placeholder = tf.placeholder(tf.float32, [vocabulary_size, embedding_size], name='word_embedding_placeholder')\n",
    "        embedding_init = embeddings.assign(embedding_placeholder)  # assign exist word embeddings\n",
    "\n",
    "        embedded_chars = tf.nn.embedding_lookup(embeddings, input_x)\n",
    "    embedded_chars_expanded = tf.expand_dims(embedded_chars, -1)\n",
    "    print(input_x.shape)\n",
    "    print(embedded_chars_expanded.shape)\n",
    "    # Keeping track of l2 regularization loss (optional)\n",
    "    l2_loss = tf.constant(0.0)\n",
    "    \n",
    "     # Add variable initializer.\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    \n",
    "    # Create first cnn : a convolution + maxpool layer for each filter size    \n",
    "    # 1st Convolution Layer\n",
    "    for i, first_filter_size in enumerate(first_filter_sizes):\n",
    "        with tf.name_scope(\"conv-maxpool-1\"):\n",
    "            # Convolution Layer\n",
    "            filter_shape = [first_filter_size, embedding_size, 1, num_filters]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "            print(\"CNN filter\", W.shape)\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "            conv = tf.nn.conv2d(\n",
    "                embedded_chars_expanded,\n",
    "                W,\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding=\"VALID\",\n",
    "                name=\"conv\")\n",
    "            # Apply nonlinearity\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "            # Maxpooling over the outputs and transform for next layer, so the \"channel\" of convolution \n",
    "            # will become \"input_width\" for next layer\n",
    "#             pooled = tf.transpose(tf.nn.max_pool(\n",
    "#                 h,\n",
    "#                 ksize=[1, first_pool_window_sizes[i], 1, 1],\n",
    "#                 strides=[1, first_pool_strides[i], 1, 1],\n",
    "#                 padding='VALID',\n",
    "#                 name=\"pool\"), perm=[0, 1, 3, 2])\n",
    "            pooled = tf.nn.max_pool(\n",
    "                h,\n",
    "                ksize=[1, first_pool_window_sizes[i], 1, 1],\n",
    "                strides=[1, first_pool_strides[i], 1, 1],\n",
    "                padding='VALID',\n",
    "                name=\"pool\")\n",
    "#     print(\"conv1\", conv.shape)\n",
    "#     print(\"h1\", h.shape)\n",
    "#     print(\"pooled1\", pooled_1.shape)\n",
    "    \n",
    "    # 2nd Convolutional Layer\n",
    "#     for i, second_filter_size in enumerate(second_filter_window_sizes):\n",
    "#         with tf.name_scope(\"conv-maxpool-2\"):\n",
    "#             # Convolution Layer\n",
    "#             filter_shape = [second_filter_size, num_filters, 1, num_filters]\n",
    "#             W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "#             print(\"CNN filter\", W.shape)\n",
    "#             b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "#             conv = tf.nn.conv2d(\n",
    "#                 pooled,\n",
    "#                 W,\n",
    "#                 strides=[1, 1, 1, 1],\n",
    "#                 padding=\"VALID\",\n",
    "#                 name=\"conv\")\n",
    "#             # Apply nonlinearity\n",
    "#             h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "#             # Maxpooling over the outputs and transform for next layer, so the \"channel\" of convolution \n",
    "#             # will become \"input_width\" for next layer\n",
    "#             pooled = tf.nn.max_pool(\n",
    "#                 h,\n",
    "#                 ksize=[1, h.shape[1], 1, 1],\n",
    "#                 strides=[1, 1, 1, 1],\n",
    "#                 padding='VALID',\n",
    "#                 name=\"pool\")\n",
    "    for i, second_filter_size in enumerate(second_filter_window_sizes):\n",
    "        with tf.name_scope(\"conv-maxpool-2\"):\n",
    "            # Convolution Layer\n",
    "            filter_shape = [second_filter_size, 1, num_filters, num_filters]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "            print(\"CNN filter\", W.shape)\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "            conv = tf.nn.conv2d(\n",
    "                pooled,\n",
    "                W,\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding=\"VALID\",\n",
    "                name=\"conv\")\n",
    "            # Apply nonlinearity\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "            print(h.shape)\n",
    "            # Maxpooling over the outputs and transform for next layer, so the \"channel\" of convolution \n",
    "            # will become \"input_width\" for next layer\n",
    "            pooled = tf.nn.max_pool(\n",
    "                h,\n",
    "                ksize=[1, h.shape[1], 1, 1],\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding='VALID',\n",
    "                name=\"pool\")\n",
    " \n",
    "\n",
    "    h_pool_flat = tf.reshape(pooled, [-1, num_filters])  # flatten pooling layers\n",
    "    print(\"h_pool_flat\", h_pool_flat.shape)\n",
    "    \n",
    "    # Add dropout\n",
    "#     with tf.name_scope(\"dropout\"):\n",
    "#         self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "    # Final (unnormalized) scores and predictions\n",
    "    \n",
    "    # Fully connected hidden layer\n",
    "    with tf.name_scope(\"hidden\"):\n",
    "        with tf.variable_scope(\"hidden\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters, num_filters],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            out = tf.nn.relu(tf.nn.xw_plus_b(h_pool_flat, W, b))\n",
    "        \n",
    "    \n",
    "    with tf.name_scope(\"output\"):\n",
    "        with tf.variable_scope(\"output\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            scores = tf.nn.xw_plus_b(out, W, b, name=\"scores\")\n",
    "            print(\"scores\", scores.shape)\n",
    "            predictions = tf.argmax(scores, 1, name=\"predictions\")\n",
    "            print(\"predictions\", predictions.shape)\n",
    "\n",
    "\n",
    "    # Calculate mean cross-entropy loss\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=scores, labels=input_y)\n",
    "        print(\"losses\", losses.shape)\n",
    "        loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "    # Accuracy\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        correct_predictions = tf.equal(predictions, input_y)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.04052442399509648&quot;).pbtxt = 'node {\\n  name: &quot;input_x&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: -1\\n        }\\n        dim {\\n          size: 53\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;input_y&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: -1\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Const&quot;\\n  op: &quot;Const&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 1859185\\n          }\\n          dim {\\n            size: 200\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;embedding&quot;\\n  op: &quot;VariableV2&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 1859185\\n        }\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;embedding/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;embedding&quot;\\n  input: &quot;Const&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;embedding/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;embedding&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;word_embedding_placeholder&quot;\\n  op: &quot;Placeholder&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 1859185\\n        }\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;embedding&quot;\\n  input: &quot;word_embedding_placeholder&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;embedding_lookup&quot;\\n  op: &quot;Gather&quot;\\n  input: &quot;embedding/read&quot;\\n  input: &quot;input_x&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;Tindices&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tparams&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_indices&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ExpandDims/dim&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: -1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ExpandDims&quot;\\n  op: &quot;ExpandDims&quot;\\n  input: &quot;embedding_lookup&quot;\\n  input: &quot;ExpandDims/dim&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tdim&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Const_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;init&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^embedding/Assign&quot;\\n  device: &quot;/device:CPU:0&quot;\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/truncated_normal/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\004\\\\000\\\\000\\\\000\\\\310\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\310\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/truncated_normal/mean&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/truncated_normal/stddev&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.10000000149011612\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/truncated_normal/TruncatedNormal&quot;\\n  op: &quot;TruncatedNormal&quot;\\n  input: &quot;conv-maxpool-1/truncated_normal/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/truncated_normal/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;conv-maxpool-1/truncated_normal/TruncatedNormal&quot;\\n  input: &quot;conv-maxpool-1/truncated_normal/stddev&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/truncated_normal&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;conv-maxpool-1/truncated_normal/mul&quot;\\n  input: &quot;conv-maxpool-1/truncated_normal/mean&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/W&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 4\\n        }\\n        dim {\\n          size: 200\\n        }\\n        dim {\\n          size: 1\\n        }\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/W/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;conv-maxpool-1/W&quot;\\n  input: &quot;conv-maxpool-1/truncated_normal&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-1/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/W/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;conv-maxpool-1/W&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-1/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 200\\n          }\\n        }\\n        float_val: 0.10000000149011612\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/b&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/b/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;conv-maxpool-1/b&quot;\\n  input: &quot;conv-maxpool-1/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-1/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/b/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;conv-maxpool-1/b&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-1/b&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/conv&quot;\\n  op: &quot;Conv2D&quot;\\n  input: &quot;ExpandDims&quot;\\n  input: &quot;conv-maxpool-1/W/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;VALID&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;conv-maxpool-1/conv&quot;\\n  input: &quot;conv-maxpool-1/b/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;conv-maxpool-1/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/pool&quot;\\n  op: &quot;MaxPool&quot;\\n  input: &quot;conv-maxpool-1/relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;ksize&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 4\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;VALID&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 2\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/truncated_normal/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\003\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\310\\\\000\\\\000\\\\000\\\\310\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/truncated_normal/mean&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/truncated_normal/stddev&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.10000000149011612\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/truncated_normal/TruncatedNormal&quot;\\n  op: &quot;TruncatedNormal&quot;\\n  input: &quot;conv-maxpool-2/truncated_normal/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/truncated_normal/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;conv-maxpool-2/truncated_normal/TruncatedNormal&quot;\\n  input: &quot;conv-maxpool-2/truncated_normal/stddev&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/truncated_normal&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;conv-maxpool-2/truncated_normal/mul&quot;\\n  input: &quot;conv-maxpool-2/truncated_normal/mean&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/W&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 3\\n        }\\n        dim {\\n          size: 1\\n        }\\n        dim {\\n          size: 200\\n        }\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/W/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;conv-maxpool-2/W&quot;\\n  input: &quot;conv-maxpool-2/truncated_normal&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-2/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/W/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;conv-maxpool-2/W&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-2/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 200\\n          }\\n        }\\n        float_val: 0.10000000149011612\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/b&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/b/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;conv-maxpool-2/b&quot;\\n  input: &quot;conv-maxpool-2/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-2/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/b/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;conv-maxpool-2/b&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-2/b&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/conv&quot;\\n  op: &quot;Conv2D&quot;\\n  input: &quot;conv-maxpool-1/pool&quot;\\n  input: &quot;conv-maxpool-2/W/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;VALID&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;conv-maxpool-2/conv&quot;\\n  input: &quot;conv-maxpool-2/b/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;conv-maxpool-2/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/pool&quot;\\n  op: &quot;MaxPool&quot;\\n  input: &quot;conv-maxpool-2/relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;ksize&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 22\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;VALID&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\310\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;conv-maxpool-2/pool&quot;\\n  input: &quot;Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\310\\\\000\\\\000\\\\000\\\\310\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.12247448414564133\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.12247448414564133\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform/max&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform/mul&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 200\\n        }\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden/W&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden/W&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 200\\n          }\\n        }\\n        float_val: 0.10000000149011612\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/b&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/b/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden/hidden/b&quot;\\n  input: &quot;hidden/hidden/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/hidden/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/b/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden/hidden/b&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/hidden/b&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/L2Loss&quot;\\n  op: &quot;L2Loss&quot;\\n  input: &quot;hidden/W/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Const_1&quot;\\n  input: &quot;hidden/hidden/L2Loss&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/L2Loss_1&quot;\\n  op: &quot;L2Loss&quot;\\n  input: &quot;hidden/hidden/b/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/add_1&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden/hidden/add&quot;\\n  input: &quot;hidden/hidden/L2Loss_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/xw_plus_b/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Reshape&quot;\\n  input: &quot;hidden/W/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/xw_plus_b&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;hidden/hidden/xw_plus_b/MatMul&quot;\\n  input: &quot;hidden/hidden/b/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;hidden/hidden/xw_plus_b&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\310\\\\000\\\\000\\\\000\\\\003\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.17192047834396362\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.17192047834396362\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;output/W/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;output/W/Initializer/random_uniform/max&quot;\\n  input: &quot;output/W/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;output/W/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;output/W/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;output/W/Initializer/random_uniform/mul&quot;\\n  input: &quot;output/W/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 200\\n        }\\n        dim {\\n          size: 3\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;output/W&quot;\\n  input: &quot;output/W/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;output/W&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        float_val: 0.10000000149011612\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/b&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 3\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/b/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;output/output/b&quot;\\n  input: &quot;output/output/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/output/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/b/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;output/output/b&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/output/b&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/L2Loss&quot;\\n  op: &quot;L2Loss&quot;\\n  input: &quot;output/W/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden/hidden/add_1&quot;\\n  input: &quot;output/output/L2Loss&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/L2Loss_1&quot;\\n  op: &quot;L2Loss&quot;\\n  input: &quot;output/output/b/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/add_1&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;output/output/add&quot;\\n  input: &quot;output/output/L2Loss_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/scores/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;hidden/hidden/Relu&quot;\\n  input: &quot;output/W/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/scores&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;output/output/scores/MatMul&quot;\\n  input: &quot;output/output/b/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/predictions/dimension&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/predictions&quot;\\n  op: &quot;ArgMax&quot;\\n  input: &quot;output/output/scores&quot;\\n  input: &quot;output/output/predictions/dimension&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;output_type&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;input_y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  op: &quot;SparseSoftmaxCrossEntropyWithLogits&quot;\\n  input: &quot;output/output/scores&quot;\\n  input: &quot;input_y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tlabels&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/Mean&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  input: &quot;loss/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/mul/x&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;loss/mul/x&quot;\\n  input: &quot;output/output/add_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;loss/Mean&quot;\\n  input: &quot;loss/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;accuracy/Equal&quot;\\n  op: &quot;Equal&quot;\\n  input: &quot;output/output/predictions&quot;\\n  input: &quot;input_y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;accuracy/Cast&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;accuracy/Equal&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_BOOL\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;accuracy/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;accuracy/accuracy&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;accuracy/Cast&quot;\\n  input: &quot;accuracy/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nversions {\\n  producer: 24\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.04052442399509648&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_graph(graph.as_graph_def())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-1/W:0/grad/hist is illegal; using conv-maxpool-1/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-1/W:0/grad/sparsity is illegal; using conv-maxpool-1/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-1/b:0/grad/hist is illegal; using conv-maxpool-1/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-1/b:0/grad/sparsity is illegal; using conv-maxpool-1/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/W:0/grad/hist is illegal; using conv-maxpool-2/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/W:0/grad/sparsity is illegal; using conv-maxpool-2/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/b:0/grad/hist is illegal; using conv-maxpool-2/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/b:0/grad/sparsity is illegal; using conv-maxpool-2/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name hidden/W:0/grad/hist is illegal; using hidden/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name hidden/W:0/grad/sparsity is illegal; using hidden/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name hidden/hidden/b:0/grad/hist is illegal; using hidden/hidden/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name hidden/hidden/b:0/grad/sparsity is illegal; using hidden/hidden/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/W:0/grad/hist is illegal; using output/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/W:0/grad/sparsity is illegal; using output/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/output/b:0/grad/hist is illegal; using output/output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/output/b:0/grad/sparsity is illegal; using output/output/b_0/grad/sparsity instead.\n",
      "Writing to E:\\HW\\Machine Learning\\Deep-Learning-Course\\Milestone2\\runs\\1510757410\n",
      "\n",
      "Current epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T22:50:22.687672: step 5, loss 1.30267, acc 0.407227, f1 0.235688\n",
      "2017-11-15T22:50:23.229151: step 10, loss 1.03652, acc 0.435547, f1 0.324913\n",
      "2017-11-15T22:50:23.777150: step 15, loss 0.983748, acc 0.470703, f1 0.428884\n",
      "Current epoch:  1\n",
      "2017-11-15T22:50:24.292058: step 20, loss 1.04705, acc 0.447266, f1 0.319714\n",
      "2017-11-15T22:50:24.806651: step 25, loss 1.01988, acc 0.444336, f1 0.329845\n",
      "2017-11-15T22:50:25.318222: step 30, loss 0.951115, acc 0.515625, f1 0.478291\n",
      "2017-11-15T22:50:25.832428: step 35, loss 0.937164, acc 0.520508, f1 0.467849\n",
      "Current epoch:  2\n",
      "2017-11-15T22:50:26.317045: step 40, loss 0.986472, acc 0.476562, f1 0.362407\n",
      "2017-11-15T22:50:26.827950: step 45, loss 1.03645, acc 0.507812, f1 0.412241\n",
      "2017-11-15T22:50:27.357403: step 50, loss 0.929724, acc 0.536133, f1 0.502296\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:50:28.061947: step 50, loss 0.947633, acc 0.50475, f1 0.481905\n",
      "\n",
      "Current epoch:  3\n",
      "2017-11-15T22:50:28.557343: step 55, loss 0.946152, acc 0.508789, f1 0.438146\n",
      "2017-11-15T22:50:29.074851: step 60, loss 0.973337, acc 0.479492, f1 0.425032\n",
      "2017-11-15T22:50:29.601355: step 65, loss 0.971117, acc 0.483398, f1 0.364229\n",
      "2017-11-15T22:50:30.119280: step 70, loss 0.954119, acc 0.487305, f1 0.395033\n",
      "Current epoch:  4\n",
      "2017-11-15T22:50:30.616895: step 75, loss 0.89412, acc 0.546875, f1 0.509311\n",
      "2017-11-15T22:50:31.124811: step 80, loss 0.97056, acc 0.489258, f1 0.464622\n",
      "2017-11-15T22:50:31.638752: step 85, loss 0.874826, acc 0.572266, f1 0.542267\n",
      "2017-11-15T22:50:32.120625: step 90, loss 0.934161, acc 0.507862, f1 0.458989\n",
      "Current epoch:  5\n",
      "2017-11-15T22:50:32.651048: step 95, loss 0.876177, acc 0.587891, f1 0.582851\n",
      "2017-11-15T22:50:33.184680: step 100, loss 0.972274, acc 0.532227, f1 0.476528\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:50:33.854761: step 100, loss 0.919905, acc 0.549631, f1 0.532648\n",
      "\n",
      "2017-11-15T22:50:34.364661: step 105, loss 0.861793, acc 0.59082, f1 0.566367\n",
      "Current epoch:  6\n",
      "2017-11-15T22:50:34.844661: step 110, loss 0.960312, acc 0.487305, f1 0.41244\n",
      "2017-11-15T22:50:35.355107: step 115, loss 0.971734, acc 0.475586, f1 0.406\n",
      "2017-11-15T22:50:35.863070: step 120, loss 0.902293, acc 0.532227, f1 0.479501\n",
      "2017-11-15T22:50:36.367975: step 125, loss 0.85449, acc 0.574219, f1 0.566398\n",
      "Current epoch:  7\n",
      "2017-11-15T22:50:36.846317: step 130, loss 0.853297, acc 0.55957, f1 0.511734\n",
      "2017-11-15T22:50:37.356817: step 135, loss 0.947647, acc 0.488281, f1 0.409915\n",
      "2017-11-15T22:50:37.862382: step 140, loss 0.837128, acc 0.552734, f1 0.518775\n",
      "Current epoch:  8\n",
      "2017-11-15T22:50:38.352810: step 145, loss 0.879153, acc 0.56543, f1 0.504821\n",
      "2017-11-15T22:50:38.870859: step 150, loss 0.856848, acc 0.597656, f1 0.601697\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:50:39.515632: step 150, loss 0.934834, acc 0.512165, f1 0.471552\n",
      "\n",
      "2017-11-15T22:50:40.034143: step 155, loss 0.853382, acc 0.56543, f1 0.508495\n",
      "2017-11-15T22:50:40.555481: step 160, loss 0.870197, acc 0.586914, f1 0.584285\n",
      "Current epoch:  9\n",
      "2017-11-15T22:50:41.048446: step 165, loss 0.895078, acc 0.511719, f1 0.42562\n",
      "2017-11-15T22:50:41.562491: step 170, loss 0.860182, acc 0.555664, f1 0.515623\n",
      "2017-11-15T22:50:42.079312: step 175, loss 0.866349, acc 0.550781, f1 0.47901\n",
      "2017-11-15T22:50:42.563111: step 180, loss 0.850834, acc 0.59434, f1 0.598741\n",
      "Current epoch:  10\n",
      "2017-11-15T22:50:43.074948: step 185, loss 0.836656, acc 0.584961, f1 0.555109\n",
      "2017-11-15T22:50:43.587722: step 190, loss 0.88554, acc 0.568359, f1 0.541059\n",
      "2017-11-15T22:50:44.108165: step 195, loss 0.867649, acc 0.555664, f1 0.503404\n",
      "Current epoch:  11\n",
      "2017-11-15T22:50:44.602601: step 200, loss 0.819353, acc 0.603516, f1 0.581277\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:50:45.260344: step 200, loss 0.885042, acc 0.556223, f1 0.553624\n",
      "\n",
      "2017-11-15T22:50:45.775561: step 205, loss 0.816827, acc 0.602539, f1 0.575795\n",
      "2017-11-15T22:50:46.289816: step 210, loss 0.826387, acc 0.614258, f1 0.621958\n",
      "2017-11-15T22:50:46.795719: step 215, loss 0.846062, acc 0.548828, f1 0.498283\n",
      "Current epoch:  12\n",
      "2017-11-15T22:50:47.271214: step 220, loss 0.870541, acc 0.556641, f1 0.529918\n",
      "2017-11-15T22:50:47.789883: step 225, loss 0.828058, acc 0.588867, f1 0.555162\n",
      "2017-11-15T22:50:48.308488: step 230, loss 0.86041, acc 0.55957, f1 0.519175\n",
      "Current epoch:  13\n",
      "2017-11-15T22:50:48.800098: step 235, loss 0.792309, acc 0.616211, f1 0.602964\n",
      "2017-11-15T22:50:49.324536: step 240, loss 0.773407, acc 0.651367, f1 0.649263\n",
      "2017-11-15T22:50:49.856682: step 245, loss 0.954563, acc 0.597656, f1 0.529679\n",
      "2017-11-15T22:50:50.373526: step 250, loss 0.836072, acc 0.585938, f1 0.542447\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:50:51.031565: step 250, loss 0.97773, acc 0.470143, f1 0.4082\n",
      "\n",
      "Current epoch:  14\n",
      "2017-11-15T22:50:51.519248: step 255, loss 0.841016, acc 0.563477, f1 0.512967\n",
      "2017-11-15T22:50:52.033209: step 260, loss 0.76397, acc 0.639648, f1 0.630931\n",
      "2017-11-15T22:50:52.551147: step 265, loss 0.845849, acc 0.567383, f1 0.522409\n",
      "2017-11-15T22:50:53.038560: step 270, loss 0.80393, acc 0.63522, f1 0.639277\n",
      "Current epoch:  15\n",
      "2017-11-15T22:50:53.559326: step 275, loss 0.79374, acc 0.632812, f1 0.602683\n",
      "2017-11-15T22:50:54.077490: step 280, loss 0.798573, acc 0.644531, f1 0.641238\n",
      "2017-11-15T22:50:54.601341: step 285, loss 0.808872, acc 0.602539, f1 0.576424\n",
      "Current epoch:  16\n",
      "2017-11-15T22:50:55.102817: step 290, loss 0.825791, acc 0.577148, f1 0.519779\n",
      "2017-11-15T22:50:55.621278: step 295, loss 0.74002, acc 0.661133, f1 0.652304\n",
      "2017-11-15T22:50:56.157230: step 300, loss 0.837818, acc 0.586914, f1 0.537242\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:50:56.804042: step 300, loss 0.916509, acc 0.548856, f1 0.545079\n",
      "\n",
      "2017-11-15T22:50:57.318863: step 305, loss 0.802687, acc 0.609375, f1 0.603832\n",
      "Current epoch:  17\n",
      "2017-11-15T22:50:57.815406: step 310, loss 0.806465, acc 0.59668, f1 0.545303\n",
      "2017-11-15T22:50:58.336701: step 315, loss 0.759199, acc 0.685547, f1 0.675838\n",
      "2017-11-15T22:50:58.857731: step 320, loss 0.741451, acc 0.672852, f1 0.657151\n",
      "Current epoch:  18\n",
      "2017-11-15T22:50:59.346149: step 325, loss 0.737103, acc 0.680664, f1 0.672415\n",
      "2017-11-15T22:50:59.860066: step 330, loss 0.758843, acc 0.635742, f1 0.632417\n",
      "2017-11-15T22:51:00.389983: step 335, loss 0.750678, acc 0.635742, f1 0.607007\n",
      "2017-11-15T22:51:00.913686: step 340, loss 0.762883, acc 0.616211, f1 0.577417\n",
      "Current epoch:  19\n",
      "2017-11-15T22:51:01.406547: step 345, loss 0.750838, acc 0.672852, f1 0.667695\n",
      "2017-11-15T22:51:01.925040: step 350, loss 0.722981, acc 0.693359, f1 0.666094\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:51:02.577826: step 350, loss 0.841296, acc 0.590006, f1 0.590135\n",
      "\n",
      "2017-11-15T22:51:03.093873: step 355, loss 0.767324, acc 0.664062, f1 0.665789\n",
      "2017-11-15T22:51:03.580805: step 360, loss 0.752528, acc 0.654088, f1 0.636428\n",
      "Current epoch:  20\n",
      "2017-11-15T22:51:04.094775: step 365, loss 0.760915, acc 0.62207, f1 0.599143\n",
      "2017-11-15T22:51:04.624608: step 370, loss 0.73167, acc 0.661133, f1 0.635635\n",
      "2017-11-15T22:51:05.153361: step 375, loss 0.7326, acc 0.650391, f1 0.641469\n",
      "Current epoch:  21\n",
      "2017-11-15T22:51:05.647905: step 380, loss 0.746517, acc 0.623047, f1 0.588317\n",
      "2017-11-15T22:51:06.179573: step 385, loss 0.761228, acc 0.65918, f1 0.662452\n",
      "2017-11-15T22:51:06.703004: step 390, loss 0.740365, acc 0.644531, f1 0.621638\n",
      "2017-11-15T22:51:07.218801: step 395, loss 0.782302, acc 0.604492, f1 0.557644\n",
      "Current epoch:  22\n",
      "2017-11-15T22:51:07.693153: step 400, loss 0.660063, acc 0.728516, f1 0.72806\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:51:08.325655: step 400, loss 0.821064, acc 0.606873, f1 0.594733\n",
      "\n",
      "2017-11-15T22:51:08.837932: step 405, loss 0.720009, acc 0.668945, f1 0.643053\n",
      "2017-11-15T22:51:09.344984: step 410, loss 0.735166, acc 0.646484, f1 0.625719\n",
      "Current epoch:  23\n",
      "2017-11-15T22:51:09.840929: step 415, loss 0.794635, acc 0.625977, f1 0.613083\n",
      "2017-11-15T22:51:10.360290: step 420, loss 0.723857, acc 0.661133, f1 0.641305\n",
      "2017-11-15T22:51:10.880916: step 425, loss 0.757066, acc 0.651367, f1 0.606294\n",
      "2017-11-15T22:51:11.416881: step 430, loss 0.703688, acc 0.674805, f1 0.656576\n",
      "Current epoch:  24\n",
      "2017-11-15T22:51:11.915656: step 435, loss 0.729955, acc 0.673828, f1 0.669679\n",
      "2017-11-15T22:51:12.439657: step 440, loss 0.661035, acc 0.717773, f1 0.711613\n",
      "2017-11-15T22:51:12.957641: step 445, loss 0.780908, acc 0.628906, f1 0.62149\n",
      "2017-11-15T22:51:13.445610: step 450, loss 0.787827, acc 0.572327, f1 0.520723\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T22:51:14.098938: step 450, loss 0.880567, acc 0.578567, f1 0.516643\n",
      "\n",
      "Current epoch:  25\n",
      "2017-11-15T22:51:14.623460: step 455, loss 0.696214, acc 0.689453, f1 0.685811\n",
      "2017-11-15T22:51:15.140942: step 460, loss 0.69546, acc 0.693359, f1 0.674622\n",
      "2017-11-15T22:51:15.652510: step 465, loss 0.763183, acc 0.642578, f1 0.655698\n",
      "Current epoch:  26\n",
      "2017-11-15T22:51:16.135349: step 470, loss 0.841944, acc 0.571289, f1 0.514248\n",
      "2017-11-15T22:51:16.649780: step 475, loss 0.682645, acc 0.702148, f1 0.695522\n",
      "2017-11-15T22:51:17.172266: step 480, loss 0.679527, acc 0.695312, f1 0.659669\n",
      "2017-11-15T22:51:17.677829: step 485, loss 0.659211, acc 0.74707, f1 0.748812\n",
      "Current epoch:  27\n",
      "2017-11-15T22:51:18.154199: step 490, loss 0.802403, acc 0.585938, f1 0.533702\n",
      "2017-11-15T22:51:18.665154: step 495, loss 0.705178, acc 0.638672, f1 0.616745\n",
      "2017-11-15T22:51:19.178081: step 500, loss 0.665066, acc 0.707031, f1 0.694725\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:51:19.813875: step 500, loss 0.802126, acc 0.622528, f1 0.611341\n",
      "\n",
      "Current epoch:  28\n",
      "2017-11-15T22:51:20.304409: step 505, loss 0.752347, acc 0.612305, f1 0.585969\n",
      "2017-11-15T22:51:20.825916: step 510, loss 0.657986, acc 0.727539, f1 0.725919\n",
      "2017-11-15T22:51:21.342420: step 515, loss 0.719194, acc 0.677734, f1 0.645622\n",
      "2017-11-15T22:51:21.859351: step 520, loss 0.674977, acc 0.731445, f1 0.73325\n",
      "Current epoch:  29\n",
      "2017-11-15T22:51:22.356095: step 525, loss 0.626391, acc 0.726562, f1 0.718715\n",
      "2017-11-15T22:51:22.880799: step 530, loss 0.707107, acc 0.669922, f1 0.657909\n",
      "2017-11-15T22:51:23.402103: step 535, loss 0.653171, acc 0.711914, f1 0.699964\n",
      "2017-11-15T22:51:23.887177: step 540, loss 0.763518, acc 0.633648, f1 0.598123\n",
      "Current epoch:  30\n",
      "2017-11-15T22:51:24.404358: step 545, loss 0.638307, acc 0.743164, f1 0.737329\n",
      "2017-11-15T22:51:24.917981: step 550, loss 0.692378, acc 0.667969, f1 0.643447\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:51:25.579811: step 550, loss 0.907638, acc 0.537999, f1 0.51658\n",
      "\n",
      "2017-11-15T22:51:26.087449: step 555, loss 0.804876, acc 0.585938, f1 0.532694\n",
      "Current epoch:  31\n",
      "2017-11-15T22:51:26.570307: step 560, loss 0.650821, acc 0.733398, f1 0.734006\n",
      "2017-11-15T22:51:27.084975: step 565, loss 0.662775, acc 0.69043, f1 0.668948\n",
      "2017-11-15T22:51:27.602086: step 570, loss 0.617964, acc 0.737305, f1 0.733491\n",
      "2017-11-15T22:51:28.118668: step 575, loss 0.632213, acc 0.724609, f1 0.711539\n",
      "Current epoch:  32\n",
      "2017-11-15T22:51:28.600067: step 580, loss 0.668216, acc 0.669922, f1 0.654865\n",
      "2017-11-15T22:51:29.109531: step 585, loss 0.637725, acc 0.734375, f1 0.724179\n",
      "2017-11-15T22:51:29.619309: step 590, loss 0.640514, acc 0.725586, f1 0.710511\n",
      "Current epoch:  33\n",
      "2017-11-15T22:51:30.101710: step 595, loss 0.708636, acc 0.643555, f1 0.623342\n",
      "2017-11-15T22:51:30.618654: step 600, loss 0.646423, acc 0.712891, f1 0.701354\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:51:31.250535: step 600, loss 0.820635, acc 0.612398, f1 0.605532\n",
      "\n",
      "2017-11-15T22:51:31.760079: step 605, loss 0.576488, acc 0.776367, f1 0.777396\n",
      "2017-11-15T22:51:32.269746: step 610, loss 0.665904, acc 0.736328, f1 0.703416\n",
      "Current epoch:  34\n",
      "2017-11-15T22:51:32.754693: step 615, loss 0.811448, acc 0.577148, f1 0.49027\n",
      "2017-11-15T22:51:33.265816: step 620, loss 0.620828, acc 0.736328, f1 0.734493\n",
      "2017-11-15T22:51:33.787187: step 625, loss 0.709546, acc 0.652344, f1 0.628837\n",
      "2017-11-15T22:51:34.267040: step 630, loss 0.635361, acc 0.745283, f1 0.750094\n",
      "Current epoch:  35\n",
      "2017-11-15T22:51:34.777579: step 635, loss 0.693226, acc 0.625, f1 0.590224\n",
      "2017-11-15T22:51:35.285242: step 640, loss 0.631047, acc 0.707031, f1 0.696562\n",
      "2017-11-15T22:51:35.798689: step 645, loss 0.727571, acc 0.678711, f1 0.676496\n",
      "Current epoch:  36\n",
      "2017-11-15T22:51:36.278138: step 650, loss 0.552717, acc 0.757812, f1 0.751206\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:51:36.919324: step 650, loss 0.90775, acc 0.546287, f1 0.530472\n",
      "\n",
      "2017-11-15T22:51:37.424848: step 655, loss 0.639805, acc 0.731445, f1 0.721299\n",
      "2017-11-15T22:51:37.936083: step 660, loss 0.692935, acc 0.668945, f1 0.631893\n",
      "2017-11-15T22:51:38.448505: step 665, loss 0.826021, acc 0.635742, f1 0.591934\n",
      "Current epoch:  37\n",
      "2017-11-15T22:51:38.936287: step 670, loss 0.650128, acc 0.692383, f1 0.670453\n",
      "2017-11-15T22:51:39.452725: step 675, loss 0.557429, acc 0.774414, f1 0.770902\n",
      "2017-11-15T22:51:39.962046: step 680, loss 0.682577, acc 0.663086, f1 0.635186\n",
      "Current epoch:  38\n",
      "2017-11-15T22:51:40.440987: step 685, loss 0.550285, acc 0.779297, f1 0.77748\n",
      "2017-11-15T22:51:40.947951: step 690, loss 0.592991, acc 0.776367, f1 0.771683\n",
      "2017-11-15T22:51:41.455838: step 695, loss 0.627411, acc 0.727539, f1 0.724537\n",
      "2017-11-15T22:51:41.975115: step 700, loss 0.619846, acc 0.734375, f1 0.725135\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:51:42.628706: step 700, loss 0.82388, acc 0.61739, f1 0.587931\n",
      "\n",
      "Current epoch:  39\n",
      "2017-11-15T22:51:43.108045: step 705, loss 0.645395, acc 0.666016, f1 0.630536\n",
      "2017-11-15T22:51:43.626519: step 710, loss 0.577184, acc 0.757812, f1 0.753424\n",
      "2017-11-15T22:51:44.143963: step 715, loss 0.611967, acc 0.719727, f1 0.710668\n",
      "2017-11-15T22:51:44.643072: step 720, loss 0.730261, acc 0.685535, f1 0.635701\n",
      "Current epoch:  40\n",
      "2017-11-15T22:51:45.182085: step 725, loss 0.692064, acc 0.640625, f1 0.617355\n",
      "2017-11-15T22:51:45.700048: step 730, loss 0.616175, acc 0.703125, f1 0.698456\n",
      "2017-11-15T22:51:46.214468: step 735, loss 0.587697, acc 0.739258, f1 0.725373\n",
      "Current epoch:  41\n",
      "2017-11-15T22:51:46.692809: step 740, loss 0.582252, acc 0.732422, f1 0.723919\n",
      "2017-11-15T22:51:47.204455: step 745, loss 0.631115, acc 0.738281, f1 0.70325\n",
      "2017-11-15T22:51:47.717243: step 750, loss 0.721663, acc 0.642578, f1 0.580854\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:51:48.344475: step 750, loss 0.802284, acc 0.622431, f1 0.599882\n",
      "\n",
      "2017-11-15T22:51:48.855815: step 755, loss 0.570238, acc 0.761719, f1 0.756939\n",
      "Current epoch:  42\n",
      "2017-11-15T22:51:49.331735: step 760, loss 0.595735, acc 0.720703, f1 0.705107\n",
      "2017-11-15T22:51:49.851253: step 765, loss 0.596723, acc 0.742188, f1 0.730356\n",
      "2017-11-15T22:51:50.367228: step 770, loss 0.506018, acc 0.818359, f1 0.812799\n",
      "Current epoch:  43\n",
      "2017-11-15T22:51:50.855636: step 775, loss 0.699106, acc 0.654297, f1 0.619457\n",
      "2017-11-15T22:51:51.375129: step 780, loss 0.573398, acc 0.788086, f1 0.78984\n",
      "2017-11-15T22:51:51.893106: step 785, loss 0.48027, acc 0.832031, f1 0.829187\n",
      "2017-11-15T22:51:52.412008: step 790, loss 0.605882, acc 0.692383, f1 0.675197\n",
      "Current epoch:  44\n",
      "2017-11-15T22:51:52.897872: step 795, loss 0.552507, acc 0.753906, f1 0.744734\n",
      "2017-11-15T22:51:53.417307: step 800, loss 0.582182, acc 0.735352, f1 0.730753\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:51:54.071952: step 800, loss 0.975838, acc 0.509645, f1 0.476714\n",
      "\n",
      "2017-11-15T22:51:54.583199: step 805, loss 0.656029, acc 0.678711, f1 0.644858\n",
      "2017-11-15T22:51:55.064038: step 810, loss 0.59409, acc 0.746855, f1 0.730669\n",
      "Current epoch:  45\n",
      "2017-11-15T22:51:55.586692: step 815, loss 0.479257, acc 0.84082, f1 0.840383\n",
      "2017-11-15T22:51:56.092228: step 820, loss 0.533265, acc 0.77832, f1 0.77626\n",
      "2017-11-15T22:51:56.621151: step 825, loss 0.543465, acc 0.760742, f1 0.750978\n",
      "Current epoch:  46\n",
      "2017-11-15T22:51:57.111122: step 830, loss 0.630201, acc 0.680664, f1 0.66261\n",
      "2017-11-15T22:51:57.633555: step 835, loss 0.539906, acc 0.806641, f1 0.808335\n",
      "2017-11-15T22:51:58.141952: step 840, loss 0.566814, acc 0.782227, f1 0.757113\n",
      "2017-11-15T22:51:58.653465: step 845, loss 0.713386, acc 0.645508, f1 0.612374\n",
      "Current epoch:  47\n",
      "2017-11-15T22:51:59.130300: step 850, loss 0.589776, acc 0.720703, f1 0.706294\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:51:59.755122: step 850, loss 0.868454, acc 0.60348, f1 0.567016\n",
      "\n",
      "2017-11-15T22:52:00.256025: step 855, loss 0.585426, acc 0.75, f1 0.716487\n",
      "2017-11-15T22:52:00.771909: step 860, loss 0.519963, acc 0.824219, f1 0.828106\n",
      "Current epoch:  48\n",
      "2017-11-15T22:52:01.274305: step 865, loss 0.620599, acc 0.713867, f1 0.689921\n",
      "2017-11-15T22:52:01.793752: step 870, loss 0.582768, acc 0.703125, f1 0.690206\n",
      "2017-11-15T22:52:02.311680: step 875, loss 0.469003, acc 0.859375, f1 0.857339\n",
      "2017-11-15T22:52:02.831239: step 880, loss 0.48685, acc 0.803711, f1 0.793124\n",
      "Current epoch:  49\n",
      "2017-11-15T22:52:03.314771: step 885, loss 0.501725, acc 0.80957, f1 0.810549\n",
      "2017-11-15T22:52:03.832219: step 890, loss 0.615064, acc 0.695312, f1 0.672352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T22:52:04.343164: step 895, loss 0.556699, acc 0.762695, f1 0.734542\n",
      "2017-11-15T22:52:04.830258: step 900, loss 0.528706, acc 0.783019, f1 0.779091\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:52:05.472223: step 900, loss 0.891266, acc 0.611865, f1 0.576822\n",
      "\n",
      "Current epoch:  50\n",
      "2017-11-15T22:52:05.985842: step 905, loss 0.599252, acc 0.753906, f1 0.720575\n",
      "2017-11-15T22:52:06.505884: step 910, loss 0.604552, acc 0.696289, f1 0.682843\n",
      "2017-11-15T22:52:07.020471: step 915, loss 0.522283, acc 0.795898, f1 0.788998\n",
      "Current epoch:  51\n",
      "2017-11-15T22:52:07.501346: step 920, loss 0.460165, acc 0.806641, f1 0.794408\n",
      "2017-11-15T22:52:08.011261: step 925, loss 0.550276, acc 0.738281, f1 0.725459\n",
      "2017-11-15T22:52:08.527353: step 930, loss 0.540491, acc 0.77832, f1 0.772678\n",
      "2017-11-15T22:52:09.036257: step 935, loss 0.464093, acc 0.807617, f1 0.792716\n",
      "Current epoch:  52\n",
      "2017-11-15T22:52:09.516615: step 940, loss 0.604742, acc 0.696289, f1 0.671039\n",
      "2017-11-15T22:52:10.022247: step 945, loss 0.440876, acc 0.833984, f1 0.833547\n",
      "2017-11-15T22:52:10.536691: step 950, loss 0.629925, acc 0.65918, f1 0.612967\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:52:11.176711: step 950, loss 0.887056, acc 0.600475, f1 0.558772\n",
      "\n",
      "Current epoch:  53\n",
      "2017-11-15T22:52:11.711315: step 955, loss 0.441241, acc 0.844727, f1 0.840225\n",
      "2017-11-15T22:52:12.241306: step 960, loss 0.499032, acc 0.823242, f1 0.831062\n",
      "2017-11-15T22:52:12.762528: step 965, loss 0.527467, acc 0.756836, f1 0.747301\n",
      "2017-11-15T22:52:13.285450: step 970, loss 0.516625, acc 0.731445, f1 0.726088\n",
      "Current epoch:  54\n",
      "2017-11-15T22:52:13.771392: step 975, loss 0.414639, acc 0.883789, f1 0.884617\n",
      "2017-11-15T22:52:14.286733: step 980, loss 0.731844, acc 0.640625, f1 0.603397\n",
      "2017-11-15T22:52:14.806277: step 985, loss 0.462913, acc 0.805664, f1 0.796064\n",
      "2017-11-15T22:52:15.288717: step 990, loss 0.464454, acc 0.825472, f1 0.823959\n",
      "Current epoch:  55\n",
      "2017-11-15T22:52:15.795110: step 995, loss 0.439035, acc 0.8125, f1 0.808411\n",
      "2017-11-15T22:52:16.301562: step 1000, loss 0.498488, acc 0.782227, f1 0.775685\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:52:16.944141: step 1000, loss 0.873907, acc 0.60348, f1 0.586301\n",
      "\n",
      "2017-11-15T22:52:17.497636: step 1005, loss 0.607593, acc 0.717773, f1 0.705403\n",
      "Current epoch:  56\n",
      "2017-11-15T22:52:17.981558: step 1010, loss 0.406695, acc 0.865234, f1 0.859015\n",
      "2017-11-15T22:52:18.493487: step 1015, loss 0.555607, acc 0.734375, f1 0.713833\n",
      "2017-11-15T22:52:19.001412: step 1020, loss 0.543296, acc 0.769531, f1 0.758677\n",
      "2017-11-15T22:52:19.514302: step 1025, loss 0.398503, acc 0.874023, f1 0.8723\n",
      "Current epoch:  57\n",
      "2017-11-15T22:52:20.004177: step 1030, loss 0.569213, acc 0.723633, f1 0.697741\n",
      "2017-11-15T22:52:20.520142: step 1035, loss 0.425955, acc 0.836914, f1 0.833048\n",
      "2017-11-15T22:52:21.041073: step 1040, loss 0.579534, acc 0.72168, f1 0.700505\n",
      "Current epoch:  58\n",
      "2017-11-15T22:52:21.531351: step 1045, loss 0.405918, acc 0.875, f1 0.878067\n",
      "2017-11-15T22:52:22.043962: step 1050, loss 0.678828, acc 0.631836, f1 0.592317\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:52:22.703311: step 1050, loss 1.20005, acc 0.453809, f1 0.362554\n",
      "\n",
      "2017-11-15T22:52:23.236328: step 1055, loss 0.405673, acc 0.873047, f1 0.872418\n",
      "2017-11-15T22:52:23.755746: step 1060, loss 0.428419, acc 0.814453, f1 0.808869\n",
      "Current epoch:  59\n",
      "2017-11-15T22:52:24.244654: step 1065, loss 0.581597, acc 0.708984, f1 0.678603\n",
      "2017-11-15T22:52:24.763958: step 1070, loss 0.467726, acc 0.81543, f1 0.790958\n",
      "2017-11-15T22:52:25.281846: step 1075, loss 0.580626, acc 0.738281, f1 0.732511\n",
      "2017-11-15T22:52:25.767367: step 1080, loss 0.539884, acc 0.732704, f1 0.703692\n",
      "Current epoch:  60\n",
      "2017-11-15T22:52:26.285297: step 1085, loss 0.383159, acc 0.864258, f1 0.862881\n",
      "2017-11-15T22:52:26.803697: step 1090, loss 0.364021, acc 0.899414, f1 0.901085\n",
      "2017-11-15T22:52:27.319361: step 1095, loss 0.542106, acc 0.689453, f1 0.669531\n",
      "Current epoch:  61\n",
      "2017-11-15T22:52:27.812322: step 1100, loss 0.46717, acc 0.827148, f1 0.810922\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:52:28.458485: step 1100, loss 0.908268, acc 0.579052, f1 0.584938\n",
      "\n",
      "2017-11-15T22:52:28.979120: step 1105, loss 0.440792, acc 0.845703, f1 0.853512\n",
      "2017-11-15T22:52:29.492007: step 1110, loss 0.373036, acc 0.871094, f1 0.868189\n",
      "2017-11-15T22:52:30.001954: step 1115, loss 0.736759, acc 0.602539, f1 0.55991\n",
      "Current epoch:  62\n",
      "2017-11-15T22:52:30.487574: step 1120, loss 0.31365, acc 0.924805, f1 0.924631\n",
      "2017-11-15T22:52:31.001472: step 1125, loss 0.310248, acc 0.917969, f1 0.918067\n",
      "2017-11-15T22:52:31.526373: step 1130, loss 0.457358, acc 0.813477, f1 0.782239\n",
      "Current epoch:  63\n",
      "2017-11-15T22:52:32.011911: step 1135, loss 0.687847, acc 0.614258, f1 0.578823\n",
      "2017-11-15T22:52:32.530266: step 1140, loss 0.33411, acc 0.924805, f1 0.924682\n",
      "2017-11-15T22:52:33.044705: step 1145, loss 0.335029, acc 0.881836, f1 0.881568\n",
      "2017-11-15T22:52:33.562630: step 1150, loss 0.5429, acc 0.716797, f1 0.687011\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:52:34.256549: step 1150, loss 0.930378, acc 0.597954, f1 0.564305\n",
      "\n",
      "Current epoch:  64\n",
      "2017-11-15T22:52:34.742817: step 1155, loss 0.409333, acc 0.836914, f1 0.832916\n",
      "2017-11-15T22:52:35.259510: step 1160, loss 0.331953, acc 0.893555, f1 0.891181\n",
      "2017-11-15T22:52:35.777473: step 1165, loss 0.486713, acc 0.756836, f1 0.745847\n",
      "2017-11-15T22:52:36.263114: step 1170, loss 0.354049, acc 0.908805, f1 0.908246\n",
      "Current epoch:  65\n",
      "2017-11-15T22:52:36.787039: step 1175, loss 0.34497, acc 0.878906, f1 0.873956\n",
      "2017-11-15T22:52:37.294946: step 1180, loss 0.410317, acc 0.858398, f1 0.866514\n",
      "2017-11-15T22:52:37.807696: step 1185, loss 0.550923, acc 0.717773, f1 0.708357\n",
      "Current epoch:  66\n",
      "2017-11-15T22:52:38.293575: step 1190, loss 0.310054, acc 0.916992, f1 0.916355\n",
      "2017-11-15T22:52:38.805385: step 1195, loss 0.325146, acc 0.896484, f1 0.896124\n",
      "2017-11-15T22:52:39.316664: step 1200, loss 0.373772, acc 0.838867, f1 0.823842\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:52:39.988715: step 1200, loss 1.06582, acc 0.513813, f1 0.492663\n",
      "\n",
      "2017-11-15T22:52:40.495682: step 1205, loss 0.428809, acc 0.800781, f1 0.791742\n",
      "Current epoch:  67\n",
      "2017-11-15T22:52:40.975779: step 1210, loss 0.408986, acc 0.808594, f1 0.799966\n",
      "2017-11-15T22:52:41.496250: step 1215, loss 0.437254, acc 0.811523, f1 0.803032\n",
      "2017-11-15T22:52:42.018057: step 1220, loss 0.294605, acc 0.94043, f1 0.940654\n",
      "Current epoch:  68\n",
      "2017-11-15T22:52:42.507107: step 1225, loss 0.683069, acc 0.716797, f1 0.670997\n",
      "2017-11-15T22:52:43.412129: step 1230, loss 0.359859, acc 0.875977, f1 0.875019\n",
      "2017-11-15T22:52:43.937115: step 1235, loss 0.31154, acc 0.895508, f1 0.894971\n",
      "2017-11-15T22:52:44.454724: step 1240, loss 0.496003, acc 0.738281, f1 0.714267\n",
      "Current epoch:  69\n",
      "2017-11-15T22:52:44.953664: step 1245, loss 0.3537, acc 0.868164, f1 0.865515\n",
      "2017-11-15T22:52:45.473899: step 1250, loss 0.357288, acc 0.878906, f1 0.883473\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:52:46.131609: step 1250, loss 1.21403, acc 0.539162, f1 0.500261\n",
      "\n",
      "2017-11-15T22:52:46.650645: step 1255, loss 0.421382, acc 0.817383, f1 0.806106\n",
      "2017-11-15T22:52:47.141112: step 1260, loss 0.307193, acc 0.889937, f1 0.890299\n",
      "Current epoch:  70\n",
      "2017-11-15T22:52:47.660560: step 1265, loss 0.321629, acc 0.898438, f1 0.89719\n",
      "2017-11-15T22:52:48.180966: step 1270, loss 0.298913, acc 0.899414, f1 0.899329\n",
      "2017-11-15T22:52:48.699611: step 1275, loss 0.500009, acc 0.736328, f1 0.726068\n",
      "Current epoch:  71\n",
      "2017-11-15T22:52:49.188841: step 1280, loss 0.294466, acc 0.905273, f1 0.904384\n",
      "2017-11-15T22:52:49.706806: step 1285, loss 0.36849, acc 0.859375, f1 0.854966\n",
      "2017-11-15T22:52:50.232997: step 1290, loss 0.285491, acc 0.905273, f1 0.904949\n",
      "2017-11-15T22:52:50.767284: step 1295, loss 0.370346, acc 0.842773, f1 0.84136\n",
      "Current epoch:  72\n",
      "2017-11-15T22:52:51.260206: step 1300, loss 0.278408, acc 0.916992, f1 0.917329\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:52:51.928896: step 1300, loss 1.06411, acc 0.527094, f1 0.504445\n",
      "\n",
      "2017-11-15T22:52:52.444849: step 1305, loss 0.438982, acc 0.80957, f1 0.80156\n",
      "2017-11-15T22:52:52.960345: step 1310, loss 0.369212, acc 0.857422, f1 0.832801\n",
      "Current epoch:  73\n",
      "2017-11-15T22:52:53.457329: step 1315, loss 0.262822, acc 0.932617, f1 0.932195\n",
      "2017-11-15T22:52:53.974681: step 1320, loss 0.31718, acc 0.878906, f1 0.877587\n",
      "2017-11-15T22:52:54.496183: step 1325, loss 0.392091, acc 0.813477, f1 0.805973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T22:52:55.015405: step 1330, loss 0.250738, acc 0.94043, f1 0.940189\n",
      "Current epoch:  74\n",
      "2017-11-15T22:52:55.500084: step 1335, loss 0.229589, acc 0.928711, f1 0.927761\n",
      "2017-11-15T22:52:56.027844: step 1340, loss 0.44302, acc 0.789062, f1 0.772027\n",
      "2017-11-15T22:52:56.547866: step 1345, loss 0.298218, acc 0.882812, f1 0.873992\n",
      "2017-11-15T22:52:57.033679: step 1350, loss 0.409259, acc 0.828616, f1 0.823213\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:52:57.695988: step 1350, loss 1.42596, acc 0.462582, f1 0.408569\n",
      "\n",
      "Current epoch:  75\n",
      "2017-11-15T22:52:58.206972: step 1355, loss 0.37522, acc 0.822266, f1 0.81472\n",
      "2017-11-15T22:52:58.711306: step 1360, loss 0.224261, acc 0.948242, f1 0.948337\n",
      "2017-11-15T22:52:59.215688: step 1365, loss 0.278762, acc 0.910156, f1 0.908418\n",
      "Current epoch:  76\n",
      "2017-11-15T22:52:59.708053: step 1370, loss 0.406592, acc 0.796875, f1 0.760427\n",
      "2017-11-15T22:53:00.225515: step 1375, loss 0.189151, acc 0.962891, f1 0.962865\n",
      "2017-11-15T22:53:00.755983: step 1380, loss 0.273438, acc 0.913086, f1 0.911887\n",
      "2017-11-15T22:53:01.278179: step 1385, loss 0.433225, acc 0.779297, f1 0.766762\n",
      "Current epoch:  77\n",
      "2017-11-15T22:53:01.784765: step 1390, loss 0.207723, acc 0.958008, f1 0.957696\n",
      "2017-11-15T22:53:02.304187: step 1395, loss 0.181341, acc 0.96875, f1 0.968711\n",
      "2017-11-15T22:53:02.828459: step 1400, loss 0.190824, acc 0.957031, f1 0.957112\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:53:03.486015: step 1400, loss 1.06186, acc 0.544251, f1 0.534089\n",
      "\n",
      "Current epoch:  78\n",
      "2017-11-15T22:53:03.969052: step 1405, loss 0.38753, acc 0.793945, f1 0.784071\n",
      "2017-11-15T22:53:04.490515: step 1410, loss 0.244199, acc 0.938477, f1 0.938477\n",
      "2017-11-15T22:53:05.004992: step 1415, loss 0.187762, acc 0.964844, f1 0.964799\n",
      "2017-11-15T22:53:05.530473: step 1420, loss 0.454335, acc 0.801758, f1 0.761003\n",
      "Current epoch:  79\n",
      "2017-11-15T22:53:06.020609: step 1425, loss 0.193655, acc 0.96582, f1 0.965846\n",
      "2017-11-15T22:53:06.545582: step 1430, loss 0.165276, acc 0.974609, f1 0.974604\n",
      "2017-11-15T22:53:07.072027: step 1435, loss 0.179909, acc 0.962891, f1 0.962893\n",
      "2017-11-15T22:53:07.561901: step 1440, loss 0.637296, acc 0.679245, f1 0.629543\n",
      "Current epoch:  80\n",
      "2017-11-15T22:53:08.082864: step 1445, loss 0.161543, acc 0.974609, f1 0.974569\n",
      "2017-11-15T22:53:08.598146: step 1450, loss 0.160458, acc 0.972656, f1 0.972622\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:53:09.256437: step 1450, loss 0.953538, acc 0.599118, f1 0.598649\n",
      "\n",
      "2017-11-15T22:53:09.775128: step 1455, loss 0.143201, acc 0.976562, f1 0.976561\n",
      "Current epoch:  81\n",
      "2017-11-15T22:53:10.269025: step 1460, loss 0.148432, acc 0.97168, f1 0.97162\n",
      "2017-11-15T22:53:10.791265: step 1465, loss 0.225556, acc 0.921875, f1 0.922185\n",
      "2017-11-15T22:53:11.316411: step 1470, loss 0.362948, acc 0.841797, f1 0.836157\n",
      "2017-11-15T22:53:11.842380: step 1475, loss 0.138814, acc 0.983398, f1 0.983403\n",
      "Current epoch:  82\n",
      "2017-11-15T22:53:12.330383: step 1480, loss 0.139227, acc 0.974609, f1 0.974613\n",
      "2017-11-15T22:53:12.868868: step 1485, loss 0.124131, acc 0.985352, f1 0.985353\n",
      "2017-11-15T22:53:13.383725: step 1490, loss 0.110846, acc 0.990234, f1 0.990238\n",
      "Current epoch:  83\n",
      "2017-11-15T22:53:13.875130: step 1495, loss 0.1156, acc 0.983398, f1 0.983374\n",
      "2017-11-15T22:53:14.393765: step 1500, loss 0.384399, acc 0.800781, f1 0.787732\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:53:15.048694: step 1500, loss 2.32603, acc 0.391091, f1 0.272422\n",
      "\n",
      "2017-11-15T22:53:15.567849: step 1505, loss 0.166933, acc 0.970703, f1 0.970631\n",
      "2017-11-15T22:53:16.082796: step 1510, loss 0.130363, acc 0.985352, f1 0.985345\n",
      "Current epoch:  84\n",
      "2017-11-15T22:53:16.571631: step 1515, loss 0.107576, acc 0.987305, f1 0.987299\n",
      "2017-11-15T22:53:17.087086: step 1520, loss 0.110148, acc 0.985352, f1 0.98533\n",
      "2017-11-15T22:53:17.608541: step 1525, loss 0.113389, acc 0.981445, f1 0.981434\n",
      "2017-11-15T22:53:18.104519: step 1530, loss 0.106391, acc 0.982704, f1 0.982706\n",
      "Current epoch:  85\n",
      "2017-11-15T22:53:18.624461: step 1535, loss 0.0935248, acc 0.994141, f1 0.994138\n",
      "2017-11-15T22:53:19.140941: step 1540, loss 0.101567, acc 0.984375, f1 0.984377\n",
      "2017-11-15T22:53:19.651340: step 1545, loss 0.155158, acc 0.956055, f1 0.955768\n",
      "Current epoch:  86\n",
      "2017-11-15T22:53:20.133214: step 1550, loss 0.140291, acc 0.980469, f1 0.98046\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:53:20.768294: step 1550, loss 0.984744, acc 0.596403, f1 0.596784\n",
      "\n",
      "2017-11-15T22:53:21.270857: step 1555, loss 0.111156, acc 0.985352, f1 0.985349\n",
      "2017-11-15T22:53:21.787001: step 1560, loss 0.0970984, acc 0.988281, f1 0.988282\n",
      "2017-11-15T22:53:22.304442: step 1565, loss 0.0982574, acc 0.989258, f1 0.98927\n",
      "Current epoch:  87\n",
      "2017-11-15T22:53:22.795830: step 1570, loss 0.0871118, acc 0.996094, f1 0.996094\n",
      "2017-11-15T22:53:23.322434: step 1575, loss 0.0866579, acc 0.988281, f1 0.98828\n",
      "2017-11-15T22:53:23.851432: step 1580, loss 0.0940619, acc 0.981445, f1 0.981424\n",
      "Current epoch:  88\n",
      "2017-11-15T22:53:24.339077: step 1585, loss 0.0850522, acc 0.990234, f1 0.990232\n",
      "2017-11-15T22:53:24.864266: step 1590, loss 0.0844588, acc 0.989258, f1 0.989269\n",
      "2017-11-15T22:53:25.382136: step 1595, loss 0.106774, acc 0.986328, f1 0.986336\n",
      "2017-11-15T22:53:25.901359: step 1600, loss 0.241798, acc 0.910156, f1 0.90937\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:53:26.558056: step 1600, loss 0.998652, acc 0.603771, f1 0.600446\n",
      "\n",
      "Current epoch:  89\n",
      "2017-11-15T22:53:27.048461: step 1605, loss 0.0931229, acc 0.987305, f1 0.987306\n",
      "2017-11-15T22:53:27.562407: step 1610, loss 0.0815357, acc 0.994141, f1 0.994143\n",
      "2017-11-15T22:53:28.081973: step 1615, loss 0.0842598, acc 0.993164, f1 0.993148\n",
      "2017-11-15T22:53:28.563804: step 1620, loss 0.0737928, acc 0.993711, f1 0.993717\n",
      "Current epoch:  90\n",
      "2017-11-15T22:53:29.079743: step 1625, loss 0.0757884, acc 0.990234, f1 0.990235\n",
      "2017-11-15T22:53:29.588433: step 1630, loss 0.074481, acc 0.995117, f1 0.995116\n",
      "2017-11-15T22:53:30.101326: step 1635, loss 0.0705982, acc 0.994141, f1 0.994135\n",
      "Current epoch:  91\n",
      "2017-11-15T22:53:30.578224: step 1640, loss 0.0706668, acc 0.995117, f1 0.995117\n",
      "2017-11-15T22:53:31.082128: step 1645, loss 0.0657459, acc 0.99707, f1 0.99707\n",
      "2017-11-15T22:53:31.597302: step 1650, loss 0.0737071, acc 0.993164, f1 0.993164\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:53:32.248615: step 1650, loss 1.11841, acc 0.587291, f1 0.586981\n",
      "\n",
      "2017-11-15T22:53:32.766618: step 1655, loss 0.0659886, acc 0.993164, f1 0.993164\n",
      "Current epoch:  92\n",
      "2017-11-15T22:53:33.257719: step 1660, loss 0.0674257, acc 0.994141, f1 0.99414\n",
      "2017-11-15T22:53:33.772149: step 1665, loss 0.0606498, acc 0.996094, f1 0.996094\n",
      "2017-11-15T22:53:34.290074: step 1670, loss 0.720209, acc 0.693359, f1 0.677311\n",
      "Current epoch:  93\n",
      "2017-11-15T22:53:34.797974: step 1675, loss 0.070035, acc 0.995117, f1 0.995109\n",
      "2017-11-15T22:53:35.315914: step 1680, loss 0.0622183, acc 0.994141, f1 0.99414\n",
      "2017-11-15T22:53:35.839856: step 1685, loss 0.0642752, acc 0.993164, f1 0.993163\n",
      "2017-11-15T22:53:36.358210: step 1690, loss 0.0620181, acc 0.996094, f1 0.996093\n",
      "Current epoch:  94\n",
      "2017-11-15T22:53:36.850288: step 1695, loss 0.0644147, acc 0.995117, f1 0.995117\n",
      "2017-11-15T22:53:37.367578: step 1700, loss 0.0596915, acc 0.992188, f1 0.992184\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:53:38.022825: step 1700, loss 1.1465, acc 0.587097, f1 0.586744\n",
      "\n",
      "2017-11-15T22:53:38.552609: step 1705, loss 0.0588882, acc 0.99707, f1 0.99707\n",
      "2017-11-15T22:53:39.055965: step 1710, loss 0.0550858, acc 0.996855, f1 0.996845\n",
      "Current epoch:  95\n",
      "2017-11-15T22:53:39.591403: step 1715, loss 0.0588016, acc 0.994141, f1 0.994144\n",
      "2017-11-15T22:53:40.122873: step 1720, loss 0.049961, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:53:40.649375: step 1725, loss 0.0604159, acc 0.993164, f1 0.993164\n",
      "Current epoch:  96\n",
      "2017-11-15T22:53:41.142041: step 1730, loss 0.851239, acc 0.658203, f1 0.601119\n",
      "2017-11-15T22:53:41.661525: step 1735, loss 0.0649405, acc 0.99707, f1 0.99707\n",
      "2017-11-15T22:53:42.177261: step 1740, loss 0.057631, acc 0.995117, f1 0.995117\n",
      "2017-11-15T22:53:42.695706: step 1745, loss 0.0575943, acc 0.996094, f1 0.996094\n",
      "Current epoch:  97\n",
      "2017-11-15T22:53:43.183066: step 1750, loss 0.057752, acc 0.991211, f1 0.991211\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:53:43.836853: step 1750, loss 1.15829, acc 0.585934, f1 0.585837\n",
      "\n",
      "2017-11-15T22:53:44.350825: step 1755, loss 0.0485905, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:53:44.867069: step 1760, loss 0.0472341, acc 0.996094, f1 0.996096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch:  98\n",
      "2017-11-15T22:53:45.359029: step 1765, loss 0.0511571, acc 0.991211, f1 0.991213\n",
      "2017-11-15T22:53:45.878466: step 1770, loss 0.0527216, acc 0.992188, f1 0.992186\n",
      "2017-11-15T22:53:46.393928: step 1775, loss 0.0520763, acc 0.993164, f1 0.993164\n",
      "2017-11-15T22:53:46.916804: step 1780, loss 0.0527071, acc 0.993164, f1 0.993164\n",
      "Current epoch:  99\n",
      "2017-11-15T22:53:47.404679: step 1785, loss 0.0437702, acc 0.99707, f1 0.99707\n",
      "2017-11-15T22:53:47.925024: step 1790, loss 0.0410258, acc 0.996094, f1 0.996092\n",
      "2017-11-15T22:53:48.443351: step 1795, loss 0.0438365, acc 0.999023, f1 0.999022\n",
      "2017-11-15T22:53:48.932237: step 1800, loss 0.0400318, acc 0.995283, f1 0.99528\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:53:49.569756: step 1800, loss 1.21893, acc 0.587582, f1 0.586472\n",
      "\n",
      "Current epoch:  100\n",
      "2017-11-15T22:53:50.085069: step 1805, loss 0.0467288, acc 0.994141, f1 0.99414\n",
      "2017-11-15T22:53:50.597653: step 1810, loss 0.0395548, acc 0.99707, f1 0.997071\n",
      "2017-11-15T22:53:51.116114: step 1815, loss 0.0632686, acc 0.992188, f1 0.992185\n",
      "Current epoch:  101\n",
      "2017-11-15T22:53:51.617532: step 1820, loss 0.143085, acc 0.951172, f1 0.949107\n",
      "2017-11-15T22:53:52.135515: step 1825, loss 0.0590758, acc 0.994141, f1 0.994141\n",
      "2017-11-15T22:53:52.657591: step 1830, loss 0.0467773, acc 0.998047, f1 0.998051\n",
      "2017-11-15T22:53:53.177027: step 1835, loss 0.0444953, acc 0.995117, f1 0.995119\n",
      "Current epoch:  102\n",
      "2017-11-15T22:53:53.664418: step 1840, loss 0.0392239, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:53:54.178929: step 1845, loss 0.0470544, acc 0.995117, f1 0.995116\n",
      "2017-11-15T22:53:54.695153: step 1850, loss 0.0435217, acc 0.996094, f1 0.996094\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:53:55.351703: step 1850, loss 1.21689, acc 0.586419, f1 0.586208\n",
      "\n",
      "Current epoch:  103\n",
      "2017-11-15T22:53:55.843107: step 1855, loss 0.0372265, acc 0.998047, f1 0.998048\n",
      "2017-11-15T22:53:56.371309: step 1860, loss 0.0396077, acc 0.99707, f1 0.997071\n",
      "2017-11-15T22:53:56.899637: step 1865, loss 0.0353052, acc 0.995117, f1 0.995119\n",
      "2017-11-15T22:53:57.415559: step 1870, loss 0.0414723, acc 0.996094, f1 0.996095\n",
      "Current epoch:  104\n",
      "2017-11-15T22:53:57.903085: step 1875, loss 0.0380636, acc 0.994141, f1 0.994134\n",
      "2017-11-15T22:53:58.422352: step 1880, loss 0.0310116, acc 1, f1 1\n",
      "2017-11-15T22:53:58.939839: step 1885, loss 0.036863, acc 0.99707, f1 0.99707\n",
      "2017-11-15T22:53:59.421719: step 1890, loss 0.0331423, acc 0.998428, f1 0.998428\n",
      "Current epoch:  105\n",
      "2017-11-15T22:53:59.932851: step 1895, loss 0.0352356, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:54:00.443203: step 1900, loss 0.15975, acc 0.950195, f1 0.94841\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:54:01.076467: step 1900, loss 2.66161, acc 0.40791, f1 0.405403\n",
      "\n",
      "2017-11-15T22:54:01.587512: step 1905, loss 0.0479627, acc 0.99707, f1 0.99707\n",
      "Current epoch:  106\n",
      "2017-11-15T22:54:02.090405: step 1910, loss 0.0380524, acc 0.996094, f1 0.996093\n",
      "2017-11-15T22:54:02.611469: step 1915, loss 0.0357465, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:54:03.128221: step 1920, loss 0.0326545, acc 0.996094, f1 0.996094\n",
      "2017-11-15T22:54:03.652715: step 1925, loss 0.034354, acc 0.998047, f1 0.998047\n",
      "Current epoch:  107\n",
      "2017-11-15T22:54:04.175704: step 1930, loss 0.028993, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:54:04.694403: step 1935, loss 0.0319494, acc 0.99707, f1 0.997072\n",
      "2017-11-15T22:54:05.207831: step 1940, loss 0.0305068, acc 1, f1 1\n",
      "Current epoch:  108\n",
      "2017-11-15T22:54:05.699218: step 1945, loss 0.0301845, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:54:06.215823: step 1950, loss 0.0281571, acc 0.998047, f1 0.998048\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:54:06.880426: step 1950, loss 1.28191, acc 0.583511, f1 0.583064\n",
      "\n",
      "2017-11-15T22:54:07.396455: step 1955, loss 0.0342908, acc 0.994141, f1 0.99414\n",
      "2017-11-15T22:54:07.919746: step 1960, loss 0.033593, acc 0.99707, f1 0.997071\n",
      "Current epoch:  109\n",
      "2017-11-15T22:54:08.408443: step 1965, loss 0.0318122, acc 0.99707, f1 0.997069\n",
      "2017-11-15T22:54:08.926378: step 1970, loss 0.0290739, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:54:09.441976: step 1975, loss 0.0321016, acc 0.995117, f1 0.995116\n",
      "2017-11-15T22:54:09.922846: step 1980, loss 0.0265866, acc 0.998428, f1 0.99843\n",
      "Current epoch:  110\n",
      "2017-11-15T22:54:10.438785: step 1985, loss 0.0728478, acc 0.996094, f1 0.996097\n",
      "2017-11-15T22:54:10.957240: step 1990, loss 0.0514788, acc 0.993164, f1 0.993166\n",
      "2017-11-15T22:54:11.502287: step 1995, loss 0.0444462, acc 0.996094, f1 0.996093\n",
      "Current epoch:  111\n",
      "2017-11-15T22:54:11.986633: step 2000, loss 0.0382781, acc 0.99707, f1 0.997067\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:54:12.650964: step 2000, loss 1.23696, acc 0.586031, f1 0.586662\n",
      "\n",
      "2017-11-15T22:54:13.177997: step 2005, loss 0.0356403, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:54:13.700668: step 2010, loss 0.0305214, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:54:14.218176: step 2015, loss 0.0298187, acc 0.99707, f1 0.99707\n",
      "Current epoch:  112\n",
      "2017-11-15T22:54:14.709739: step 2020, loss 0.032333, acc 0.99707, f1 0.997069\n",
      "2017-11-15T22:54:15.226079: step 2025, loss 0.0259507, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:54:15.752022: step 2030, loss 0.0290882, acc 0.995117, f1 0.995118\n",
      "Current epoch:  113\n",
      "2017-11-15T22:54:16.242474: step 2035, loss 0.0246158, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:54:16.767065: step 2040, loss 0.0257625, acc 0.998047, f1 0.998048\n",
      "2017-11-15T22:54:17.290001: step 2045, loss 0.0358962, acc 0.992188, f1 0.99218\n",
      "2017-11-15T22:54:17.810457: step 2050, loss 0.027118, acc 0.99707, f1 0.997066\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:54:18.480040: step 2050, loss 1.31379, acc 0.582299, f1 0.583229\n",
      "\n",
      "Current epoch:  114\n",
      "2017-11-15T22:54:19.004975: step 2055, loss 0.0256886, acc 0.99707, f1 0.99707\n",
      "2017-11-15T22:54:19.528982: step 2060, loss 0.0266593, acc 0.996094, f1 0.996094\n",
      "2017-11-15T22:54:20.040910: step 2065, loss 0.0264893, acc 0.99707, f1 0.997074\n",
      "2017-11-15T22:54:20.523819: step 2070, loss 0.0313279, acc 0.998428, f1 0.998425\n",
      "Current epoch:  115\n",
      "2017-11-15T22:54:21.036636: step 2075, loss 0.02687, acc 0.99707, f1 0.997069\n",
      "2017-11-15T22:54:21.551557: step 2080, loss 0.0221066, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:54:22.075336: step 2085, loss 0.0310153, acc 0.99707, f1 0.997068\n",
      "Current epoch:  116\n",
      "2017-11-15T22:54:22.567986: step 2090, loss 0.339907, acc 0.87207, f1 0.82982\n",
      "2017-11-15T22:54:23.092935: step 2095, loss 0.0306794, acc 0.99707, f1 0.99707\n",
      "2017-11-15T22:54:23.611854: step 2100, loss 0.0275575, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:54:24.300774: step 2100, loss 1.30589, acc 0.586516, f1 0.586448\n",
      "\n",
      "2017-11-15T22:54:24.821316: step 2105, loss 0.026473, acc 0.99707, f1 0.997071\n",
      "Current epoch:  117\n",
      "2017-11-15T22:54:25.310623: step 2110, loss 0.0244634, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:54:25.826536: step 2115, loss 0.0280703, acc 0.996094, f1 0.996094\n",
      "2017-11-15T22:54:26.341897: step 2120, loss 0.0273599, acc 0.996094, f1 0.996094\n",
      "Current epoch:  118\n",
      "2017-11-15T22:54:26.832439: step 2125, loss 0.0246335, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:54:27.351410: step 2130, loss 0.0258763, acc 0.995117, f1 0.995117\n",
      "2017-11-15T22:54:27.873347: step 2135, loss 0.0246741, acc 0.996094, f1 0.996094\n",
      "2017-11-15T22:54:28.390275: step 2140, loss 0.0237086, acc 0.996094, f1 0.996093\n",
      "Current epoch:  119\n",
      "2017-11-15T22:54:28.892399: step 2145, loss 0.0223479, acc 1, f1 1\n",
      "2017-11-15T22:54:29.417348: step 2150, loss 0.0253592, acc 0.99707, f1 0.997071\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:54:30.094243: step 2150, loss 1.37313, acc 0.574738, f1 0.574208\n",
      "\n",
      "2017-11-15T22:54:30.613088: step 2155, loss 0.0225226, acc 0.998047, f1 0.998046\n",
      "2017-11-15T22:54:31.105952: step 2160, loss 0.0206412, acc 1, f1 1\n",
      "Current epoch:  120\n",
      "2017-11-15T22:54:31.630711: step 2165, loss 0.0213952, acc 0.99707, f1 0.99707\n",
      "2017-11-15T22:54:32.154273: step 2170, loss 0.0430484, acc 0.996094, f1 0.996094\n",
      "2017-11-15T22:54:32.673724: step 2175, loss 0.114073, acc 0.964844, f1 0.964628\n",
      "Current epoch:  121\n",
      "2017-11-15T22:54:33.161595: step 2180, loss 0.0228334, acc 1, f1 1\n",
      "2017-11-15T22:54:33.680671: step 2185, loss 0.026071, acc 0.99707, f1 0.997071\n",
      "2017-11-15T22:54:34.197381: step 2190, loss 0.0240391, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:54:34.719124: step 2195, loss 0.023223, acc 0.999023, f1 0.999023\n",
      "Current epoch:  122\n",
      "2017-11-15T22:54:35.215504: step 2200, loss 0.0213018, acc 0.996094, f1 0.996094\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T22:54:35.882982: step 2200, loss 1.33153, acc 0.583705, f1 0.58345\n",
      "\n",
      "2017-11-15T22:54:36.407647: step 2205, loss 0.0208995, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:54:36.924691: step 2210, loss 0.0218766, acc 0.998047, f1 0.998047\n",
      "Current epoch:  123\n",
      "2017-11-15T22:54:37.416099: step 2215, loss 0.022933, acc 0.996094, f1 0.996096\n",
      "2017-11-15T22:54:37.933141: step 2220, loss 0.0200357, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:54:38.452138: step 2225, loss 0.0175253, acc 1, f1 1\n",
      "2017-11-15T22:54:38.971580: step 2230, loss 0.019087, acc 0.999023, f1 0.999024\n",
      "Current epoch:  124\n",
      "2017-11-15T22:54:39.453931: step 2235, loss 0.0168685, acc 1, f1 1\n",
      "2017-11-15T22:54:39.968474: step 2240, loss 0.0208636, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:54:40.499358: step 2245, loss 0.0182959, acc 0.999023, f1 0.999024\n",
      "2017-11-15T22:54:40.988255: step 2250, loss 0.0202866, acc 0.996855, f1 0.996855\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:54:41.648352: step 2250, loss 1.38993, acc 0.577598, f1 0.577991\n",
      "\n",
      "Current epoch:  125\n",
      "2017-11-15T22:54:42.168068: step 2255, loss 0.0194535, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:54:42.688197: step 2260, loss 0.0177837, acc 0.99707, f1 0.997066\n",
      "2017-11-15T22:54:43.202175: step 2265, loss 0.0204152, acc 0.998047, f1 0.998047\n",
      "Current epoch:  126\n",
      "2017-11-15T22:54:43.696125: step 2270, loss 0.0181858, acc 0.998047, f1 0.998046\n",
      "2017-11-15T22:54:44.207084: step 2275, loss 0.0186541, acc 1, f1 1\n",
      "2017-11-15T22:54:44.734258: step 2280, loss 0.695335, acc 0.725586, f1 0.755515\n",
      "2017-11-15T22:54:45.254846: step 2285, loss 0.0251572, acc 0.998047, f1 0.998047\n",
      "Current epoch:  127\n",
      "2017-11-15T22:54:45.754775: step 2290, loss 0.0200666, acc 0.998047, f1 0.998046\n",
      "2017-11-15T22:54:46.280744: step 2295, loss 0.0179596, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:54:46.797744: step 2300, loss 0.0174491, acc 0.999023, f1 0.999024\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:54:47.449616: step 2300, loss 1.38034, acc 0.585256, f1 0.585393\n",
      "\n",
      "Current epoch:  128\n",
      "2017-11-15T22:54:47.938983: step 2305, loss 0.0161811, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:54:48.454164: step 2310, loss 0.0171039, acc 0.99707, f1 0.997072\n",
      "2017-11-15T22:54:48.974293: step 2315, loss 0.0197999, acc 0.996094, f1 0.996095\n",
      "2017-11-15T22:54:49.485191: step 2320, loss 0.0180322, acc 0.996094, f1 0.996093\n",
      "Current epoch:  129\n",
      "2017-11-15T22:54:49.970162: step 2325, loss 0.01465, acc 1, f1 1\n",
      "2017-11-15T22:54:50.479075: step 2330, loss 0.0186378, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:54:50.989500: step 2335, loss 0.0160379, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:54:51.469902: step 2340, loss 0.0121879, acc 1, f1 1\n",
      "Current epoch:  130\n",
      "2017-11-15T22:54:51.988697: step 2345, loss 0.0178954, acc 0.99707, f1 0.997072\n",
      "2017-11-15T22:54:52.512663: step 2350, loss 0.018233, acc 0.99707, f1 0.997069\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:54:53.176253: step 2350, loss 1.41549, acc 0.588745, f1 0.588337\n",
      "\n",
      "2017-11-15T22:54:53.693704: step 2355, loss 0.0164618, acc 0.998047, f1 0.998046\n",
      "Current epoch:  131\n",
      "2017-11-15T22:54:54.180717: step 2360, loss 0.017408, acc 0.996094, f1 0.996094\n",
      "2017-11-15T22:54:54.706352: step 2365, loss 0.0184146, acc 1, f1 1\n",
      "2017-11-15T22:54:55.226892: step 2370, loss 1.19127, acc 0.651367, f1 0.581231\n",
      "2017-11-15T22:54:55.753363: step 2375, loss 0.025963, acc 1, f1 1\n",
      "Current epoch:  132\n",
      "2017-11-15T22:54:56.243050: step 2380, loss 0.0214302, acc 0.999023, f1 0.999024\n",
      "2017-11-15T22:54:56.758698: step 2385, loss 0.023087, acc 0.99707, f1 0.997072\n",
      "2017-11-15T22:54:57.290454: step 2390, loss 0.0224118, acc 0.999023, f1 0.999023\n",
      "Current epoch:  133\n",
      "2017-11-15T22:54:57.780641: step 2395, loss 0.0171293, acc 1, f1 1\n",
      "2017-11-15T22:54:58.293110: step 2400, loss 0.0171591, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:54:58.955258: step 2400, loss 1.37948, acc 0.581524, f1 0.581823\n",
      "\n",
      "2017-11-15T22:54:59.469174: step 2405, loss 0.0162503, acc 1, f1 1\n",
      "2017-11-15T22:54:59.986914: step 2410, loss 0.0181357, acc 0.998047, f1 0.998047\n",
      "Current epoch:  134\n",
      "2017-11-15T22:55:00.479310: step 2415, loss 0.0168227, acc 0.998047, f1 0.998046\n",
      "2017-11-15T22:55:00.995132: step 2420, loss 0.0169644, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:55:01.516557: step 2425, loss 0.0172445, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:55:02.005196: step 2430, loss 0.017516, acc 0.995283, f1 0.995283\n",
      "Current epoch:  135\n",
      "2017-11-15T22:55:02.531462: step 2435, loss 0.0151044, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:55:03.045914: step 2440, loss 0.0171366, acc 0.99707, f1 0.997069\n",
      "2017-11-15T22:55:03.555044: step 2445, loss 0.0143846, acc 0.999023, f1 0.999024\n",
      "Current epoch:  136\n",
      "2017-11-15T22:55:04.033745: step 2450, loss 0.0138329, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:55:04.667400: step 2450, loss 1.43049, acc 0.584383, f1 0.584138\n",
      "\n",
      "2017-11-15T22:55:05.168335: step 2455, loss 0.0153202, acc 0.99707, f1 0.99707\n",
      "2017-11-15T22:55:05.675534: step 2460, loss 0.0170959, acc 0.995117, f1 0.995115\n",
      "2017-11-15T22:55:06.185470: step 2465, loss 0.0143536, acc 0.998047, f1 0.998047\n",
      "Current epoch:  137\n",
      "2017-11-15T22:55:06.664310: step 2470, loss 0.0149608, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:55:07.172017: step 2475, loss 0.0155688, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:55:07.689804: step 2480, loss 0.0163517, acc 0.996094, f1 0.996094\n",
      "Current epoch:  138\n",
      "2017-11-15T22:55:08.191485: step 2485, loss 0.0145716, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:55:08.711204: step 2490, loss 0.020133, acc 0.99707, f1 0.997066\n",
      "2017-11-15T22:55:09.222519: step 2495, loss 0.214124, acc 0.891602, f1 0.891701\n",
      "2017-11-15T22:55:09.734707: step 2500, loss 0.0197163, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:55:10.365522: step 2500, loss 1.40547, acc 0.577258, f1 0.577725\n",
      "\n",
      "Current epoch:  139\n",
      "2017-11-15T22:55:10.844417: step 2505, loss 0.0168042, acc 0.999023, f1 0.999025\n",
      "2017-11-15T22:55:11.351330: step 2510, loss 0.0133758, acc 0.999023, f1 0.999025\n",
      "2017-11-15T22:55:11.871289: step 2515, loss 0.0139914, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:55:12.358003: step 2520, loss 0.0142757, acc 0.996855, f1 0.996856\n",
      "Current epoch:  140\n",
      "2017-11-15T22:55:12.878927: step 2525, loss 0.0119157, acc 1, f1 1\n",
      "2017-11-15T22:55:13.404875: step 2530, loss 0.0163716, acc 0.99707, f1 0.99707\n",
      "2017-11-15T22:55:13.931734: step 2535, loss 0.0101472, acc 1, f1 1\n",
      "Current epoch:  141\n",
      "2017-11-15T22:55:14.429522: step 2540, loss 0.0134262, acc 0.99707, f1 0.99707\n",
      "2017-11-15T22:55:14.951678: step 2545, loss 0.0151248, acc 0.996094, f1 0.996091\n",
      "2017-11-15T22:55:15.471639: step 2550, loss 0.0162713, acc 0.995117, f1 0.995117\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:55:16.124959: step 2550, loss 1.4739, acc 0.578179, f1 0.578922\n",
      "\n",
      "2017-11-15T22:55:16.648718: step 2555, loss 0.0113667, acc 1, f1 1\n",
      "Current epoch:  142\n",
      "2017-11-15T22:55:17.129063: step 2560, loss 0.0141481, acc 0.99707, f1 0.997069\n",
      "2017-11-15T22:55:17.764393: step 2565, loss 0.0105885, acc 0.999023, f1 0.999024\n",
      "2017-11-15T22:55:18.285858: step 2570, loss 0.0157135, acc 0.996094, f1 0.996094\n",
      "Current epoch:  143\n",
      "2017-11-15T22:55:18.776720: step 2575, loss 0.0130319, acc 0.998047, f1 0.998048\n",
      "2017-11-15T22:55:19.308779: step 2580, loss 0.0135655, acc 0.99707, f1 0.997069\n",
      "2017-11-15T22:55:19.830204: step 2585, loss 0.0105525, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:55:20.354657: step 2590, loss 0.0138124, acc 0.99707, f1 0.997069\n",
      "Current epoch:  144\n",
      "2017-11-15T22:55:20.845079: step 2595, loss 0.00876222, acc 1, f1 1\n",
      "2017-11-15T22:55:21.365029: step 2600, loss 0.0152714, acc 0.99707, f1 0.997069\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:55:22.023330: step 2600, loss 1.49574, acc 0.589569, f1 0.587923\n",
      "\n",
      "2017-11-15T22:55:22.544504: step 2605, loss 0.0217916, acc 0.99707, f1 0.997071\n",
      "2017-11-15T22:55:23.033663: step 2610, loss 0.257773, acc 0.875786, f1 0.872887\n",
      "Current epoch:  145\n",
      "2017-11-15T22:55:23.560525: step 2615, loss 0.0188945, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:55:24.076547: step 2620, loss 0.0159295, acc 1, f1 1\n",
      "2017-11-15T22:55:24.609835: step 2625, loss 0.0205994, acc 0.996094, f1 0.996094\n",
      "Current epoch:  146\n",
      "2017-11-15T22:55:25.101916: step 2630, loss 0.0167658, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:55:25.623449: step 2635, loss 0.0207242, acc 0.994141, f1 0.994141\n",
      "2017-11-15T22:55:26.141849: step 2640, loss 0.0120811, acc 0.999023, f1 0.999025\n",
      "2017-11-15T22:55:26.663621: step 2645, loss 0.0127223, acc 0.998047, f1 0.998047\n",
      "Current epoch:  147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T22:55:27.151366: step 2650, loss 0.0137769, acc 0.99707, f1 0.997069\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:55:27.804528: step 2650, loss 1.45883, acc 0.585547, f1 0.584791\n",
      "\n",
      "2017-11-15T22:55:28.321491: step 2655, loss 0.010641, acc 0.999023, f1 0.999022\n",
      "2017-11-15T22:55:28.844462: step 2660, loss 0.0126246, acc 0.998047, f1 0.998047\n",
      "Current epoch:  148\n",
      "2017-11-15T22:55:29.330810: step 2665, loss 0.0125145, acc 0.998047, f1 0.998046\n",
      "2017-11-15T22:55:29.857702: step 2670, loss 0.0132121, acc 0.996094, f1 0.996095\n",
      "2017-11-15T22:55:30.382718: step 2675, loss 0.0102024, acc 0.998047, f1 0.998048\n",
      "2017-11-15T22:55:30.898430: step 2680, loss 0.0126284, acc 0.998047, f1 0.998047\n",
      "Current epoch:  149\n",
      "2017-11-15T22:55:31.383320: step 2685, loss 0.00934468, acc 1, f1 1\n",
      "2017-11-15T22:55:31.901283: step 2690, loss 0.0124953, acc 0.999023, f1 0.999024\n",
      "2017-11-15T22:55:32.421257: step 2695, loss 0.0102665, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:55:32.908297: step 2700, loss 0.0157761, acc 0.995283, f1 0.99528\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:55:33.561099: step 2700, loss 1.5019, acc 0.59335, f1 0.590273\n",
      "\n",
      "Current epoch:  150\n",
      "2017-11-15T22:55:34.078976: step 2705, loss 0.0140028, acc 0.99707, f1 0.99707\n",
      "2017-11-15T22:55:34.595889: step 2710, loss 0.00971962, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:55:35.112300: step 2715, loss 0.0143604, acc 0.99707, f1 0.997066\n",
      "Current epoch:  151\n",
      "2017-11-15T22:55:35.618698: step 2720, loss 0.0103814, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:55:36.140361: step 2725, loss 0.00785068, acc 1, f1 1\n",
      "2017-11-15T22:55:36.656815: step 2730, loss 0.0166898, acc 0.996094, f1 0.996093\n",
      "2017-11-15T22:55:37.176547: step 2735, loss 0.0118782, acc 0.998047, f1 0.998047\n",
      "Current epoch:  152\n",
      "2017-11-15T22:55:37.669420: step 2740, loss 0.0189734, acc 0.996094, f1 0.996093\n",
      "2017-11-15T22:55:38.192675: step 2745, loss 1.18777, acc 0.652344, f1 0.592563\n",
      "2017-11-15T22:55:38.712765: step 2750, loss 0.0192445, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:55:39.376244: step 2750, loss 1.40016, acc 0.587534, f1 0.587946\n",
      "\n",
      "Current epoch:  153\n",
      "2017-11-15T22:55:39.859045: step 2755, loss 0.0173776, acc 0.998047, f1 0.998051\n",
      "2017-11-15T22:55:40.372959: step 2760, loss 0.0230969, acc 0.994141, f1 0.994142\n",
      "2017-11-15T22:55:40.886385: step 2765, loss 0.0129274, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:55:41.406425: step 2770, loss 0.0141787, acc 0.99707, f1 0.997072\n",
      "Current epoch:  154\n",
      "2017-11-15T22:55:41.892291: step 2775, loss 0.0118709, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:55:42.424747: step 2780, loss 0.0118803, acc 0.998047, f1 0.998048\n",
      "2017-11-15T22:55:42.944742: step 2785, loss 0.0122035, acc 0.998047, f1 0.998048\n",
      "2017-11-15T22:55:43.436660: step 2790, loss 0.0083463, acc 1, f1 1\n",
      "Current epoch:  155\n",
      "2017-11-15T22:55:43.961739: step 2795, loss 0.0115088, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:55:44.484690: step 2800, loss 0.00993858, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:55:45.138324: step 2800, loss 1.50105, acc 0.582396, f1 0.582422\n",
      "\n",
      "2017-11-15T22:55:45.661768: step 2805, loss 0.0103307, acc 0.999023, f1 0.999023\n",
      "Current epoch:  156\n",
      "2017-11-15T22:55:46.154217: step 2810, loss 0.0105916, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:55:46.680800: step 2815, loss 0.010896, acc 0.998047, f1 0.998048\n",
      "2017-11-15T22:55:47.205287: step 2820, loss 0.0129279, acc 0.99707, f1 0.997071\n",
      "2017-11-15T22:55:47.728268: step 2825, loss 0.0113658, acc 0.99707, f1 0.997071\n",
      "Current epoch:  157\n",
      "2017-11-15T22:55:48.214423: step 2830, loss 0.0101248, acc 0.99707, f1 0.99707\n",
      "2017-11-15T22:55:48.737923: step 2835, loss 0.0135479, acc 0.996094, f1 0.996094\n",
      "2017-11-15T22:55:49.258846: step 2840, loss 0.00806714, acc 1, f1 1\n",
      "Current epoch:  158\n",
      "2017-11-15T22:55:49.748214: step 2845, loss 0.0067196, acc 1, f1 1\n",
      "2017-11-15T22:55:50.262757: step 2850, loss 0.00954473, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:55:50.914558: step 2850, loss 1.54069, acc 0.580748, f1 0.580733\n",
      "\n",
      "2017-11-15T22:55:51.426424: step 2855, loss 0.0138069, acc 0.996094, f1 0.996095\n",
      "2017-11-15T22:55:51.956263: step 2860, loss 0.0131979, acc 0.998047, f1 0.998047\n",
      "Current epoch:  159\n",
      "2017-11-15T22:55:52.460226: step 2865, loss 0.0102787, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:55:52.975132: step 2870, loss 0.015542, acc 0.995117, f1 0.995121\n",
      "2017-11-15T22:55:53.493519: step 2875, loss 0.00999096, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:55:53.983997: step 2880, loss 0.0144033, acc 0.996855, f1 0.996855\n",
      "Current epoch:  160\n",
      "2017-11-15T22:55:54.506500: step 2885, loss 0.0137884, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:55:55.019445: step 2890, loss 0.0103144, acc 1, f1 1\n",
      "2017-11-15T22:55:55.537131: step 2895, loss 1.35666, acc 0.84668, f1 0.776605\n",
      "Current epoch:  161\n",
      "2017-11-15T22:55:56.027974: step 2900, loss 0.0121573, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:55:56.687322: step 2900, loss 1.50675, acc 0.58923, f1 0.587581\n",
      "\n",
      "2017-11-15T22:55:57.204751: step 2905, loss 0.0100188, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:55:57.742618: step 2910, loss 0.0112344, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:55:58.258426: step 2915, loss 0.0100065, acc 0.998047, f1 0.998048\n",
      "Current epoch:  162\n",
      "2017-11-15T22:55:58.758736: step 2920, loss 0.00904191, acc 1, f1 1\n",
      "2017-11-15T22:55:59.275890: step 2925, loss 0.0115472, acc 0.995117, f1 0.995116\n",
      "2017-11-15T22:55:59.805856: step 2930, loss 0.00794935, acc 0.999023, f1 0.999023\n",
      "Current epoch:  163\n",
      "2017-11-15T22:56:00.304388: step 2935, loss 0.00938902, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:56:00.831042: step 2940, loss 0.00859218, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:56:01.352953: step 2945, loss 0.00897753, acc 0.998047, f1 0.998048\n",
      "2017-11-15T22:56:01.869407: step 2950, loss 0.0152035, acc 0.993164, f1 0.993163\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:56:02.523081: step 2950, loss 1.56803, acc 0.586613, f1 0.586315\n",
      "\n",
      "Current epoch:  164\n",
      "2017-11-15T22:56:03.044383: step 2955, loss 0.00631504, acc 1, f1 1\n",
      "2017-11-15T22:56:03.574326: step 2960, loss 0.0113423, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:56:04.103283: step 2965, loss 0.00919084, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:56:04.595298: step 2970, loss 0.0189927, acc 0.993711, f1 0.993712\n",
      "Current epoch:  165\n",
      "2017-11-15T22:56:05.120349: step 2975, loss 0.00855692, acc 0.998047, f1 0.998042\n",
      "2017-11-15T22:56:05.642851: step 2980, loss 0.00541112, acc 1, f1 1\n",
      "2017-11-15T22:56:06.158274: step 2985, loss 0.0159427, acc 0.999023, f1 0.999023\n",
      "Current epoch:  166\n",
      "2017-11-15T22:56:06.654154: step 2990, loss 0.952739, acc 0.671875, f1 0.615793\n",
      "2017-11-15T22:56:07.176233: step 2995, loss 0.0152459, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:56:07.696867: step 3000, loss 0.0125723, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:56:08.362186: step 3000, loss 1.46683, acc 0.585789, f1 0.586159\n",
      "\n",
      "2017-11-15T22:56:08.890271: step 3005, loss 0.0116656, acc 1, f1 1\n",
      "Current epoch:  167\n",
      "2017-11-15T22:56:09.376994: step 3010, loss 0.0116367, acc 0.998047, f1 0.998048\n",
      "2017-11-15T22:56:09.900030: step 3015, loss 0.0112597, acc 0.999023, f1 0.999024\n",
      "2017-11-15T22:56:10.433704: step 3020, loss 0.0118358, acc 0.999023, f1 0.999024\n",
      "Current epoch:  168\n",
      "2017-11-15T22:56:10.928104: step 3025, loss 0.0104487, acc 1, f1 1\n",
      "2017-11-15T22:56:11.451244: step 3030, loss 0.00806732, acc 1, f1 1\n",
      "2017-11-15T22:56:12.008269: step 3035, loss 0.0114725, acc 0.99707, f1 0.997069\n",
      "2017-11-15T22:56:12.541222: step 3040, loss 0.0134734, acc 0.995117, f1 0.995118\n",
      "Current epoch:  169\n",
      "2017-11-15T22:56:13.029697: step 3045, loss 0.0102175, acc 0.999023, f1 0.999022\n",
      "2017-11-15T22:56:13.549173: step 3050, loss 0.00719907, acc 0.999023, f1 0.999022\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:56:14.224044: step 3050, loss 1.54214, acc 0.582202, f1 0.582592\n",
      "\n",
      "2017-11-15T22:56:14.746710: step 3055, loss 0.00996632, acc 0.99707, f1 0.99707\n",
      "2017-11-15T22:56:15.235286: step 3060, loss 0.00651059, acc 1, f1 1\n",
      "Current epoch:  170\n",
      "2017-11-15T22:56:15.761571: step 3065, loss 0.00724662, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:56:16.278597: step 3070, loss 0.00726324, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:56:16.800064: step 3075, loss 0.00648711, acc 1, f1 1\n",
      "Current epoch:  171\n",
      "2017-11-15T22:56:17.292003: step 3080, loss 0.00766904, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:56:17.816984: step 3085, loss 0.00583785, acc 1, f1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T22:56:18.335968: step 3090, loss 0.00808241, acc 0.998047, f1 0.998046\n",
      "2017-11-15T22:56:18.862970: step 3095, loss 0.00762315, acc 0.999023, f1 0.999023\n",
      "Current epoch:  172\n",
      "2017-11-15T22:56:19.352977: step 3100, loss 0.0106798, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:56:20.040355: step 3100, loss 1.59217, acc 0.582008, f1 0.582101\n",
      "\n",
      "2017-11-15T22:56:20.633845: step 3105, loss 0.00983369, acc 0.998047, f1 0.998043\n",
      "2017-11-15T22:56:21.151782: step 3110, loss 0.00837324, acc 0.999023, f1 0.999023\n",
      "Current epoch:  173\n",
      "2017-11-15T22:56:21.645259: step 3115, loss 0.00593383, acc 1, f1 1\n",
      "2017-11-15T22:56:22.168705: step 3120, loss 0.00812194, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:56:22.689651: step 3125, loss 0.0100755, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:56:23.210706: step 3130, loss 0.0105748, acc 0.99707, f1 0.99707\n",
      "Current epoch:  174\n",
      "2017-11-15T22:56:23.700376: step 3135, loss 0.00530884, acc 1, f1 1\n",
      "2017-11-15T22:56:24.219376: step 3140, loss 0.00990918, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:56:24.739305: step 3145, loss 0.0425349, acc 0.992188, f1 0.992158\n",
      "2017-11-15T22:56:25.237192: step 3150, loss 0.0167926, acc 0.996855, f1 0.996855\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:56:25.903156: step 3150, loss 1.5662, acc 0.588406, f1 0.585759\n",
      "\n",
      "Current epoch:  175\n",
      "2017-11-15T22:56:26.426558: step 3155, loss 0.018672, acc 0.996094, f1 0.996091\n",
      "2017-11-15T22:56:26.950468: step 3160, loss 0.0111568, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:56:27.474593: step 3165, loss 0.0130734, acc 0.998047, f1 0.998047\n",
      "Current epoch:  176\n",
      "2017-11-15T22:56:27.965666: step 3170, loss 0.00831141, acc 0.998047, f1 0.998048\n",
      "2017-11-15T22:56:28.493868: step 3175, loss 0.0144572, acc 0.996094, f1 0.996094\n",
      "2017-11-15T22:56:29.020727: step 3180, loss 0.00897724, acc 0.99707, f1 0.997072\n",
      "2017-11-15T22:56:29.550802: step 3185, loss 0.0077183, acc 0.999023, f1 0.999022\n",
      "Current epoch:  177\n",
      "2017-11-15T22:56:30.032756: step 3190, loss 0.00793811, acc 0.99707, f1 0.997075\n",
      "2017-11-15T22:56:30.559415: step 3195, loss 0.0127551, acc 0.99707, f1 0.997069\n",
      "2017-11-15T22:56:31.072342: step 3200, loss 0.00686577, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:56:31.703896: step 3200, loss 1.60574, acc 0.580845, f1 0.581233\n",
      "\n",
      "Current epoch:  178\n",
      "2017-11-15T22:56:32.192089: step 3205, loss 0.00658224, acc 1, f1 1\n",
      "2017-11-15T22:56:32.718007: step 3210, loss 0.00898711, acc 0.99707, f1 0.997069\n",
      "2017-11-15T22:56:33.237425: step 3215, loss 0.00698925, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:56:33.761895: step 3220, loss 0.00638746, acc 0.999023, f1 0.999024\n",
      "Current epoch:  179\n",
      "2017-11-15T22:56:34.248943: step 3225, loss 0.00475792, acc 1, f1 1\n",
      "2017-11-15T22:56:34.773018: step 3230, loss 0.00748116, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:56:35.299832: step 3235, loss 0.00825144, acc 0.998047, f1 0.998046\n",
      "2017-11-15T22:56:35.789941: step 3240, loss 0.00763983, acc 0.996855, f1 0.996853\n",
      "Current epoch:  180\n",
      "2017-11-15T22:56:36.317926: step 3245, loss 0.00466712, acc 1, f1 1\n",
      "2017-11-15T22:56:36.844316: step 3250, loss 0.00837653, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:56:37.503639: step 3250, loss 1.62936, acc 0.584529, f1 0.584705\n",
      "\n",
      "2017-11-15T22:56:38.021421: step 3255, loss 0.00623714, acc 0.999023, f1 0.999025\n",
      "Current epoch:  181\n",
      "2017-11-15T22:56:38.513629: step 3260, loss 0.0150477, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:56:39.038101: step 3265, loss 1.41257, acc 0.635742, f1 0.540466\n",
      "2017-11-15T22:56:39.556630: step 3270, loss 0.0139215, acc 0.999023, f1 0.999024\n",
      "2017-11-15T22:56:40.080339: step 3275, loss 0.0232436, acc 0.994141, f1 0.994138\n",
      "Current epoch:  182\n",
      "2017-11-15T22:56:40.567221: step 3280, loss 0.0110689, acc 0.998047, f1 0.998048\n",
      "2017-11-15T22:56:41.089129: step 3285, loss 0.00946145, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:56:41.615160: step 3290, loss 0.00902769, acc 0.999023, f1 0.999023\n",
      "Current epoch:  183\n",
      "2017-11-15T22:56:42.108108: step 3295, loss 0.0092984, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:56:42.629046: step 3300, loss 0.00997685, acc 0.998047, f1 0.998048\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:56:43.286316: step 3300, loss 1.55368, acc 0.583511, f1 0.583384\n",
      "\n",
      "2017-11-15T22:56:43.804002: step 3305, loss 0.0114984, acc 0.99707, f1 0.997067\n",
      "2017-11-15T22:56:44.323572: step 3310, loss 0.00721505, acc 1, f1 1\n",
      "Current epoch:  184\n",
      "2017-11-15T22:56:44.820724: step 3315, loss 0.00960402, acc 0.99707, f1 0.99707\n",
      "2017-11-15T22:56:45.341424: step 3320, loss 0.00811499, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:56:45.861693: step 3325, loss 0.0080179, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:56:46.350529: step 3330, loss 0.0103958, acc 0.996855, f1 0.996843\n",
      "Current epoch:  185\n",
      "2017-11-15T22:56:46.875808: step 3335, loss 0.00660244, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:56:47.402756: step 3340, loss 0.00805406, acc 0.99707, f1 0.997071\n",
      "2017-11-15T22:56:47.926938: step 3345, loss 0.00669959, acc 0.999023, f1 0.999023\n",
      "Current epoch:  186\n",
      "2017-11-15T22:56:48.423453: step 3350, loss 0.00669876, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:56:49.084529: step 3350, loss 1.61537, acc 0.583026, f1 0.582815\n",
      "\n",
      "2017-11-15T22:56:49.607479: step 3355, loss 0.0101046, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:56:50.131570: step 3360, loss 0.0109869, acc 0.998047, f1 0.998046\n",
      "2017-11-15T22:56:50.655816: step 3365, loss 0.00927554, acc 0.995117, f1 0.99512\n",
      "Current epoch:  187\n",
      "2017-11-15T22:56:51.141827: step 3370, loss 0.0082584, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:56:51.663316: step 3375, loss 0.0079439, acc 0.998047, f1 0.998046\n",
      "2017-11-15T22:56:52.173760: step 3380, loss 0.00731428, acc 0.998047, f1 0.998046\n",
      "Current epoch:  188\n",
      "2017-11-15T22:56:52.683187: step 3385, loss 0.0126183, acc 0.996094, f1 0.996089\n",
      "2017-11-15T22:56:53.201194: step 3390, loss 0.00623712, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:56:53.727253: step 3395, loss 0.005199, acc 1, f1 1\n",
      "2017-11-15T22:56:54.246216: step 3400, loss 0.0110902, acc 0.99707, f1 0.997066\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:56:54.916593: step 3400, loss 1.72911, acc 0.564027, f1 0.567396\n",
      "\n",
      "Current epoch:  189\n",
      "2017-11-15T22:56:55.404779: step 3405, loss 0.00909701, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:56:55.920796: step 3410, loss 0.0159056, acc 0.995117, f1 0.995118\n",
      "2017-11-15T22:56:56.441121: step 3415, loss 0.00397181, acc 1, f1 1\n",
      "2017-11-15T22:56:56.930988: step 3420, loss 0.0306429, acc 0.992138, f1 0.992135\n",
      "Current epoch:  190\n",
      "2017-11-15T22:56:57.452989: step 3425, loss 2.09822, acc 0.837891, f1 0.767161\n",
      "2017-11-15T22:56:57.975430: step 3430, loss 0.0115256, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:56:58.506713: step 3435, loss 0.0120729, acc 0.999023, f1 0.999024\n",
      "Current epoch:  191\n",
      "2017-11-15T22:56:58.997038: step 3440, loss 0.0111212, acc 0.998047, f1 0.998046\n",
      "2017-11-15T22:56:59.514999: step 3445, loss 0.0144468, acc 0.996094, f1 0.996094\n",
      "2017-11-15T22:57:00.031599: step 3450, loss 0.00967175, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:57:00.688232: step 3450, loss 1.57671, acc 0.583462, f1 0.583738\n",
      "\n",
      "2017-11-15T22:57:01.207017: step 3455, loss 0.00637921, acc 1, f1 1\n",
      "Current epoch:  192\n",
      "2017-11-15T22:57:01.692354: step 3460, loss 0.00730646, acc 0.999023, f1 0.999022\n",
      "2017-11-15T22:57:02.213085: step 3465, loss 0.00997495, acc 0.99707, f1 0.997069\n",
      "2017-11-15T22:57:02.738545: step 3470, loss 0.00521926, acc 1, f1 1\n",
      "Current epoch:  193\n",
      "2017-11-15T22:57:03.227000: step 3475, loss 0.00727118, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:57:03.769972: step 3480, loss 0.00771773, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:57:04.291928: step 3485, loss 0.00825421, acc 0.99707, f1 0.997067\n",
      "2017-11-15T22:57:04.810139: step 3490, loss 0.00952002, acc 0.998047, f1 0.998047\n",
      "Current epoch:  194\n",
      "2017-11-15T22:57:05.300518: step 3495, loss 0.00561908, acc 1, f1 1\n",
      "2017-11-15T22:57:05.820242: step 3500, loss 0.00759832, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:57:06.474944: step 3500, loss 1.64601, acc 0.580845, f1 0.581121\n",
      "\n",
      "2017-11-15T22:57:06.992874: step 3505, loss 0.00919509, acc 0.99707, f1 0.997075\n",
      "2017-11-15T22:57:07.480395: step 3510, loss 0.0131305, acc 0.996855, f1 0.996855\n",
      "Current epoch:  195\n",
      "2017-11-15T22:57:07.999862: step 3515, loss 0.00554714, acc 0.999023, f1 0.999022\n",
      "2017-11-15T22:57:08.517805: step 3520, loss 0.00897426, acc 0.99707, f1 0.997069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T22:57:09.048196: step 3525, loss 0.00971416, acc 0.998047, f1 0.998047\n",
      "Current epoch:  196\n",
      "2017-11-15T22:57:09.542875: step 3530, loss 0.0119504, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:57:10.057616: step 3535, loss 0.00411928, acc 1, f1 1\n",
      "2017-11-15T22:57:10.563011: step 3540, loss 0.0048527, acc 1, f1 1\n",
      "2017-11-15T22:57:11.072912: step 3545, loss 0.0130673, acc 0.99707, f1 0.997069\n",
      "Current epoch:  197\n",
      "2017-11-15T22:57:11.554794: step 3550, loss 1.54835, acc 0.636719, f1 0.535513\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:57:12.196547: step 3550, loss 2.16438, acc 0.571151, f1 0.523928\n",
      "\n",
      "2017-11-15T22:57:12.717962: step 3555, loss 0.0160439, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:57:13.236300: step 3560, loss 0.0175002, acc 0.998047, f1 0.998048\n",
      "Current epoch:  198\n",
      "2017-11-15T22:57:13.729259: step 3565, loss 0.00858359, acc 1, f1 1\n",
      "2017-11-15T22:57:14.249446: step 3570, loss 0.0100134, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:57:14.785099: step 3575, loss 0.00663079, acc 1, f1 1\n",
      "2017-11-15T22:57:15.306045: step 3580, loss 0.00791219, acc 0.999023, f1 0.999023\n",
      "Current epoch:  199\n",
      "2017-11-15T22:57:15.794551: step 3585, loss 0.00810055, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:57:16.316974: step 3590, loss 0.00976856, acc 0.99707, f1 0.997067\n",
      "2017-11-15T22:57:16.839473: step 3595, loss 0.0116014, acc 0.99707, f1 0.99707\n",
      "2017-11-15T22:57:17.326859: step 3600, loss 0.00540711, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:57:18.001561: step 3600, loss 1.589, acc 0.58036, f1 0.580775\n",
      "\n",
      "Current epoch:  200\n",
      "2017-11-15T22:57:18.524890: step 3605, loss 0.00682299, acc 1, f1 1\n",
      "2017-11-15T22:57:19.043337: step 3610, loss 0.00871364, acc 0.998047, f1 0.998048\n",
      "2017-11-15T22:57:19.558301: step 3615, loss 0.0111983, acc 0.996094, f1 0.996095\n",
      "Current epoch:  201\n",
      "2017-11-15T22:57:20.053717: step 3620, loss 0.00832584, acc 0.99707, f1 0.997072\n",
      "2017-11-15T22:57:20.581158: step 3625, loss 0.00788301, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:57:21.095939: step 3630, loss 0.0101569, acc 0.995117, f1 0.995117\n",
      "2017-11-15T22:57:21.614875: step 3635, loss 0.0117687, acc 0.995117, f1 0.995118\n",
      "Current epoch:  202\n",
      "2017-11-15T22:57:22.100544: step 3640, loss 0.00719391, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:57:22.618026: step 3645, loss 0.00673769, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:57:23.133567: step 3650, loss 0.00831242, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:57:23.791775: step 3650, loss 1.63749, acc 0.583075, f1 0.582958\n",
      "\n",
      "Current epoch:  203\n",
      "2017-11-15T22:57:24.277603: step 3655, loss 0.00415815, acc 1, f1 1\n",
      "2017-11-15T22:57:24.794873: step 3660, loss 0.00491937, acc 1, f1 1\n",
      "2017-11-15T22:57:25.317966: step 3665, loss 0.00686986, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:57:25.846940: step 3670, loss 0.0111361, acc 0.996094, f1 0.996094\n",
      "Current epoch:  204\n",
      "2017-11-15T22:57:26.331152: step 3675, loss 0.00494431, acc 1, f1 1\n",
      "2017-11-15T22:57:26.863544: step 3680, loss 0.00549876, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:57:27.381340: step 3685, loss 0.00678029, acc 0.999023, f1 0.999022\n",
      "2017-11-15T22:57:27.867794: step 3690, loss 0.00372267, acc 1, f1 1\n",
      "Current epoch:  205\n",
      "2017-11-15T22:57:28.392546: step 3695, loss 0.0041332, acc 0.999023, f1 0.999022\n",
      "2017-11-15T22:57:28.914113: step 3700, loss 0.0110167, acc 0.99707, f1 0.997069\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:57:29.567449: step 3700, loss 1.69185, acc 0.593592, f1 0.589889\n",
      "\n",
      "2017-11-15T22:57:30.085729: step 3705, loss 0.0111102, acc 0.99707, f1 0.99707\n",
      "Current epoch:  206\n",
      "2017-11-15T22:57:30.572119: step 3710, loss 0.0128168, acc 0.99707, f1 0.997069\n",
      "2017-11-15T22:57:31.100591: step 3715, loss 0.00544353, acc 0.999023, f1 0.999024\n",
      "2017-11-15T22:57:31.632471: step 3720, loss 0.00359644, acc 1, f1 1\n",
      "2017-11-15T22:57:32.157176: step 3725, loss 0.0427084, acc 0.994141, f1 0.994156\n",
      "Current epoch:  207\n",
      "2017-11-15T22:57:32.655109: step 3730, loss 0.0190558, acc 0.99707, f1 0.99707\n",
      "2017-11-15T22:57:33.180126: step 3735, loss 0.00963821, acc 1, f1 1\n",
      "2017-11-15T22:57:33.700064: step 3740, loss 0.0110694, acc 0.998047, f1 0.998046\n",
      "Current epoch:  208\n",
      "2017-11-15T22:57:34.193986: step 3745, loss 0.0107486, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:57:34.719472: step 3750, loss 0.00600488, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:57:35.375799: step 3750, loss 1.60956, acc 0.582832, f1 0.582796\n",
      "\n",
      "2017-11-15T22:57:35.891876: step 3755, loss 0.00778933, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:57:36.419300: step 3760, loss 0.00832296, acc 0.998047, f1 0.998047\n",
      "Current epoch:  209\n",
      "2017-11-15T22:57:36.917316: step 3765, loss 0.00696652, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:57:37.435106: step 3770, loss 0.00890908, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:57:37.955047: step 3775, loss 0.00823061, acc 0.99707, f1 0.997071\n",
      "2017-11-15T22:57:38.451633: step 3780, loss 0.00949899, acc 0.996855, f1 0.996856\n",
      "Current epoch:  210\n",
      "2017-11-15T22:57:38.973262: step 3785, loss 0.00582077, acc 0.999023, f1 0.999024\n",
      "2017-11-15T22:57:39.491201: step 3790, loss 0.00588218, acc 0.999023, f1 0.999024\n",
      "2017-11-15T22:57:40.006121: step 3795, loss 0.00652205, acc 0.998047, f1 0.998046\n",
      "Current epoch:  211\n",
      "2017-11-15T22:57:40.493973: step 3800, loss 0.0037029, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:57:41.148186: step 3800, loss 1.66864, acc 0.580457, f1 0.580823\n",
      "\n",
      "2017-11-15T22:57:41.693304: step 3805, loss 0.00724751, acc 0.99707, f1 0.997072\n",
      "2017-11-15T22:57:42.227463: step 3810, loss 0.00909725, acc 0.99707, f1 0.99707\n",
      "2017-11-15T22:57:42.751932: step 3815, loss 0.0126728, acc 0.995117, f1 0.995119\n",
      "Current epoch:  212\n",
      "2017-11-15T22:57:43.238884: step 3820, loss 0.0087977, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:57:43.759619: step 3825, loss 0.0050208, acc 0.999023, f1 0.999024\n",
      "2017-11-15T22:57:44.280042: step 3830, loss 0.00700386, acc 0.998047, f1 0.998048\n",
      "Current epoch:  213\n",
      "2017-11-15T22:57:44.773969: step 3835, loss 0.00649376, acc 0.999023, f1 0.999022\n",
      "2017-11-15T22:57:45.292907: step 3840, loss 0.0029462, acc 1, f1 1\n",
      "2017-11-15T22:57:45.812870: step 3845, loss 0.00940316, acc 0.99707, f1 0.99707\n",
      "2017-11-15T22:57:46.330418: step 3850, loss 0.00942651, acc 0.99707, f1 0.997071\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:57:46.993817: step 3850, loss 1.71225, acc 0.57847, f1 0.5797\n",
      "\n",
      "Current epoch:  214\n",
      "2017-11-15T22:57:47.485903: step 3855, loss 0.0135264, acc 0.99707, f1 0.997071\n",
      "2017-11-15T22:57:48.017968: step 3860, loss 0.00316005, acc 1, f1 1\n",
      "2017-11-15T22:57:48.541758: step 3865, loss 0.00529541, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:57:49.034651: step 3870, loss 0.0077286, acc 0.998428, f1 0.99843\n",
      "Current epoch:  215\n",
      "2017-11-15T22:57:49.553106: step 3875, loss 0.00961497, acc 0.99707, f1 0.99707\n",
      "2017-11-15T22:57:50.071062: step 3880, loss 0.00716266, acc 0.999023, f1 0.999024\n",
      "2017-11-15T22:57:50.588331: step 3885, loss 0.00907884, acc 0.999023, f1 0.999022\n",
      "Current epoch:  216\n",
      "2017-11-15T22:57:51.080210: step 3890, loss 0.00447465, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:57:51.606111: step 3895, loss 0.0192303, acc 0.995117, f1 0.995115\n",
      "2017-11-15T22:57:52.238302: step 3900, loss 1.57456, acc 0.624023, f1 0.537704\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:57:52.913843: step 3900, loss 2.53334, acc 0.464473, f1 0.419105\n",
      "\n",
      "2017-11-15T22:57:53.440249: step 3905, loss 0.0173747, acc 0.998047, f1 0.998047\n",
      "Current epoch:  217\n",
      "2017-11-15T22:57:53.929793: step 3910, loss 0.0100182, acc 1, f1 1\n",
      "2017-11-15T22:57:54.451855: step 3915, loss 0.010859, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:57:54.968780: step 3920, loss 0.00990056, acc 0.999023, f1 0.999023\n",
      "Current epoch:  218\n",
      "2017-11-15T22:57:55.462183: step 3925, loss 0.00867803, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:57:55.977582: step 3930, loss 0.00995459, acc 0.99707, f1 0.997072\n",
      "2017-11-15T22:57:56.496620: step 3935, loss 0.00686134, acc 0.999023, f1 0.999022\n",
      "2017-11-15T22:57:57.009545: step 3940, loss 0.0049929, acc 1, f1 1\n",
      "Current epoch:  219\n",
      "2017-11-15T22:57:57.500788: step 3945, loss 0.00782838, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:57:58.016266: step 3950, loss 0.0079943, acc 0.999023, f1 0.999024\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:57:58.681594: step 3950, loss 1.62816, acc 0.582299, f1 0.582118\n",
      "\n",
      "2017-11-15T22:57:59.204678: step 3955, loss 0.00708799, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:57:59.693183: step 3960, loss 0.00653956, acc 0.998428, f1 0.998429\n",
      "Current epoch:  220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T22:58:00.208679: step 3965, loss 0.0081262, acc 0.99707, f1 0.997069\n",
      "2017-11-15T22:58:00.726995: step 3970, loss 0.00711066, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:58:01.247563: step 3975, loss 0.00775417, acc 0.99707, f1 0.99707\n",
      "Current epoch:  221\n",
      "2017-11-15T22:58:01.739752: step 3980, loss 0.00505339, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:58:02.277732: step 3985, loss 0.00349663, acc 1, f1 1\n",
      "2017-11-15T22:58:02.810666: step 3990, loss 0.00326682, acc 1, f1 1\n",
      "2017-11-15T22:58:03.337617: step 3995, loss 0.00702166, acc 0.998047, f1 0.998047\n",
      "Current epoch:  222\n",
      "2017-11-15T22:58:03.849114: step 4000, loss 0.0052403, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:58:04.567204: step 4000, loss 1.71422, acc 0.578519, f1 0.5773\n",
      "\n",
      "2017-11-15T22:58:05.085621: step 4005, loss 0.00800784, acc 0.998047, f1 0.998046\n",
      "2017-11-15T22:58:05.608566: step 4010, loss 0.00584017, acc 0.999023, f1 0.999023\n",
      "Current epoch:  223\n",
      "2017-11-15T22:58:06.098495: step 4015, loss 0.00349129, acc 1, f1 1\n",
      "2017-11-15T22:58:06.615565: step 4020, loss 0.0047383, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:58:07.130608: step 4025, loss 0.00715268, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:58:07.656045: step 4030, loss 0.00966381, acc 0.999023, f1 0.999025\n",
      "Current epoch:  224\n",
      "2017-11-15T22:58:08.144215: step 4035, loss 0.00369858, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:58:08.667365: step 4040, loss 0.00658183, acc 0.998047, f1 0.998048\n",
      "2017-11-15T22:58:09.187328: step 4045, loss 0.0118504, acc 0.996094, f1 0.996097\n",
      "2017-11-15T22:58:09.684216: step 4050, loss 0.00361065, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:58:10.350189: step 4050, loss 1.75474, acc 0.576822, f1 0.577539\n",
      "\n",
      "Current epoch:  225\n",
      "2017-11-15T22:58:10.875515: step 4055, loss 0.00546933, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:58:11.393936: step 4060, loss 0.00524431, acc 0.999023, f1 0.999024\n",
      "2017-11-15T22:58:11.945049: step 4065, loss 0.0270837, acc 0.999023, f1 0.999023\n",
      "Current epoch:  226\n",
      "2017-11-15T22:58:12.436653: step 4070, loss 0.153953, acc 0.94043, f1 0.940322\n",
      "2017-11-15T22:58:12.956457: step 4075, loss 0.00973452, acc 1, f1 1\n",
      "2017-11-15T22:58:13.468409: step 4080, loss 0.0108157, acc 0.999023, f1 0.999025\n",
      "2017-11-15T22:58:13.988389: step 4085, loss 0.00989277, acc 0.998047, f1 0.998046\n",
      "Current epoch:  227\n",
      "2017-11-15T22:58:14.471299: step 4090, loss 0.00735071, acc 0.999023, f1 0.999024\n",
      "2017-11-15T22:58:14.998279: step 4095, loss 0.00619147, acc 1, f1 1\n",
      "2017-11-15T22:58:15.530733: step 4100, loss 0.0109476, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:58:16.201053: step 4100, loss 1.62431, acc 0.582784, f1 0.583093\n",
      "\n",
      "Current epoch:  228\n",
      "2017-11-15T22:58:16.696578: step 4105, loss 0.00861054, acc 0.99707, f1 0.997071\n",
      "2017-11-15T22:58:17.212521: step 4110, loss 0.00924816, acc 0.99707, f1 0.997074\n",
      "2017-11-15T22:58:17.733141: step 4115, loss 0.00441692, acc 1, f1 1\n",
      "2017-11-15T22:58:18.250132: step 4120, loss 0.00874973, acc 0.998047, f1 0.998047\n",
      "Current epoch:  229\n",
      "2017-11-15T22:58:18.744099: step 4125, loss 0.00547326, acc 0.999023, f1 0.999022\n",
      "2017-11-15T22:58:19.259189: step 4130, loss 0.00375739, acc 1, f1 1\n",
      "2017-11-15T22:58:19.769969: step 4135, loss 0.00729024, acc 0.99707, f1 0.997074\n",
      "2017-11-15T22:58:20.252382: step 4140, loss 0.00777144, acc 0.996855, f1 0.996855\n",
      "Current epoch:  230\n",
      "2017-11-15T22:58:20.775614: step 4145, loss 0.00740914, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:58:21.285644: step 4150, loss 0.00578138, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:58:21.918898: step 4150, loss 1.67817, acc 0.585013, f1 0.58474\n",
      "\n",
      "2017-11-15T22:58:22.501778: step 4155, loss 0.00482537, acc 0.999023, f1 0.999023\n",
      "Current epoch:  231\n",
      "2017-11-15T22:58:22.999062: step 4160, loss 0.00538415, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:58:23.522015: step 4165, loss 0.00471059, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:58:24.039067: step 4170, loss 0.0101334, acc 0.996094, f1 0.996096\n",
      "2017-11-15T22:58:24.553922: step 4175, loss 0.00683013, acc 0.998047, f1 0.998046\n",
      "Current epoch:  232\n",
      "2017-11-15T22:58:25.043678: step 4180, loss 0.00487559, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:58:25.560904: step 4185, loss 0.00992702, acc 0.99707, f1 0.99707\n",
      "2017-11-15T22:58:26.091406: step 4190, loss 0.00920212, acc 0.99707, f1 0.997074\n",
      "Current epoch:  233\n",
      "2017-11-15T22:58:26.592088: step 4195, loss 0.0048642, acc 0.999023, f1 0.999024\n",
      "2017-11-15T22:58:27.108008: step 4200, loss 0.00343572, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:58:27.762132: step 4200, loss 1.74584, acc 0.579343, f1 0.580024\n",
      "\n",
      "2017-11-15T22:58:28.280599: step 4205, loss 0.0111151, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:58:28.804759: step 4210, loss 0.00638615, acc 0.998047, f1 0.998047\n",
      "Current epoch:  234\n",
      "2017-11-15T22:58:29.295956: step 4215, loss 0.00971575, acc 0.99707, f1 0.997071\n",
      "2017-11-15T22:58:29.823384: step 4220, loss 0.0142033, acc 0.99707, f1 0.99707\n",
      "2017-11-15T22:58:30.343836: step 4225, loss 0.00824839, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:58:30.834729: step 4230, loss 0.00422822, acc 1, f1 1\n",
      "Current epoch:  235\n",
      "2017-11-15T22:58:31.359621: step 4235, loss 0.00349522, acc 1, f1 1\n",
      "2017-11-15T22:58:31.890581: step 4240, loss 0.00818355, acc 0.998047, f1 0.998042\n",
      "2017-11-15T22:58:32.412043: step 4245, loss 0.00556017, acc 0.999023, f1 0.999022\n",
      "Current epoch:  236\n",
      "2017-11-15T22:58:32.909121: step 4250, loss 0.0098405, acc 0.99707, f1 0.99707\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:58:33.563451: step 4250, loss 1.79605, acc 0.59621, f1 0.591799\n",
      "\n",
      "2017-11-15T22:58:34.082405: step 4255, loss 0.0089162, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:58:34.608384: step 4260, loss 2.09045, acc 0.610352, f1 0.526997\n",
      "2017-11-15T22:58:35.130594: step 4265, loss 0.0254271, acc 0.999023, f1 0.999023\n",
      "Current epoch:  237\n",
      "2017-11-15T22:58:35.620951: step 4270, loss 0.0149789, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:58:36.138006: step 4275, loss 0.0140222, acc 0.998047, f1 0.998046\n",
      "2017-11-15T22:58:36.659415: step 4280, loss 0.0106963, acc 0.999023, f1 0.999023\n",
      "Current epoch:  238\n",
      "2017-11-15T22:58:37.160585: step 4285, loss 0.00979152, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:58:37.683774: step 4290, loss 0.0118894, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:58:38.200226: step 4295, loss 0.00838396, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:58:38.722204: step 4300, loss 0.00978788, acc 0.999023, f1 0.999024\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:58:39.382548: step 4300, loss 1.61031, acc 0.584141, f1 0.584282\n",
      "\n",
      "Current epoch:  239\n",
      "2017-11-15T22:58:39.878145: step 4305, loss 0.00631536, acc 1, f1 1\n",
      "2017-11-15T22:58:40.405598: step 4310, loss 0.00695408, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:58:40.928126: step 4315, loss 0.0053012, acc 1, f1 1\n",
      "2017-11-15T22:58:41.415806: step 4320, loss 0.0105195, acc 0.996855, f1 0.996854\n",
      "Current epoch:  240\n",
      "2017-11-15T22:58:41.941752: step 4325, loss 0.00769994, acc 0.99707, f1 0.997071\n",
      "2017-11-15T22:58:42.467063: step 4330, loss 0.00617854, acc 1, f1 1\n",
      "2017-11-15T22:58:42.994014: step 4335, loss 0.00644617, acc 0.998047, f1 0.998047\n",
      "Current epoch:  241\n",
      "2017-11-15T22:58:43.489424: step 4340, loss 0.00652268, acc 0.999023, f1 0.999022\n",
      "2017-11-15T22:58:44.009479: step 4345, loss 0.00555903, acc 0.998047, f1 0.998046\n",
      "2017-11-15T22:58:44.532462: step 4350, loss 0.00799829, acc 0.99707, f1 0.99707\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:58:45.186000: step 4350, loss 1.67502, acc 0.580845, f1 0.581524\n",
      "\n",
      "2017-11-15T22:58:45.704934: step 4355, loss 0.00622543, acc 0.998047, f1 0.998046\n",
      "Current epoch:  242\n",
      "2017-11-15T22:58:46.194304: step 4360, loss 0.00849299, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:58:46.714117: step 4365, loss 0.0112845, acc 0.995117, f1 0.995123\n",
      "2017-11-15T22:58:47.232050: step 4370, loss 0.00443405, acc 0.999023, f1 0.999023\n",
      "Current epoch:  243\n",
      "2017-11-15T22:58:47.729311: step 4375, loss 0.00785541, acc 0.996094, f1 0.996094\n",
      "2017-11-15T22:58:48.263912: step 4380, loss 0.00537324, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:58:48.785383: step 4385, loss 0.00586843, acc 0.999023, f1 0.999022\n",
      "2017-11-15T22:58:49.309893: step 4390, loss 0.00986946, acc 0.996094, f1 0.996095\n",
      "Current epoch:  244\n",
      "2017-11-15T22:58:49.798450: step 4395, loss 0.00766776, acc 0.99707, f1 0.99707\n",
      "2017-11-15T22:58:50.321397: step 4400, loss 0.00702251, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T22:58:50.987599: step 4400, loss 1.74913, acc 0.575611, f1 0.576252\n",
      "\n",
      "2017-11-15T22:58:51.509651: step 4405, loss 0.00285293, acc 1, f1 1\n",
      "2017-11-15T22:58:51.995469: step 4410, loss 0.00665047, acc 0.998428, f1 0.998425\n",
      "Current epoch:  245\n",
      "2017-11-15T22:58:52.514841: step 4415, loss 0.00434916, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:58:53.036871: step 4420, loss 0.0130823, acc 0.99707, f1 0.997073\n",
      "2017-11-15T22:58:53.563896: step 4425, loss 0.00679477, acc 0.99707, f1 0.997069\n",
      "Current epoch:  246\n",
      "2017-11-15T22:58:54.056803: step 4430, loss 0.00220122, acc 1, f1 1\n",
      "2017-11-15T22:58:54.574767: step 4435, loss 0.00794867, acc 0.99707, f1 0.997072\n",
      "2017-11-15T22:58:55.092706: step 4440, loss 0.0134217, acc 0.99707, f1 0.997071\n",
      "2017-11-15T22:58:55.613625: step 4445, loss 0.00979553, acc 0.998047, f1 0.998048\n",
      "Current epoch:  247\n",
      "2017-11-15T22:58:56.098500: step 4450, loss 2.56327, acc 0.552734, f1 0.454165\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:58:56.750708: step 4450, loss 5.57474, acc 0.40505, f1 0.273846\n",
      "\n",
      "2017-11-15T22:58:57.268438: step 4455, loss 0.018699, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:58:57.784196: step 4460, loss 0.0194881, acc 0.998047, f1 0.998047\n",
      "Current epoch:  248\n",
      "2017-11-15T22:58:58.271527: step 4465, loss 0.0106642, acc 0.999023, f1 0.999025\n",
      "2017-11-15T22:58:58.789444: step 4470, loss 0.0166753, acc 0.99707, f1 0.997075\n",
      "2017-11-15T22:58:59.322548: step 4475, loss 0.00932575, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:58:59.845270: step 4480, loss 0.0073866, acc 1, f1 1\n",
      "Current epoch:  249\n",
      "2017-11-15T22:59:00.344661: step 4485, loss 0.00586177, acc 1, f1 1\n",
      "2017-11-15T22:59:00.867435: step 4490, loss 0.0069446, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:59:01.390470: step 4495, loss 0.00778026, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:59:01.878362: step 4500, loss 0.00549009, acc 0.998428, f1 0.998428\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:59:02.536558: step 4500, loss 1.65814, acc 0.579876, f1 0.580565\n",
      "\n",
      "Current epoch:  250\n",
      "2017-11-15T22:59:03.055793: step 4505, loss 0.00817426, acc 0.99707, f1 0.997071\n",
      "2017-11-15T22:59:03.576714: step 4510, loss 0.0055391, acc 0.998047, f1 0.998048\n",
      "2017-11-15T22:59:04.099180: step 4515, loss 0.00518483, acc 0.998047, f1 0.998047\n",
      "Current epoch:  251\n",
      "2017-11-15T22:59:04.596596: step 4520, loss 0.00407145, acc 1, f1 1\n",
      "2017-11-15T22:59:05.130589: step 4525, loss 0.00586043, acc 0.999023, f1 0.999022\n",
      "2017-11-15T22:59:05.657057: step 4530, loss 0.00460252, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:59:06.178471: step 4535, loss 0.00637162, acc 0.998047, f1 0.998047\n",
      "Current epoch:  252\n",
      "2017-11-15T22:59:06.665659: step 4540, loss 0.00471247, acc 0.999023, f1 0.999025\n",
      "2017-11-15T22:59:07.180703: step 4545, loss 0.0090608, acc 0.996094, f1 0.996095\n",
      "2017-11-15T22:59:07.702672: step 4550, loss 0.00470009, acc 0.999023, f1 0.999024\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:59:08.355337: step 4550, loss 1.72426, acc 0.587001, f1 0.58551\n",
      "\n",
      "Current epoch:  253\n",
      "2017-11-15T22:59:08.849336: step 4555, loss 0.00292944, acc 1, f1 1\n",
      "2017-11-15T22:59:09.365228: step 4560, loss 0.00683727, acc 0.99707, f1 0.997071\n",
      "2017-11-15T22:59:09.876735: step 4565, loss 0.00566118, acc 0.998047, f1 0.998042\n",
      "2017-11-15T22:59:10.400699: step 4570, loss 0.00597864, acc 0.998047, f1 0.998046\n",
      "Current epoch:  254\n",
      "2017-11-15T22:59:10.884565: step 4575, loss 0.00500618, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:59:11.394477: step 4580, loss 0.0151065, acc 0.994141, f1 0.994139\n",
      "2017-11-15T22:59:11.909280: step 4585, loss 0.00610279, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:59:12.403795: step 4590, loss 0.00372576, acc 0.998428, f1 0.99843\n",
      "Current epoch:  255\n",
      "2017-11-15T22:59:12.931740: step 4595, loss 0.00543817, acc 0.998047, f1 0.998046\n",
      "2017-11-15T22:59:13.448648: step 4600, loss 0.00580782, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:59:14.101391: step 4600, loss 1.77815, acc 0.580942, f1 0.583365\n",
      "\n",
      "2017-11-15T22:59:14.618657: step 4605, loss 0.00839908, acc 0.998047, f1 0.998048\n",
      "Current epoch:  256\n",
      "2017-11-15T22:59:15.115533: step 4610, loss 0.00674167, acc 0.999023, f1 0.999025\n",
      "2017-11-15T22:59:15.655232: step 4615, loss 0.00443703, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:59:16.181192: step 4620, loss 0.0058425, acc 0.998047, f1 0.998048\n",
      "2017-11-15T22:59:16.712487: step 4625, loss 0.00709578, acc 0.999023, f1 0.999023\n",
      "Current epoch:  257\n",
      "2017-11-15T22:59:17.197639: step 4630, loss 1.28295, acc 0.646484, f1 0.5499\n",
      "2017-11-15T22:59:17.718425: step 4635, loss 0.0124255, acc 0.99707, f1 0.997069\n",
      "2017-11-15T22:59:18.234959: step 4640, loss 0.00865141, acc 0.998047, f1 0.998047\n",
      "Current epoch:  258\n",
      "2017-11-15T22:59:18.721708: step 4645, loss 0.00676895, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:59:19.243535: step 4650, loss 0.0073677, acc 0.998047, f1 0.998046\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:59:19.908339: step 4650, loss 1.66071, acc 0.587291, f1 0.587656\n",
      "\n",
      "2017-11-15T22:59:20.423568: step 4655, loss 0.00535353, acc 0.999023, f1 0.999025\n",
      "2017-11-15T22:59:20.937014: step 4660, loss 0.00722242, acc 0.99707, f1 0.997074\n",
      "Current epoch:  259\n",
      "2017-11-15T22:59:21.425401: step 4665, loss 0.0074533, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:59:21.937594: step 4670, loss 0.00608286, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:59:22.453107: step 4675, loss 0.0058673, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:59:22.940999: step 4680, loss 0.00403455, acc 1, f1 1\n",
      "Current epoch:  260\n",
      "2017-11-15T22:59:23.462849: step 4685, loss 0.00561554, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:59:23.982409: step 4690, loss 0.0081472, acc 0.996094, f1 0.996095\n",
      "2017-11-15T22:59:24.503819: step 4695, loss 0.00478048, acc 0.999023, f1 0.999023\n",
      "Current epoch:  261\n",
      "2017-11-15T22:59:24.994196: step 4700, loss 0.00471744, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:59:25.653615: step 4700, loss 1.71688, acc 0.586758, f1 0.587104\n",
      "\n",
      "2017-11-15T22:59:26.168073: step 4705, loss 0.00366915, acc 1, f1 1\n",
      "2017-11-15T22:59:26.702231: step 4710, loss 0.00484653, acc 0.999023, f1 0.999022\n",
      "2017-11-15T22:59:27.225181: step 4715, loss 0.0110479, acc 0.996094, f1 0.996103\n",
      "Current epoch:  262\n",
      "2017-11-15T22:59:27.717285: step 4720, loss 0.00312644, acc 1, f1 1\n",
      "2017-11-15T22:59:28.232286: step 4725, loss 0.00627641, acc 0.99707, f1 0.99707\n",
      "2017-11-15T22:59:28.761827: step 4730, loss 0.00850446, acc 0.99707, f1 0.99707\n",
      "Current epoch:  263\n",
      "2017-11-15T22:59:29.250692: step 4735, loss 0.00421642, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:59:29.766081: step 4740, loss 0.00721105, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:59:30.283551: step 4745, loss 0.0028714, acc 1, f1 1\n",
      "2017-11-15T22:59:30.800450: step 4750, loss 0.008284, acc 0.99707, f1 0.997069\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:59:31.426376: step 4750, loss 1.79865, acc 0.575756, f1 0.576205\n",
      "\n",
      "Current epoch:  264\n",
      "2017-11-15T22:59:31.910747: step 4755, loss 0.011922, acc 0.996094, f1 0.996094\n",
      "2017-11-15T22:59:32.445761: step 4760, loss 0.00410043, acc 0.999023, f1 0.999022\n",
      "2017-11-15T22:59:32.967701: step 4765, loss 0.0150772, acc 0.996094, f1 0.996093\n",
      "2017-11-15T22:59:33.453102: step 4770, loss 0.0151626, acc 0.996855, f1 0.996856\n",
      "Current epoch:  265\n",
      "2017-11-15T22:59:33.970511: step 4775, loss 0.00227409, acc 1, f1 1\n",
      "2017-11-15T22:59:34.490455: step 4780, loss 0.018482, acc 0.994141, f1 0.994138\n",
      "2017-11-15T22:59:35.012674: step 4785, loss 0.00373159, acc 0.999023, f1 0.999022\n",
      "Current epoch:  266\n",
      "2017-11-15T22:59:35.508288: step 4790, loss 0.0074746, acc 0.998047, f1 0.998046\n",
      "2017-11-15T22:59:36.028736: step 4795, loss 0.00216352, acc 1, f1 1\n",
      "2017-11-15T22:59:36.552737: step 4800, loss 0.00595316, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:59:37.211281: step 4800, loss 1.90437, acc 0.568534, f1 0.566931\n",
      "\n",
      "2017-11-15T22:59:37.743940: step 4805, loss 0.0118261, acc 0.99707, f1 0.997071\n",
      "Current epoch:  267\n",
      "2017-11-15T22:59:38.233835: step 4810, loss 0.0111134, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:59:38.757835: step 4815, loss 0.00785564, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:59:39.279025: step 4820, loss 0.0323348, acc 0.993164, f1 0.993167\n",
      "Current epoch:  268\n",
      "2017-11-15T22:59:39.773093: step 4825, loss 0.776705, acc 0.821289, f1 0.764402\n",
      "2017-11-15T22:59:40.309608: step 4830, loss 0.011906, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:59:40.835390: step 4835, loss 0.00702069, acc 1, f1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T22:59:41.370367: step 4840, loss 0.00952513, acc 0.998047, f1 0.998048\n",
      "Current epoch:  269\n",
      "2017-11-15T22:59:41.871006: step 4845, loss 0.00677608, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:59:42.391537: step 4850, loss 0.00921994, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:59:43.060672: step 4850, loss 1.69265, acc 0.58385, f1 0.583887\n",
      "\n",
      "2017-11-15T22:59:43.585657: step 4855, loss 0.009124, acc 0.998047, f1 0.998048\n",
      "2017-11-15T22:59:44.074045: step 4860, loss 0.00901409, acc 0.998428, f1 0.998428\n",
      "Current epoch:  270\n",
      "2017-11-15T22:59:44.597153: step 4865, loss 0.00622599, acc 0.998047, f1 0.998046\n",
      "2017-11-15T22:59:45.118523: step 4870, loss 0.00484807, acc 1, f1 1\n",
      "2017-11-15T22:59:45.642910: step 4875, loss 0.00403325, acc 1, f1 1\n",
      "Current epoch:  271\n",
      "2017-11-15T22:59:46.140804: step 4880, loss 0.00494545, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:59:46.662345: step 4885, loss 0.00387799, acc 1, f1 1\n",
      "2017-11-15T22:59:47.180282: step 4890, loss 0.00328978, acc 1, f1 1\n",
      "2017-11-15T22:59:47.702301: step 4895, loss 0.00343156, acc 1, f1 1\n",
      "Current epoch:  272\n",
      "2017-11-15T22:59:48.197187: step 4900, loss 0.00568634, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:59:48.904022: step 4900, loss 1.7335, acc 0.585692, f1 0.585468\n",
      "\n",
      "2017-11-15T22:59:49.432474: step 4905, loss 0.00372511, acc 1, f1 1\n",
      "2017-11-15T22:59:49.954938: step 4910, loss 0.00331378, acc 1, f1 1\n",
      "Current epoch:  273\n",
      "2017-11-15T22:59:50.454702: step 4915, loss 0.00498121, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:59:50.971103: step 4920, loss 0.00336492, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:59:51.491613: step 4925, loss 0.00825961, acc 0.996094, f1 0.996092\n",
      "2017-11-15T22:59:52.012265: step 4930, loss 0.00595566, acc 0.998047, f1 0.998047\n",
      "Current epoch:  274\n",
      "2017-11-15T22:59:52.500280: step 4935, loss 0.00277196, acc 1, f1 1\n",
      "2017-11-15T22:59:53.020620: step 4940, loss 0.00213554, acc 1, f1 1\n",
      "2017-11-15T22:59:53.539747: step 4945, loss 0.00305714, acc 1, f1 1\n",
      "2017-11-15T22:59:54.038720: step 4950, loss 0.0116464, acc 0.995283, f1 0.99528\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T22:59:54.719926: step 4950, loss 1.77423, acc 0.592914, f1 0.591111\n",
      "\n",
      "Current epoch:  275\n",
      "2017-11-15T22:59:55.233628: step 4955, loss 0.00721179, acc 0.998047, f1 0.998047\n",
      "2017-11-15T22:59:55.761740: step 4960, loss 0.0131746, acc 0.99707, f1 0.997069\n",
      "2017-11-15T22:59:56.276725: step 4965, loss 0.00879764, acc 0.99707, f1 0.99707\n",
      "Current epoch:  276\n",
      "2017-11-15T22:59:56.770617: step 4970, loss 0.00531603, acc 0.999023, f1 0.999023\n",
      "2017-11-15T22:59:57.291623: step 4975, loss 0.00204407, acc 1, f1 1\n",
      "2017-11-15T22:59:57.816073: step 4980, loss 0.00225368, acc 1, f1 1\n",
      "2017-11-15T22:59:58.333990: step 4985, loss 0.0517915, acc 0.994141, f1 0.994137\n",
      "Current epoch:  277\n",
      "2017-11-15T22:59:58.824864: step 4990, loss 0.0190111, acc 0.996094, f1 0.996093\n",
      "2017-11-15T22:59:59.346716: step 4995, loss 0.00740275, acc 1, f1 1\n",
      "2017-11-15T22:59:59.880203: step 5000, loss 0.00683843, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:00:00.545483: step 5000, loss 1.64516, acc 0.576095, f1 0.576409\n",
      "\n",
      "Current epoch:  278\n",
      "2017-11-15T23:00:01.031168: step 5005, loss 0.00774217, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:00:01.541091: step 5010, loss 0.007766, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:00:02.059138: step 5015, loss 0.0127636, acc 0.996094, f1 0.99609\n",
      "2017-11-15T23:00:02.574706: step 5020, loss 0.00689339, acc 0.998047, f1 0.998047\n",
      "Current epoch:  279\n",
      "2017-11-15T23:00:03.064430: step 5025, loss 0.00712634, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:00:03.582866: step 5030, loss 0.00540608, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:00:04.099682: step 5035, loss 0.00399449, acc 1, f1 1\n",
      "2017-11-15T23:00:04.587809: step 5040, loss 0.00507165, acc 0.998428, f1 0.998428\n",
      "Current epoch:  280\n",
      "2017-11-15T23:00:05.121876: step 5045, loss 0.00371534, acc 1, f1 1\n",
      "2017-11-15T23:00:05.647813: step 5050, loss 0.00531751, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:00:06.304175: step 5050, loss 1.70114, acc 0.580651, f1 0.580577\n",
      "\n",
      "2017-11-15T23:00:06.828342: step 5055, loss 0.00551292, acc 0.998047, f1 0.998046\n",
      "Current epoch:  281\n",
      "2017-11-15T23:00:07.320001: step 5060, loss 0.0055503, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:00:07.847007: step 5065, loss 0.00347369, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:00:08.373245: step 5070, loss 0.00475729, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:00:08.896187: step 5075, loss 0.00725494, acc 0.99707, f1 0.99707\n",
      "Current epoch:  282\n",
      "2017-11-15T23:00:09.392079: step 5080, loss 0.00390736, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:00:09.904003: step 5085, loss 0.00623802, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:00:10.429489: step 5090, loss 0.00598112, acc 0.998047, f1 0.998047\n",
      "Current epoch:  283\n",
      "2017-11-15T23:00:10.922469: step 5095, loss 0.0104561, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:00:11.434428: step 5100, loss 0.0082957, acc 0.995117, f1 0.995117\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:00:12.070775: step 5100, loss 1.7714, acc 0.584189, f1 0.583287\n",
      "\n",
      "2017-11-15T23:00:12.632474: step 5105, loss 0.00322375, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:00:13.151522: step 5110, loss 0.00454831, acc 0.999023, f1 0.999024\n",
      "Current epoch:  284\n",
      "2017-11-15T23:00:13.651484: step 5115, loss 0.0029115, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:00:14.171107: step 5120, loss 0.0135282, acc 0.996094, f1 0.996095\n",
      "2017-11-15T23:00:14.690900: step 5125, loss 0.00819001, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:00:15.178746: step 5130, loss 0.0150862, acc 0.995283, f1 0.99528\n",
      "Current epoch:  285\n",
      "2017-11-15T23:00:15.700026: step 5135, loss 0.00636694, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:00:16.230986: step 5140, loss 0.00332083, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:00:16.760466: step 5145, loss 0.00847764, acc 0.998047, f1 0.998047\n",
      "Current epoch:  286\n",
      "2017-11-15T23:00:17.247864: step 5150, loss 0.00728089, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:00:17.917380: step 5150, loss 1.80748, acc 0.585692, f1 0.586892\n",
      "\n",
      "2017-11-15T23:00:18.437718: step 5155, loss 0.0040746, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:00:18.961818: step 5160, loss 0.0088472, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:00:19.482255: step 5165, loss 0.0194982, acc 0.995117, f1 0.995114\n",
      "Current epoch:  287\n",
      "2017-11-15T23:00:19.967498: step 5170, loss 0.573521, acc 0.817383, f1 0.774358\n",
      "2017-11-15T23:00:20.481915: step 5175, loss 0.012386, acc 1, f1 1\n",
      "2017-11-15T23:00:20.991674: step 5180, loss 0.0090887, acc 0.999023, f1 0.999023\n",
      "Current epoch:  288\n",
      "2017-11-15T23:00:21.475019: step 5185, loss 0.00912852, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:00:21.987452: step 5190, loss 0.00433485, acc 1, f1 1\n",
      "2017-11-15T23:00:22.508787: step 5195, loss 0.0122426, acc 0.996094, f1 0.996092\n",
      "2017-11-15T23:00:23.026986: step 5200, loss 0.00523289, acc 0.999023, f1 0.999024\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:00:23.687216: step 5200, loss 1.68451, acc 0.584626, f1 0.584803\n",
      "\n",
      "Current epoch:  289\n",
      "2017-11-15T23:00:24.186204: step 5205, loss 0.00463409, acc 1, f1 1\n",
      "2017-11-15T23:00:24.705129: step 5210, loss 0.00385479, acc 1, f1 1\n",
      "2017-11-15T23:00:25.222993: step 5215, loss 0.00442854, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:00:25.717556: step 5220, loss 0.00561779, acc 0.996855, f1 0.996853\n",
      "Current epoch:  290\n",
      "2017-11-15T23:00:26.238552: step 5225, loss 0.00501723, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:00:26.761320: step 5230, loss 0.0060206, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:00:27.415030: step 5235, loss 0.00529818, acc 0.998047, f1 0.998047\n",
      "Current epoch:  291\n",
      "2017-11-15T23:00:27.905419: step 5240, loss 0.00415474, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:00:28.429118: step 5245, loss 0.00313424, acc 1, f1 1\n",
      "2017-11-15T23:00:28.951377: step 5250, loss 0.0080229, acc 0.996094, f1 0.996092\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:00:29.603786: step 5250, loss 1.73863, acc 0.585983, f1 0.585891\n",
      "\n",
      "2017-11-15T23:00:30.118931: step 5255, loss 0.00999304, acc 0.995117, f1 0.995126\n",
      "Current epoch:  292\n",
      "2017-11-15T23:00:30.609036: step 5260, loss 0.00803672, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:00:31.127378: step 5265, loss 0.00476471, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:00:31.653234: step 5270, loss 0.0077565, acc 0.99707, f1 0.99707\n",
      "Current epoch:  293\n",
      "2017-11-15T23:00:32.148163: step 5275, loss 0.00557198, acc 0.998047, f1 0.998046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:00:32.688679: step 5280, loss 0.00828634, acc 0.99707, f1 0.997074\n",
      "2017-11-15T23:00:33.212667: step 5285, loss 0.00407454, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:00:33.737326: step 5290, loss 0.0142625, acc 0.995117, f1 0.995118\n",
      "Current epoch:  294\n",
      "2017-11-15T23:00:34.224406: step 5295, loss 0.004346, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:00:34.749374: step 5300, loss 0.00200193, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:00:35.403017: step 5300, loss 1.8114, acc 0.586128, f1 0.585886\n",
      "\n",
      "2017-11-15T23:00:35.922862: step 5305, loss 0.0104514, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:00:36.416800: step 5310, loss 0.0103647, acc 0.996855, f1 0.996855\n",
      "Current epoch:  295\n",
      "2017-11-15T23:00:36.940542: step 5315, loss 0.0108041, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:00:37.455263: step 5320, loss 0.00209044, acc 1, f1 1\n",
      "2017-11-15T23:00:37.985063: step 5325, loss 0.00924864, acc 0.99707, f1 0.99707\n",
      "Current epoch:  296\n",
      "2017-11-15T23:00:38.482204: step 5330, loss 0.00233654, acc 1, f1 1\n",
      "2017-11-15T23:00:38.998116: step 5335, loss 0.0017821, acc 1, f1 1\n",
      "2017-11-15T23:00:39.524267: step 5340, loss 0.00383512, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:00:40.045275: step 5345, loss 0.0171741, acc 0.994141, f1 0.994141\n",
      "Current epoch:  297\n",
      "2017-11-15T23:00:40.535148: step 5350, loss 1.67103, acc 0.634766, f1 0.547558\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:00:41.184609: step 5350, loss 2.29755, acc 0.496025, f1 0.470487\n",
      "\n",
      "2017-11-15T23:00:41.694744: step 5355, loss 0.00984566, acc 1, f1 1\n",
      "2017-11-15T23:00:42.204887: step 5360, loss 0.0110754, acc 0.999023, f1 0.999023\n",
      "Current epoch:  298\n",
      "2017-11-15T23:00:42.698906: step 5365, loss 0.0067865, acc 1, f1 1\n",
      "2017-11-15T23:00:43.221854: step 5370, loss 0.00538485, acc 1, f1 1\n",
      "2017-11-15T23:00:43.765719: step 5375, loss 0.00741883, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:00:44.289662: step 5380, loss 0.00767662, acc 0.998047, f1 0.998046\n",
      "Current epoch:  299\n",
      "2017-11-15T23:00:44.784604: step 5385, loss 0.00914905, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:00:45.304178: step 5390, loss 0.00723169, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:00:45.828612: step 5395, loss 0.00807593, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:00:46.312789: step 5400, loss 0.00969533, acc 0.996855, f1 0.996855\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:00:46.980966: step 5400, loss 1.70306, acc 0.582299, f1 0.582061\n",
      "\n",
      "Current epoch:  300\n",
      "2017-11-15T23:00:47.504033: step 5405, loss 0.00356257, acc 1, f1 1\n",
      "2017-11-15T23:00:48.024526: step 5410, loss 0.007351, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:00:48.549771: step 5415, loss 0.00281361, acc 1, f1 1\n",
      "Current epoch:  301\n",
      "2017-11-15T23:00:49.048438: step 5420, loss 0.00624028, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:00:49.576106: step 5425, loss 0.00621016, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:00:50.100211: step 5430, loss 0.00494233, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:00:50.622301: step 5435, loss 0.00395486, acc 0.999023, f1 0.999024\n",
      "Current epoch:  302\n",
      "2017-11-15T23:00:51.111542: step 5440, loss 0.00481977, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:00:51.646307: step 5445, loss 0.00191057, acc 1, f1 1\n",
      "2017-11-15T23:00:52.170176: step 5450, loss 0.00438432, acc 0.999023, f1 0.999024\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:00:52.839189: step 5450, loss 1.77695, acc 0.583753, f1 0.58272\n",
      "\n",
      "Current epoch:  303\n",
      "2017-11-15T23:00:53.327732: step 5455, loss 0.00241888, acc 1, f1 1\n",
      "2017-11-15T23:00:53.848205: step 5460, loss 0.00725378, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:00:54.371889: step 5465, loss 0.00747418, acc 0.996094, f1 0.996091\n",
      "2017-11-15T23:00:54.903518: step 5470, loss 0.00796404, acc 0.99707, f1 0.997069\n",
      "Current epoch:  304\n",
      "2017-11-15T23:00:55.399905: step 5475, loss 0.00334072, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:00:55.921832: step 5480, loss 0.00475818, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:00:56.445015: step 5485, loss 0.00613274, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:00:56.938976: step 5490, loss 0.00476085, acc 0.998428, f1 0.99843\n",
      "Current epoch:  305\n",
      "2017-11-15T23:00:57.466462: step 5495, loss 0.00211818, acc 1, f1 1\n",
      "2017-11-15T23:00:57.988421: step 5500, loss 0.0173183, acc 0.995117, f1 0.995113\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:00:58.646721: step 5500, loss 1.82068, acc 0.591944, f1 0.590645\n",
      "\n",
      "2017-11-15T23:00:59.161203: step 5505, loss 0.00169039, acc 1, f1 1\n",
      "Current epoch:  306\n",
      "2017-11-15T23:00:59.649919: step 5510, loss 0.00277449, acc 1, f1 1\n",
      "2017-11-15T23:01:00.178044: step 5515, loss 0.00958544, acc 0.99707, f1 0.997072\n",
      "2017-11-15T23:01:00.702048: step 5520, loss 0.00182397, acc 1, f1 1\n",
      "2017-11-15T23:01:01.216974: step 5525, loss 0.00826956, acc 0.998047, f1 0.998046\n",
      "Current epoch:  307\n",
      "2017-11-15T23:01:01.706900: step 5530, loss 0.00456902, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:01:02.217840: step 5535, loss 0.015483, acc 0.998047, f1 0.998043\n",
      "2017-11-15T23:01:02.741296: step 5540, loss 0.00473299, acc 0.999023, f1 0.999023\n",
      "Current epoch:  308\n",
      "2017-11-15T23:01:03.245695: step 5545, loss 0.0033615, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:01:03.777896: step 5550, loss 0.00383254, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:01:04.473859: step 5550, loss 1.85656, acc 0.591169, f1 0.589053\n",
      "\n",
      "2017-11-15T23:01:05.010011: step 5555, loss 2.41067, acc 0.743164, f1 0.689022\n",
      "2017-11-15T23:01:05.567525: step 5560, loss 0.0203591, acc 0.996094, f1 0.996094\n",
      "Current epoch:  309\n",
      "2017-11-15T23:01:06.065194: step 5565, loss 0.0121912, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:01:06.584392: step 5570, loss 0.0115906, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:01:07.105604: step 5575, loss 0.00724215, acc 1, f1 1\n",
      "2017-11-15T23:01:07.594680: step 5580, loss 0.00655104, acc 1, f1 1\n",
      "Current epoch:  310\n",
      "2017-11-15T23:01:08.115942: step 5585, loss 0.00992094, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:01:08.636624: step 5590, loss 0.00960299, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:01:09.155213: step 5595, loss 0.00565417, acc 0.999023, f1 0.999023\n",
      "Current epoch:  311\n",
      "2017-11-15T23:01:09.646069: step 5600, loss 0.00601915, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:01:10.301315: step 5600, loss 1.69315, acc 0.581136, f1 0.581349\n",
      "\n",
      "2017-11-15T23:01:10.820685: step 5605, loss 0.00621642, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:01:11.350777: step 5610, loss 0.010748, acc 0.995117, f1 0.995117\n",
      "2017-11-15T23:01:11.872055: step 5615, loss 0.00415646, acc 0.999023, f1 0.999024\n",
      "Current epoch:  312\n",
      "2017-11-15T23:01:12.363956: step 5620, loss 0.00486852, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:01:12.883395: step 5625, loss 0.00573309, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:01:13.404665: step 5630, loss 0.00544424, acc 0.998047, f1 0.998048\n",
      "Current epoch:  313\n",
      "2017-11-15T23:01:13.897492: step 5635, loss 0.00353305, acc 1, f1 1\n",
      "2017-11-15T23:01:14.424930: step 5640, loss 0.00379746, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:01:14.944362: step 5645, loss 0.00262546, acc 1, f1 1\n",
      "2017-11-15T23:01:15.467413: step 5650, loss 0.00764025, acc 0.996094, f1 0.996093\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:01:16.126809: step 5650, loss 1.75579, acc 0.584286, f1 0.58399\n",
      "\n",
      "Current epoch:  314\n",
      "2017-11-15T23:01:16.629875: step 5655, loss 0.00288292, acc 1, f1 1\n",
      "2017-11-15T23:01:17.156763: step 5660, loss 0.00258839, acc 1, f1 1\n",
      "2017-11-15T23:01:17.671843: step 5665, loss 0.00191169, acc 1, f1 1\n",
      "2017-11-15T23:01:18.159677: step 5670, loss 0.00406409, acc 0.998428, f1 0.998428\n",
      "Current epoch:  315\n",
      "2017-11-15T23:01:18.675088: step 5675, loss 0.00179472, acc 1, f1 1\n",
      "2017-11-15T23:01:19.195034: step 5680, loss 0.00308077, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:01:19.716967: step 5685, loss 0.00310977, acc 0.999023, f1 0.999024\n",
      "Current epoch:  316\n",
      "2017-11-15T23:01:20.203405: step 5690, loss 0.00478242, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:01:20.723351: step 5695, loss 0.00708935, acc 0.99707, f1 0.997075\n",
      "2017-11-15T23:01:21.229926: step 5700, loss 0.0111096, acc 0.996094, f1 0.996085\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:01:21.869063: step 5700, loss 1.8689, acc 0.568825, f1 0.57231\n",
      "\n",
      "2017-11-15T23:01:22.397186: step 5705, loss 0.00161379, acc 1, f1 1\n",
      "Current epoch:  317\n",
      "2017-11-15T23:01:22.888903: step 5710, loss 0.0038719, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:01:23.408836: step 5715, loss 0.0102198, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:01:23.929773: step 5720, loss 0.00814453, acc 0.998047, f1 0.998048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch:  318\n",
      "2017-11-15T23:01:24.426649: step 5725, loss 0.0034001, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:01:24.949647: step 5730, loss 0.00358835, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:01:25.470570: step 5735, loss 0.00905005, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:01:25.995559: step 5740, loss 0.0106924, acc 0.998047, f1 0.998047\n",
      "Current epoch:  319\n",
      "2017-11-15T23:01:26.482431: step 5745, loss 0.00788228, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:01:27.003382: step 5750, loss 0.00906109, acc 0.996094, f1 0.996094\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:01:27.670470: step 5750, loss 1.8736, acc 0.584916, f1 0.584277\n",
      "\n",
      "2017-11-15T23:01:28.191304: step 5755, loss 0.00453011, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:01:28.683291: step 5760, loss 0.00961093, acc 0.998428, f1 0.99843\n",
      "Current epoch:  320\n",
      "2017-11-15T23:01:29.207453: step 5765, loss 0.00852945, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:01:29.727876: step 5770, loss 0.0199705, acc 0.99707, f1 0.997067\n",
      "2017-11-15T23:01:30.249163: step 5775, loss 0.0071989, acc 0.998047, f1 0.998047\n",
      "Current epoch:  321\n",
      "2017-11-15T23:01:30.741054: step 5780, loss 0.0714138, acc 0.985352, f1 0.985343\n",
      "2017-11-15T23:01:31.259063: step 5785, loss 0.875459, acc 0.725586, f1 0.693993\n",
      "2017-11-15T23:01:31.771049: step 5790, loss 0.0162356, acc 0.99707, f1 0.997072\n",
      "2017-11-15T23:01:32.278559: step 5795, loss 0.0155286, acc 0.99707, f1 0.99707\n",
      "Current epoch:  322\n",
      "2017-11-15T23:01:32.767928: step 5800, loss 0.00788991, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:01:33.483818: step 5800, loss 1.62993, acc 0.589472, f1 0.589721\n",
      "\n",
      "2017-11-15T23:01:34.007249: step 5805, loss 0.00959261, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:01:34.539656: step 5810, loss 0.00999726, acc 0.99707, f1 0.99707\n",
      "Current epoch:  323\n",
      "2017-11-15T23:01:35.042034: step 5815, loss 0.0107943, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:01:35.569978: step 5820, loss 0.00668416, acc 1, f1 1\n",
      "2017-11-15T23:01:36.096018: step 5825, loss 0.00442471, acc 1, f1 1\n",
      "2017-11-15T23:01:36.618996: step 5830, loss 0.00680117, acc 0.998047, f1 0.998047\n",
      "Current epoch:  324\n",
      "2017-11-15T23:01:37.109365: step 5835, loss 0.00754144, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:01:37.631123: step 5840, loss 0.00699059, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:01:38.153612: step 5845, loss 0.00518298, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:01:38.655701: step 5850, loss 0.00564293, acc 0.998428, f1 0.998428\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:01:39.343484: step 5850, loss 1.7051, acc 0.582687, f1 0.582966\n",
      "\n",
      "Current epoch:  325\n",
      "2017-11-15T23:01:39.862319: step 5855, loss 0.00334196, acc 1, f1 1\n",
      "2017-11-15T23:01:40.374380: step 5860, loss 0.00680765, acc 0.998047, f1 0.998044\n",
      "2017-11-15T23:01:40.889314: step 5865, loss 0.0102003, acc 0.995117, f1 0.995119\n",
      "Current epoch:  326\n",
      "2017-11-15T23:01:41.373263: step 5870, loss 0.00547174, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:01:41.883247: step 5875, loss 0.00466748, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:01:42.397242: step 5880, loss 0.00389928, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:01:42.923201: step 5885, loss 0.00789285, acc 0.996094, f1 0.996094\n",
      "Current epoch:  327\n",
      "2017-11-15T23:01:43.415961: step 5890, loss 0.005066, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:01:43.935381: step 5895, loss 0.00856279, acc 0.996094, f1 0.996092\n",
      "2017-11-15T23:01:44.460318: step 5900, loss 0.00241191, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:01:45.115578: step 5900, loss 1.75911, acc 0.579633, f1 0.580346\n",
      "\n",
      "Current epoch:  328\n",
      "2017-11-15T23:01:45.610465: step 5905, loss 0.00467495, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:01:46.132440: step 5910, loss 0.00486246, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:01:46.650980: step 5915, loss 0.00199054, acc 1, f1 1\n",
      "2017-11-15T23:01:47.170419: step 5920, loss 0.0118963, acc 0.996094, f1 0.996095\n",
      "Current epoch:  329\n",
      "2017-11-15T23:01:47.662949: step 5925, loss 0.00260976, acc 1, f1 1\n",
      "2017-11-15T23:01:48.185581: step 5930, loss 0.00233502, acc 1, f1 1\n",
      "2017-11-15T23:01:48.709036: step 5935, loss 0.00723944, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:01:49.197501: step 5940, loss 0.0087253, acc 0.996855, f1 0.996854\n",
      "Current epoch:  330\n",
      "2017-11-15T23:01:49.733188: step 5945, loss 0.0116762, acc 0.996094, f1 0.996091\n",
      "2017-11-15T23:01:50.253632: step 5950, loss 0.00225429, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:01:50.904622: step 5950, loss 1.81618, acc 0.588552, f1 0.587813\n",
      "\n",
      "2017-11-15T23:01:51.412413: step 5955, loss 0.00170577, acc 1, f1 1\n",
      "Current epoch:  331\n",
      "2017-11-15T23:01:51.891841: step 5960, loss 0.0108921, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:01:52.411167: step 5965, loss 0.00758994, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:01:52.934719: step 5970, loss 0.0116727, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:01:53.457774: step 5975, loss 0.00152892, acc 1, f1 1\n",
      "Current epoch:  332\n",
      "2017-11-15T23:01:53.948783: step 5980, loss 0.00177591, acc 1, f1 1\n",
      "2017-11-15T23:01:54.471313: step 5985, loss 0.00470709, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:01:55.000444: step 5990, loss 0.0123265, acc 0.998047, f1 0.998047\n",
      "Current epoch:  333\n",
      "2017-11-15T23:01:55.501072: step 5995, loss 0.00281046, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:01:56.023593: step 6000, loss 0.00634584, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:01:56.686557: step 6000, loss 1.99961, acc 0.560198, f1 0.557667\n",
      "\n",
      "2017-11-15T23:01:57.204595: step 6005, loss 0.00847797, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:01:57.730601: step 6010, loss 0.303998, acc 0.848633, f1 0.844938\n",
      "Current epoch:  334\n",
      "2017-11-15T23:01:58.224086: step 6015, loss 0.0125217, acc 0.995117, f1 0.995119\n",
      "2017-11-15T23:01:58.757090: step 6020, loss 0.00396931, acc 1, f1 1\n",
      "2017-11-15T23:01:59.283136: step 6025, loss 0.00323991, acc 1, f1 1\n",
      "2017-11-15T23:01:59.776010: step 6030, loss 0.0114493, acc 0.996855, f1 0.996855\n",
      "Current epoch:  335\n",
      "2017-11-15T23:02:00.307295: step 6035, loss 0.00656328, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:02:00.846547: step 6040, loss 0.00661581, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:02:01.374524: step 6045, loss 0.00577552, acc 0.998047, f1 0.998047\n",
      "Current epoch:  336\n",
      "2017-11-15T23:02:01.868797: step 6050, loss 0.00585955, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:02:02.529130: step 6050, loss 1.77825, acc 0.579633, f1 0.579876\n",
      "\n",
      "2017-11-15T23:02:03.056114: step 6055, loss 0.00753884, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:02:03.583738: step 6060, loss 0.00275241, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:02:04.112714: step 6065, loss 0.00310962, acc 1, f1 1\n",
      "Current epoch:  337\n",
      "2017-11-15T23:02:04.616306: step 6070, loss 0.00342692, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:02:05.140242: step 6075, loss 0.00507682, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:02:05.671259: step 6080, loss 0.00638015, acc 0.996094, f1 0.996094\n",
      "Current epoch:  338\n",
      "2017-11-15T23:02:06.180097: step 6085, loss 0.0026513, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:02:06.717106: step 6090, loss 0.00505414, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:02:07.238756: step 6095, loss 0.00657262, acc 0.996094, f1 0.996097\n",
      "2017-11-15T23:02:07.761884: step 6100, loss 0.00527879, acc 0.99707, f1 0.997069\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:02:08.422745: step 6100, loss 1.82363, acc 0.577404, f1 0.577593\n",
      "\n",
      "Current epoch:  339\n",
      "2017-11-15T23:02:08.912552: step 6105, loss 0.00207405, acc 1, f1 1\n",
      "2017-11-15T23:02:09.433986: step 6110, loss 0.00292804, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:02:09.953509: step 6115, loss 0.00208839, acc 1, f1 1\n",
      "2017-11-15T23:02:10.442325: step 6120, loss 0.00418958, acc 0.998428, f1 0.998425\n",
      "Current epoch:  340\n",
      "2017-11-15T23:02:10.967399: step 6125, loss 0.00188311, acc 1, f1 1\n",
      "2017-11-15T23:02:11.489195: step 6130, loss 0.00622404, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:02:12.018012: step 6135, loss 0.00156767, acc 1, f1 1\n",
      "Current epoch:  341\n",
      "2017-11-15T23:02:12.557685: step 6140, loss 0.00380486, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:02:13.081725: step 6145, loss 0.00617413, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:02:13.602266: step 6150, loss 0.00253909, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:02:14.260097: step 6150, loss 1.85453, acc 0.579827, f1 0.581164\n",
      "\n",
      "2017-11-15T23:02:14.778010: step 6155, loss 0.00216481, acc 1, f1 1\n",
      "Current epoch:  342\n",
      "2017-11-15T23:02:15.269991: step 6160, loss 0.0016624, acc 1, f1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:02:15.790546: step 6165, loss 0.00527897, acc 0.998047, f1 0.998051\n",
      "2017-11-15T23:02:16.315026: step 6170, loss 0.00333106, acc 0.999023, f1 0.999024\n",
      "Current epoch:  343\n",
      "2017-11-15T23:02:16.805396: step 6175, loss 0.00143048, acc 1, f1 1\n",
      "2017-11-15T23:02:17.335427: step 6180, loss 0.00120368, acc 1, f1 1\n",
      "2017-11-15T23:02:17.862933: step 6185, loss 0.0186349, acc 0.995117, f1 0.995114\n",
      "2017-11-15T23:02:18.384447: step 6190, loss 0.0186303, acc 0.996094, f1 0.996093\n",
      "Current epoch:  344\n",
      "2017-11-15T23:02:18.874320: step 6195, loss 0.010087, acc 0.996094, f1 0.996089\n",
      "2017-11-15T23:02:19.395266: step 6200, loss 0.00663878, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:02:20.057434: step 6200, loss 1.93145, acc 0.577695, f1 0.576897\n",
      "\n",
      "2017-11-15T23:02:20.577869: step 6205, loss 0.00897828, acc 0.996094, f1 0.996092\n",
      "2017-11-15T23:02:21.065553: step 6210, loss 0.00602158, acc 0.998428, f1 0.998428\n",
      "Current epoch:  345\n",
      "2017-11-15T23:02:21.586378: step 6215, loss 0.00365844, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:02:22.101456: step 6220, loss 0.00525119, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:02:22.630386: step 6225, loss 3.41196, acc 0.575195, f1 0.456494\n",
      "Current epoch:  346\n",
      "2017-11-15T23:02:23.131303: step 6230, loss 0.00884504, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:02:23.655554: step 6235, loss 0.00786331, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:02:24.176608: step 6240, loss 0.0109082, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:02:24.696318: step 6245, loss 0.0157796, acc 0.994141, f1 0.994142\n",
      "Current epoch:  347\n",
      "2017-11-15T23:02:25.181872: step 6250, loss 0.00544779, acc 0.999023, f1 0.999022\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:02:25.849633: step 6250, loss 1.72694, acc 0.583414, f1 0.584278\n",
      "\n",
      "2017-11-15T23:02:26.401699: step 6255, loss 0.00505889, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:02:26.933666: step 6260, loss 0.00856713, acc 0.996094, f1 0.996094\n",
      "Current epoch:  348\n",
      "2017-11-15T23:02:27.434813: step 6265, loss 0.00451511, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:02:27.966239: step 6270, loss 0.00310806, acc 1, f1 1\n",
      "2017-11-15T23:02:28.497615: step 6275, loss 0.00884111, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:02:29.015552: step 6280, loss 0.00527947, acc 0.998047, f1 0.998047\n",
      "Current epoch:  349\n",
      "2017-11-15T23:02:29.508454: step 6285, loss 0.00400029, acc 1, f1 1\n",
      "2017-11-15T23:02:30.029432: step 6290, loss 0.00408361, acc 1, f1 1\n",
      "2017-11-15T23:02:30.556200: step 6295, loss 0.00390436, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:02:31.047072: step 6300, loss 0.00440959, acc 0.998428, f1 0.998428\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:02:31.681224: step 6300, loss 1.78748, acc 0.581136, f1 0.581624\n",
      "\n",
      "Current epoch:  350\n",
      "2017-11-15T23:02:32.192369: step 6305, loss 0.00363539, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:02:32.722806: step 6310, loss 0.00436817, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:02:33.246828: step 6315, loss 0.0030874, acc 0.999023, f1 0.999022\n",
      "Current epoch:  351\n",
      "2017-11-15T23:02:33.752813: step 6320, loss 0.00214211, acc 1, f1 1\n",
      "2017-11-15T23:02:34.290038: step 6325, loss 0.00312414, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:02:34.815791: step 6330, loss 0.00317846, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:02:35.339721: step 6335, loss 0.00256294, acc 0.999023, f1 0.999025\n",
      "Current epoch:  352\n",
      "2017-11-15T23:02:35.837609: step 6340, loss 0.0100739, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:02:36.359704: step 6345, loss 0.00674776, acc 0.998047, f1 0.998043\n",
      "2017-11-15T23:02:36.879392: step 6350, loss 0.00306907, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:02:37.534404: step 6350, loss 1.85862, acc 0.576628, f1 0.577543\n",
      "\n",
      "Current epoch:  353\n",
      "2017-11-15T23:02:38.024258: step 6355, loss 0.00396687, acc 0.998047, f1 0.998042\n",
      "2017-11-15T23:02:38.551758: step 6360, loss 0.00465196, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:02:39.085263: step 6365, loss 0.0068306, acc 0.99707, f1 0.997072\n",
      "2017-11-15T23:02:39.635265: step 6370, loss 0.00480428, acc 0.998047, f1 0.998047\n",
      "Current epoch:  354\n",
      "2017-11-15T23:02:40.128754: step 6375, loss 0.00272036, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:02:40.657503: step 6380, loss 0.00692898, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:02:41.175859: step 6385, loss 0.00657054, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:02:41.654340: step 6390, loss 0.00519094, acc 0.998428, f1 0.99843\n",
      "Current epoch:  355\n",
      "2017-11-15T23:02:42.173369: step 6395, loss 0.0131267, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:02:42.698329: step 6400, loss 0.00191433, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:02:43.362472: step 6400, loss 1.94096, acc 0.575271, f1 0.573858\n",
      "\n",
      "2017-11-15T23:02:43.883485: step 6405, loss 0.00380579, acc 0.999023, f1 0.999024\n",
      "Current epoch:  356\n",
      "2017-11-15T23:02:44.376194: step 6410, loss 0.00523492, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:02:44.926314: step 6415, loss 0.00373601, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:02:45.457324: step 6420, loss 0.0167815, acc 0.996094, f1 0.996093\n",
      "2017-11-15T23:02:45.985823: step 6425, loss 0.00565772, acc 0.999023, f1 0.999025\n",
      "Current epoch:  357\n",
      "2017-11-15T23:02:46.481428: step 6430, loss 0.30708, acc 0.880859, f1 0.855002\n",
      "2017-11-15T23:02:47.004869: step 6435, loss 1.40778, acc 0.735352, f1 0.677857\n",
      "2017-11-15T23:02:47.534920: step 6440, loss 0.0189053, acc 1, f1 1\n",
      "Current epoch:  358\n",
      "2017-11-15T23:02:48.037530: step 6445, loss 0.0147252, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:02:48.557981: step 6450, loss 0.0156753, acc 0.99707, f1 0.99707\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:02:49.237839: step 6450, loss 1.56857, acc 0.584868, f1 0.585048\n",
      "\n",
      "2017-11-15T23:02:49.768289: step 6455, loss 0.0102278, acc 1, f1 1\n",
      "2017-11-15T23:02:50.300267: step 6460, loss 0.00993161, acc 0.999023, f1 0.999023\n",
      "Current epoch:  359\n",
      "2017-11-15T23:02:50.801456: step 6465, loss 0.00838106, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:02:51.316903: step 6470, loss 0.00756587, acc 1, f1 1\n",
      "2017-11-15T23:02:51.842660: step 6475, loss 0.00959164, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:02:52.330847: step 6480, loss 0.0102777, acc 0.996855, f1 0.996855\n",
      "Current epoch:  360\n",
      "2017-11-15T23:02:52.851834: step 6485, loss 0.00885675, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:02:53.377462: step 6490, loss 0.00970955, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:02:53.889938: step 6495, loss 0.00471911, acc 1, f1 1\n",
      "Current epoch:  361\n",
      "2017-11-15T23:02:54.389847: step 6500, loss 0.00702909, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:02:55.048291: step 6500, loss 1.66437, acc 0.584626, f1 0.584867\n",
      "\n",
      "2017-11-15T23:02:55.569409: step 6505, loss 0.00634205, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:02:56.096102: step 6510, loss 0.00752056, acc 0.996094, f1 0.99609\n",
      "2017-11-15T23:02:56.618017: step 6515, loss 0.00674811, acc 0.998047, f1 0.998047\n",
      "Current epoch:  362\n",
      "2017-11-15T23:02:57.109672: step 6520, loss 0.00413962, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:02:57.628550: step 6525, loss 0.00696193, acc 0.996094, f1 0.996093\n",
      "2017-11-15T23:02:58.151436: step 6530, loss 0.00761801, acc 0.99707, f1 0.997072\n",
      "Current epoch:  363\n",
      "2017-11-15T23:02:58.639433: step 6535, loss 0.00328216, acc 1, f1 1\n",
      "2017-11-15T23:02:59.160585: step 6540, loss 0.0047841, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:02:59.684115: step 6545, loss 0.00529708, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:03:00.207519: step 6550, loss 0.00752874, acc 0.99707, f1 0.99707\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:03:00.872619: step 6550, loss 1.74399, acc 0.578373, f1 0.579177\n",
      "\n",
      "Current epoch:  364\n",
      "2017-11-15T23:03:01.372331: step 6555, loss 0.00624129, acc 0.99707, f1 0.997074\n",
      "2017-11-15T23:03:01.896784: step 6560, loss 0.00237665, acc 1, f1 1\n",
      "2017-11-15T23:03:02.408730: step 6565, loss 0.0102756, acc 0.995117, f1 0.995116\n",
      "2017-11-15T23:03:02.902117: step 6570, loss 0.00567734, acc 0.998428, f1 0.99843\n",
      "Current epoch:  365\n",
      "2017-11-15T23:03:03.534360: step 6575, loss 0.00744187, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:03:04.056298: step 6580, loss 0.00611815, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:03:04.580231: step 6585, loss 0.00395228, acc 0.999023, f1 0.999023\n",
      "Current epoch:  366\n",
      "2017-11-15T23:03:05.073393: step 6590, loss 0.00659715, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:03:05.594301: step 6595, loss 0.00672273, acc 0.998047, f1 0.998053\n",
      "2017-11-15T23:03:06.115243: step 6600, loss 0.00463534, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:03:06.791091: step 6600, loss 1.85442, acc 0.571733, f1 0.571998\n",
      "\n",
      "2017-11-15T23:03:07.315418: step 6605, loss 0.0139147, acc 0.995117, f1 0.995117\n",
      "Current epoch:  367\n",
      "2017-11-15T23:03:07.822037: step 6610, loss 0.00549198, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:03:08.342322: step 6615, loss 0.00212406, acc 1, f1 1\n",
      "2017-11-15T23:03:08.868380: step 6620, loss 0.00751082, acc 0.998047, f1 0.998048\n",
      "Current epoch:  368\n",
      "2017-11-15T23:03:09.362274: step 6625, loss 0.00889339, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:03:09.884819: step 6630, loss 0.00692537, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:03:10.406059: step 6635, loss 0.00572081, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:03:10.925996: step 6640, loss 0.00395119, acc 0.999023, f1 0.999023\n",
      "Current epoch:  369\n",
      "2017-11-15T23:03:11.417838: step 6645, loss 0.00669458, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:03:11.947776: step 6650, loss 0.0181236, acc 0.994141, f1 0.99415\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:03:12.612750: step 6650, loss 1.92766, acc 0.59112, f1 0.584196\n",
      "\n",
      "2017-11-15T23:03:13.140277: step 6655, loss 0.00442539, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:03:13.630822: step 6660, loss 0.00542264, acc 0.998428, f1 0.998428\n",
      "Current epoch:  370\n",
      "2017-11-15T23:03:14.153251: step 6665, loss 0.00148201, acc 1, f1 1\n",
      "2017-11-15T23:03:14.674332: step 6670, loss 0.00411347, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:03:15.197824: step 6675, loss 0.0291977, acc 0.992188, f1 0.992195\n",
      "Current epoch:  371\n",
      "2017-11-15T23:03:15.698721: step 6680, loss 0.00390694, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:03:16.223001: step 6685, loss 0.101334, acc 0.96582, f1 0.965697\n",
      "2017-11-15T23:03:16.754459: step 6690, loss 0.0188367, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:03:17.275840: step 6695, loss 0.0105304, acc 1, f1 1\n",
      "Current epoch:  372\n",
      "2017-11-15T23:03:17.772598: step 6700, loss 0.0115284, acc 0.998047, f1 0.998052\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:03:18.449447: step 6700, loss 1.63798, acc 0.587146, f1 0.587609\n",
      "\n",
      "2017-11-15T23:03:18.971808: step 6705, loss 0.00854827, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:03:19.491738: step 6710, loss 0.00625513, acc 1, f1 1\n",
      "Current epoch:  373\n",
      "2017-11-15T23:03:19.981328: step 6715, loss 0.00658055, acc 1, f1 1\n",
      "2017-11-15T23:03:20.504250: step 6720, loss 0.00901862, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:03:21.016163: step 6725, loss 0.00517922, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:03:21.530223: step 6730, loss 0.00393481, acc 1, f1 1\n",
      "Current epoch:  374\n",
      "2017-11-15T23:03:22.010067: step 6735, loss 0.00640685, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:03:22.529580: step 6740, loss 0.0055982, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:03:23.048104: step 6745, loss 0.00664528, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:03:23.540117: step 6750, loss 0.00863109, acc 0.996855, f1 0.996866\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:03:24.235376: step 6750, loss 1.72013, acc 0.584626, f1 0.5848\n",
      "\n",
      "Current epoch:  375\n",
      "2017-11-15T23:03:24.760252: step 6755, loss 0.00390348, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:03:25.278831: step 6760, loss 0.00363862, acc 1, f1 1\n",
      "2017-11-15T23:03:25.798359: step 6765, loss 0.00257474, acc 1, f1 1\n",
      "Current epoch:  376\n",
      "2017-11-15T23:03:26.283502: step 6770, loss 0.00393062, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:03:26.811166: step 6775, loss 0.00494098, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:03:27.331187: step 6780, loss 0.00601172, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:03:27.857746: step 6785, loss 0.0035434, acc 0.999023, f1 0.999023\n",
      "Current epoch:  377\n",
      "2017-11-15T23:03:28.346234: step 6790, loss 0.00631892, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:03:28.865773: step 6795, loss 0.00608742, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:03:29.401100: step 6800, loss 0.00258871, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:03:30.072191: step 6800, loss 1.79106, acc 0.582154, f1 0.58251\n",
      "\n",
      "Current epoch:  378\n",
      "2017-11-15T23:03:30.562898: step 6805, loss 0.00199171, acc 1, f1 1\n",
      "2017-11-15T23:03:31.097857: step 6810, loss 0.00454676, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:03:31.616785: step 6815, loss 0.00715807, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:03:32.135700: step 6820, loss 0.00696982, acc 0.99707, f1 0.99707\n",
      "Current epoch:  379\n",
      "2017-11-15T23:03:32.637596: step 6825, loss 0.00686435, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:03:33.159517: step 6830, loss 0.00674843, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:03:33.685464: step 6835, loss 0.00809522, acc 0.995117, f1 0.995114\n",
      "2017-11-15T23:03:34.180188: step 6840, loss 0.00624409, acc 0.998428, f1 0.99843\n",
      "Current epoch:  380\n",
      "2017-11-15T23:03:34.719647: step 6845, loss 0.00812163, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:03:35.263620: step 6850, loss 0.00548978, acc 0.999023, f1 0.999024\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:03:35.956661: step 6850, loss 1.86657, acc 0.589424, f1 0.587716\n",
      "\n",
      "2017-11-15T23:03:36.473129: step 6855, loss 0.00654964, acc 0.998047, f1 0.998047\n",
      "Current epoch:  381\n",
      "2017-11-15T23:03:36.970131: step 6860, loss 0.00435523, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:03:37.492885: step 6865, loss 0.00529349, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:03:38.013358: step 6870, loss 0.00140019, acc 1, f1 1\n",
      "2017-11-15T23:03:38.544412: step 6875, loss 2.21009, acc 0.615234, f1 0.523483\n",
      "Current epoch:  382\n",
      "2017-11-15T23:03:39.037945: step 6880, loss 0.209454, acc 0.90332, f1 0.902201\n",
      "2017-11-15T23:03:39.559554: step 6885, loss 0.0106504, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:03:40.085100: step 6890, loss 0.00983915, acc 0.998047, f1 0.998047\n",
      "Current epoch:  383\n",
      "2017-11-15T23:03:40.597022: step 6895, loss 0.00890883, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:03:41.112911: step 6900, loss 0.0126909, acc 0.99707, f1 0.99707\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:03:41.754445: step 6900, loss 1.66877, acc 0.579585, f1 0.580185\n",
      "\n",
      "2017-11-15T23:03:42.263868: step 6905, loss 0.0102485, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:03:42.785468: step 6910, loss 0.00382617, acc 1, f1 1\n",
      "Current epoch:  384\n",
      "2017-11-15T23:03:43.279958: step 6915, loss 0.00522311, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:03:43.802155: step 6920, loss 0.00346258, acc 1, f1 1\n",
      "2017-11-15T23:03:44.332223: step 6925, loss 0.00413082, acc 1, f1 1\n",
      "2017-11-15T23:03:44.824615: step 6930, loss 0.00813791, acc 0.996855, f1 0.996853\n",
      "Current epoch:  385\n",
      "2017-11-15T23:03:45.340502: step 6935, loss 0.00607954, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:03:45.872132: step 6940, loss 0.00596583, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:03:46.399358: step 6945, loss 0.00699511, acc 0.99707, f1 0.99707\n",
      "Current epoch:  386\n",
      "2017-11-15T23:03:46.893717: step 6950, loss 0.0032105, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:03:47.558590: step 6950, loss 1.7374, acc 0.583172, f1 0.582931\n",
      "\n",
      "2017-11-15T23:03:48.079012: step 6955, loss 0.0033807, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:03:48.601363: step 6960, loss 0.00441899, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:03:49.122714: step 6965, loss 0.00346007, acc 0.999023, f1 0.999023\n",
      "Current epoch:  387\n",
      "2017-11-15T23:03:49.618219: step 6970, loss 0.00435458, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:03:50.139761: step 6975, loss 0.00318661, acc 1, f1 1\n",
      "2017-11-15T23:03:50.660691: step 6980, loss 0.00520019, acc 0.998047, f1 0.998047\n",
      "Current epoch:  388\n",
      "2017-11-15T23:03:51.154254: step 6985, loss 0.00272786, acc 1, f1 1\n",
      "2017-11-15T23:03:51.675189: step 6990, loss 0.00240705, acc 1, f1 1\n",
      "2017-11-15T23:03:52.183601: step 6995, loss 0.00796129, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:03:52.704893: step 7000, loss 0.00338702, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:03:53.369727: step 7000, loss 1.80656, acc 0.579924, f1 0.580333\n",
      "\n",
      "Current epoch:  389\n",
      "2017-11-15T23:03:53.861571: step 7005, loss 0.0075227, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:03:54.378944: step 7010, loss 0.00703351, acc 0.99707, f1 0.997068\n",
      "2017-11-15T23:03:54.901483: step 7015, loss 0.00709565, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:03:55.398382: step 7020, loss 0.00503645, acc 0.998428, f1 0.998425\n",
      "Current epoch:  390\n",
      "2017-11-15T23:03:55.920345: step 7025, loss 0.00555342, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:03:56.442785: step 7030, loss 0.00337493, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:03:56.975733: step 7035, loss 0.00193768, acc 1, f1 1\n",
      "Current epoch:  391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:03:57.469497: step 7040, loss 0.0049939, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:03:57.986060: step 7045, loss 0.00552613, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:03:58.511005: step 7050, loss 0.00812951, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:03:59.165808: step 7050, loss 1.94859, acc 0.566353, f1 0.566857\n",
      "\n",
      "2017-11-15T23:03:59.683744: step 7055, loss 0.0118877, acc 0.996094, f1 0.996091\n",
      "Current epoch:  392\n",
      "2017-11-15T23:04:00.175636: step 7060, loss 0.00143492, acc 1, f1 1\n",
      "2017-11-15T23:04:00.695971: step 7065, loss 0.0120313, acc 0.99707, f1 0.997067\n",
      "2017-11-15T23:04:01.217809: step 7070, loss 0.0116829, acc 0.996094, f1 0.996094\n",
      "Current epoch:  393\n",
      "2017-11-15T23:04:01.715008: step 7075, loss 0.00169143, acc 1, f1 1\n",
      "2017-11-15T23:04:02.238309: step 7080, loss 0.0091561, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:04:02.765289: step 7085, loss 0.0270542, acc 0.994141, f1 0.994141\n",
      "2017-11-15T23:04:03.291242: step 7090, loss 0.308044, acc 0.852539, f1 0.847677\n",
      "Current epoch:  394\n",
      "2017-11-15T23:04:03.787121: step 7095, loss 0.0344907, acc 0.99707, f1 0.997081\n",
      "2017-11-15T23:04:04.306743: step 7100, loss 0.0166008, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:04:04.969435: step 7100, loss 1.63592, acc 0.584965, f1 0.585857\n",
      "\n",
      "2017-11-15T23:04:05.486368: step 7105, loss 0.0100094, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:04:05.976767: step 7110, loss 0.0126065, acc 0.996855, f1 0.996855\n",
      "Current epoch:  395\n",
      "2017-11-15T23:04:06.499611: step 7115, loss 0.0100698, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:04:07.020550: step 7120, loss 0.00570369, acc 1, f1 1\n",
      "2017-11-15T23:04:07.548624: step 7125, loss 0.00602058, acc 0.999023, f1 0.999024\n",
      "Current epoch:  396\n",
      "2017-11-15T23:04:08.060206: step 7130, loss 0.00583397, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:04:08.579818: step 7135, loss 0.00382173, acc 1, f1 1\n",
      "2017-11-15T23:04:09.095360: step 7140, loss 0.010076, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:04:09.617380: step 7145, loss 0.00589612, acc 0.99707, f1 0.99707\n",
      "Current epoch:  397\n",
      "2017-11-15T23:04:10.108976: step 7150, loss 0.00270517, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:04:10.785837: step 7150, loss 1.75377, acc 0.57973, f1 0.579605\n",
      "\n",
      "2017-11-15T23:04:11.305365: step 7155, loss 0.0040049, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:04:11.831831: step 7160, loss 0.00520433, acc 0.998047, f1 0.998047\n",
      "Current epoch:  398\n",
      "2017-11-15T23:04:12.323208: step 7165, loss 0.00313419, acc 1, f1 1\n",
      "2017-11-15T23:04:12.904855: step 7170, loss 0.00361042, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:04:13.441814: step 7175, loss 0.00452409, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:04:13.966755: step 7180, loss 0.00774714, acc 0.996094, f1 0.996094\n",
      "Current epoch:  399\n",
      "2017-11-15T23:04:14.456299: step 7185, loss 0.00546355, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:04:14.977671: step 7190, loss 0.00280964, acc 1, f1 1\n",
      "2017-11-15T23:04:15.500555: step 7195, loss 0.00351425, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:04:15.990462: step 7200, loss 0.00786191, acc 0.996855, f1 0.996855\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:04:16.646077: step 7200, loss 1.8344, acc 0.576483, f1 0.576869\n",
      "\n",
      "Current epoch:  400\n",
      "2017-11-15T23:04:17.165110: step 7205, loss 0.00405318, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:04:17.686031: step 7210, loss 0.00304381, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:04:18.208501: step 7215, loss 0.00919821, acc 0.995117, f1 0.995118\n",
      "Current epoch:  401\n",
      "2017-11-15T23:04:18.707735: step 7220, loss 0.00826291, acc 0.996094, f1 0.996091\n",
      "2017-11-15T23:04:19.238137: step 7225, loss 0.00539289, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:04:19.764082: step 7230, loss 0.00633409, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:04:20.288019: step 7235, loss 0.0124083, acc 0.995117, f1 0.995116\n",
      "Current epoch:  402\n",
      "2017-11-15T23:04:20.783967: step 7240, loss 0.0129862, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:04:21.303976: step 7245, loss 0.0065389, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:04:21.827151: step 7250, loss 0.00916728, acc 0.99707, f1 0.99707\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:04:22.466417: step 7250, loss 1.99076, acc 0.565335, f1 0.563862\n",
      "\n",
      "Current epoch:  403\n",
      "2017-11-15T23:04:22.952341: step 7255, loss 0.00644165, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:04:23.472305: step 7260, loss 0.00180883, acc 1, f1 1\n",
      "2017-11-15T23:04:23.992942: step 7265, loss 0.022397, acc 0.995117, f1 0.995117\n",
      "2017-11-15T23:04:24.530013: step 7270, loss 0.003394, acc 0.999023, f1 0.999024\n",
      "Current epoch:  404\n",
      "2017-11-15T23:04:25.019057: step 7275, loss 0.00810635, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:04:25.544730: step 7280, loss 0.00329372, acc 0.998047, f1 0.998041\n",
      "2017-11-15T23:04:26.066413: step 7285, loss 0.0102271, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:04:26.559769: step 7290, loss 0.0112082, acc 0.995283, f1 0.995283\n",
      "Current epoch:  405\n",
      "2017-11-15T23:04:27.078687: step 7295, loss 0.00667749, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:04:27.604856: step 7300, loss 0.0049264, acc 0.999023, f1 0.999024\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:04:28.262586: step 7300, loss 1.957, acc 0.588212, f1 0.584234\n",
      "\n",
      "2017-11-15T23:04:28.810016: step 7305, loss 0.0121116, acc 0.996094, f1 0.996093\n",
      "Current epoch:  406\n",
      "2017-11-15T23:04:29.299912: step 7310, loss 0.00322865, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:04:29.833307: step 7315, loss 0.00414278, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:04:30.363839: step 7320, loss 0.00926498, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:04:30.887914: step 7325, loss 0.0218588, acc 0.995117, f1 0.99512\n",
      "Current epoch:  407\n",
      "2017-11-15T23:04:31.375296: step 7330, loss 0.00616347, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:04:31.897544: step 7335, loss 0.00981752, acc 0.996094, f1 0.996092\n",
      "2017-11-15T23:04:32.416126: step 7340, loss 0.00764959, acc 0.998047, f1 0.998047\n",
      "Current epoch:  408\n",
      "2017-11-15T23:04:32.906543: step 7345, loss 0.00879871, acc 0.99707, f1 0.997067\n",
      "2017-11-15T23:04:33.433476: step 7350, loss 0.00611215, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:04:34.090218: step 7350, loss 2.12479, acc 0.552782, f1 0.550909\n",
      "\n",
      "2017-11-15T23:04:34.608655: step 7355, loss 0.00100968, acc 1, f1 1\n",
      "2017-11-15T23:04:35.131083: step 7360, loss 4.30238, acc 0.475586, f1 0.351425\n",
      "Current epoch:  409\n",
      "2017-11-15T23:04:35.639474: step 7365, loss 0.0182509, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:04:36.155509: step 7370, loss 0.00875021, acc 1, f1 1\n",
      "2017-11-15T23:04:36.681046: step 7375, loss 0.00721501, acc 1, f1 1\n",
      "2017-11-15T23:04:37.171690: step 7380, loss 0.0131294, acc 0.995283, f1 0.995282\n",
      "Current epoch:  410\n",
      "2017-11-15T23:04:37.689630: step 7385, loss 0.00578063, acc 1, f1 1\n",
      "2017-11-15T23:04:38.204073: step 7390, loss 0.00529382, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:04:38.729588: step 7395, loss 0.00844263, acc 0.99707, f1 0.997071\n",
      "Current epoch:  411\n",
      "2017-11-15T23:04:39.219475: step 7400, loss 0.00445806, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:04:39.885188: step 7400, loss 1.70572, acc 0.583947, f1 0.584394\n",
      "\n",
      "2017-11-15T23:04:40.402951: step 7405, loss 0.00476765, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:04:40.943872: step 7410, loss 0.00734542, acc 0.99707, f1 0.997074\n",
      "2017-11-15T23:04:41.469988: step 7415, loss 0.0056789, acc 0.998047, f1 0.998047\n",
      "Current epoch:  412\n",
      "2017-11-15T23:04:41.959147: step 7420, loss 0.00337651, acc 1, f1 1\n",
      "2017-11-15T23:04:42.480068: step 7425, loss 0.00302115, acc 1, f1 1\n",
      "2017-11-15T23:04:42.988571: step 7430, loss 0.00482182, acc 0.998047, f1 0.998046\n",
      "Current epoch:  413\n",
      "2017-11-15T23:04:43.482208: step 7435, loss 0.00407649, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:04:43.998261: step 7440, loss 0.00376393, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:04:44.516704: step 7445, loss 0.00367582, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:04:45.038906: step 7450, loss 0.00311387, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:04:45.698343: step 7450, loss 1.77997, acc 0.583123, f1 0.583778\n",
      "\n",
      "Current epoch:  414\n",
      "2017-11-15T23:04:46.194215: step 7455, loss 0.00196912, acc 1, f1 1\n",
      "2017-11-15T23:04:46.719593: step 7460, loss 0.00332766, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:04:47.238163: step 7465, loss 0.00474089, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:04:47.728273: step 7470, loss 0.00196642, acc 1, f1 1\n",
      "Current epoch:  415\n",
      "2017-11-15T23:04:48.247717: step 7475, loss 0.00234591, acc 0.999023, f1 0.999023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:04:48.769156: step 7480, loss 0.00268677, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:04:49.288904: step 7485, loss 0.00575435, acc 0.99707, f1 0.997069\n",
      "Current epoch:  416\n",
      "2017-11-15T23:04:49.783976: step 7490, loss 0.00708751, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:04:50.303395: step 7495, loss 0.00382465, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:04:50.827360: step 7500, loss 0.00816606, acc 0.996094, f1 0.99609\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:04:51.483654: step 7500, loss 1.88906, acc 0.572654, f1 0.574596\n",
      "\n",
      "2017-11-15T23:04:52.020713: step 7505, loss 0.00669958, acc 0.99707, f1 0.997069\n",
      "Current epoch:  417\n",
      "2017-11-15T23:04:52.514791: step 7510, loss 0.0116736, acc 0.99707, f1 0.997072\n",
      "2017-11-15T23:04:53.037100: step 7515, loss 0.00276691, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:04:53.561036: step 7520, loss 0.00285316, acc 0.999023, f1 0.999023\n",
      "Current epoch:  418\n",
      "2017-11-15T23:04:54.058503: step 7525, loss 0.00518998, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:04:54.579333: step 7530, loss 0.0011811, acc 1, f1 1\n",
      "2017-11-15T23:04:55.095282: step 7535, loss 0.00389948, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:04:55.619315: step 7540, loss 0.00608813, acc 0.998047, f1 0.998047\n",
      "Current epoch:  419\n",
      "2017-11-15T23:04:56.116217: step 7545, loss 0.00133451, acc 1, f1 1\n",
      "2017-11-15T23:04:56.645683: step 7550, loss 0.0077698, acc 0.99707, f1 0.99707\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:04:57.319526: step 7550, loss 1.92964, acc 0.583947, f1 0.584619\n",
      "\n",
      "2017-11-15T23:04:57.856882: step 7555, loss 0.01263, acc 0.995117, f1 0.995117\n",
      "2017-11-15T23:04:58.343713: step 7560, loss 0.00696182, acc 0.998428, f1 0.998428\n",
      "Current epoch:  420\n",
      "2017-11-15T23:04:58.867161: step 7565, loss 0.000946139, acc 1, f1 1\n",
      "2017-11-15T23:04:59.390703: step 7570, loss 0.0100956, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:04:59.912133: step 7575, loss 0.0143556, acc 0.996094, f1 0.996096\n",
      "Current epoch:  421\n",
      "2017-11-15T23:05:00.408033: step 7580, loss 0.00538818, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:05:00.924340: step 7585, loss 0.00817261, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:05:01.450310: step 7590, loss 0.0136801, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:05:01.973280: step 7595, loss 0.00369631, acc 0.999023, f1 0.999022\n",
      "Current epoch:  422\n",
      "2017-11-15T23:05:02.457126: step 7600, loss 1.56784, acc 0.857422, f1 0.793099\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:05:03.141720: step 7600, loss 2.6125, acc 0.587146, f1 0.541963\n",
      "\n",
      "2017-11-15T23:05:03.656270: step 7605, loss 0.00941906, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:05:04.175631: step 7610, loss 0.00662447, acc 0.999023, f1 0.999023\n",
      "Current epoch:  423\n",
      "2017-11-15T23:05:04.670009: step 7615, loss 0.00611899, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:05:05.188261: step 7620, loss 0.0124645, acc 0.996094, f1 0.996097\n",
      "2017-11-15T23:05:05.712230: step 7625, loss 0.00641021, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:05:06.238182: step 7630, loss 0.0124222, acc 0.996094, f1 0.996097\n",
      "Current epoch:  424\n",
      "2017-11-15T23:05:06.726570: step 7635, loss 0.00391235, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:05:07.244989: step 7640, loss 0.00371937, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:05:07.766414: step 7645, loss 0.00705949, acc 0.99707, f1 0.997074\n",
      "2017-11-15T23:05:08.262352: step 7650, loss 0.00326599, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:05:08.953654: step 7650, loss 1.82857, acc 0.584819, f1 0.584756\n",
      "\n",
      "Current epoch:  425\n",
      "2017-11-15T23:05:09.472796: step 7655, loss 0.00813849, acc 0.996094, f1 0.996096\n",
      "2017-11-15T23:05:09.995244: step 7660, loss 0.00425394, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:05:10.518851: step 7665, loss 0.00351469, acc 0.999023, f1 0.999023\n",
      "Current epoch:  426\n",
      "2017-11-15T23:05:11.013577: step 7670, loss 0.00750538, acc 0.996094, f1 0.996097\n",
      "2017-11-15T23:05:11.537047: step 7675, loss 0.00370733, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:05:12.060994: step 7680, loss 0.00372009, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:05:12.576914: step 7685, loss 0.003913, acc 0.998047, f1 0.998047\n",
      "Current epoch:  427\n",
      "2017-11-15T23:05:13.067832: step 7690, loss 0.00456894, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:05:13.590833: step 7695, loss 0.00258382, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:05:14.126991: step 7700, loss 0.00311206, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:05:14.795905: step 7700, loss 1.88626, acc 0.583317, f1 0.583215\n",
      "\n",
      "Current epoch:  428\n",
      "2017-11-15T23:05:15.288531: step 7705, loss 0.00203025, acc 1, f1 1\n",
      "2017-11-15T23:05:15.811814: step 7710, loss 0.00316594, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:05:16.332773: step 7715, loss 0.0060792, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:05:16.862395: step 7720, loss 0.00804973, acc 0.99707, f1 0.997069\n",
      "Current epoch:  429\n",
      "2017-11-15T23:05:17.355903: step 7725, loss 0.00502515, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:05:17.874348: step 7730, loss 0.00124985, acc 1, f1 1\n",
      "2017-11-15T23:05:18.393767: step 7735, loss 0.00121861, acc 1, f1 1\n",
      "2017-11-15T23:05:18.886643: step 7740, loss 0.0101584, acc 0.995283, f1 0.995283\n",
      "Current epoch:  430\n",
      "2017-11-15T23:05:19.418016: step 7745, loss 0.00484486, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:05:19.938470: step 7750, loss 0.000873535, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:05:20.592980: step 7750, loss 1.93281, acc 0.584916, f1 0.584435\n",
      "\n",
      "2017-11-15T23:05:21.108716: step 7755, loss 0.00764608, acc 0.99707, f1 0.997061\n",
      "Current epoch:  431\n",
      "2017-11-15T23:05:21.611620: step 7760, loss 0.00729602, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:05:22.137537: step 7765, loss 0.0140546, acc 0.995117, f1 0.995118\n",
      "2017-11-15T23:05:22.658800: step 7770, loss 0.000817881, acc 1, f1 1\n",
      "2017-11-15T23:05:23.180836: step 7775, loss 0.00771173, acc 0.996094, f1 0.996089\n",
      "Current epoch:  432\n",
      "2017-11-15T23:05:23.662973: step 7780, loss 0.000897124, acc 1, f1 1\n",
      "2017-11-15T23:05:24.180216: step 7785, loss 0.00638629, acc 0.998047, f1 0.998043\n",
      "2017-11-15T23:05:24.702500: step 7790, loss 0.00484229, acc 0.998047, f1 0.998048\n",
      "Current epoch:  433\n",
      "2017-11-15T23:05:25.210370: step 7795, loss 0.00551009, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:05:25.734377: step 7800, loss 0.00501588, acc 0.999023, f1 0.999024\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:05:26.390149: step 7800, loss 1.98547, acc 0.592865, f1 0.588673\n",
      "\n",
      "2017-11-15T23:05:26.905585: step 7805, loss 0.0116687, acc 0.99707, f1 0.997066\n",
      "2017-11-15T23:05:27.431973: step 7810, loss 0.00415304, acc 0.999023, f1 0.999023\n",
      "Current epoch:  434\n",
      "2017-11-15T23:05:27.923245: step 7815, loss 0.0204942, acc 0.994141, f1 0.994141\n",
      "2017-11-15T23:05:28.452355: step 7820, loss 0.000944099, acc 1, f1 1\n",
      "2017-11-15T23:05:28.974297: step 7825, loss 0.005871, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:05:29.466939: step 7830, loss 0.0022239, acc 1, f1 1\n",
      "Current epoch:  435\n",
      "2017-11-15T23:05:29.994691: step 7835, loss 0.0107426, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:05:30.542796: step 7840, loss 0.00893214, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:05:31.065736: step 7845, loss 0.00777807, acc 0.998047, f1 0.998046\n",
      "Current epoch:  436\n",
      "2017-11-15T23:05:31.559753: step 7850, loss 4.25562, acc 0.538086, f1 0.412396\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:05:32.221332: step 7850, loss 3.98341, acc 0.425068, f1 0.314343\n",
      "\n",
      "2017-11-15T23:05:32.748291: step 7855, loss 0.0141356, acc 0.99707, f1 0.997072\n",
      "2017-11-15T23:05:33.268306: step 7860, loss 0.00600286, acc 1, f1 1\n",
      "2017-11-15T23:05:33.785187: step 7865, loss 0.0130356, acc 0.99707, f1 0.99707\n",
      "Current epoch:  437\n",
      "2017-11-15T23:05:34.272083: step 7870, loss 0.0104875, acc 0.996094, f1 0.996092\n",
      "2017-11-15T23:05:34.784796: step 7875, loss 0.00365557, acc 1, f1 1\n",
      "2017-11-15T23:05:35.305521: step 7880, loss 0.00928191, acc 0.998047, f1 0.998047\n",
      "Current epoch:  438\n",
      "2017-11-15T23:05:35.813015: step 7885, loss 0.0046782, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:05:36.334668: step 7890, loss 0.0077405, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:05:36.856189: step 7895, loss 0.00338845, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:05:37.372038: step 7900, loss 0.00526963, acc 0.998047, f1 0.998046\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:05:38.029304: step 7900, loss 1.78079, acc 0.583656, f1 0.583558\n",
      "\n",
      "Current epoch:  439\n",
      "2017-11-15T23:05:38.521425: step 7905, loss 0.00338908, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:05:39.154565: step 7910, loss 0.00533113, acc 0.998047, f1 0.998047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:05:39.684913: step 7915, loss 0.00367429, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:05:40.178281: step 7920, loss 0.00191102, acc 1, f1 1\n",
      "Current epoch:  440\n",
      "2017-11-15T23:05:40.702637: step 7925, loss 0.00279478, acc 1, f1 1\n",
      "2017-11-15T23:05:41.238614: step 7930, loss 0.00321087, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:05:41.771822: step 7935, loss 0.00574088, acc 0.99707, f1 0.997074\n",
      "Current epoch:  441\n",
      "2017-11-15T23:05:42.257178: step 7940, loss 0.00254386, acc 1, f1 1\n",
      "2017-11-15T23:05:42.780050: step 7945, loss 0.00204298, acc 1, f1 1\n",
      "2017-11-15T23:05:43.296593: step 7950, loss 0.00560324, acc 0.99707, f1 0.997071\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:05:43.965436: step 7950, loss 1.84325, acc 0.580651, f1 0.581403\n",
      "\n",
      "2017-11-15T23:05:44.482288: step 7955, loss 0.00154542, acc 1, f1 1\n",
      "Current epoch:  442\n",
      "2017-11-15T23:05:44.978371: step 7960, loss 0.00268475, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:05:45.499814: step 7965, loss 0.00329302, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:05:46.019303: step 7970, loss 0.00427885, acc 0.998047, f1 0.998046\n",
      "Current epoch:  443\n",
      "2017-11-15T23:05:46.513760: step 7975, loss 0.00355463, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:05:47.052787: step 7980, loss 0.00582487, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:05:47.574754: step 7985, loss 0.00492481, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:05:48.101287: step 7990, loss 0.00293515, acc 0.999023, f1 0.999025\n",
      "Current epoch:  444\n",
      "2017-11-15T23:05:48.590204: step 7995, loss 0.00131521, acc 1, f1 1\n",
      "2017-11-15T23:05:49.114819: step 8000, loss 0.00770808, acc 0.996094, f1 0.996092\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:05:49.765874: step 8000, loss 1.94564, acc 0.581475, f1 0.579822\n",
      "\n",
      "2017-11-15T23:05:50.284520: step 8005, loss 0.0052742, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:05:50.775401: step 8010, loss 0.00704541, acc 0.998428, f1 0.998428\n",
      "Current epoch:  445\n",
      "2017-11-15T23:05:51.305863: step 8015, loss 0.00241197, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:05:51.835869: step 8020, loss 0.00186312, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:05:52.371530: step 8025, loss 0.00829928, acc 0.99707, f1 0.997069\n",
      "Current epoch:  446\n",
      "2017-11-15T23:05:52.889901: step 8030, loss 0.000801782, acc 1, f1 1\n",
      "2017-11-15T23:05:53.420035: step 8035, loss 0.00224641, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:05:53.942981: step 8040, loss 0.00348879, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:05:54.464910: step 8045, loss 0.00883808, acc 0.996094, f1 0.996095\n",
      "Current epoch:  447\n",
      "2017-11-15T23:05:54.955518: step 8050, loss 0.000867316, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:05:55.613184: step 8050, loss 1.98354, acc 0.580748, f1 0.579921\n",
      "\n",
      "2017-11-15T23:05:56.133475: step 8055, loss 0.00588604, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:05:56.659589: step 8060, loss 0.00677316, acc 0.998047, f1 0.998047\n",
      "Current epoch:  448\n",
      "2017-11-15T23:05:57.155461: step 8065, loss 0.00555339, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:05:57.679380: step 8070, loss 0.0034161, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:05:58.211842: step 8075, loss 0.0124447, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:05:58.743778: step 8080, loss 0.00494152, acc 0.999023, f1 0.999023\n",
      "Current epoch:  449\n",
      "2017-11-15T23:05:59.233178: step 8085, loss 0.00463223, acc 1, f1 1\n",
      "2017-11-15T23:05:59.752726: step 8090, loss 0.00947889, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:06:00.271701: step 8095, loss 0.00645471, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:06:00.762793: step 8100, loss 0.0103564, acc 0.996855, f1 0.996853\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:06:01.419083: step 8100, loss 2.08826, acc 0.574205, f1 0.579829\n",
      "\n",
      "Current epoch:  450\n",
      "2017-11-15T23:06:01.948613: step 8105, loss 0.0105186, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:06:02.462215: step 8110, loss 0.00731969, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:06:02.987148: step 8115, loss 0.00956301, acc 0.996094, f1 0.996094\n",
      "Current epoch:  451\n",
      "2017-11-15T23:06:03.508758: step 8120, loss 0.00843975, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:06:04.039232: step 8125, loss 0.00320718, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:06:04.563698: step 8130, loss 0.0158276, acc 0.996094, f1 0.996095\n",
      "2017-11-15T23:06:05.086967: step 8135, loss 0.852974, acc 0.726562, f1 0.69545\n",
      "Current epoch:  452\n",
      "2017-11-15T23:06:05.582665: step 8140, loss 0.0220279, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:06:06.107512: step 8145, loss 0.00833189, acc 1, f1 1\n",
      "2017-11-15T23:06:06.642000: step 8150, loss 0.00856191, acc 0.999023, f1 0.999024\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:06:07.328657: step 8150, loss 1.70914, acc 0.585062, f1 0.584995\n",
      "\n",
      "Current epoch:  453\n",
      "2017-11-15T23:06:07.836172: step 8155, loss 0.00731378, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:06:08.363142: step 8160, loss 0.00910219, acc 0.99707, f1 0.997068\n",
      "2017-11-15T23:06:08.893242: step 8165, loss 0.00622234, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:06:09.417194: step 8170, loss 0.00506609, acc 1, f1 1\n",
      "Current epoch:  454\n",
      "2017-11-15T23:06:09.907120: step 8175, loss 0.00587176, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:06:10.432075: step 8180, loss 0.00527312, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:06:10.952279: step 8185, loss 0.00692894, acc 0.99707, f1 0.997073\n",
      "2017-11-15T23:06:11.444123: step 8190, loss 0.00362404, acc 1, f1 1\n",
      "Current epoch:  455\n",
      "2017-11-15T23:06:11.967104: step 8195, loss 0.00538402, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:06:12.526209: step 8200, loss 0.00325622, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:06:13.183656: step 8200, loss 1.78408, acc 0.584868, f1 0.584519\n",
      "\n",
      "2017-11-15T23:06:13.707970: step 8205, loss 0.00411316, acc 0.998047, f1 0.998047\n",
      "Current epoch:  456\n",
      "2017-11-15T23:06:14.211367: step 8210, loss 0.00644213, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:06:14.770184: step 8215, loss 0.00337391, acc 1, f1 1\n",
      "2017-11-15T23:06:15.306676: step 8220, loss 0.00217496, acc 1, f1 1\n",
      "2017-11-15T23:06:15.833605: step 8225, loss 0.0084119, acc 0.996094, f1 0.996096\n",
      "Current epoch:  457\n",
      "2017-11-15T23:06:16.332959: step 8230, loss 0.00251099, acc 1, f1 1\n",
      "2017-11-15T23:06:16.846585: step 8235, loss 0.00379523, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:06:17.360605: step 8240, loss 0.00365681, acc 0.998047, f1 0.998048\n",
      "Current epoch:  458\n",
      "2017-11-15T23:06:17.844949: step 8245, loss 0.00335129, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:06:18.354704: step 8250, loss 0.0039592, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:06:18.993103: step 8250, loss 1.84649, acc 0.582638, f1 0.582355\n",
      "\n",
      "2017-11-15T23:06:19.509707: step 8255, loss 0.00638728, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:06:20.036181: step 8260, loss 0.00237678, acc 0.999023, f1 0.999023\n",
      "Current epoch:  459\n",
      "2017-11-15T23:06:20.533106: step 8265, loss 0.00302166, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:06:21.047584: step 8270, loss 0.00261703, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:06:21.567595: step 8275, loss 0.00689561, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:06:22.053060: step 8280, loss 0.00369483, acc 0.998428, f1 0.998428\n",
      "Current epoch:  460\n",
      "2017-11-15T23:06:22.572181: step 8285, loss 0.002929, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:06:23.090280: step 8290, loss 0.00302405, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:06:23.609002: step 8295, loss 0.00477055, acc 0.998047, f1 0.99805\n",
      "Current epoch:  461\n",
      "2017-11-15T23:06:24.095303: step 8300, loss 0.00110883, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:06:24.728446: step 8300, loss 1.93175, acc 0.587873, f1 0.5858\n",
      "\n",
      "2017-11-15T23:06:25.245853: step 8305, loss 0.00231042, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:06:25.764885: step 8310, loss 0.00108982, acc 1, f1 1\n",
      "2017-11-15T23:06:26.276782: step 8315, loss 0.00234882, acc 0.999023, f1 0.999023\n",
      "Current epoch:  462\n",
      "2017-11-15T23:06:26.766603: step 8320, loss 0.0013679, acc 1, f1 1\n",
      "2017-11-15T23:06:27.284063: step 8325, loss 0.00191862, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:06:27.795600: step 8330, loss 0.00539954, acc 0.998047, f1 0.998047\n",
      "Current epoch:  463\n",
      "2017-11-15T23:06:28.281816: step 8335, loss 0.00673855, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:06:28.794212: step 8340, loss 0.00778852, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:06:29.305022: step 8345, loss 0.017185, acc 0.996094, f1 0.996095\n",
      "2017-11-15T23:06:29.820389: step 8350, loss 0.892337, acc 0.680664, f1 0.655896\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:06:30.463838: step 8350, loss 3.47422, acc 0.545172, f1 0.472479\n",
      "\n",
      "Current epoch:  464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:06:30.993877: step 8355, loss 0.00920848, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:06:31.508565: step 8360, loss 0.0116809, acc 0.996094, f1 0.996095\n",
      "2017-11-15T23:06:32.018102: step 8365, loss 0.00317708, acc 1, f1 1\n",
      "2017-11-15T23:06:32.500624: step 8370, loss 0.0108713, acc 0.995283, f1 0.995283\n",
      "Current epoch:  465\n",
      "2017-11-15T23:06:33.015040: step 8375, loss 0.00564094, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:06:33.536717: step 8380, loss 0.00255121, acc 1, f1 1\n",
      "2017-11-15T23:06:34.061455: step 8385, loss 0.00265619, acc 1, f1 1\n",
      "Current epoch:  466\n",
      "2017-11-15T23:06:34.556843: step 8390, loss 0.00457183, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:06:35.066152: step 8395, loss 0.00266391, acc 1, f1 1\n",
      "2017-11-15T23:06:35.584362: step 8400, loss 0.00317119, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:06:36.221525: step 8400, loss 1.78956, acc 0.585886, f1 0.585524\n",
      "\n",
      "2017-11-15T23:06:36.740738: step 8405, loss 0.00256935, acc 1, f1 1\n",
      "Current epoch:  467\n",
      "2017-11-15T23:06:37.223182: step 8410, loss 0.00309021, acc 1, f1 1\n",
      "2017-11-15T23:06:37.743418: step 8415, loss 0.00418042, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:06:38.257580: step 8420, loss 0.00264956, acc 0.999023, f1 0.999023\n",
      "Current epoch:  468\n",
      "2017-11-15T23:06:38.756481: step 8425, loss 0.00240357, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:06:39.281654: step 8430, loss 0.00237631, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:06:39.799007: step 8435, loss 0.00211248, acc 1, f1 1\n",
      "2017-11-15T23:06:40.309709: step 8440, loss 0.00544209, acc 0.99707, f1 0.997069\n",
      "Current epoch:  469\n",
      "2017-11-15T23:06:40.798063: step 8445, loss 0.00454639, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:06:41.306114: step 8450, loss 0.00106694, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:06:41.966680: step 8450, loss 1.86106, acc 0.584577, f1 0.583973\n",
      "\n",
      "2017-11-15T23:06:42.483223: step 8455, loss 0.00584993, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:06:42.969695: step 8460, loss 0.00518287, acc 0.996855, f1 0.996852\n",
      "Current epoch:  470\n",
      "2017-11-15T23:06:43.490696: step 8465, loss 0.00211139, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:06:44.018223: step 8470, loss 0.00307028, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:06:44.556332: step 8475, loss 0.0101685, acc 0.996094, f1 0.996094\n",
      "Current epoch:  471\n",
      "2017-11-15T23:06:45.047017: step 8480, loss 0.000894639, acc 1, f1 1\n",
      "2017-11-15T23:06:45.569583: step 8485, loss 0.00440426, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:06:46.095993: step 8490, loss 0.00317315, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:06:46.624555: step 8495, loss 0.00699732, acc 0.99707, f1 0.997069\n",
      "Current epoch:  472\n",
      "2017-11-15T23:06:47.127470: step 8500, loss 0.0020666, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:06:47.840706: step 8500, loss 1.92975, acc 0.582444, f1 0.582966\n",
      "\n",
      "2017-11-15T23:06:48.352738: step 8505, loss 0.00368416, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:06:48.877591: step 8510, loss 0.00380782, acc 0.998047, f1 0.998047\n",
      "Current epoch:  473\n",
      "2017-11-15T23:06:49.370075: step 8515, loss 0.00629649, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:06:49.881138: step 8520, loss 0.00441066, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:06:50.402639: step 8525, loss 0.000657901, acc 1, f1 1\n",
      "2017-11-15T23:06:50.920732: step 8530, loss 0.00200485, acc 0.999023, f1 0.999023\n",
      "Current epoch:  474\n",
      "2017-11-15T23:06:51.409061: step 8535, loss 0.00279812, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:06:51.925457: step 8540, loss 0.00723877, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:06:52.443595: step 8545, loss 0.006967, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:06:52.938595: step 8550, loss 0.000952553, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:06:53.579523: step 8550, loss 2.00481, acc 0.576047, f1 0.576678\n",
      "\n",
      "Current epoch:  475\n",
      "2017-11-15T23:06:54.104074: step 8555, loss 0.00374991, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:06:54.632669: step 8560, loss 0.0044591, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:06:55.154105: step 8565, loss 0.01174, acc 0.996094, f1 0.996097\n",
      "Current epoch:  476\n",
      "2017-11-15T23:06:55.648688: step 8570, loss 0.00972316, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:06:56.173190: step 8575, loss 0.0064427, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:06:56.693981: step 8580, loss 0.00840157, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:06:57.217888: step 8585, loss 0.00797482, acc 0.998047, f1 0.998047\n",
      "Current epoch:  477\n",
      "2017-11-15T23:06:57.706632: step 8590, loss 0.0187975, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:06:58.233572: step 8595, loss 0.00533433, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:06:58.768492: step 8600, loss 0.00629532, acc 0.998047, f1 0.998043\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:06:59.423471: step 8600, loss 2.19111, acc 0.547402, f1 0.547786\n",
      "\n",
      "Current epoch:  478\n",
      "2017-11-15T23:06:59.913263: step 8605, loss 0.0125689, acc 0.996094, f1 0.996092\n",
      "2017-11-15T23:07:00.440784: step 8610, loss 0.00259133, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:07:00.965742: step 8615, loss 0.0122371, acc 0.996094, f1 0.996093\n",
      "2017-11-15T23:07:01.487574: step 8620, loss 0.00318522, acc 1, f1 1\n",
      "Current epoch:  479\n",
      "2017-11-15T23:07:01.984170: step 8625, loss 0.00055198, acc 1, f1 1\n",
      "2017-11-15T23:07:02.506946: step 8630, loss 0.244778, acc 0.875, f1 0.872494\n",
      "2017-11-15T23:07:03.031200: step 8635, loss 0.170966, acc 0.93457, f1 0.93401\n",
      "2017-11-15T23:07:03.524637: step 8640, loss 0.0092709, acc 0.998428, f1 0.998428\n",
      "Current epoch:  480\n",
      "2017-11-15T23:07:04.056876: step 8645, loss 0.00655086, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:07:04.579870: step 8650, loss 0.00834868, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:07:05.238616: step 8650, loss 1.70394, acc 0.585498, f1 0.585135\n",
      "\n",
      "2017-11-15T23:07:05.760511: step 8655, loss 0.00368911, acc 1, f1 1\n",
      "Current epoch:  481\n",
      "2017-11-15T23:07:06.252823: step 8660, loss 0.00519524, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:07:06.771820: step 8665, loss 0.00647283, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:07:07.291086: step 8670, loss 0.00405075, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:07:07.820641: step 8675, loss 0.0047551, acc 0.999023, f1 0.999023\n",
      "Current epoch:  482\n",
      "2017-11-15T23:07:08.307971: step 8680, loss 0.00738822, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:07:08.832245: step 8685, loss 0.00397529, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:07:09.353125: step 8690, loss 0.00244676, acc 1, f1 1\n",
      "Current epoch:  483\n",
      "2017-11-15T23:07:09.846033: step 8695, loss 0.0083239, acc 0.996094, f1 0.996095\n",
      "2017-11-15T23:07:10.354768: step 8700, loss 0.00346537, acc 0.999023, f1 0.999022\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:07:10.991595: step 8700, loss 1.77646, acc 0.58608, f1 0.585452\n",
      "\n",
      "2017-11-15T23:07:11.507736: step 8705, loss 0.00413136, acc 0.998047, f1 0.998042\n",
      "2017-11-15T23:07:12.020385: step 8710, loss 0.00334777, acc 0.999023, f1 0.999023\n",
      "Current epoch:  484\n",
      "2017-11-15T23:07:12.507840: step 8715, loss 0.00224173, acc 1, f1 1\n",
      "2017-11-15T23:07:13.027541: step 8720, loss 0.00651038, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:07:13.548054: step 8725, loss 0.00470914, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:07:14.037313: step 8730, loss 0.00265538, acc 0.998428, f1 0.99843\n",
      "Current epoch:  485\n",
      "2017-11-15T23:07:14.560936: step 8735, loss 0.00293878, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:07:15.094777: step 8740, loss 0.00281175, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:07:15.617108: step 8745, loss 0.00723647, acc 0.99707, f1 0.99707\n",
      "Current epoch:  486\n",
      "2017-11-15T23:07:16.109080: step 8750, loss 0.00270712, acc 0.999023, f1 0.999024\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:07:16.764129: step 8750, loss 1.85443, acc 0.585595, f1 0.584237\n",
      "\n",
      "2017-11-15T23:07:17.284101: step 8755, loss 0.00577878, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:07:17.802864: step 8760, loss 0.00810515, acc 0.99707, f1 0.997067\n",
      "2017-11-15T23:07:18.323691: step 8765, loss 0.000996604, acc 1, f1 1\n",
      "Current epoch:  487\n",
      "2017-11-15T23:07:18.818401: step 8770, loss 0.00298465, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:07:19.339545: step 8775, loss 0.0062371, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:07:19.861791: step 8780, loss 0.0053792, acc 0.998047, f1 0.998047\n",
      "Current epoch:  488\n",
      "2017-11-15T23:07:20.367573: step 8785, loss 0.00605727, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:07:20.895724: step 8790, loss 0.00104799, acc 1, f1 1\n",
      "2017-11-15T23:07:21.416199: step 8795, loss 0.00772157, acc 0.99707, f1 0.99707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:07:21.956696: step 8800, loss 0.00498775, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:07:22.610690: step 8800, loss 1.94157, acc 0.577743, f1 0.578308\n",
      "\n",
      "Current epoch:  489\n",
      "2017-11-15T23:07:23.101031: step 8805, loss 0.0041356, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:07:23.622162: step 8810, loss 0.00377619, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:07:24.141433: step 8815, loss 0.00190328, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:07:24.632336: step 8820, loss 0.0111573, acc 0.996855, f1 0.996853\n",
      "Current epoch:  490\n",
      "2017-11-15T23:07:25.155315: step 8825, loss 0.00615085, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:07:25.682160: step 8830, loss 0.0033426, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:07:26.218535: step 8835, loss 0.00782201, acc 0.998047, f1 0.998046\n",
      "Current epoch:  491\n",
      "2017-11-15T23:07:26.723063: step 8840, loss 0.00873986, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:07:27.246371: step 8845, loss 0.00268834, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:07:27.771460: step 8850, loss 0.00345175, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:07:28.423657: step 8850, loss 1.98284, acc 0.594513, f1 0.592771\n",
      "\n",
      "2017-11-15T23:07:28.946636: step 8855, loss 0.00715386, acc 0.99707, f1 0.997069\n",
      "Current epoch:  492\n",
      "2017-11-15T23:07:29.444584: step 8860, loss 0.00599038, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:07:29.973558: step 8865, loss 0.00538389, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:07:30.494365: step 8870, loss 0.0152949, acc 0.996094, f1 0.996091\n",
      "Current epoch:  493\n",
      "2017-11-15T23:07:30.984097: step 8875, loss 0.00076387, acc 1, f1 1\n",
      "2017-11-15T23:07:31.515949: step 8880, loss 0.020584, acc 0.99707, f1 0.997073\n",
      "2017-11-15T23:07:32.039548: step 8885, loss 0.00143201, acc 1, f1 1\n",
      "2017-11-15T23:07:32.561182: step 8890, loss 0.00450873, acc 0.998047, f1 0.998046\n",
      "Current epoch:  494\n",
      "2017-11-15T23:07:33.049029: step 8895, loss 0.000606843, acc 1, f1 1\n",
      "2017-11-15T23:07:33.569546: step 8900, loss 0.0135964, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:07:34.224614: step 8900, loss 2.15699, acc 0.561652, f1 0.560077\n",
      "\n",
      "2017-11-15T23:07:34.746798: step 8905, loss 0.0113318, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:07:35.233960: step 8910, loss 0.0190329, acc 0.996855, f1 0.996857\n",
      "Current epoch:  495\n",
      "2017-11-15T23:07:35.759143: step 8915, loss 0.0243541, acc 0.996094, f1 0.996096\n",
      "2017-11-15T23:07:36.288629: step 8920, loss 0.00237448, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:07:36.820646: step 8925, loss 0.0191636, acc 0.994141, f1 0.994144\n",
      "Current epoch:  496\n",
      "2017-11-15T23:07:37.319055: step 8930, loss 2.73117, acc 0.474609, f1 0.460042\n",
      "2017-11-15T23:07:37.848482: step 8935, loss 0.0313761, acc 0.994141, f1 0.994143\n",
      "2017-11-15T23:07:38.366982: step 8940, loss 0.0140194, acc 1, f1 1\n",
      "2017-11-15T23:07:38.881637: step 8945, loss 0.0139253, acc 0.999023, f1 0.999023\n",
      "Current epoch:  497\n",
      "2017-11-15T23:07:39.373481: step 8950, loss 0.00907935, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:07:40.028335: step 8950, loss 1.69338, acc 0.576338, f1 0.576696\n",
      "\n",
      "2017-11-15T23:07:40.539897: step 8955, loss 0.0133623, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:07:41.049436: step 8960, loss 0.0113831, acc 0.995117, f1 0.995118\n",
      "Current epoch:  498\n",
      "2017-11-15T23:07:41.531284: step 8965, loss 0.00564, acc 1, f1 1\n",
      "2017-11-15T23:07:42.044262: step 8970, loss 0.00872561, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:07:42.567333: step 8975, loss 0.00536532, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:07:43.077287: step 8980, loss 0.00522616, acc 0.999023, f1 0.999023\n",
      "Current epoch:  499\n",
      "2017-11-15T23:07:43.565171: step 8985, loss 0.00561134, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:07:44.083147: step 8990, loss 0.00763016, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:07:44.603997: step 8995, loss 0.00916211, acc 0.996094, f1 0.996089\n",
      "2017-11-15T23:07:45.090454: step 9000, loss 0.00642653, acc 0.998428, f1 0.998428\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:07:45.746431: step 9000, loss 1.77977, acc 0.577985, f1 0.578299\n",
      "\n",
      "Current epoch:  500\n",
      "2017-11-15T23:07:46.264798: step 9005, loss 0.00475725, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:07:46.782063: step 9010, loss 0.00336715, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:07:47.301586: step 9015, loss 0.00490715, acc 0.998047, f1 0.998047\n",
      "Current epoch:  501\n",
      "2017-11-15T23:07:47.804871: step 9020, loss 0.00352574, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:07:48.327963: step 9025, loss 0.00421336, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:07:48.854041: step 9030, loss 0.00349433, acc 0.998047, f1 0.998051\n",
      "2017-11-15T23:07:49.377469: step 9035, loss 0.00842674, acc 0.994141, f1 0.994139\n",
      "Current epoch:  502\n",
      "2017-11-15T23:07:49.864039: step 9040, loss 0.00415252, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:07:50.383175: step 9045, loss 0.00433732, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:07:50.896915: step 9050, loss 0.00537458, acc 0.99707, f1 0.997075\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:07:51.526599: step 9050, loss 1.83686, acc 0.587728, f1 0.586694\n",
      "\n",
      "Current epoch:  503\n",
      "2017-11-15T23:07:52.018442: step 9055, loss 0.00214969, acc 1, f1 1\n",
      "2017-11-15T23:07:52.538780: step 9060, loss 0.00208182, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:07:53.057092: step 9065, loss 0.00639509, acc 0.99707, f1 0.997068\n",
      "2017-11-15T23:07:53.590056: step 9070, loss 0.00270356, acc 0.999023, f1 0.999023\n",
      "Current epoch:  504\n",
      "2017-11-15T23:07:54.086610: step 9075, loss 0.00513158, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:07:54.606863: step 9080, loss 0.00437931, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:07:55.123922: step 9085, loss 0.00608321, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:07:55.614641: step 9090, loss 0.013774, acc 0.993711, f1 0.993712\n",
      "Current epoch:  505\n",
      "2017-11-15T23:07:56.134513: step 9095, loss 0.00326083, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:07:56.655483: step 9100, loss 0.00754771, acc 0.996094, f1 0.996095\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:07:57.312120: step 9100, loss 1.94432, acc 0.581136, f1 0.579723\n",
      "\n",
      "2017-11-15T23:07:57.832597: step 9105, loss 0.00275605, acc 0.999023, f1 0.999023\n",
      "Current epoch:  506\n",
      "2017-11-15T23:07:58.321987: step 9110, loss 0.00594597, acc 0.996094, f1 0.996091\n",
      "2017-11-15T23:07:58.855495: step 9115, loss 0.00119826, acc 1, f1 1\n",
      "2017-11-15T23:07:59.382067: step 9120, loss 0.0045391, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:07:59.901231: step 9125, loss 0.00388676, acc 0.999023, f1 0.999023\n",
      "Current epoch:  507\n",
      "2017-11-15T23:08:00.397334: step 9130, loss 0.00113505, acc 1, f1 1\n",
      "2017-11-15T23:08:00.914789: step 9135, loss 0.000815735, acc 1, f1 1\n",
      "2017-11-15T23:08:01.439379: step 9140, loss 0.00485, acc 0.998047, f1 0.998048\n",
      "Current epoch:  508\n",
      "2017-11-15T23:08:01.940613: step 9145, loss 0.00333148, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:08:02.461613: step 9150, loss 0.00579511, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:08:03.122217: step 9150, loss 2.03936, acc 0.575659, f1 0.572915\n",
      "\n",
      "2017-11-15T23:08:03.646785: step 9155, loss 0.0066579, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:08:04.174477: step 9160, loss 0.00672323, acc 0.998047, f1 0.998046\n",
      "Current epoch:  509\n",
      "2017-11-15T23:08:04.683259: step 9165, loss 0.00640044, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:08:05.204920: step 9170, loss 0.00392531, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:08:05.734483: step 9175, loss 0.00492612, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:08:06.222764: step 9180, loss 0.00361919, acc 0.998428, f1 0.99843\n",
      "Current epoch:  510\n",
      "2017-11-15T23:08:06.753133: step 9185, loss 0.00701792, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:08:07.277012: step 9190, loss 0.00963291, acc 0.996094, f1 0.996096\n",
      "2017-11-15T23:08:07.796235: step 9195, loss 0.00320734, acc 0.999023, f1 0.999023\n",
      "Current epoch:  511\n",
      "2017-11-15T23:08:08.285669: step 9200, loss 0.00225211, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:08:08.949428: step 9200, loss 2.02153, acc 0.581136, f1 0.581467\n",
      "\n",
      "2017-11-15T23:08:09.463764: step 9205, loss 0.00595763, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:08:09.994920: step 9210, loss 0.00455172, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:08:10.597746: step 9215, loss 0.00614005, acc 0.998047, f1 0.998052\n",
      "Current epoch:  512\n",
      "2017-11-15T23:08:11.089253: step 9220, loss 0.00485785, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:08:11.616775: step 9225, loss 0.000816933, acc 1, f1 1\n",
      "2017-11-15T23:08:12.144281: step 9230, loss 0.0057952, acc 0.999023, f1 0.999022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch:  513\n",
      "2017-11-15T23:08:12.664744: step 9235, loss 3.58226, acc 0.570312, f1 0.447717\n",
      "2017-11-15T23:08:13.186333: step 9240, loss 0.0103473, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:08:13.821506: step 9245, loss 0.00645178, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:08:14.338566: step 9250, loss 0.0102322, acc 0.996094, f1 0.996091\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:08:15.005898: step 9250, loss 1.73509, acc 0.580118, f1 0.580803\n",
      "\n",
      "Current epoch:  514\n",
      "2017-11-15T23:08:15.499247: step 9255, loss 0.00805209, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:08:16.021738: step 9260, loss 0.00630857, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:08:16.542310: step 9265, loss 0.00804321, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:08:17.036797: step 9270, loss 0.00304724, acc 1, f1 1\n",
      "Current epoch:  515\n",
      "2017-11-15T23:08:17.555521: step 9275, loss 0.00510078, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:08:18.075487: step 9280, loss 0.00500078, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:08:18.595523: step 9285, loss 0.0086796, acc 0.99707, f1 0.997075\n",
      "Current epoch:  516\n",
      "2017-11-15T23:08:19.088926: step 9290, loss 0.00326496, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:08:19.610529: step 9295, loss 0.00761829, acc 0.996094, f1 0.996101\n",
      "2017-11-15T23:08:20.134283: step 9300, loss 0.00357005, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:08:20.791874: step 9300, loss 1.82103, acc 0.581911, f1 0.581862\n",
      "\n",
      "2017-11-15T23:08:21.310577: step 9305, loss 0.00766055, acc 0.996094, f1 0.996096\n",
      "Current epoch:  517\n",
      "2017-11-15T23:08:21.796132: step 9310, loss 0.00213063, acc 1, f1 1\n",
      "2017-11-15T23:08:22.322239: step 9315, loss 0.00776337, acc 0.995117, f1 0.995112\n",
      "2017-11-15T23:08:22.853750: step 9320, loss 0.00351236, acc 0.998047, f1 0.998047\n",
      "Current epoch:  518\n",
      "2017-11-15T23:08:23.343747: step 9325, loss 0.00690804, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:08:23.866049: step 9330, loss 0.00574289, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:08:24.387779: step 9335, loss 0.00364725, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:08:24.906865: step 9340, loss 0.00339658, acc 0.998047, f1 0.998047\n",
      "Current epoch:  519\n",
      "2017-11-15T23:08:25.395654: step 9345, loss 0.00227657, acc 1, f1 1\n",
      "2017-11-15T23:08:25.918464: step 9350, loss 0.00282963, acc 0.999023, f1 0.999022\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:08:26.598831: step 9350, loss 1.89487, acc 0.580409, f1 0.581764\n",
      "\n",
      "2017-11-15T23:08:27.123560: step 9355, loss 0.00393064, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:08:27.616277: step 9360, loss 0.00845452, acc 0.995283, f1 0.995283\n",
      "Current epoch:  520\n",
      "2017-11-15T23:08:28.135210: step 9365, loss 0.00124969, acc 1, f1 1\n",
      "2017-11-15T23:08:28.659802: step 9370, loss 0.00407356, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:08:29.182337: step 9375, loss 0.00926591, acc 0.995117, f1 0.995121\n",
      "Current epoch:  521\n",
      "2017-11-15T23:08:29.679997: step 9380, loss 0.00115575, acc 1, f1 1\n",
      "2017-11-15T23:08:30.198395: step 9385, loss 0.00185899, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:08:30.727828: step 9390, loss 0.00353296, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:08:31.252693: step 9395, loss 0.00109414, acc 1, f1 1\n",
      "Current epoch:  522\n",
      "2017-11-15T23:08:31.761663: step 9400, loss 0.00848743, acc 0.99707, f1 0.997074\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:08:32.443112: step 9400, loss 2.00259, acc 0.592574, f1 0.587299\n",
      "\n",
      "2017-11-15T23:08:33.000694: step 9405, loss 0.00169176, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:08:33.537647: step 9410, loss 0.0084126, acc 0.99707, f1 0.997071\n",
      "Current epoch:  523\n",
      "2017-11-15T23:08:34.033676: step 9415, loss 0.00326512, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:08:34.556654: step 9420, loss 0.00625761, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:08:35.081197: step 9425, loss 0.00940597, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:08:35.601679: step 9430, loss 0.0034453, acc 0.999023, f1 0.999023\n",
      "Current epoch:  524\n",
      "2017-11-15T23:08:36.095086: step 9435, loss 0.00301878, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:08:36.622128: step 9440, loss 0.00774928, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:08:37.147064: step 9445, loss 0.0031172, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:08:37.653893: step 9450, loss 0.0141445, acc 0.995283, f1 0.995286\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:08:38.327324: step 9450, loss 1.97575, acc 0.587097, f1 0.587962\n",
      "\n",
      "Current epoch:  525\n",
      "2017-11-15T23:08:38.850672: step 9455, loss 0.0092038, acc 0.99707, f1 0.997079\n",
      "2017-11-15T23:08:39.374538: step 9460, loss 0.0131918, acc 0.99707, f1 0.997068\n",
      "2017-11-15T23:08:39.895928: step 9465, loss 0.000900661, acc 1, f1 1\n",
      "Current epoch:  526\n",
      "2017-11-15T23:08:40.393448: step 9470, loss 0.00128353, acc 1, f1 1\n",
      "2017-11-15T23:08:40.916471: step 9475, loss 0.000996183, acc 1, f1 1\n",
      "2017-11-15T23:08:41.433211: step 9480, loss 0.00429649, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:08:41.951656: step 9485, loss 0.00438874, acc 0.999023, f1 0.999025\n",
      "Current epoch:  527\n",
      "2017-11-15T23:08:42.444081: step 9490, loss 0.466349, acc 0.796875, f1 0.791682\n",
      "2017-11-15T23:08:42.966751: step 9495, loss 0.0977451, acc 0.959961, f1 0.959885\n",
      "2017-11-15T23:08:43.485408: step 9500, loss 0.010697, acc 0.998047, f1 0.998048\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:08:44.115079: step 9500, loss 1.65423, acc 0.587097, f1 0.587564\n",
      "\n",
      "Current epoch:  528\n",
      "2017-11-15T23:08:44.615351: step 9505, loss 0.00759918, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:08:45.129857: step 9510, loss 0.0116742, acc 0.998047, f1 0.998043\n",
      "2017-11-15T23:08:45.652068: step 9515, loss 0.00699113, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:08:46.169482: step 9520, loss 0.0114882, acc 0.996094, f1 0.996095\n",
      "Current epoch:  529\n",
      "2017-11-15T23:08:46.658564: step 9525, loss 0.00496798, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:08:47.178558: step 9530, loss 0.00425299, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:08:47.699034: step 9535, loss 0.00464674, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:08:48.187408: step 9540, loss 0.00719713, acc 0.996855, f1 0.996852\n",
      "Current epoch:  530\n",
      "2017-11-15T23:08:48.723493: step 9545, loss 0.00632746, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:08:49.243505: step 9550, loss 0.00317625, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:08:49.906208: step 9550, loss 1.75641, acc 0.583705, f1 0.584128\n",
      "\n",
      "2017-11-15T23:08:50.431169: step 9555, loss 0.002222, acc 1, f1 1\n",
      "Current epoch:  531\n",
      "2017-11-15T23:08:50.920017: step 9560, loss 0.00347848, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:08:51.439939: step 9565, loss 0.00503752, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:08:51.957901: step 9570, loss 0.00406874, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:08:52.469789: step 9575, loss 0.00211544, acc 1, f1 1\n",
      "Current epoch:  532\n",
      "2017-11-15T23:08:52.948980: step 9580, loss 0.0037372, acc 0.998047, f1 0.998043\n",
      "2017-11-15T23:08:53.467393: step 9585, loss 0.00708861, acc 0.996094, f1 0.996098\n",
      "2017-11-15T23:08:53.997678: step 9590, loss 0.0060604, acc 0.99707, f1 0.997072\n",
      "Current epoch:  533\n",
      "2017-11-15T23:08:54.498462: step 9595, loss 0.00435547, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:08:55.019675: step 9600, loss 0.00415736, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:08:55.676427: step 9600, loss 1.82642, acc 0.584965, f1 0.585123\n",
      "\n",
      "2017-11-15T23:08:56.194520: step 9605, loss 0.00353491, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:08:56.715341: step 9610, loss 0.00121127, acc 1, f1 1\n",
      "Current epoch:  534\n",
      "2017-11-15T23:08:57.204450: step 9615, loss 0.00302489, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:08:57.727246: step 9620, loss 0.00173692, acc 1, f1 1\n",
      "2017-11-15T23:08:58.250731: step 9625, loss 0.00158438, acc 1, f1 1\n",
      "2017-11-15T23:08:58.738968: step 9630, loss 0.00317551, acc 0.998428, f1 0.998428\n",
      "Current epoch:  535\n",
      "2017-11-15T23:08:59.269774: step 9635, loss 0.00358402, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:08:59.800375: step 9640, loss 0.00123759, acc 1, f1 1\n",
      "2017-11-15T23:09:00.315213: step 9645, loss 0.0118562, acc 0.994141, f1 0.994142\n",
      "Current epoch:  536\n",
      "2017-11-15T23:09:00.808631: step 9650, loss 0.00722663, acc 0.996094, f1 0.996094\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:09:01.467642: step 9650, loss 1.91779, acc 0.582202, f1 0.582509\n",
      "\n",
      "2017-11-15T23:09:01.987189: step 9655, loss 0.00590914, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:09:02.507265: step 9660, loss 0.000956722, acc 1, f1 1\n",
      "2017-11-15T23:09:03.027656: step 9665, loss 0.00651064, acc 0.998047, f1 0.998048\n",
      "Current epoch:  537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:09:03.522226: step 9670, loss 0.00462095, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:09:04.043682: step 9675, loss 0.00738617, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:09:04.571431: step 9680, loss 0.011199, acc 0.995117, f1 0.995115\n",
      "Current epoch:  538\n",
      "2017-11-15T23:09:05.079353: step 9685, loss 0.00240213, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:09:05.601422: step 9690, loss 0.00230209, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:09:06.124543: step 9695, loss 0.00407252, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:09:06.647078: step 9700, loss 0.00380397, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:09:07.308963: step 9700, loss 2.03443, acc 0.578519, f1 0.577679\n",
      "\n",
      "Current epoch:  539\n",
      "2017-11-15T23:09:07.800306: step 9705, loss 0.00962035, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:09:08.316750: step 9710, loss 0.0131571, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:09:08.841972: step 9715, loss 0.000653125, acc 1, f1 1\n",
      "2017-11-15T23:09:09.330350: step 9720, loss 0.0157618, acc 0.995283, f1 0.99528\n",
      "Current epoch:  540\n",
      "2017-11-15T23:09:09.858675: step 9725, loss 0.000794033, acc 1, f1 1\n",
      "2017-11-15T23:09:10.386843: step 9730, loss 0.0139932, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:09:10.910020: step 9735, loss 0.00933217, acc 0.99707, f1 0.997071\n",
      "Current epoch:  541\n",
      "2017-11-15T23:09:11.407554: step 9740, loss 0.0131543, acc 0.995117, f1 0.995119\n",
      "2017-11-15T23:09:11.925477: step 9745, loss 0.00295965, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:09:12.439068: step 9750, loss 0.0114639, acc 0.996094, f1 0.996094\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:09:13.074681: step 9750, loss 2.03322, acc 0.5696, f1 0.571847\n",
      "\n",
      "2017-11-15T23:09:13.592415: step 9755, loss 0.00187107, acc 0.999023, f1 0.999023\n",
      "Current epoch:  542\n",
      "2017-11-15T23:09:14.081100: step 9760, loss 0.00197269, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:09:14.613097: step 9765, loss 0.232308, acc 0.895508, f1 0.900853\n",
      "2017-11-15T23:09:15.134025: step 9770, loss 2.22807, acc 0.606445, f1 0.529341\n",
      "Current epoch:  543\n",
      "2017-11-15T23:09:15.630980: step 9775, loss 0.0218996, acc 0.995117, f1 0.995117\n",
      "2017-11-15T23:09:16.168190: step 9780, loss 0.0176502, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:09:16.688734: step 9785, loss 0.0132288, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:09:17.210245: step 9790, loss 0.00826693, acc 1, f1 1\n",
      "Current epoch:  544\n",
      "2017-11-15T23:09:17.700498: step 9795, loss 0.00778449, acc 1, f1 1\n",
      "2017-11-15T23:09:18.221742: step 9800, loss 0.0136238, acc 0.99707, f1 0.997069\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:09:18.893260: step 9800, loss 1.65518, acc 0.581087, f1 0.581054\n",
      "\n",
      "2017-11-15T23:09:19.411176: step 9805, loss 0.0115064, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:09:19.902046: step 9810, loss 0.00747249, acc 0.998428, f1 0.998428\n",
      "Current epoch:  545\n",
      "2017-11-15T23:09:20.426813: step 9815, loss 0.00849022, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:09:20.947398: step 9820, loss 0.00480133, acc 1, f1 1\n",
      "2017-11-15T23:09:21.470624: step 9825, loss 0.00643994, acc 0.998047, f1 0.998048\n",
      "Current epoch:  546\n",
      "2017-11-15T23:09:21.965027: step 9830, loss 0.00638328, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:09:22.477954: step 9835, loss 0.00628302, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:09:22.988038: step 9840, loss 0.00488179, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:09:23.493505: step 9845, loss 0.00270299, acc 1, f1 1\n",
      "Current epoch:  547\n",
      "2017-11-15T23:09:23.971369: step 9850, loss 0.00695503, acc 0.99707, f1 0.99707\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:09:24.634194: step 9850, loss 1.745, acc 0.584819, f1 0.584837\n",
      "\n",
      "2017-11-15T23:09:25.151958: step 9855, loss 0.00444331, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:09:25.669180: step 9860, loss 0.00225046, acc 1, f1 1\n",
      "Current epoch:  548\n",
      "2017-11-15T23:09:26.163439: step 9865, loss 0.0022115, acc 1, f1 1\n",
      "2017-11-15T23:09:26.690014: step 9870, loss 0.00555817, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:09:27.224815: step 9875, loss 0.00367232, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:09:27.747877: step 9880, loss 0.00654328, acc 0.996094, f1 0.996085\n",
      "Current epoch:  549\n",
      "2017-11-15T23:09:28.243276: step 9885, loss 0.00436509, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:09:28.762438: step 9890, loss 0.00289865, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:09:29.287312: step 9895, loss 0.00480377, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:09:29.776946: step 9900, loss 0.00361177, acc 0.998428, f1 0.998428\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:09:30.446856: step 9900, loss 1.83481, acc 0.582832, f1 0.58361\n",
      "\n",
      "Current epoch:  550\n",
      "2017-11-15T23:09:30.968684: step 9905, loss 0.00379177, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:09:31.484330: step 9910, loss 0.00122702, acc 1, f1 1\n",
      "2017-11-15T23:09:32.006469: step 9915, loss 0.00616161, acc 0.99707, f1 0.99707\n",
      "Current epoch:  551\n",
      "2017-11-15T23:09:32.512241: step 9920, loss 0.0029015, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:09:33.035178: step 9925, loss 0.002351, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:09:33.560661: step 9930, loss 0.0035112, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:09:34.084613: step 9935, loss 0.00329903, acc 0.999023, f1 0.999024\n",
      "Current epoch:  552\n",
      "2017-11-15T23:09:34.577586: step 9940, loss 0.0057186, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:09:35.097838: step 9945, loss 0.00362047, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:09:35.621369: step 9950, loss 0.00144588, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:09:36.275238: step 9950, loss 1.92806, acc 0.586661, f1 0.587744\n",
      "\n",
      "Current epoch:  553\n",
      "2017-11-15T23:09:36.769174: step 9955, loss 0.00302541, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:09:37.286934: step 9960, loss 0.00257956, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:09:37.832019: step 9965, loss 0.00112545, acc 1, f1 1\n",
      "2017-11-15T23:09:38.362093: step 9970, loss 0.00846969, acc 0.99707, f1 0.997072\n",
      "Current epoch:  554\n",
      "2017-11-15T23:09:38.862560: step 9975, loss 0.00127775, acc 1, f1 1\n",
      "2017-11-15T23:09:39.382024: step 9980, loss 0.0010686, acc 1, f1 1\n",
      "2017-11-15T23:09:39.908015: step 9985, loss 0.00648036, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:09:40.397468: step 9990, loss 0.0124676, acc 0.995283, f1 0.995284\n",
      "Current epoch:  555\n",
      "2017-11-15T23:09:40.919465: step 9995, loss 0.00104769, acc 1, f1 1\n",
      "2017-11-15T23:09:41.435879: step 10000, loss 0.0106511, acc 0.996094, f1 0.996094\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:09:42.079067: step 10000, loss 1.98779, acc 0.578616, f1 0.579273\n",
      "\n",
      "2017-11-15T23:09:42.588879: step 10005, loss 0.000884368, acc 1, f1 1\n",
      "Current epoch:  556\n",
      "2017-11-15T23:09:43.070684: step 10010, loss 0.00786118, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:09:43.597522: step 10015, loss 0.00902147, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:09:44.115505: step 10020, loss 0.000680171, acc 1, f1 1\n",
      "2017-11-15T23:09:44.642887: step 10025, loss 0.00900616, acc 0.99707, f1 0.99707\n",
      "Current epoch:  557\n",
      "2017-11-15T23:09:45.140309: step 10030, loss 0.0112991, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:09:45.662380: step 10035, loss 0.0068055, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:09:46.188165: step 10040, loss 0.0056708, acc 0.99707, f1 0.997069\n",
      "Current epoch:  558\n",
      "2017-11-15T23:09:46.677502: step 10045, loss 0.00624934, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:09:47.197317: step 10050, loss 0.000518809, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:09:47.854941: step 10050, loss 2.03377, acc 0.581863, f1 0.580234\n",
      "\n",
      "2017-11-15T23:09:48.377090: step 10055, loss 0.0119943, acc 1, f1 1\n",
      "2017-11-15T23:09:48.910286: step 10060, loss 0.0139767, acc 0.999023, f1 0.999022\n",
      "Current epoch:  559\n",
      "2017-11-15T23:09:49.410184: step 10065, loss 0.0177112, acc 0.995117, f1 0.995116\n",
      "2017-11-15T23:09:49.928738: step 10070, loss 0.0072574, acc 1, f1 1\n",
      "2017-11-15T23:09:50.450568: step 10075, loss 0.00592867, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:09:50.939304: step 10080, loss 0.00721339, acc 0.998428, f1 0.99843\n",
      "Current epoch:  560\n",
      "2017-11-15T23:09:51.461048: step 10085, loss 0.0055254, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:09:51.979039: step 10090, loss 0.00654854, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:09:52.489681: step 10095, loss 0.00644788, acc 0.998047, f1 0.998048\n",
      "Current epoch:  561\n",
      "2017-11-15T23:09:52.972549: step 10100, loss 0.00480227, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:09:53.636388: step 10100, loss 1.77924, acc 0.580312, f1 0.580595\n",
      "\n",
      "2017-11-15T23:09:54.155021: step 10105, loss 0.00382597, acc 0.999023, f1 0.999024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:09:54.688971: step 10110, loss 0.00571763, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:09:55.207670: step 10115, loss 0.00474772, acc 0.998047, f1 0.998047\n",
      "Current epoch:  562\n",
      "2017-11-15T23:09:55.706609: step 10120, loss 0.00300813, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:09:56.225903: step 10125, loss 0.0041203, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:09:56.747454: step 10130, loss 0.00550153, acc 0.99707, f1 0.997074\n",
      "Current epoch:  563\n",
      "2017-11-15T23:09:57.239325: step 10135, loss 0.00221302, acc 1, f1 1\n",
      "2017-11-15T23:09:57.764638: step 10140, loss 0.00477686, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:09:58.280153: step 10145, loss 0.00362624, acc 0.998047, f1 0.998041\n",
      "2017-11-15T23:09:58.799073: step 10150, loss 0.00208236, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:09:59.451206: step 10150, loss 1.84368, acc 0.583511, f1 0.58336\n",
      "\n",
      "Current epoch:  564\n",
      "2017-11-15T23:09:59.950645: step 10155, loss 0.00475328, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:10:00.478067: step 10160, loss 0.00335861, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:10:01.001027: step 10165, loss 0.00318295, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:10:01.493578: step 10170, loss 0.00441466, acc 0.996855, f1 0.996858\n",
      "Current epoch:  565\n",
      "2017-11-15T23:10:02.014924: step 10175, loss 0.00226481, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:10:02.539531: step 10180, loss 0.00807113, acc 0.995117, f1 0.995115\n",
      "2017-11-15T23:10:03.061396: step 10185, loss 0.00398973, acc 0.998047, f1 0.998047\n",
      "Current epoch:  566\n",
      "2017-11-15T23:10:03.554808: step 10190, loss 0.000921043, acc 1, f1 1\n",
      "2017-11-15T23:10:04.064373: step 10195, loss 0.00377836, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:10:04.584633: step 10200, loss 0.00237397, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:10:05.244393: step 10200, loss 1.93053, acc 0.582202, f1 0.58212\n",
      "\n",
      "2017-11-15T23:10:05.779894: step 10205, loss 0.00430536, acc 0.999023, f1 0.999022\n",
      "Current epoch:  567\n",
      "2017-11-15T23:10:06.269862: step 10210, loss 0.00159753, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:10:06.789136: step 10215, loss 0.000756361, acc 1, f1 1\n",
      "2017-11-15T23:10:07.311980: step 10220, loss 0.00185372, acc 0.999023, f1 0.999023\n",
      "Current epoch:  568\n",
      "2017-11-15T23:10:07.808838: step 10225, loss 0.00484745, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:10:08.321342: step 10230, loss 0.00261219, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:10:08.845928: step 10235, loss 0.00066991, acc 1, f1 1\n",
      "2017-11-15T23:10:09.368571: step 10240, loss 0.000621009, acc 1, f1 1\n",
      "Current epoch:  569\n",
      "2017-11-15T23:10:09.856163: step 10245, loss 0.00196374, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:10:10.383793: step 10250, loss 0.00381247, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:10:11.059423: step 10250, loss 2.02086, acc 0.589133, f1 0.585947\n",
      "\n",
      "2017-11-15T23:10:11.587404: step 10255, loss 0.000590477, acc 1, f1 1\n",
      "2017-11-15T23:10:12.090827: step 10260, loss 0.0102291, acc 0.996855, f1 0.996854\n",
      "Current epoch:  570\n",
      "2017-11-15T23:10:12.649939: step 10265, loss 0.00741475, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:10:13.181063: step 10270, loss 0.0136423, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:10:13.699784: step 10275, loss 0.00221833, acc 0.999023, f1 0.999023\n",
      "Current epoch:  571\n",
      "2017-11-15T23:10:14.190292: step 10280, loss 0.0056852, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:10:14.711376: step 10285, loss 0.00108752, acc 1, f1 1\n",
      "2017-11-15T23:10:15.229288: step 10290, loss 0.00162819, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:10:15.758126: step 10295, loss 0.0201959, acc 0.998047, f1 0.998046\n",
      "Current epoch:  572\n",
      "2017-11-15T23:10:16.247829: step 10300, loss 0.106985, acc 0.963867, f1 0.964083\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:10:16.938308: step 10300, loss 1.78834, acc 0.56931, f1 0.565269\n",
      "\n",
      "2017-11-15T23:10:17.458276: step 10305, loss 0.00940046, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:10:17.980332: step 10310, loss 0.0108078, acc 0.998047, f1 0.998046\n",
      "Current epoch:  573\n",
      "2017-11-15T23:10:18.468901: step 10315, loss 0.00389461, acc 1, f1 1\n",
      "2017-11-15T23:10:18.990039: step 10320, loss 0.00541937, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:10:19.509426: step 10325, loss 0.00364408, acc 1, f1 1\n",
      "2017-11-15T23:10:20.025771: step 10330, loss 0.00568047, acc 0.998047, f1 0.998047\n",
      "Current epoch:  574\n",
      "2017-11-15T23:10:20.517070: step 10335, loss 0.00669142, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:10:21.037353: step 10340, loss 0.00626739, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:10:21.557591: step 10345, loss 0.00407577, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:10:22.057511: step 10350, loss 0.00589001, acc 0.998428, f1 0.998428\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:10:22.722090: step 10350, loss 1.78898, acc 0.583123, f1 0.582637\n",
      "\n",
      "Current epoch:  575\n",
      "2017-11-15T23:10:23.243677: step 10355, loss 0.00394245, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:10:23.766009: step 10360, loss 0.00566355, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:10:24.283497: step 10365, loss 0.00501256, acc 0.99707, f1 0.997071\n",
      "Current epoch:  576\n",
      "2017-11-15T23:10:24.778372: step 10370, loss 0.00162185, acc 1, f1 1\n",
      "2017-11-15T23:10:25.299973: step 10375, loss 0.00373422, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:10:25.823035: step 10380, loss 0.00164415, acc 1, f1 1\n",
      "2017-11-15T23:10:26.344622: step 10385, loss 0.00441803, acc 0.99707, f1 0.997072\n",
      "Current epoch:  577\n",
      "2017-11-15T23:10:26.844028: step 10390, loss 0.00200653, acc 1, f1 1\n",
      "2017-11-15T23:10:27.371187: step 10395, loss 0.00247284, acc 1, f1 1\n",
      "2017-11-15T23:10:27.899652: step 10400, loss 0.00829711, acc 0.995117, f1 0.995122\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:10:28.569344: step 10400, loss 1.86618, acc 0.582784, f1 0.582437\n",
      "\n",
      "Current epoch:  578\n",
      "2017-11-15T23:10:29.061142: step 10405, loss 0.00344682, acc 1, f1 1\n",
      "2017-11-15T23:10:29.584969: step 10410, loss 0.00270583, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:10:30.107059: step 10415, loss 0.00239035, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:10:30.636608: step 10420, loss 0.00394763, acc 0.998047, f1 0.998048\n",
      "Current epoch:  579\n",
      "2017-11-15T23:10:31.129878: step 10425, loss 0.00522989, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:10:31.653061: step 10430, loss 0.00427553, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:10:32.176705: step 10435, loss 0.00273039, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:10:32.662615: step 10440, loss 0.000985726, acc 1, f1 1\n",
      "Current epoch:  580\n",
      "2017-11-15T23:10:33.200341: step 10445, loss 0.0055658, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:10:33.727801: step 10450, loss 0.00361662, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:10:34.388462: step 10450, loss 1.96003, acc 0.586952, f1 0.585037\n",
      "\n",
      "2017-11-15T23:10:34.949609: step 10455, loss 0.000685769, acc 1, f1 1\n",
      "Current epoch:  581\n",
      "2017-11-15T23:10:35.444921: step 10460, loss 0.00339937, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:10:35.964555: step 10465, loss 0.00220821, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:10:36.483711: step 10470, loss 0.00769062, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:10:37.007142: step 10475, loss 0.00840549, acc 0.99707, f1 0.997069\n",
      "Current epoch:  582\n",
      "2017-11-15T23:10:37.500545: step 10480, loss 0.00196484, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:10:38.020980: step 10485, loss 0.00361602, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:10:38.567145: step 10490, loss 0.00101496, acc 1, f1 1\n",
      "Current epoch:  583\n",
      "2017-11-15T23:10:39.062794: step 10495, loss 0.00438759, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:10:39.593606: step 10500, loss 0.00110255, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:10:40.248599: step 10500, loss 2.03325, acc 0.570764, f1 0.57304\n",
      "\n",
      "2017-11-15T23:10:40.773757: step 10505, loss 0.00702395, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:10:41.298556: step 10510, loss 0.00358771, acc 0.998047, f1 0.998047\n",
      "Current epoch:  584\n",
      "2017-11-15T23:10:41.795148: step 10515, loss 0.000530078, acc 1, f1 1\n",
      "2017-11-15T23:10:42.311539: step 10520, loss 0.0127982, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:10:42.841050: step 10525, loss 0.00951289, acc 0.996094, f1 0.996093\n",
      "2017-11-15T23:10:43.332411: step 10530, loss 0.00516089, acc 0.996855, f1 0.996855\n",
      "Current epoch:  585\n",
      "2017-11-15T23:10:43.871382: step 10535, loss 0.00476988, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:10:44.411879: step 10540, loss 0.0120964, acc 0.996094, f1 0.996095\n",
      "2017-11-15T23:10:44.939751: step 10545, loss 0.00858352, acc 0.998047, f1 0.998047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch:  586\n",
      "2017-11-15T23:10:45.441971: step 10550, loss 0.00746221, acc 0.99707, f1 0.99707\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:10:46.106660: step 10550, loss 2.35595, acc 0.545899, f1 0.540091\n",
      "\n",
      "2017-11-15T23:10:46.630249: step 10555, loss 0.00849598, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:10:47.160270: step 10560, loss 0.00831044, acc 0.99707, f1 0.997074\n",
      "2017-11-15T23:10:47.678894: step 10565, loss 0.00564887, acc 0.999023, f1 0.999024\n",
      "Current epoch:  587\n",
      "2017-11-15T23:10:48.168322: step 10570, loss 0.00584278, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:10:48.691438: step 10575, loss 0.00700599, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:10:49.328268: step 10580, loss 0.00590228, acc 0.998047, f1 0.998047\n",
      "Current epoch:  588\n",
      "2017-11-15T23:10:49.844311: step 10585, loss 0.00612418, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:10:50.367899: step 10590, loss 0.0034429, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:10:50.889348: step 10595, loss 0.00180181, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:10:51.415761: step 10600, loss 0.00455225, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:10:52.051200: step 10600, loss 2.04555, acc 0.593253, f1 0.589717\n",
      "\n",
      "Current epoch:  589\n",
      "2017-11-15T23:10:52.542895: step 10605, loss 0.00626464, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:10:53.055821: step 10610, loss 0.00446872, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:10:53.578235: step 10615, loss 0.00589214, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:10:54.067875: step 10620, loss 0.115284, acc 0.951258, f1 0.951416\n",
      "Current epoch:  590\n",
      "2017-11-15T23:10:54.591650: step 10625, loss 0.873489, acc 0.71875, f1 0.695516\n",
      "2017-11-15T23:10:55.127728: step 10630, loss 0.0132486, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:10:55.649007: step 10635, loss 0.0157213, acc 0.995117, f1 0.995116\n",
      "Current epoch:  591\n",
      "2017-11-15T23:10:56.141075: step 10640, loss 0.0101536, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:10:56.662009: step 10645, loss 0.0101503, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:10:57.182499: step 10650, loss 0.00797413, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:10:57.848538: step 10650, loss 1.67661, acc 0.588406, f1 0.588437\n",
      "\n",
      "2017-11-15T23:10:58.359973: step 10655, loss 0.00473009, acc 1, f1 1\n",
      "Current epoch:  592\n",
      "2017-11-15T23:10:58.850592: step 10660, loss 0.00546078, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:10:59.369794: step 10665, loss 0.00910034, acc 0.996094, f1 0.996093\n",
      "2017-11-15T23:10:59.889232: step 10670, loss 0.00308009, acc 1, f1 1\n",
      "Current epoch:  593\n",
      "2017-11-15T23:11:00.393831: step 10675, loss 0.00473828, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:11:00.921893: step 10680, loss 0.00360109, acc 1, f1 1\n",
      "2017-11-15T23:11:01.442788: step 10685, loss 0.00333493, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:11:01.963269: step 10690, loss 0.00446765, acc 0.998047, f1 0.998047\n",
      "Current epoch:  594\n",
      "2017-11-15T23:11:02.454134: step 10695, loss 0.00517056, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:11:02.979202: step 10700, loss 0.00561855, acc 0.99707, f1 0.99707\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:11:03.696781: step 10700, loss 1.76377, acc 0.587437, f1 0.587455\n",
      "\n",
      "2017-11-15T23:11:04.219630: step 10705, loss 0.0069851, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:11:04.709003: step 10710, loss 0.00497643, acc 0.996855, f1 0.996855\n",
      "Current epoch:  595\n",
      "2017-11-15T23:11:05.220931: step 10715, loss 0.00305465, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:11:05.741395: step 10720, loss 0.00704671, acc 0.996094, f1 0.996093\n",
      "2017-11-15T23:11:06.261067: step 10725, loss 0.00388857, acc 0.998047, f1 0.998047\n",
      "Current epoch:  596\n",
      "2017-11-15T23:11:06.746703: step 10730, loss 0.00356594, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:11:07.266056: step 10735, loss 0.00353057, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:11:07.782959: step 10740, loss 0.0046101, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:11:08.299650: step 10745, loss 0.00270949, acc 0.999023, f1 0.999023\n",
      "Current epoch:  597\n",
      "2017-11-15T23:11:08.790843: step 10750, loss 0.00466337, acc 0.99707, f1 0.99707\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:11:09.455313: step 10750, loss 1.8536, acc 0.582348, f1 0.583325\n",
      "\n",
      "2017-11-15T23:11:09.971402: step 10755, loss 0.00443312, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:11:10.496838: step 10760, loss 0.0101289, acc 0.995117, f1 0.995117\n",
      "Current epoch:  598\n",
      "2017-11-15T23:11:10.989353: step 10765, loss 0.00298597, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:11:11.515489: step 10770, loss 0.00165968, acc 1, f1 1\n",
      "2017-11-15T23:11:12.041591: step 10775, loss 0.0104339, acc 0.994141, f1 0.994141\n",
      "2017-11-15T23:11:12.561527: step 10780, loss 0.00611787, acc 0.99707, f1 0.997071\n",
      "Current epoch:  599\n",
      "2017-11-15T23:11:13.049655: step 10785, loss 0.00540391, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:11:13.568842: step 10790, loss 0.00713219, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:11:14.090880: step 10795, loss 0.00318764, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:11:14.582790: step 10800, loss 0.00826861, acc 0.995283, f1 0.995273\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:11:15.243302: step 10800, loss 1.95681, acc 0.577452, f1 0.580568\n",
      "\n",
      "Current epoch:  600\n",
      "2017-11-15T23:11:15.768043: step 10805, loss 0.0024523, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:11:16.288791: step 10810, loss 0.00421352, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:11:16.815007: step 10815, loss 0.0129918, acc 0.996094, f1 0.996095\n",
      "Current epoch:  601\n",
      "2017-11-15T23:11:17.319835: step 10820, loss 0.00756985, acc 0.998047, f1 0.998052\n",
      "2017-11-15T23:11:17.842820: step 10825, loss 0.00216936, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:11:18.363061: step 10830, loss 0.0104443, acc 0.996094, f1 0.996092\n",
      "2017-11-15T23:11:18.886790: step 10835, loss 0.002706, acc 0.999023, f1 0.999023\n",
      "Current epoch:  602\n",
      "2017-11-15T23:11:19.374370: step 10840, loss 0.00873346, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:11:19.899799: step 10845, loss 0.00472844, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:11:20.425259: step 10850, loss 0.0010283, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:11:21.081105: step 10850, loss 2.0194, acc 0.585934, f1 0.585054\n",
      "\n",
      "Current epoch:  603\n",
      "2017-11-15T23:11:21.576493: step 10855, loss 0.00120471, acc 1, f1 1\n",
      "2017-11-15T23:11:22.088614: step 10860, loss 0.0039369, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:11:22.612929: step 10865, loss 0.000679565, acc 1, f1 1\n",
      "2017-11-15T23:11:23.129851: step 10870, loss 0.0068869, acc 0.998047, f1 0.998047\n",
      "Current epoch:  604\n",
      "2017-11-15T23:11:23.616550: step 10875, loss 0.00596948, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:11:24.131551: step 10880, loss 0.0116568, acc 0.996094, f1 0.996096\n",
      "2017-11-15T23:11:24.657890: step 10885, loss 0.00366988, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:11:25.150904: step 10890, loss 0.00349845, acc 0.998428, f1 0.998428\n",
      "Current epoch:  605\n",
      "2017-11-15T23:11:25.676598: step 10895, loss 0.00395701, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:11:26.201438: step 10900, loss 0.0068979, acc 0.99707, f1 0.99707\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:11:26.862577: step 10900, loss 2.04175, acc 0.589036, f1 0.585762\n",
      "\n",
      "2017-11-15T23:11:27.374228: step 10905, loss 0.00489511, acc 0.998047, f1 0.998047\n",
      "Current epoch:  606\n",
      "2017-11-15T23:11:27.876900: step 10910, loss 0.00593809, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:11:28.405852: step 10915, loss 0.004323, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:11:28.931784: step 10920, loss 0.00356803, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:11:29.456592: step 10925, loss 0.00866191, acc 0.99707, f1 0.997071\n",
      "Current epoch:  607\n",
      "2017-11-15T23:11:29.950073: step 10930, loss 0.00649279, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:11:30.476043: step 10935, loss 0.000971601, acc 1, f1 1\n",
      "2017-11-15T23:11:30.999461: step 10940, loss 0.00809122, acc 0.998047, f1 0.998046\n",
      "Current epoch:  608\n",
      "2017-11-15T23:11:31.498580: step 10945, loss 0.0131847, acc 0.996094, f1 0.996093\n",
      "2017-11-15T23:11:32.015800: step 10950, loss 0.00696506, acc 0.99707, f1 0.99707\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:11:32.670434: step 10950, loss 2.08012, acc 0.570521, f1 0.573804\n",
      "\n",
      "2017-11-15T23:11:33.187298: step 10955, loss 0.00364006, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:11:33.723959: step 10960, loss 0.0111488, acc 0.996094, f1 0.996094\n",
      "Current epoch:  609\n",
      "2017-11-15T23:11:34.214078: step 10965, loss 0.000779578, acc 1, f1 1\n",
      "2017-11-15T23:11:34.736516: step 10970, loss 0.00087553, acc 1, f1 1\n",
      "2017-11-15T23:11:35.261341: step 10975, loss 0.0067306, acc 0.99707, f1 0.99707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:11:35.751766: step 10980, loss 0.0238534, acc 0.995283, f1 0.995286\n",
      "Current epoch:  610\n",
      "2017-11-15T23:11:36.278671: step 10985, loss 0.348101, acc 0.855469, f1 0.854141\n",
      "2017-11-15T23:11:36.797938: step 10990, loss 0.00893418, acc 1, f1 1\n",
      "2017-11-15T23:11:37.317838: step 10995, loss 0.0107831, acc 0.998047, f1 0.998046\n",
      "Current epoch:  611\n",
      "2017-11-15T23:11:37.812375: step 11000, loss 0.00585775, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:11:38.477088: step 11000, loss 1.71046, acc 0.581863, f1 0.581918\n",
      "\n",
      "2017-11-15T23:11:39.004045: step 11005, loss 0.0061886, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:11:39.534948: step 11010, loss 0.00914884, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:11:40.055208: step 11015, loss 0.00491055, acc 0.999023, f1 0.999022\n",
      "Current epoch:  612\n",
      "2017-11-15T23:11:40.555407: step 11020, loss 0.00468465, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:11:41.071848: step 11025, loss 0.0058803, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:11:41.587559: step 11030, loss 0.00734826, acc 0.996094, f1 0.996094\n",
      "Current epoch:  613\n",
      "2017-11-15T23:11:42.080203: step 11035, loss 0.00303802, acc 1, f1 1\n",
      "2017-11-15T23:11:42.602365: step 11040, loss 0.00552862, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:11:43.117559: step 11045, loss 0.00207139, acc 1, f1 1\n",
      "2017-11-15T23:11:43.633566: step 11050, loss 0.00463013, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:11:44.277107: step 11050, loss 1.80604, acc 0.583559, f1 0.583452\n",
      "\n",
      "Current epoch:  614\n",
      "2017-11-15T23:11:44.775912: step 11055, loss 0.00216606, acc 1, f1 1\n",
      "2017-11-15T23:11:45.294830: step 11060, loss 0.0017435, acc 1, f1 1\n",
      "2017-11-15T23:11:45.816255: step 11065, loss 0.00593167, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:11:46.301750: step 11070, loss 0.0024238, acc 1, f1 1\n",
      "Current epoch:  615\n",
      "2017-11-15T23:11:46.822893: step 11075, loss 0.00131399, acc 1, f1 1\n",
      "2017-11-15T23:11:47.341675: step 11080, loss 0.00420915, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:11:47.861262: step 11085, loss 0.00123174, acc 1, f1 1\n",
      "Current epoch:  616\n",
      "2017-11-15T23:11:48.357115: step 11090, loss 0.0024763, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:11:48.882055: step 11095, loss 0.00369938, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:11:49.401405: step 11100, loss 0.00531431, acc 0.99707, f1 0.99707\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:11:50.076128: step 11100, loss 1.89163, acc 0.583462, f1 0.582833\n",
      "\n",
      "2017-11-15T23:11:50.603551: step 11105, loss 0.00876805, acc 0.995117, f1 0.995121\n",
      "Current epoch:  617\n",
      "2017-11-15T23:11:51.096889: step 11110, loss 0.00308961, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:11:51.627876: step 11115, loss 0.00355258, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:11:52.150751: step 11120, loss 0.00447604, acc 0.998047, f1 0.998047\n",
      "Current epoch:  618\n",
      "2017-11-15T23:11:52.646759: step 11125, loss 0.00185171, acc 1, f1 1\n",
      "2017-11-15T23:11:53.164085: step 11130, loss 0.00400683, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:11:53.687692: step 11135, loss 0.00381667, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:11:54.210939: step 11140, loss 0.00439769, acc 0.998047, f1 0.998047\n",
      "Current epoch:  619\n",
      "2017-11-15T23:11:54.705314: step 11145, loss 0.0039069, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:11:55.223128: step 11150, loss 0.00107204, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:11:55.925581: step 11150, loss 1.9853, acc 0.584819, f1 0.584796\n",
      "\n",
      "2017-11-15T23:11:56.442565: step 11155, loss 0.00859743, acc 0.99707, f1 0.997066\n",
      "2017-11-15T23:11:56.933696: step 11160, loss 0.0100365, acc 0.995283, f1 0.995285\n",
      "Current epoch:  620\n",
      "2017-11-15T23:11:57.458896: step 11165, loss 0.0020433, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:11:57.976861: step 11170, loss 0.00607765, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:11:58.497931: step 11175, loss 0.00432542, acc 0.999023, f1 0.999022\n",
      "Current epoch:  621\n",
      "2017-11-15T23:11:58.993319: step 11180, loss 0.00525172, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:11:59.515767: step 11185, loss 0.00646261, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:12:00.031678: step 11190, loss 0.000522421, acc 1, f1 1\n",
      "2017-11-15T23:12:00.559160: step 11195, loss 0.0102276, acc 0.996094, f1 0.996102\n",
      "Current epoch:  622\n",
      "2017-11-15T23:12:01.069256: step 11200, loss 0.00540574, acc 0.998047, f1 0.998048\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:12:01.737468: step 11200, loss 2.06198, acc 0.585401, f1 0.584489\n",
      "\n",
      "2017-11-15T23:12:02.258562: step 11205, loss 0.00417563, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:12:02.782783: step 11210, loss 0.00563967, acc 0.999023, f1 0.999022\n",
      "Current epoch:  623\n",
      "2017-11-15T23:12:03.275630: step 11215, loss 0.00580985, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:12:03.795100: step 11220, loss 0.00569498, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:12:04.316329: step 11225, loss 0.00351547, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:12:04.843973: step 11230, loss 0.00305787, acc 0.999023, f1 0.999022\n",
      "Current epoch:  624\n",
      "2017-11-15T23:12:05.331874: step 11235, loss 0.00461042, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:12:05.862854: step 11240, loss 0.00254557, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:12:06.394569: step 11245, loss 0.00336051, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:12:06.890759: step 11250, loss 0.0141264, acc 0.995283, f1 0.995283\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:12:07.547725: step 11250, loss 2.16829, acc 0.568195, f1 0.566017\n",
      "\n",
      "Current epoch:  625\n",
      "2017-11-15T23:12:08.069538: step 11255, loss 0.000601553, acc 1, f1 1\n",
      "2017-11-15T23:12:08.587145: step 11260, loss 0.00348756, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:12:09.109076: step 11265, loss 0.0131297, acc 0.995117, f1 0.995119\n",
      "Current epoch:  626\n",
      "2017-11-15T23:12:09.607135: step 11270, loss 0.00325238, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:12:10.132338: step 11275, loss 0.00257305, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:12:10.651969: step 11280, loss 0.0103264, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:12:11.173024: step 11285, loss 0.00919139, acc 0.998047, f1 0.998043\n",
      "Current epoch:  627\n",
      "2017-11-15T23:12:11.667801: step 11290, loss 0.0057156, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:12:12.203128: step 11295, loss 0.00211126, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:12:12.769688: step 11300, loss 0.00696123, acc 0.996094, f1 0.996094\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:12:13.430173: step 11300, loss 2.07165, acc 0.594949, f1 0.593712\n",
      "\n",
      "Current epoch:  628\n",
      "2017-11-15T23:12:13.929203: step 11305, loss 0.0014272, acc 1, f1 1\n",
      "2017-11-15T23:12:14.452423: step 11310, loss 0.00036141, acc 1, f1 1\n",
      "2017-11-15T23:12:14.976171: step 11315, loss 0.00289185, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:12:15.501160: step 11320, loss 0.00705286, acc 0.998047, f1 0.998047\n",
      "Current epoch:  629\n",
      "2017-11-15T23:12:15.991012: step 11325, loss 0.00434496, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:12:16.518345: step 11330, loss 0.00180593, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:12:17.041228: step 11335, loss 0.0027358, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:12:17.546121: step 11340, loss 0.0252408, acc 0.993711, f1 0.993688\n",
      "Current epoch:  630\n",
      "2017-11-15T23:12:18.078596: step 11345, loss 0.805446, acc 0.817383, f1 0.763295\n",
      "2017-11-15T23:12:18.603643: step 11350, loss 1.08912, acc 0.669922, f1 0.619172\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:12:19.265385: step 11350, loss 2.54133, acc 0.565481, f1 0.507092\n",
      "\n",
      "2017-11-15T23:12:19.784388: step 11355, loss 0.0166151, acc 0.99707, f1 0.99707\n",
      "Current epoch:  631\n",
      "2017-11-15T23:12:20.271236: step 11360, loss 0.0134411, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:12:20.791185: step 11365, loss 0.00752344, acc 1, f1 1\n",
      "2017-11-15T23:12:21.311286: step 11370, loss 0.0084781, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:12:21.835827: step 11375, loss 0.00850138, acc 0.999023, f1 0.999023\n",
      "Current epoch:  632\n",
      "2017-11-15T23:12:22.325735: step 11380, loss 0.014514, acc 0.996094, f1 0.996096\n",
      "2017-11-15T23:12:22.860142: step 11385, loss 0.0106471, acc 0.996094, f1 0.996093\n",
      "2017-11-15T23:12:23.390929: step 11390, loss 0.0066304, acc 0.998047, f1 0.998047\n",
      "Current epoch:  633\n",
      "2017-11-15T23:12:23.872785: step 11395, loss 0.00401292, acc 1, f1 1\n",
      "2017-11-15T23:12:24.391906: step 11400, loss 0.00683553, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:12:25.045419: step 11400, loss 1.75648, acc 0.585159, f1 0.584046\n",
      "\n",
      "2017-11-15T23:12:25.561452: step 11405, loss 0.00835119, acc 0.998047, f1 0.998047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:12:26.075545: step 11410, loss 0.00413535, acc 0.999023, f1 0.999023\n",
      "Current epoch:  634\n",
      "2017-11-15T23:12:26.559856: step 11415, loss 0.0069412, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:12:27.077451: step 11420, loss 0.00316292, acc 1, f1 1\n",
      "2017-11-15T23:12:27.598941: step 11425, loss 0.0041147, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:12:28.087662: step 11430, loss 0.00318913, acc 1, f1 1\n",
      "Current epoch:  635\n",
      "2017-11-15T23:12:28.618696: step 11435, loss 0.00267778, acc 1, f1 1\n",
      "2017-11-15T23:12:29.141030: step 11440, loss 0.00247791, acc 1, f1 1\n",
      "2017-11-15T23:12:29.664022: step 11445, loss 0.00561918, acc 0.999023, f1 0.999024\n",
      "Current epoch:  636\n",
      "2017-11-15T23:12:30.162037: step 11450, loss 0.00454094, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:12:30.823381: step 11450, loss 1.82707, acc 0.585692, f1 0.583644\n",
      "\n",
      "2017-11-15T23:12:31.340709: step 11455, loss 0.00358464, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:12:31.867563: step 11460, loss 0.00598318, acc 0.996094, f1 0.996092\n",
      "2017-11-15T23:12:32.385004: step 11465, loss 0.00340461, acc 0.998047, f1 0.998047\n",
      "Current epoch:  637\n",
      "2017-11-15T23:12:32.874967: step 11470, loss 0.00318688, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:12:33.394493: step 11475, loss 0.00142126, acc 1, f1 1\n",
      "2017-11-15T23:12:33.929949: step 11480, loss 0.00547951, acc 0.99707, f1 0.997067\n",
      "Current epoch:  638\n",
      "2017-11-15T23:12:34.438637: step 11485, loss 0.00200308, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:12:34.954123: step 11490, loss 0.00181735, acc 1, f1 1\n",
      "2017-11-15T23:12:35.475828: step 11495, loss 0.00408566, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:12:36.000761: step 11500, loss 0.00324167, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:12:36.653701: step 11500, loss 1.92204, acc 0.580215, f1 0.579805\n",
      "\n",
      "Current epoch:  639\n",
      "2017-11-15T23:12:37.182121: step 11505, loss 0.00086718, acc 1, f1 1\n",
      "2017-11-15T23:12:37.702322: step 11510, loss 0.00497015, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:12:38.222831: step 11515, loss 0.00311381, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:12:38.717209: step 11520, loss 0.00932104, acc 0.995283, f1 0.995283\n",
      "Current epoch:  640\n",
      "2017-11-15T23:12:39.240372: step 11525, loss 0.00780437, acc 0.995117, f1 0.995118\n",
      "2017-11-15T23:12:39.774835: step 11530, loss 0.00824267, acc 0.996094, f1 0.996085\n",
      "2017-11-15T23:12:40.297006: step 11535, loss 0.000770469, acc 1, f1 1\n",
      "Current epoch:  641\n",
      "2017-11-15T23:12:40.793919: step 11540, loss 0.00919845, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:12:41.311594: step 11545, loss 0.003769, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:12:41.832833: step 11550, loss 0.0031398, acc 0.999023, f1 0.999022\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:12:42.493138: step 11550, loss 2.03547, acc 0.573284, f1 0.573732\n",
      "\n",
      "2017-11-15T23:12:43.011053: step 11555, loss 0.00247085, acc 0.999023, f1 0.999023\n",
      "Current epoch:  642\n",
      "2017-11-15T23:12:43.501698: step 11560, loss 0.000881778, acc 1, f1 1\n",
      "2017-11-15T23:12:44.022634: step 11565, loss 0.0052752, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:12:44.545565: step 11570, loss 0.00285042, acc 0.999023, f1 0.999023\n",
      "Current epoch:  643\n",
      "2017-11-15T23:12:45.051139: step 11575, loss 0.00128438, acc 1, f1 1\n",
      "2017-11-15T23:12:45.575329: step 11580, loss 0.00288427, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:12:46.097331: step 11585, loss 0.00691963, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:12:46.626767: step 11590, loss 0.00446609, acc 0.998047, f1 0.998052\n",
      "Current epoch:  644\n",
      "2017-11-15T23:12:47.118631: step 11595, loss 0.00863806, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:12:47.642562: step 11600, loss 0.000414239, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:12:48.301270: step 11600, loss 2.05734, acc 0.58608, f1 0.585011\n",
      "\n",
      "2017-11-15T23:12:48.823903: step 11605, loss 0.0118311, acc 0.995117, f1 0.995118\n",
      "2017-11-15T23:12:49.310445: step 11610, loss 0.0119369, acc 0.995283, f1 0.995283\n",
      "Current epoch:  645\n",
      "2017-11-15T23:12:49.850748: step 11615, loss 0.00375762, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:12:50.378085: step 11620, loss 0.000469273, acc 1, f1 1\n",
      "2017-11-15T23:12:50.912025: step 11625, loss 0.00428153, acc 0.998047, f1 0.998047\n",
      "Current epoch:  646\n",
      "2017-11-15T23:12:51.406830: step 11630, loss 0.00175915, acc 1, f1 1\n",
      "2017-11-15T23:12:51.933530: step 11635, loss 0.00419609, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:12:52.457631: step 11640, loss 0.00514108, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:12:52.982542: step 11645, loss 0.00553448, acc 0.998047, f1 0.998047\n",
      "Current epoch:  647\n",
      "2017-11-15T23:12:53.470639: step 11650, loss 0.00469921, acc 0.998047, f1 0.998051\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:12:54.126714: step 11650, loss 2.23743, acc 0.576047, f1 0.56913\n",
      "\n",
      "2017-11-15T23:12:54.653270: step 11655, loss 0.000931224, acc 1, f1 1\n",
      "2017-11-15T23:12:55.178241: step 11660, loss 0.00801913, acc 0.998047, f1 0.998046\n",
      "Current epoch:  648\n",
      "2017-11-15T23:12:55.674446: step 11665, loss 0.0024151, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:12:56.212106: step 11670, loss 0.00915496, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:12:56.751361: step 11675, loss 0.0107741, acc 0.996094, f1 0.996093\n",
      "2017-11-15T23:12:57.265356: step 11680, loss 0.000846531, acc 1, f1 1\n",
      "Current epoch:  649\n",
      "2017-11-15T23:12:57.757215: step 11685, loss 0.00651191, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:12:58.265898: step 11690, loss 0.000624943, acc 1, f1 1\n",
      "2017-11-15T23:12:58.777818: step 11695, loss 0.0101216, acc 0.996094, f1 0.996104\n",
      "2017-11-15T23:12:59.263037: step 11700, loss 2.39855, acc 0.841195, f1 0.77469\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:12:59.898342: step 11700, loss 3.84523, acc 0.453567, f1 0.391844\n",
      "\n",
      "Current epoch:  650\n",
      "2017-11-15T23:13:00.417168: step 11705, loss 0.764457, acc 0.734375, f1 0.694276\n",
      "2017-11-15T23:13:00.927238: step 11710, loss 0.0266286, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:13:01.444195: step 11715, loss 0.0194293, acc 0.999023, f1 0.999025\n",
      "Current epoch:  651\n",
      "2017-11-15T23:13:01.931693: step 11720, loss 0.014803, acc 0.996094, f1 0.996092\n",
      "2017-11-15T23:13:02.441095: step 11725, loss 0.0146969, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:13:02.951428: step 11730, loss 0.00950575, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:13:03.458194: step 11735, loss 0.00830889, acc 0.999023, f1 0.999025\n",
      "Current epoch:  652\n",
      "2017-11-15T23:13:03.940656: step 11740, loss 0.00936, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:13:04.449482: step 11745, loss 0.00749436, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:13:04.960557: step 11750, loss 0.00468226, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:13:05.603910: step 11750, loss 1.7058, acc 0.58671, f1 0.585907\n",
      "\n",
      "Current epoch:  653\n",
      "2017-11-15T23:13:06.085864: step 11755, loss 0.00570267, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:13:06.599089: step 11760, loss 0.0040142, acc 1, f1 1\n",
      "2017-11-15T23:13:07.124849: step 11765, loss 0.0060371, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:13:07.639768: step 11770, loss 0.00379265, acc 0.999023, f1 0.999023\n",
      "Current epoch:  654\n",
      "2017-11-15T23:13:08.121699: step 11775, loss 0.0045421, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:13:08.635841: step 11780, loss 0.00498467, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:13:09.143908: step 11785, loss 0.00708859, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:13:09.621770: step 11790, loss 0.0020295, acc 1, f1 1\n",
      "Current epoch:  655\n",
      "2017-11-15T23:13:10.133162: step 11795, loss 0.00323427, acc 1, f1 1\n",
      "2017-11-15T23:13:10.645617: step 11800, loss 0.00557139, acc 0.99707, f1 0.997069\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:13:11.282150: step 11800, loss 1.80368, acc 0.586177, f1 0.585412\n",
      "\n",
      "2017-11-15T23:13:11.786144: step 11805, loss 0.00400145, acc 0.998047, f1 0.998048\n",
      "Current epoch:  656\n",
      "2017-11-15T23:13:12.271083: step 11810, loss 0.00185897, acc 1, f1 1\n",
      "2017-11-15T23:13:12.796615: step 11815, loss 0.00237837, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:13:13.311299: step 11820, loss 0.0031262, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:13:13.824534: step 11825, loss 0.00459061, acc 0.99707, f1 0.997071\n",
      "Current epoch:  657\n",
      "2017-11-15T23:13:14.302257: step 11830, loss 0.00374215, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:13:14.813803: step 11835, loss 0.00126945, acc 1, f1 1\n",
      "2017-11-15T23:13:15.325485: step 11840, loss 0.00874724, acc 0.994141, f1 0.99414\n",
      "Current epoch:  658\n",
      "2017-11-15T23:13:15.818570: step 11845, loss 0.00311222, acc 0.998047, f1 0.998048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:13:16.329712: step 11850, loss 0.00208486, acc 0.999023, f1 0.999022\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:13:16.967146: step 11850, loss 1.90176, acc 0.583317, f1 0.582687\n",
      "\n",
      "2017-11-15T23:13:17.475941: step 11855, loss 0.00239735, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:13:17.998427: step 11860, loss 0.0070958, acc 0.996094, f1 0.996094\n",
      "Current epoch:  659\n",
      "2017-11-15T23:13:18.488179: step 11865, loss 0.0035007, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:13:18.998287: step 11870, loss 0.00762168, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:13:19.509697: step 11875, loss 0.00472496, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:13:19.994572: step 11880, loss 0.00454055, acc 0.996855, f1 0.996855\n",
      "Current epoch:  660\n",
      "2017-11-15T23:13:20.509536: step 11885, loss 0.00299595, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:13:21.022068: step 11890, loss 0.00599136, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:13:21.539018: step 11895, loss 0.00359228, acc 0.999023, f1 0.999024\n",
      "Current epoch:  661\n",
      "2017-11-15T23:13:22.031240: step 11900, loss 0.0056979, acc 0.998047, f1 0.998042\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:13:22.659615: step 11900, loss 2.00635, acc 0.57309, f1 0.574771\n",
      "\n",
      "2017-11-15T23:13:23.168537: step 11905, loss 0.00508632, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:13:23.693786: step 11910, loss 0.00303393, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:13:24.307610: step 11915, loss 0.00171752, acc 0.999023, f1 0.999023\n",
      "Current epoch:  662\n",
      "2017-11-15T23:13:24.793346: step 11920, loss 0.00180563, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:13:25.304778: step 11925, loss 0.0079608, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:13:25.823212: step 11930, loss 0.00624647, acc 0.99707, f1 0.99707\n",
      "Current epoch:  663\n",
      "2017-11-15T23:13:26.309655: step 11935, loss 0.00995728, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:13:26.824442: step 11940, loss 0.00921482, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:13:27.336566: step 11945, loss 0.00521114, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:13:27.848167: step 11950, loss 0.00501046, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:13:28.506223: step 11950, loss 2.11609, acc 0.572024, f1 0.570332\n",
      "\n",
      "Current epoch:  664\n",
      "2017-11-15T23:13:28.999604: step 11955, loss 0.00262066, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:13:29.521023: step 11960, loss 0.00880282, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:13:30.030949: step 11965, loss 0.000714052, acc 1, f1 1\n",
      "2017-11-15T23:13:30.516623: step 11970, loss 0.00799813, acc 0.996855, f1 0.996859\n",
      "Current epoch:  665\n",
      "2017-11-15T23:13:31.030037: step 11975, loss 0.0119087, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:13:31.546964: step 11980, loss 0.0126723, acc 0.995117, f1 0.995116\n",
      "2017-11-15T23:13:32.059161: step 11985, loss 0.00463865, acc 0.998047, f1 0.99805\n",
      "Current epoch:  666\n",
      "2017-11-15T23:13:32.547649: step 11990, loss 0.00662391, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:13:33.056666: step 11995, loss 0.000659432, acc 1, f1 1\n",
      "2017-11-15T23:13:33.584139: step 12000, loss 0.00587688, acc 0.99707, f1 0.99707\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:13:34.242007: step 12000, loss 2.06177, acc 0.591314, f1 0.58953\n",
      "\n",
      "2017-11-15T23:13:34.772643: step 12005, loss 0.0054838, acc 0.999023, f1 0.999023\n",
      "Current epoch:  667\n",
      "2017-11-15T23:13:35.255408: step 12010, loss 0.00465029, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:13:35.762534: step 12015, loss 0.00175196, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:13:36.276146: step 12020, loss 0.0072789, acc 0.99707, f1 0.99707\n",
      "Current epoch:  668\n",
      "2017-11-15T23:13:36.762757: step 12025, loss 0.00103144, acc 1, f1 1\n",
      "2017-11-15T23:13:37.273506: step 12030, loss 0.000733635, acc 1, f1 1\n",
      "2017-11-15T23:13:37.786215: step 12035, loss 0.00923736, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:13:38.294929: step 12040, loss 0.0128384, acc 0.99707, f1 0.997071\n",
      "Current epoch:  669\n",
      "2017-11-15T23:13:38.780892: step 12045, loss 0.000539241, acc 1, f1 1\n",
      "2017-11-15T23:13:39.299814: step 12050, loss 0.000911072, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:13:39.950519: step 12050, loss 2.1041, acc 0.583075, f1 0.579413\n",
      "\n",
      "2017-11-15T23:13:40.470137: step 12055, loss 0.00328307, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:13:40.957258: step 12060, loss 0.00348795, acc 0.998428, f1 0.998428\n",
      "Current epoch:  670\n",
      "2017-11-15T23:13:41.476909: step 12065, loss 0.0107018, acc 0.996094, f1 0.996103\n",
      "2017-11-15T23:13:41.996001: step 12070, loss 0.00488503, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:13:42.531615: step 12075, loss 0.0185843, acc 0.996094, f1 0.99609\n",
      "Current epoch:  671\n",
      "2017-11-15T23:13:43.017078: step 12080, loss 0.654474, acc 0.731445, f1 0.721896\n",
      "2017-11-15T23:13:43.528455: step 12085, loss 0.00406677, acc 1, f1 1\n",
      "2017-11-15T23:13:44.038290: step 12090, loss 0.0110549, acc 0.99707, f1 0.997074\n",
      "2017-11-15T23:13:44.549988: step 12095, loss 0.00532442, acc 0.999023, f1 0.999025\n",
      "Current epoch:  672\n",
      "2017-11-15T23:13:45.031697: step 12100, loss 0.00547247, acc 0.998047, f1 0.998046\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:13:45.678234: step 12100, loss 1.74238, acc 0.584141, f1 0.583857\n",
      "\n",
      "2017-11-15T23:13:46.186762: step 12105, loss 0.00585387, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:13:46.701554: step 12110, loss 0.00289113, acc 1, f1 1\n",
      "Current epoch:  673\n",
      "2017-11-15T23:13:47.188595: step 12115, loss 0.00331454, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:13:47.699664: step 12120, loss 0.00384295, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:13:48.207062: step 12125, loss 0.00259496, acc 1, f1 1\n",
      "2017-11-15T23:13:48.716433: step 12130, loss 0.00273247, acc 0.999023, f1 0.999023\n",
      "Current epoch:  674\n",
      "2017-11-15T23:13:49.199309: step 12135, loss 0.00441423, acc 0.998047, f1 0.998051\n",
      "2017-11-15T23:13:49.712029: step 12140, loss 0.00266227, acc 1, f1 1\n",
      "2017-11-15T23:13:50.229374: step 12145, loss 0.0015227, acc 1, f1 1\n",
      "2017-11-15T23:13:50.709734: step 12150, loss 0.00840728, acc 0.993711, f1 0.993711\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:13:51.382597: step 12150, loss 1.81061, acc 0.584286, f1 0.584247\n",
      "\n",
      "Current epoch:  675\n",
      "2017-11-15T23:13:51.902180: step 12155, loss 0.00275511, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:13:52.420421: step 12160, loss 0.00624046, acc 0.995117, f1 0.995117\n",
      "2017-11-15T23:13:52.941243: step 12165, loss 0.0011662, acc 1, f1 1\n",
      "Current epoch:  676\n",
      "2017-11-15T23:13:53.431760: step 12170, loss 0.00178613, acc 1, f1 1\n",
      "2017-11-15T23:13:53.961865: step 12175, loss 0.00251713, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:13:54.486013: step 12180, loss 0.00390839, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:13:55.018619: step 12185, loss 0.00331091, acc 0.998047, f1 0.998048\n",
      "Current epoch:  677\n",
      "2017-11-15T23:13:55.507491: step 12190, loss 0.00277533, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:13:56.032451: step 12195, loss 0.00440161, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:13:56.573480: step 12200, loss 0.00328683, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:13:57.272360: step 12200, loss 1.88306, acc 0.58797, f1 0.586764\n",
      "\n",
      "Current epoch:  678\n",
      "2017-11-15T23:13:57.772930: step 12205, loss 0.00457721, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:13:58.295899: step 12210, loss 0.00461156, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:13:58.835432: step 12215, loss 0.00075706, acc 1, f1 1\n",
      "2017-11-15T23:13:59.363228: step 12220, loss 0.00403537, acc 0.998047, f1 0.998046\n",
      "Current epoch:  679\n",
      "2017-11-15T23:13:59.858290: step 12225, loss 0.00485477, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:14:00.373382: step 12230, loss 0.00535278, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:14:00.890307: step 12235, loss 0.000728054, acc 1, f1 1\n",
      "2017-11-15T23:14:01.378857: step 12240, loss 0.00631797, acc 0.996855, f1 0.996855\n",
      "Current epoch:  680\n",
      "2017-11-15T23:14:01.908510: step 12245, loss 0.00217585, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:14:02.431658: step 12250, loss 0.00231785, acc 0.999023, f1 0.999024\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:14:03.065460: step 12250, loss 1.99258, acc 0.588503, f1 0.585943\n",
      "\n",
      "2017-11-15T23:14:03.577220: step 12255, loss 0.00585039, acc 0.998047, f1 0.998047\n",
      "Current epoch:  681\n",
      "2017-11-15T23:14:04.066481: step 12260, loss 0.00341268, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:14:04.585911: step 12265, loss 0.00462064, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:14:05.098447: step 12270, loss 0.00540757, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:14:05.618413: step 12275, loss 0.0108606, acc 0.995117, f1 0.995117\n",
      "Current epoch:  682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:14:06.099401: step 12280, loss 0.00139231, acc 1, f1 1\n",
      "2017-11-15T23:14:06.622785: step 12285, loss 0.00333047, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:14:07.144664: step 12290, loss 0.0181312, acc 0.994141, f1 0.994138\n",
      "Current epoch:  683\n",
      "2017-11-15T23:14:07.655560: step 12295, loss 0.000734925, acc 1, f1 1\n",
      "2017-11-15T23:14:08.181746: step 12300, loss 0.0101857, acc 0.996094, f1 0.996098\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:14:08.837156: step 12300, loss 2.09128, acc 0.589036, f1 0.583151\n",
      "\n",
      "2017-11-15T23:14:09.358979: step 12305, loss 0.00193069, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:14:09.875281: step 12310, loss 0.00581745, acc 0.998047, f1 0.998047\n",
      "Current epoch:  684\n",
      "2017-11-15T23:14:10.363137: step 12315, loss 0.0100695, acc 0.996094, f1 0.996096\n",
      "2017-11-15T23:14:10.879581: step 12320, loss 0.00338866, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:14:11.398482: step 12325, loss 0.000431159, acc 1, f1 1\n",
      "2017-11-15T23:14:11.883346: step 12330, loss 0.00898634, acc 0.996855, f1 0.996853\n",
      "Current epoch:  685\n",
      "2017-11-15T23:14:12.401803: step 12335, loss 0.000374143, acc 1, f1 1\n",
      "2017-11-15T23:14:12.934997: step 12340, loss 0.00527086, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:14:13.451921: step 12345, loss 0.00324475, acc 0.998047, f1 0.998047\n",
      "Current epoch:  686\n",
      "2017-11-15T23:14:13.943884: step 12350, loss 0.000922654, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:14:14.607785: step 12350, loss 2.08874, acc 0.585934, f1 0.584562\n",
      "\n",
      "2017-11-15T23:14:15.125417: step 12355, loss 0.00043826, acc 1, f1 1\n",
      "2017-11-15T23:14:15.645116: step 12360, loss 0.00802483, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:14:16.168164: step 12365, loss 0.00128579, acc 1, f1 1\n",
      "Current epoch:  687\n",
      "2017-11-15T23:14:16.661078: step 12370, loss 0.00213231, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:14:17.186731: step 12375, loss 0.00997622, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:14:17.710493: step 12380, loss 0.00372853, acc 0.999023, f1 0.999023\n",
      "Current epoch:  688\n",
      "2017-11-15T23:14:18.204496: step 12385, loss 0.00312889, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:14:18.742278: step 12390, loss 0.00645581, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:14:19.262164: step 12395, loss 0.000457339, acc 1, f1 1\n",
      "2017-11-15T23:14:19.786165: step 12400, loss 0.00801077, acc 0.99707, f1 0.997072\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:14:20.441828: step 12400, loss 2.28758, acc 0.557241, f1 0.553952\n",
      "\n",
      "Current epoch:  689\n",
      "2017-11-15T23:14:20.927548: step 12405, loss 0.00288536, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:14:21.442850: step 12410, loss 0.0053351, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:14:21.959535: step 12415, loss 0.00190904, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:14:22.447799: step 12420, loss 0.00592128, acc 0.996855, f1 0.996856\n",
      "Current epoch:  690\n",
      "2017-11-15T23:14:22.970306: step 12425, loss 0.00176247, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:14:23.494874: step 12430, loss 0.0068121, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:14:24.023468: step 12435, loss 0.0135287, acc 0.99707, f1 0.997069\n",
      "Current epoch:  691\n",
      "2017-11-15T23:14:24.526524: step 12440, loss 0.00246143, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:14:25.052016: step 12445, loss 0.00351033, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:14:25.576295: step 12450, loss 0.00340969, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:14:26.235549: step 12450, loss 2.15526, acc 0.56897, f1 0.570365\n",
      "\n",
      "2017-11-15T23:14:26.754487: step 12455, loss 0.0115581, acc 0.996094, f1 0.99609\n",
      "Current epoch:  692\n",
      "2017-11-15T23:14:27.248250: step 12460, loss 0.000380208, acc 1, f1 1\n",
      "2017-11-15T23:14:27.770678: step 12465, loss 0.00511744, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:14:28.292255: step 12470, loss 0.00301929, acc 0.999023, f1 0.999023\n",
      "Current epoch:  693\n",
      "2017-11-15T23:14:28.790260: step 12475, loss 0.0157102, acc 0.995117, f1 0.995115\n",
      "2017-11-15T23:14:29.317538: step 12480, loss 1.93906, acc 0.623047, f1 0.503217\n",
      "2017-11-15T23:14:29.859703: step 12485, loss 0.0116784, acc 1, f1 1\n",
      "2017-11-15T23:14:30.380252: step 12490, loss 0.0115896, acc 0.998047, f1 0.998046\n",
      "Current epoch:  694\n",
      "2017-11-15T23:14:30.869309: step 12495, loss 0.00744553, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:14:31.390386: step 12500, loss 0.00833902, acc 0.998047, f1 0.998052\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:14:32.042564: step 12500, loss 1.67022, acc 0.581911, f1 0.581786\n",
      "\n",
      "2017-11-15T23:14:32.560950: step 12505, loss 0.00619243, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:14:33.044050: step 12510, loss 0.00435001, acc 1, f1 1\n",
      "Current epoch:  695\n",
      "2017-11-15T23:14:33.561470: step 12515, loss 0.0099941, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:14:34.081115: step 12520, loss 0.00545595, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:14:34.597762: step 12525, loss 0.00349673, acc 1, f1 1\n",
      "Current epoch:  696\n",
      "2017-11-15T23:14:35.094603: step 12530, loss 0.00488939, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:14:35.625624: step 12535, loss 0.00274923, acc 1, f1 1\n",
      "2017-11-15T23:14:36.140061: step 12540, loss 0.00236302, acc 1, f1 1\n",
      "2017-11-15T23:14:36.659988: step 12545, loss 0.00401171, acc 0.999023, f1 0.999023\n",
      "Current epoch:  697\n",
      "2017-11-15T23:14:37.149556: step 12550, loss 0.00679465, acc 0.996094, f1 0.996096\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:14:37.807794: step 12550, loss 1.77635, acc 0.58545, f1 0.585063\n",
      "\n",
      "2017-11-15T23:14:38.360339: step 12555, loss 0.00341117, acc 0.998047, f1 0.998043\n",
      "2017-11-15T23:14:38.881513: step 12560, loss 0.00506957, acc 0.99707, f1 0.99707\n",
      "Current epoch:  698\n",
      "2017-11-15T23:14:39.373355: step 12565, loss 0.00379162, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:14:39.891009: step 12570, loss 0.0036543, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:14:40.418987: step 12575, loss 0.0033609, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:14:40.951441: step 12580, loss 0.00216228, acc 0.999023, f1 0.999023\n",
      "Current epoch:  699\n",
      "2017-11-15T23:14:41.444745: step 12585, loss 0.0012727, acc 1, f1 1\n",
      "2017-11-15T23:14:41.964434: step 12590, loss 0.00258236, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:14:42.483857: step 12595, loss 0.00346038, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:14:42.970784: step 12600, loss 0.00109197, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:14:43.626216: step 12600, loss 1.88491, acc 0.585643, f1 0.58451\n",
      "\n",
      "Current epoch:  700\n",
      "2017-11-15T23:14:44.146186: step 12605, loss 0.00156687, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:14:44.664719: step 12610, loss 0.00171622, acc 1, f1 1\n",
      "2017-11-15T23:14:45.188703: step 12615, loss 0.00505235, acc 0.998047, f1 0.998048\n",
      "Current epoch:  701\n",
      "2017-11-15T23:14:45.680609: step 12620, loss 0.00196904, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:14:46.207530: step 12625, loss 0.00290114, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:14:46.728259: step 12630, loss 0.0064067, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:14:47.250202: step 12635, loss 0.00812051, acc 0.996094, f1 0.996094\n",
      "Current epoch:  702\n",
      "2017-11-15T23:14:47.746333: step 12640, loss 0.00382502, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:14:48.267076: step 12645, loss 0.00284371, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:14:48.789137: step 12650, loss 0.00330226, acc 0.999023, f1 0.999024\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:14:49.439432: step 12650, loss 1.97298, acc 0.590151, f1 0.587767\n",
      "\n",
      "Current epoch:  703\n",
      "2017-11-15T23:14:49.932328: step 12655, loss 0.000923805, acc 1, f1 1\n",
      "2017-11-15T23:14:50.456262: step 12660, loss 0.00407371, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:14:50.977576: step 12665, loss 0.0110686, acc 0.995117, f1 0.995119\n",
      "2017-11-15T23:14:51.502592: step 12670, loss 0.00236562, acc 0.999023, f1 0.999023\n",
      "Current epoch:  704\n",
      "2017-11-15T23:14:52.009620: step 12675, loss 0.00149194, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:14:52.533337: step 12680, loss 0.0083182, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:14:53.044269: step 12685, loss 0.00593381, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:14:53.532131: step 12690, loss 0.00437162, acc 0.998428, f1 0.998425\n",
      "Current epoch:  705\n",
      "2017-11-15T23:14:54.059977: step 12695, loss 0.00665445, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:14:54.579020: step 12700, loss 0.0062052, acc 0.998047, f1 0.998046\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:14:55.236641: step 12700, loss 2.05283, acc 0.578276, f1 0.579336\n",
      "\n",
      "2017-11-15T23:14:55.761499: step 12705, loss 0.00872119, acc 0.99707, f1 0.997068\n",
      "Current epoch:  706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:14:56.253175: step 12710, loss 0.00166288, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:14:56.774403: step 12715, loss 0.00474507, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:14:57.303410: step 12720, loss 0.00732511, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:14:57.848394: step 12725, loss 0.00623611, acc 0.998047, f1 0.998047\n",
      "Current epoch:  707\n",
      "2017-11-15T23:14:58.341370: step 12730, loss 0.000445186, acc 1, f1 1\n",
      "2017-11-15T23:14:58.869371: step 12735, loss 0.00447128, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:14:59.390832: step 12740, loss 0.00411249, acc 0.998047, f1 0.998047\n",
      "Current epoch:  708\n",
      "2017-11-15T23:14:59.881296: step 12745, loss 0.0296766, acc 0.990234, f1 0.99012\n",
      "2017-11-15T23:15:00.408641: step 12750, loss 0.00842878, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:15:01.068894: step 12750, loss 1.91388, acc 0.591896, f1 0.582248\n",
      "\n",
      "2017-11-15T23:15:01.589831: step 12755, loss 0.00388455, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:15:02.108063: step 12760, loss 0.00824317, acc 0.99707, f1 0.99707\n",
      "Current epoch:  709\n",
      "2017-11-15T23:15:02.607927: step 12765, loss 0.00503556, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:15:03.136871: step 12770, loss 0.00273712, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:15:03.656786: step 12775, loss 0.00282492, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:15:04.145551: step 12780, loss 0.00262761, acc 0.998428, f1 0.998425\n",
      "Current epoch:  710\n",
      "2017-11-15T23:15:04.666919: step 12785, loss 0.00103003, acc 1, f1 1\n",
      "2017-11-15T23:15:05.182022: step 12790, loss 0.00963739, acc 0.994141, f1 0.994136\n",
      "2017-11-15T23:15:05.702477: step 12795, loss 0.00310825, acc 0.999023, f1 0.999023\n",
      "Current epoch:  711\n",
      "2017-11-15T23:15:06.193477: step 12800, loss 0.00229434, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:15:06.854467: step 12800, loss 1.9177, acc 0.587534, f1 0.585907\n",
      "\n",
      "2017-11-15T23:15:07.381017: step 12805, loss 0.00220109, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:15:07.902271: step 12810, loss 0.00250063, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:15:08.440585: step 12815, loss 0.0008261, acc 1, f1 1\n",
      "Current epoch:  712\n",
      "2017-11-15T23:15:08.933608: step 12820, loss 0.00233885, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:15:09.455351: step 12825, loss 0.00484593, acc 0.996094, f1 0.996098\n",
      "2017-11-15T23:15:09.977762: step 12830, loss 0.00337127, acc 0.99707, f1 0.99707\n",
      "Current epoch:  713\n",
      "2017-11-15T23:15:10.473314: step 12835, loss 0.00415978, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:15:10.999783: step 12840, loss 0.000687042, acc 1, f1 1\n",
      "2017-11-15T23:15:11.520973: step 12845, loss 0.00318696, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:15:12.038420: step 12850, loss 0.00486949, acc 0.99707, f1 0.997071\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:15:12.671819: step 12850, loss 1.99574, acc 0.583656, f1 0.582724\n",
      "\n",
      "Current epoch:  714\n",
      "2017-11-15T23:15:13.184737: step 12855, loss 0.0029308, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:15:13.718858: step 12860, loss 0.00328857, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:15:14.245531: step 12865, loss 0.0046588, acc 0.99707, f1 0.997072\n",
      "2017-11-15T23:15:14.739161: step 12870, loss 0.0083895, acc 0.995283, f1 0.99528\n",
      "Current epoch:  715\n",
      "2017-11-15T23:15:15.261136: step 12875, loss 0.00114301, acc 1, f1 1\n",
      "2017-11-15T23:15:15.782097: step 12880, loss 0.0049222, acc 0.99707, f1 0.997075\n",
      "2017-11-15T23:15:16.299912: step 12885, loss 0.0057844, acc 0.996094, f1 0.996094\n",
      "Current epoch:  716\n",
      "2017-11-15T23:15:16.792941: step 12890, loss 0.000715809, acc 1, f1 1\n",
      "2017-11-15T23:15:17.317624: step 12895, loss 0.00178836, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:15:17.843212: step 12900, loss 0.0040404, acc 0.998047, f1 0.998048\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:15:18.500723: step 12900, loss 2.07028, acc 0.578567, f1 0.578496\n",
      "\n",
      "2017-11-15T23:15:19.031188: step 12905, loss 0.00474034, acc 0.998047, f1 0.998047\n",
      "Current epoch:  717\n",
      "2017-11-15T23:15:19.533854: step 12910, loss 0.000438134, acc 1, f1 1\n",
      "2017-11-15T23:15:20.052778: step 12915, loss 0.00513566, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:15:20.576272: step 12920, loss 0.00564254, acc 0.99707, f1 0.997067\n",
      "Current epoch:  718\n",
      "2017-11-15T23:15:21.070781: step 12925, loss 0.00819773, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:15:21.600731: step 12930, loss 0.00263771, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:15:22.126468: step 12935, loss 0.00512495, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:15:22.640394: step 12940, loss 0.00961628, acc 0.996094, f1 0.996096\n",
      "Current epoch:  719\n",
      "2017-11-15T23:15:23.131784: step 12945, loss 0.00401847, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:15:23.646744: step 12950, loss 0.00499427, acc 0.998047, f1 0.998046\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:15:24.288488: step 12950, loss 2.07195, acc 0.580215, f1 0.582461\n",
      "\n",
      "2017-11-15T23:15:24.810056: step 12955, loss 0.00793527, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:15:25.288411: step 12960, loss 0.00719834, acc 0.996855, f1 0.996855\n",
      "Current epoch:  720\n",
      "2017-11-15T23:15:25.811960: step 12965, loss 0.000382556, acc 1, f1 1\n",
      "2017-11-15T23:15:26.324874: step 12970, loss 0.0104523, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:15:26.846665: step 12975, loss 0.00640625, acc 0.99707, f1 0.99707\n",
      "Current epoch:  721\n",
      "2017-11-15T23:15:27.341617: step 12980, loss 0.000630165, acc 1, f1 1\n",
      "2017-11-15T23:15:27.865877: step 12985, loss 0.00256517, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:15:28.388522: step 12990, loss 0.000631995, acc 1, f1 1\n",
      "2017-11-15T23:15:28.907507: step 12995, loss 0.00539424, acc 0.998047, f1 0.998046\n",
      "Current epoch:  722\n",
      "2017-11-15T23:15:29.403014: step 13000, loss 2.03018, acc 0.602539, f1 0.536926\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:15:30.066296: step 13000, loss 5.51237, acc 0.524282, f1 0.411836\n",
      "\n",
      "2017-11-15T23:15:30.595790: step 13005, loss 0.00873135, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:15:31.117242: step 13010, loss 0.00575848, acc 0.999023, f1 0.999023\n",
      "Current epoch:  723\n",
      "2017-11-15T23:15:31.609316: step 13015, loss 0.00500919, acc 1, f1 1\n",
      "2017-11-15T23:15:32.129291: step 13020, loss 0.00459415, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:15:32.650347: step 13025, loss 0.0048236, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:15:33.173288: step 13030, loss 0.00609697, acc 0.998047, f1 0.998047\n",
      "Current epoch:  724\n",
      "2017-11-15T23:15:33.668019: step 13035, loss 0.00509011, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:15:34.187540: step 13040, loss 0.00442483, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:15:34.707050: step 13045, loss 0.00418934, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:15:35.195605: step 13050, loss 0.0060811, acc 0.996855, f1 0.996855\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:15:35.901426: step 13050, loss 1.8246, acc 0.583656, f1 0.582876\n",
      "\n",
      "Current epoch:  725\n",
      "2017-11-15T23:15:36.425887: step 13055, loss 0.00317419, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:15:36.943934: step 13060, loss 0.00760193, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:15:37.466846: step 13065, loss 0.00169175, acc 1, f1 1\n",
      "Current epoch:  726\n",
      "2017-11-15T23:15:37.956838: step 13070, loss 0.0019995, acc 1, f1 1\n",
      "2017-11-15T23:15:38.474753: step 13075, loss 0.00248213, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:15:38.994963: step 13080, loss 0.00231161, acc 1, f1 1\n",
      "2017-11-15T23:15:39.516937: step 13085, loss 0.00439358, acc 0.998047, f1 0.998046\n",
      "Current epoch:  727\n",
      "2017-11-15T23:15:40.006674: step 13090, loss 0.00163496, acc 1, f1 1\n",
      "2017-11-15T23:15:40.525755: step 13095, loss 0.00214729, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:15:41.053400: step 13100, loss 0.0019916, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:15:41.736461: step 13100, loss 1.90799, acc 0.580797, f1 0.580622\n",
      "\n",
      "Current epoch:  728\n",
      "2017-11-15T23:15:42.224201: step 13105, loss 0.00479803, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:15:42.748460: step 13110, loss 0.00397936, acc 0.998047, f1 0.998043\n",
      "2017-11-15T23:15:43.266403: step 13115, loss 0.000948136, acc 1, f1 1\n",
      "2017-11-15T23:15:43.780853: step 13120, loss 0.00423029, acc 0.99707, f1 0.997071\n",
      "Current epoch:  729\n",
      "2017-11-15T23:15:44.263874: step 13125, loss 0.00340301, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:15:44.777422: step 13130, loss 0.00886641, acc 0.995117, f1 0.995122\n",
      "2017-11-15T23:15:45.290704: step 13135, loss 0.0026323, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:15:45.779485: step 13140, loss 0.00312457, acc 0.998428, f1 0.998427\n",
      "Current epoch:  730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:15:46.295928: step 13145, loss 0.00329203, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:15:46.829171: step 13150, loss 0.00389806, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:15:47.488215: step 13150, loss 1.98324, acc 0.581717, f1 0.581635\n",
      "\n",
      "2017-11-15T23:15:48.005686: step 13155, loss 0.00291482, acc 0.999023, f1 0.999023\n",
      "Current epoch:  731\n",
      "2017-11-15T23:15:48.499938: step 13160, loss 0.00094115, acc 1, f1 1\n",
      "2017-11-15T23:15:49.021930: step 13165, loss 0.00356729, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:15:49.544853: step 13170, loss 0.00187845, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:15:50.067015: step 13175, loss 0.00809865, acc 0.996094, f1 0.996094\n",
      "Current epoch:  732\n",
      "2017-11-15T23:15:50.561909: step 13180, loss 0.00419051, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:15:51.085261: step 13185, loss 0.00682557, acc 0.995117, f1 0.995113\n",
      "2017-11-15T23:15:51.607447: step 13190, loss 0.00196503, acc 0.999023, f1 0.999022\n",
      "Current epoch:  733\n",
      "2017-11-15T23:15:52.119357: step 13195, loss 0.00518349, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:15:52.648805: step 13200, loss 0.0049825, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:15:53.308174: step 13200, loss 2.09513, acc 0.580167, f1 0.579127\n",
      "\n",
      "2017-11-15T23:15:53.831596: step 13205, loss 0.00864463, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:15:54.356523: step 13210, loss 0.00545367, acc 0.998047, f1 0.998047\n",
      "Current epoch:  734\n",
      "2017-11-15T23:15:54.847367: step 13215, loss 0.00547523, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:15:55.368444: step 13220, loss 0.000834906, acc 1, f1 1\n",
      "2017-11-15T23:15:55.889111: step 13225, loss 0.0115754, acc 0.995117, f1 0.995114\n",
      "2017-11-15T23:15:56.380455: step 13230, loss 0.00850103, acc 0.995283, f1 0.995284\n",
      "Current epoch:  735\n",
      "2017-11-15T23:15:56.904574: step 13235, loss 0.000415448, acc 1, f1 1\n",
      "2017-11-15T23:15:57.442307: step 13240, loss 0.00296383, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:15:57.984087: step 13245, loss 0.00296585, acc 0.998047, f1 0.998047\n",
      "Current epoch:  736\n",
      "2017-11-15T23:15:58.588868: step 13250, loss 0.00271214, acc 0.999023, f1 0.999024\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:15:59.248631: step 13250, loss 2.14827, acc 0.587824, f1 0.583566\n",
      "\n",
      "2017-11-15T23:15:59.769898: step 13255, loss 0.000883172, acc 1, f1 1\n",
      "2017-11-15T23:16:00.287602: step 13260, loss 0.00520992, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:16:00.813631: step 13265, loss 0.00434893, acc 0.998047, f1 0.998048\n",
      "Current epoch:  737\n",
      "2017-11-15T23:16:01.303511: step 13270, loss 0.00232676, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:16:01.823897: step 13275, loss 0.00694892, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:16:02.350276: step 13280, loss 0.00583509, acc 0.998047, f1 0.998043\n",
      "Current epoch:  738\n",
      "2017-11-15T23:16:02.851185: step 13285, loss 0.00810823, acc 0.99707, f1 0.997075\n",
      "2017-11-15T23:16:03.387343: step 13290, loss 0.00533788, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:16:03.909310: step 13295, loss 0.0055549, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:16:04.434614: step 13300, loss 0.00908813, acc 0.995117, f1 0.995114\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:16:05.094625: step 13300, loss 2.23937, acc 0.603383, f1 0.591243\n",
      "\n",
      "Current epoch:  739\n",
      "2017-11-15T23:16:05.576033: step 13305, loss 0.00370804, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:16:06.094993: step 13310, loss 0.00267873, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:16:06.613933: step 13315, loss 0.00568612, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:16:07.098469: step 13320, loss 0.00604832, acc 0.998428, f1 0.998428\n",
      "Current epoch:  740\n",
      "2017-11-15T23:16:07.628948: step 13325, loss 0.000330582, acc 1, f1 1\n",
      "2017-11-15T23:16:08.149962: step 13330, loss 0.0112779, acc 0.995117, f1 0.995119\n",
      "2017-11-15T23:16:08.687200: step 13335, loss 0.000440937, acc 1, f1 1\n",
      "Current epoch:  741\n",
      "2017-11-15T23:16:09.181577: step 13340, loss 0.00694672, acc 0.99707, f1 0.997072\n",
      "2017-11-15T23:16:09.703017: step 13345, loss 0.0123621, acc 0.996094, f1 0.996091\n",
      "2017-11-15T23:16:10.228950: step 13350, loss 0.015596, acc 0.995117, f1 0.995111\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:16:10.899051: step 13350, loss 2.27989, acc 0.553945, f1 0.558289\n",
      "\n",
      "2017-11-15T23:16:11.422313: step 13355, loss 0.00672331, acc 0.99707, f1 0.997071\n",
      "Current epoch:  742\n",
      "2017-11-15T23:16:11.917036: step 13360, loss 0.000890092, acc 1, f1 1\n",
      "2017-11-15T23:16:12.442031: step 13365, loss 0.000330573, acc 1, f1 1\n",
      "2017-11-15T23:16:13.009867: step 13370, loss 0.000541462, acc 1, f1 1\n",
      "Current epoch:  743\n",
      "2017-11-15T23:16:13.514289: step 13375, loss 0.00445068, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:16:14.044447: step 13380, loss 0.00297266, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:16:14.573927: step 13385, loss 0.00859471, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:16:15.101789: step 13390, loss 0.0058473, acc 0.99707, f1 0.997066\n",
      "Current epoch:  744\n",
      "2017-11-15T23:16:15.600882: step 13395, loss 0.00403698, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:16:16.129947: step 13400, loss 0.00690735, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:16:16.789926: step 13400, loss 2.11007, acc 0.588309, f1 0.588075\n",
      "\n",
      "2017-11-15T23:16:17.314768: step 13405, loss 0.0114286, acc 0.995117, f1 0.995114\n",
      "2017-11-15T23:16:17.812672: step 13410, loss 0.00025249, acc 1, f1 1\n",
      "Current epoch:  745\n",
      "2017-11-15T23:16:18.341113: step 13415, loss 0.00426928, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:16:18.866400: step 13420, loss 0.00418294, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:16:19.398188: step 13425, loss 0.0105222, acc 0.996094, f1 0.996093\n",
      "Current epoch:  746\n",
      "2017-11-15T23:16:19.904366: step 13430, loss 0.000687301, acc 1, f1 1\n",
      "2017-11-15T23:16:20.434858: step 13435, loss 0.000604668, acc 1, f1 1\n",
      "2017-11-15T23:16:20.955834: step 13440, loss 0.00130232, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:16:21.480894: step 13445, loss 0.00665617, acc 0.99707, f1 0.997075\n",
      "Current epoch:  747\n",
      "2017-11-15T23:16:21.976492: step 13450, loss 0.00392408, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:16:22.633473: step 13450, loss 2.22438, acc 0.567274, f1 0.570806\n",
      "\n",
      "2017-11-15T23:16:23.155384: step 13455, loss 0.00253517, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:16:23.674545: step 13460, loss 0.00509687, acc 0.998047, f1 0.998047\n",
      "Current epoch:  748\n",
      "2017-11-15T23:16:24.167434: step 13465, loss 0.0048483, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:16:24.684400: step 13470, loss 0.000236823, acc 1, f1 1\n",
      "2017-11-15T23:16:25.214977: step 13475, loss 0.00507333, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:16:25.738886: step 13480, loss 0.00953658, acc 0.996094, f1 0.996094\n",
      "Current epoch:  749\n",
      "2017-11-15T23:16:26.221298: step 13485, loss 0.00439358, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:16:26.738891: step 13490, loss 0.00351976, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:16:27.250806: step 13495, loss 0.00578095, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:16:27.735272: step 13500, loss 0.0042855, acc 0.996855, f1 0.996858\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:16:28.362671: step 13500, loss 2.26675, acc 0.587728, f1 0.577512\n",
      "\n",
      "Current epoch:  750\n",
      "2017-11-15T23:16:28.882972: step 13505, loss 0.00745379, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:16:29.403000: step 13510, loss 0.00163529, acc 1, f1 1\n",
      "2017-11-15T23:16:29.929432: step 13515, loss 0.00512056, acc 0.998047, f1 0.998046\n",
      "Current epoch:  751\n",
      "2017-11-15T23:16:30.426319: step 13520, loss 0.00429873, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:16:30.965993: step 13525, loss 0.00208459, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:16:31.483908: step 13530, loss 0.00201832, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:16:32.000880: step 13535, loss 0.0139032, acc 0.996094, f1 0.996094\n",
      "Current epoch:  752\n",
      "2017-11-15T23:16:32.484763: step 13540, loss 0.000326911, acc 1, f1 1\n",
      "2017-11-15T23:16:33.000291: step 13545, loss 0.00711344, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:16:33.514750: step 13550, loss 0.00811515, acc 0.99707, f1 0.997067\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:16:34.156693: step 13550, loss 2.17629, acc 0.578422, f1 0.581256\n",
      "\n",
      "Current epoch:  753\n",
      "2017-11-15T23:16:34.648095: step 13555, loss 0.000684048, acc 1, f1 1\n",
      "2017-11-15T23:16:35.172055: step 13560, loss 0.0047062, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:16:35.692990: step 13565, loss 0.0136355, acc 0.99707, f1 0.99706\n",
      "2017-11-15T23:16:36.216937: step 13570, loss 0.00397353, acc 0.998047, f1 0.998047\n",
      "Current epoch:  754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:16:36.707828: step 13575, loss 0.00502955, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:16:37.217777: step 13580, loss 0.00394817, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:16:37.732724: step 13585, loss 0.0125437, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:16:38.216155: step 13590, loss 0.0050181, acc 0.998428, f1 0.998428\n",
      "Current epoch:  755\n",
      "2017-11-15T23:16:38.733177: step 13595, loss 0.00112667, acc 1, f1 1\n",
      "2017-11-15T23:16:39.241161: step 13600, loss 0.00479898, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:16:39.875247: step 13600, loss 2.30292, acc 0.557483, f1 0.555042\n",
      "\n",
      "2017-11-15T23:16:40.425228: step 13605, loss 0.00309181, acc 0.999023, f1 0.999023\n",
      "Current epoch:  756\n",
      "2017-11-15T23:16:40.910893: step 13610, loss 0.00111892, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:16:41.433355: step 13615, loss 0.00333705, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:16:41.952923: step 13620, loss 0.00355662, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:16:42.466861: step 13625, loss 0.00500581, acc 0.998047, f1 0.998047\n",
      "Current epoch:  757\n",
      "2017-11-15T23:16:42.952761: step 13630, loss 0.00759306, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:16:43.474713: step 13635, loss 0.00747839, acc 0.99707, f1 0.997074\n",
      "2017-11-15T23:16:43.994633: step 13640, loss 0.00625914, acc 0.998047, f1 0.998047\n",
      "Current epoch:  758\n",
      "2017-11-15T23:16:44.479488: step 13645, loss 0.00248942, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:16:44.989266: step 13650, loss 0.00527616, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:16:45.634828: step 13650, loss 2.14001, acc 0.582832, f1 0.584168\n",
      "\n",
      "2017-11-15T23:16:46.144442: step 13655, loss 0.0122681, acc 0.996094, f1 0.996095\n",
      "2017-11-15T23:16:46.658717: step 13660, loss 0.000252394, acc 1, f1 1\n",
      "Current epoch:  759\n",
      "2017-11-15T23:16:47.155261: step 13665, loss 0.0035557, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:16:47.681241: step 13670, loss 0.00204571, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:16:48.203409: step 13675, loss 0.00204031, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:16:48.698804: step 13680, loss 0.0091777, acc 0.996855, f1 0.996857\n",
      "Current epoch:  760\n",
      "2017-11-15T23:16:49.214410: step 13685, loss 0.000518487, acc 1, f1 1\n",
      "2017-11-15T23:16:49.734215: step 13690, loss 0.00217298, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:16:50.246276: step 13695, loss 0.00464242, acc 0.999023, f1 0.999023\n",
      "Current epoch:  761\n",
      "2017-11-15T23:16:50.738698: step 13700, loss 0.00533005, acc 0.99707, f1 0.99707\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:16:51.373319: step 13700, loss 2.24651, acc 0.565578, f1 0.563284\n",
      "\n",
      "2017-11-15T23:16:51.892752: step 13705, loss 0.0077857, acc 0.99707, f1 0.997074\n",
      "2017-11-15T23:16:52.412225: step 13710, loss 0.00449171, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:16:52.938718: step 13715, loss 0.00454503, acc 0.998047, f1 0.998047\n",
      "Current epoch:  762\n",
      "2017-11-15T23:16:53.423574: step 13720, loss 0.00549935, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:16:53.939506: step 13725, loss 0.00556042, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:16:54.453850: step 13730, loss 0.000288102, acc 1, f1 1\n",
      "Current epoch:  763\n",
      "2017-11-15T23:16:54.941259: step 13735, loss 0.00225759, acc 1, f1 1\n",
      "2017-11-15T23:16:55.453244: step 13740, loss 0.00563942, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:16:55.970700: step 13745, loss 0.000852065, acc 1, f1 1\n",
      "2017-11-15T23:16:56.486128: step 13750, loss 0.00308276, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:16:57.116483: step 13750, loss 2.26972, acc 0.558937, f1 0.558808\n",
      "\n",
      "Current epoch:  764\n",
      "2017-11-15T23:16:57.603982: step 13755, loss 0.00413835, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:16:58.143458: step 13760, loss 0.000432652, acc 1, f1 1\n",
      "2017-11-15T23:16:58.660410: step 13765, loss 0.00450752, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:16:59.144962: step 13770, loss 0.00575572, acc 0.996855, f1 0.996855\n",
      "Current epoch:  765\n",
      "2017-11-15T23:16:59.662815: step 13775, loss 0.00282829, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:17:00.179241: step 13780, loss 0.0052027, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:17:00.692661: step 13785, loss 0.00849611, acc 0.996094, f1 0.996095\n",
      "Current epoch:  766\n",
      "2017-11-15T23:17:01.179050: step 13790, loss 0.00451639, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:17:01.699074: step 13795, loss 0.00290884, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:17:02.212041: step 13800, loss 0.00253938, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:17:02.856213: step 13800, loss 2.32657, acc 0.56233, f1 0.559517\n",
      "\n",
      "2017-11-15T23:17:03.378157: step 13805, loss 0.0111786, acc 0.99707, f1 0.997067\n",
      "Current epoch:  767\n",
      "2017-11-15T23:17:03.890096: step 13810, loss 0.0147409, acc 0.995117, f1 0.995118\n",
      "2017-11-15T23:17:04.407004: step 13815, loss 0.00518724, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:17:04.917793: step 13820, loss 0.000647323, acc 1, f1 1\n",
      "Current epoch:  768\n",
      "2017-11-15T23:17:05.403463: step 13825, loss 0.00559524, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:17:05.910389: step 13830, loss 0.00259925, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:17:06.433122: step 13835, loss 0.00312333, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:17:06.945061: step 13840, loss 0.00204558, acc 0.999023, f1 0.999023\n",
      "Current epoch:  769\n",
      "2017-11-15T23:17:07.433959: step 13845, loss 0.000505849, acc 1, f1 1\n",
      "2017-11-15T23:17:07.950908: step 13850, loss 0.000446424, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:17:08.581560: step 13850, loss 2.15306, acc 0.591023, f1 0.587823\n",
      "\n",
      "2017-11-15T23:17:09.095485: step 13855, loss 0.00027315, acc 1, f1 1\n",
      "2017-11-15T23:17:09.582855: step 13860, loss 0.0106885, acc 0.995283, f1 0.995283\n",
      "Current epoch:  770\n",
      "2017-11-15T23:17:10.106418: step 13865, loss 0.000399562, acc 1, f1 1\n",
      "2017-11-15T23:17:10.623360: step 13870, loss 0.00535109, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:17:11.138293: step 13875, loss 0.00751509, acc 0.99707, f1 0.99707\n",
      "Current epoch:  771\n",
      "2017-11-15T23:17:11.629935: step 13880, loss 0.00228209, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:17:12.154881: step 13885, loss 0.00435369, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:17:12.670124: step 13890, loss 0.0057073, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:17:13.181441: step 13895, loss 0.00978213, acc 0.99707, f1 0.997069\n",
      "Current epoch:  772\n",
      "2017-11-15T23:17:13.668858: step 13900, loss 0.000361372, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:17:14.322311: step 13900, loss 2.13304, acc 0.586516, f1 0.586859\n",
      "\n",
      "2017-11-15T23:17:14.863665: step 13905, loss 0.00855665, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:17:15.392700: step 13910, loss 0.00335337, acc 0.999023, f1 0.999024\n",
      "Current epoch:  773\n",
      "2017-11-15T23:17:15.889086: step 13915, loss 0.000470284, acc 1, f1 1\n",
      "2017-11-15T23:17:16.404859: step 13920, loss 0.00291018, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:17:16.918176: step 13925, loss 0.00776878, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:17:17.437090: step 13930, loss 0.00241497, acc 0.999023, f1 0.999023\n",
      "Current epoch:  774\n",
      "2017-11-15T23:17:17.922034: step 13935, loss 1.33607, acc 0.666992, f1 0.616981\n",
      "2017-11-15T23:17:18.444051: step 13940, loss 0.00944686, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:17:18.970052: step 13945, loss 0.00408363, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:17:19.460673: step 13950, loss 0.00888247, acc 0.995283, f1 0.995282\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:17:20.134145: step 13950, loss 1.80935, acc 0.584286, f1 0.583939\n",
      "\n",
      "Current epoch:  775\n",
      "2017-11-15T23:17:20.654180: step 13955, loss 0.0068797, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:17:21.171282: step 13960, loss 0.00500359, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:17:21.691295: step 13965, loss 0.0045403, acc 0.999023, f1 0.999023\n",
      "Current epoch:  776\n",
      "2017-11-15T23:17:22.173192: step 13970, loss 0.00187314, acc 1, f1 1\n",
      "2017-11-15T23:17:22.687410: step 13975, loss 0.00453795, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:17:23.200832: step 13980, loss 0.00388854, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:17:23.710972: step 13985, loss 0.00428788, acc 0.999023, f1 0.999023\n",
      "Current epoch:  777\n",
      "2017-11-15T23:17:24.193085: step 13990, loss 0.00367511, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:17:24.706020: step 13995, loss 0.00247773, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:17:25.218441: step 14000, loss 0.00394513, acc 0.998047, f1 0.998046\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:17:25.899819: step 14000, loss 1.86152, acc 0.586225, f1 0.586291\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch:  778\n",
      "2017-11-15T23:17:26.383872: step 14005, loss 0.00320635, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:17:26.895821: step 14010, loss 0.00415888, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:17:27.403332: step 14015, loss 0.00215562, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:17:27.914847: step 14020, loss 0.00459698, acc 0.99707, f1 0.997071\n",
      "Current epoch:  779\n",
      "2017-11-15T23:17:28.403862: step 14025, loss 0.00249769, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:17:28.922550: step 14030, loss 0.00233036, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:17:29.447141: step 14035, loss 0.00375417, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:17:29.938810: step 14040, loss 0.00419908, acc 0.996855, f1 0.996854\n",
      "Current epoch:  780\n",
      "2017-11-15T23:17:30.457102: step 14045, loss 0.000822632, acc 1, f1 1\n",
      "2017-11-15T23:17:30.975020: step 14050, loss 0.00262214, acc 0.998047, f1 0.998046\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:17:31.634438: step 14050, loss 1.93459, acc 0.587049, f1 0.586637\n",
      "\n",
      "2017-11-15T23:17:32.149354: step 14055, loss 0.00265964, acc 0.998047, f1 0.998047\n",
      "Current epoch:  781\n",
      "2017-11-15T23:17:32.643262: step 14060, loss 0.00488919, acc 0.996094, f1 0.996093\n",
      "2017-11-15T23:17:33.165557: step 14065, loss 0.00217669, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:17:33.682385: step 14070, loss 0.00219922, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:17:34.199362: step 14075, loss 0.0046, acc 0.99707, f1 0.997074\n",
      "Current epoch:  782\n",
      "2017-11-15T23:17:34.690263: step 14080, loss 0.00189671, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:17:35.211245: step 14085, loss 0.00283792, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:17:35.725766: step 14090, loss 0.00715713, acc 0.996094, f1 0.996094\n",
      "Current epoch:  783\n",
      "2017-11-15T23:17:36.216164: step 14095, loss 0.00352909, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:17:36.747778: step 14100, loss 0.00169201, acc 0.999023, f1 0.999024\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:17:37.393092: step 14100, loss 2.0316, acc 0.588648, f1 0.58582\n",
      "\n",
      "2017-11-15T23:17:37.901347: step 14105, loss 0.00528167, acc 0.99707, f1 0.997073\n",
      "2017-11-15T23:17:38.412278: step 14110, loss 0.00518578, acc 0.99707, f1 0.997071\n",
      "Current epoch:  784\n",
      "2017-11-15T23:17:38.898817: step 14115, loss 0.000737048, acc 1, f1 1\n",
      "2017-11-15T23:17:39.421810: step 14120, loss 0.00186514, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:17:39.946913: step 14125, loss 0.00947635, acc 0.994141, f1 0.994141\n",
      "2017-11-15T23:17:40.438225: step 14130, loss 0.000381492, acc 1, f1 1\n",
      "Current epoch:  785\n",
      "2017-11-15T23:17:40.955175: step 14135, loss 0.00357631, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:17:41.482669: step 14140, loss 0.00379499, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:17:42.016141: step 14145, loss 0.00190345, acc 0.999023, f1 0.999022\n",
      "Current epoch:  786\n",
      "2017-11-15T23:17:42.508542: step 14150, loss 0.00459279, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:17:43.138509: step 14150, loss 2.07745, acc 0.581911, f1 0.58279\n",
      "\n",
      "2017-11-15T23:17:43.648252: step 14155, loss 0.00595725, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:17:44.167666: step 14160, loss 0.00426902, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:17:44.676591: step 14165, loss 0.0116156, acc 0.995117, f1 0.995113\n",
      "Current epoch:  787\n",
      "2017-11-15T23:17:45.155354: step 14170, loss 0.000624751, acc 1, f1 1\n",
      "2017-11-15T23:17:45.664391: step 14175, loss 0.016148, acc 0.993164, f1 0.993166\n",
      "2017-11-15T23:17:46.176313: step 14180, loss 0.00287663, acc 0.999023, f1 0.999023\n",
      "Current epoch:  788\n",
      "2017-11-15T23:17:46.659202: step 14185, loss 0.00778694, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:17:47.175221: step 14190, loss 0.00942715, acc 0.996094, f1 0.996096\n",
      "2017-11-15T23:17:47.696684: step 14195, loss 0.00334528, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:17:48.223858: step 14200, loss 0.00155879, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:17:48.891200: step 14200, loss 2.10103, acc 0.589909, f1 0.590181\n",
      "\n",
      "Current epoch:  789\n",
      "2017-11-15T23:17:49.381251: step 14205, loss 0.00414563, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:17:49.904246: step 14210, loss 0.00569389, acc 0.99707, f1 0.997068\n",
      "2017-11-15T23:17:50.427383: step 14215, loss 0.00704792, acc 0.99707, f1 0.997072\n",
      "2017-11-15T23:17:50.921891: step 14220, loss 0.00391693, acc 0.998428, f1 0.99843\n",
      "Current epoch:  790\n",
      "2017-11-15T23:17:51.443825: step 14225, loss 0.00340442, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:17:51.969783: step 14230, loss 0.00297859, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:17:52.497284: step 14235, loss 0.00454741, acc 0.998047, f1 0.998047\n",
      "Current epoch:  791\n",
      "2017-11-15T23:17:52.986670: step 14240, loss 0.000646636, acc 1, f1 1\n",
      "2017-11-15T23:17:53.508629: step 14245, loss 0.0102853, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:17:54.028285: step 14250, loss 0.00894738, acc 0.99707, f1 0.997071\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:17:54.689491: step 14250, loss 2.299, acc 0.560585, f1 0.557979\n",
      "\n",
      "2017-11-15T23:17:55.209740: step 14255, loss 0.000278625, acc 1, f1 1\n",
      "Current epoch:  792\n",
      "2017-11-15T23:17:55.699675: step 14260, loss 0.00160596, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:17:56.224133: step 14265, loss 0.00363113, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:17:56.757244: step 14270, loss 0.015292, acc 0.994141, f1 0.994141\n",
      "Current epoch:  793\n",
      "2017-11-15T23:17:57.251117: step 14275, loss 0.00569562, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:17:57.761728: step 14280, loss 0.00514048, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:17:58.276194: step 14285, loss 0.000646133, acc 1, f1 1\n",
      "2017-11-15T23:17:58.804294: step 14290, loss 0.00611093, acc 0.99707, f1 0.997069\n",
      "Current epoch:  794\n",
      "2017-11-15T23:17:59.297706: step 14295, loss 0.00223498, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:17:59.825325: step 14300, loss 0.000330274, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:18:00.488657: step 14300, loss 2.13745, acc 0.579343, f1 0.581782\n",
      "\n",
      "2017-11-15T23:18:01.012151: step 14305, loss 0.00278881, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:18:01.507443: step 14310, loss 0.0004029, acc 1, f1 1\n",
      "Current epoch:  795\n",
      "2017-11-15T23:18:02.031280: step 14315, loss 0.000229681, acc 1, f1 1\n",
      "2017-11-15T23:18:02.550785: step 14320, loss 0.00276399, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:18:03.074965: step 14325, loss 0.000488485, acc 1, f1 1\n",
      "Current epoch:  796\n",
      "2017-11-15T23:18:03.571878: step 14330, loss 0.00139436, acc 1, f1 1\n",
      "2017-11-15T23:18:04.103836: step 14335, loss 0.00199707, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:18:04.640707: step 14340, loss 0.0024289, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:18:05.160637: step 14345, loss 0.00876682, acc 0.996094, f1 0.996094\n",
      "Current epoch:  797\n",
      "2017-11-15T23:18:05.659387: step 14350, loss 0.00936734, acc 0.99707, f1 0.997075\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:18:06.316214: step 14350, loss 2.15943, acc 0.58545, f1 0.5824\n",
      "\n",
      "2017-11-15T23:18:06.843198: step 14355, loss 0.00546448, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:18:07.368869: step 14360, loss 0.00575969, acc 0.998047, f1 0.998047\n",
      "Current epoch:  798\n",
      "2017-11-15T23:18:07.860917: step 14365, loss 0.00244913, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:18:08.379134: step 14370, loss 0.00596294, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:18:08.893252: step 14375, loss 0.00290369, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:18:09.423710: step 14380, loss 0.00582357, acc 0.99707, f1 0.99707\n",
      "Current epoch:  799\n",
      "2017-11-15T23:18:09.934978: step 14385, loss 0.000459335, acc 1, f1 1\n",
      "2017-11-15T23:18:10.460916: step 14390, loss 0.00180052, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:18:10.981338: step 14395, loss 0.00716508, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:18:11.473722: step 14400, loss 0.00816209, acc 0.996855, f1 0.996855\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:18:12.136032: step 14400, loss 2.26201, acc 0.570909, f1 0.568327\n",
      "\n",
      "Current epoch:  800\n",
      "2017-11-15T23:18:12.662279: step 14405, loss 0.00233909, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:18:13.240828: step 14410, loss 0.000636037, acc 1, f1 1\n",
      "2017-11-15T23:18:13.767779: step 14415, loss 0.00507683, acc 0.99707, f1 0.99707\n",
      "Current epoch:  801\n",
      "2017-11-15T23:18:14.257481: step 14420, loss 0.0003298, acc 1, f1 1\n",
      "2017-11-15T23:18:14.781961: step 14425, loss 0.0056753, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:18:15.323942: step 14430, loss 0.00808129, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:18:15.854069: step 14435, loss 0.000491384, acc 1, f1 1\n",
      "Current epoch:  802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:18:16.338922: step 14440, loss 0.00169132, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:18:16.863579: step 14445, loss 0.00197674, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:18:17.382862: step 14450, loss 0.00579662, acc 0.998047, f1 0.998046\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:18:18.041658: step 14450, loss 2.19498, acc 0.575853, f1 0.58043\n",
      "\n",
      "Current epoch:  803\n",
      "2017-11-15T23:18:18.532500: step 14455, loss 0.000516413, acc 1, f1 1\n",
      "2017-11-15T23:18:19.054576: step 14460, loss 0.00339845, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:18:19.578157: step 14465, loss 0.00442824, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:18:20.099849: step 14470, loss 0.000339484, acc 1, f1 1\n",
      "Current epoch:  804\n",
      "2017-11-15T23:18:20.596853: step 14475, loss 0.000522132, acc 1, f1 1\n",
      "2017-11-15T23:18:21.114822: step 14480, loss 0.00374917, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:18:21.628732: step 14485, loss 0.0126699, acc 0.994141, f1 0.994139\n",
      "2017-11-15T23:18:22.112413: step 14490, loss 0.00838491, acc 0.996855, f1 0.996855\n",
      "Current epoch:  805\n",
      "2017-11-15T23:18:22.637873: step 14495, loss 0.0063307, acc 0.996094, f1 0.996092\n",
      "2017-11-15T23:18:23.154326: step 14500, loss 0.000267276, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:18:23.786652: step 14500, loss 2.20786, acc 0.574399, f1 0.574701\n",
      "\n",
      "2017-11-15T23:18:24.300550: step 14505, loss 0.0043421, acc 0.99707, f1 0.99707\n",
      "Current epoch:  806\n",
      "2017-11-15T23:18:24.788939: step 14510, loss 0.00276755, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:18:25.306438: step 14515, loss 0.00183812, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:18:25.824874: step 14520, loss 0.00151306, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:18:26.349814: step 14525, loss 0.00259598, acc 0.998047, f1 0.998047\n",
      "Current epoch:  807\n",
      "2017-11-15T23:18:26.839685: step 14530, loss 0.00238713, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:18:27.362180: step 14535, loss 0.00216047, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:18:27.876669: step 14540, loss 0.00269, acc 0.999023, f1 0.999024\n",
      "Current epoch:  808\n",
      "2017-11-15T23:18:28.368581: step 14545, loss 0.000851161, acc 1, f1 1\n",
      "2017-11-15T23:18:28.880011: step 14550, loss 0.000706328, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:18:29.511021: step 14550, loss 2.22719, acc 0.570376, f1 0.569985\n",
      "\n",
      "2017-11-15T23:18:30.031479: step 14555, loss 0.00501724, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:18:30.555935: step 14560, loss 0.00027669, acc 1, f1 1\n",
      "Current epoch:  809\n",
      "2017-11-15T23:18:31.045805: step 14565, loss 0.00891956, acc 0.995117, f1 0.995125\n",
      "2017-11-15T23:18:31.596894: step 14570, loss 0.00227272, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:18:32.118814: step 14575, loss 0.0116544, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:18:32.605572: step 14580, loss 0.00430847, acc 0.998428, f1 0.998427\n",
      "Current epoch:  810\n",
      "2017-11-15T23:18:33.230229: step 14585, loss 0.00854355, acc 0.996094, f1 0.996091\n",
      "2017-11-15T23:18:33.754654: step 14590, loss 0.00295814, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:18:34.274625: step 14595, loss 0.0106552, acc 0.995117, f1 0.995118\n",
      "Current epoch:  811\n",
      "2017-11-15T23:18:34.770713: step 14600, loss 0.00125339, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:18:35.410352: step 14600, loss 2.27661, acc 0.564657, f1 0.562382\n",
      "\n",
      "2017-11-15T23:18:35.923913: step 14605, loss 0.00247716, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:18:36.446428: step 14610, loss 0.00311204, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:18:36.972522: step 14615, loss 0.00416187, acc 0.998047, f1 0.998047\n",
      "Current epoch:  812\n",
      "2017-11-15T23:18:37.483476: step 14620, loss 0.0032599, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:18:37.999424: step 14625, loss 0.00366386, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:18:38.520058: step 14630, loss 0.00952687, acc 0.99707, f1 0.99707\n",
      "Current epoch:  813\n",
      "2017-11-15T23:18:39.006432: step 14635, loss 0.00369602, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:18:39.522878: step 14640, loss 0.00420716, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:18:40.046992: step 14645, loss 0.0065853, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:18:40.562957: step 14650, loss 0.00732314, acc 0.998047, f1 0.998042\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:18:41.199908: step 14650, loss 2.1782, acc 0.580603, f1 0.583824\n",
      "\n",
      "Current epoch:  814\n",
      "2017-11-15T23:18:41.730380: step 14655, loss 0.00122058, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:18:42.248367: step 14660, loss 0.00447688, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:18:42.784922: step 14665, loss 0.00022892, acc 1, f1 1\n",
      "2017-11-15T23:18:43.269490: step 14670, loss 0.000212903, acc 1, f1 1\n",
      "Current epoch:  815\n",
      "2017-11-15T23:18:43.788478: step 14675, loss 0.00372511, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:18:44.306263: step 14680, loss 0.00956669, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:18:44.832475: step 14685, loss 0.00425805, acc 0.998047, f1 0.998047\n",
      "Current epoch:  816\n",
      "2017-11-15T23:18:45.324472: step 14690, loss 0.00966717, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:18:45.836201: step 14695, loss 0.00035493, acc 1, f1 1\n",
      "2017-11-15T23:18:46.347293: step 14700, loss 0.00382825, acc 0.998047, f1 0.998052\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:18:46.975119: step 14700, loss 2.23945, acc 0.593835, f1 0.583945\n",
      "\n",
      "2017-11-15T23:18:47.486035: step 14705, loss 0.00773826, acc 0.996094, f1 0.996092\n",
      "Current epoch:  817\n",
      "2017-11-15T23:18:47.973885: step 14710, loss 0.0026951, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:18:48.496617: step 14715, loss 0.00767633, acc 0.998047, f1 0.998044\n",
      "2017-11-15T23:18:49.006532: step 14720, loss 0.00719183, acc 0.99707, f1 0.997071\n",
      "Current epoch:  818\n",
      "2017-11-15T23:18:49.505560: step 14725, loss 0.00255528, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:18:50.022471: step 14730, loss 0.00444511, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:18:50.543976: step 14735, loss 0.00170154, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:18:51.068091: step 14740, loss 0.00298078, acc 0.999023, f1 0.999024\n",
      "Current epoch:  819\n",
      "2017-11-15T23:18:51.561071: step 14745, loss 0.00472784, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:18:52.080018: step 14750, loss 0.00188255, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:18:52.727272: step 14750, loss 2.15664, acc 0.590878, f1 0.58792\n",
      "\n",
      "2017-11-15T23:18:53.246920: step 14755, loss 0.00718425, acc 0.996094, f1 0.996095\n",
      "2017-11-15T23:18:53.744954: step 14760, loss 0.00693937, acc 0.996855, f1 0.996855\n",
      "Current epoch:  820\n",
      "2017-11-15T23:18:54.275903: step 14765, loss 0.00678878, acc 0.995117, f1 0.995118\n",
      "2017-11-15T23:18:54.798830: step 14770, loss 0.000556296, acc 1, f1 1\n",
      "2017-11-15T23:18:55.313304: step 14775, loss 0.000252208, acc 1, f1 1\n",
      "Current epoch:  821\n",
      "2017-11-15T23:18:55.805942: step 14780, loss 0.00491647, acc 0.99707, f1 0.997075\n",
      "2017-11-15T23:18:56.322605: step 14785, loss 0.00133681, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:18:56.842588: step 14790, loss 0.00147396, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:18:57.363334: step 14795, loss 0.00298046, acc 0.999023, f1 0.999022\n",
      "Current epoch:  822\n",
      "2017-11-15T23:18:57.848255: step 14800, loss 0.000308344, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:18:58.493564: step 14800, loss 2.2172, acc 0.584723, f1 0.582627\n",
      "\n",
      "2017-11-15T23:18:59.023384: step 14805, loss 0.00215109, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:18:59.550421: step 14810, loss 0.00581286, acc 0.99707, f1 0.997074\n",
      "Current epoch:  823\n",
      "2017-11-15T23:19:00.040976: step 14815, loss 0.00320987, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:19:00.563418: step 14820, loss 0.00504989, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:19:01.092353: step 14825, loss 0.00473526, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:19:01.621240: step 14830, loss 0.00704336, acc 0.998047, f1 0.998047\n",
      "Current epoch:  824\n",
      "2017-11-15T23:19:02.109189: step 14835, loss 0.00317845, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:19:02.639995: step 14840, loss 0.00196133, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:19:03.159429: step 14845, loss 0.00809945, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:19:03.649268: step 14850, loss 0.00567957, acc 0.998428, f1 0.998425\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:19:04.292066: step 14850, loss 2.15378, acc 0.580845, f1 0.582643\n",
      "\n",
      "Current epoch:  825\n",
      "2017-11-15T23:19:04.832042: step 14855, loss 0.00259826, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:19:05.353499: step 14860, loss 0.00477042, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:19:05.868896: step 14865, loss 0.0018825, acc 0.999023, f1 0.999023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch:  826\n",
      "2017-11-15T23:19:06.357783: step 14870, loss 0.000266219, acc 1, f1 1\n",
      "2017-11-15T23:19:06.875717: step 14875, loss 0.00700008, acc 0.996094, f1 0.996093\n",
      "2017-11-15T23:19:07.394168: step 14880, loss 0.00330885, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:19:07.911648: step 14885, loss 0.000501838, acc 1, f1 1\n",
      "Current epoch:  827\n",
      "2017-11-15T23:19:08.400896: step 14890, loss 0.00302394, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:19:08.913833: step 14895, loss 0.0100261, acc 0.99707, f1 0.997072\n",
      "2017-11-15T23:19:09.427870: step 14900, loss 0.00027428, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:19:10.070141: step 14900, loss 2.22083, acc 0.574059, f1 0.572837\n",
      "\n",
      "Current epoch:  828\n",
      "2017-11-15T23:19:10.571072: step 14905, loss 0.00198818, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:19:11.082884: step 14910, loss 0.00162792, acc 1, f1 1\n",
      "2017-11-15T23:19:11.602068: step 14915, loss 0.00276326, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:19:12.114609: step 14920, loss 0.00522385, acc 0.998047, f1 0.998047\n",
      "Current epoch:  829\n",
      "2017-11-15T23:19:12.609412: step 14925, loss 3.14811, acc 0.583008, f1 0.474215\n",
      "2017-11-15T23:19:13.123242: step 14930, loss 0.0105206, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:19:13.648590: step 14935, loss 0.00758108, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:19:14.149027: step 14940, loss 0.011841, acc 0.998428, f1 0.998427\n",
      "Current epoch:  830\n",
      "2017-11-15T23:19:14.676980: step 14945, loss 0.00601113, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:19:15.190922: step 14950, loss 0.00519766, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:19:15.862394: step 14950, loss 1.77335, acc 0.585207, f1 0.584378\n",
      "\n",
      "2017-11-15T23:19:16.387849: step 14955, loss 0.0035672, acc 1, f1 1\n",
      "Current epoch:  831\n",
      "2017-11-15T23:19:16.876993: step 14960, loss 0.00191388, acc 1, f1 1\n",
      "2017-11-15T23:19:17.395231: step 14965, loss 0.00409986, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:19:17.909786: step 14970, loss 0.00519734, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:19:18.428621: step 14975, loss 0.00349239, acc 0.999023, f1 0.999024\n",
      "Current epoch:  832\n",
      "2017-11-15T23:19:18.913522: step 14980, loss 0.00284482, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:19:19.430048: step 14985, loss 0.00239267, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:19:19.942010: step 14990, loss 0.00442177, acc 0.99707, f1 0.99707\n",
      "Current epoch:  833\n",
      "2017-11-15T23:19:20.431073: step 14995, loss 0.00181274, acc 1, f1 1\n",
      "2017-11-15T23:19:20.957121: step 15000, loss 0.00370386, acc 0.99707, f1 0.997069\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:19:21.660658: step 15000, loss 1.86043, acc 0.586322, f1 0.585136\n",
      "\n",
      "2017-11-15T23:19:22.183506: step 15005, loss 0.0011929, acc 1, f1 1\n",
      "2017-11-15T23:19:22.697727: step 15010, loss 0.00173444, acc 0.999023, f1 0.999024\n",
      "Current epoch:  834\n",
      "2017-11-15T23:19:23.179496: step 15015, loss 0.00546942, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:19:23.690468: step 15020, loss 0.00240334, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:19:24.204089: step 15025, loss 0.00326546, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:19:24.691889: step 15030, loss 0.00700449, acc 0.993711, f1 0.993713\n",
      "Current epoch:  835\n",
      "2017-11-15T23:19:25.205753: step 15035, loss 0.00297031, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:19:25.724620: step 15040, loss 0.00414517, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:19:26.243277: step 15045, loss 0.00696575, acc 0.995117, f1 0.995117\n",
      "Current epoch:  836\n",
      "2017-11-15T23:19:26.737290: step 15050, loss 0.00226992, acc 0.999023, f1 0.999024\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:19:27.388033: step 15050, loss 1.93957, acc 0.58259, f1 0.583199\n",
      "\n",
      "2017-11-15T23:19:27.897641: step 15055, loss 0.00304977, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:19:28.424981: step 15060, loss 0.00547267, acc 0.996094, f1 0.996095\n",
      "2017-11-15T23:19:28.944916: step 15065, loss 0.000502332, acc 1, f1 1\n",
      "Current epoch:  837\n",
      "2017-11-15T23:19:29.440817: step 15070, loss 0.00205687, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:19:29.961758: step 15075, loss 0.00686598, acc 0.995117, f1 0.995117\n",
      "2017-11-15T23:19:30.470696: step 15080, loss 0.000409587, acc 1, f1 1\n",
      "Current epoch:  838\n",
      "2017-11-15T23:19:30.961585: step 15085, loss 0.00041639, acc 1, f1 1\n",
      "2017-11-15T23:19:31.491462: step 15090, loss 0.00418399, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:19:32.018033: step 15095, loss 0.00482425, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:19:32.565793: step 15100, loss 0.00257264, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:19:33.234431: step 15100, loss 2.0634, acc 0.585983, f1 0.583581\n",
      "\n",
      "Current epoch:  839\n",
      "2017-11-15T23:19:33.733468: step 15105, loss 0.00300059, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:19:34.254420: step 15110, loss 0.000656389, acc 1, f1 1\n",
      "2017-11-15T23:19:34.786718: step 15115, loss 0.00260761, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:19:35.277251: step 15120, loss 0.0106299, acc 0.993711, f1 0.993709\n",
      "Current epoch:  840\n",
      "2017-11-15T23:19:35.806846: step 15125, loss 0.00798303, acc 0.996094, f1 0.996092\n",
      "2017-11-15T23:19:36.336415: step 15130, loss 0.0031898, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:19:36.860383: step 15135, loss 0.0062329, acc 0.996094, f1 0.996094\n",
      "Current epoch:  841\n",
      "2017-11-15T23:19:37.358745: step 15140, loss 0.00151309, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:19:37.895217: step 15145, loss 0.00209075, acc 1, f1 1\n",
      "2017-11-15T23:19:38.420762: step 15150, loss 0.00395505, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:19:39.099136: step 15150, loss 2.11662, acc 0.581184, f1 0.581677\n",
      "\n",
      "2017-11-15T23:19:39.621772: step 15155, loss 0.000425098, acc 1, f1 1\n",
      "Current epoch:  842\n",
      "2017-11-15T23:19:40.114898: step 15160, loss 0.00446873, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:19:40.637915: step 15165, loss 0.0032238, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:19:41.165379: step 15170, loss 0.00563514, acc 0.998047, f1 0.998051\n",
      "Current epoch:  843\n",
      "2017-11-15T23:19:41.672581: step 15175, loss 0.00114534, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:19:42.195725: step 15180, loss 0.0020045, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:19:42.721010: step 15185, loss 0.00219222, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:19:43.278535: step 15190, loss 0.00741402, acc 0.99707, f1 0.997071\n",
      "Current epoch:  844\n",
      "2017-11-15T23:19:43.778072: step 15195, loss 0.00529637, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:19:44.299574: step 15200, loss 0.00190227, acc 0.999023, f1 0.999024\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:19:44.976053: step 15200, loss 2.17942, acc 0.582784, f1 0.579126\n",
      "\n",
      "2017-11-15T23:19:45.495064: step 15205, loss 0.00599863, acc 0.998047, f1 0.998043\n",
      "2017-11-15T23:19:45.976990: step 15210, loss 0.00334415, acc 0.998428, f1 0.998425\n",
      "Current epoch:  845\n",
      "2017-11-15T23:19:46.506987: step 15215, loss 0.000492973, acc 1, f1 1\n",
      "2017-11-15T23:19:47.029922: step 15220, loss 0.00431533, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:19:47.547456: step 15225, loss 0.00949379, acc 0.996094, f1 0.996094\n",
      "Current epoch:  846\n",
      "2017-11-15T23:19:48.041885: step 15230, loss 0.000700254, acc 1, f1 1\n",
      "2017-11-15T23:19:48.568270: step 15235, loss 0.00253724, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:19:49.113942: step 15240, loss 0.00577524, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:19:49.643403: step 15245, loss 0.000251619, acc 1, f1 1\n",
      "Current epoch:  847\n",
      "2017-11-15T23:19:50.134434: step 15250, loss 0.000489683, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:19:50.791165: step 15250, loss 2.1798, acc 0.587291, f1 0.583834\n",
      "\n",
      "2017-11-15T23:19:51.319061: step 15255, loss 0.0056621, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:19:51.849333: step 15260, loss 0.00663829, acc 0.998047, f1 0.998048\n",
      "Current epoch:  848\n",
      "2017-11-15T23:19:52.355077: step 15265, loss 0.00206432, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:19:52.880832: step 15270, loss 0.000236304, acc 1, f1 1\n",
      "2017-11-15T23:19:53.408601: step 15275, loss 0.000991818, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:19:53.941185: step 15280, loss 0.00740179, acc 0.99707, f1 0.997069\n",
      "Current epoch:  849\n",
      "2017-11-15T23:19:54.450262: step 15285, loss 2.43841, acc 0.624023, f1 0.534843\n",
      "2017-11-15T23:19:54.984211: step 15290, loss 0.0112832, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:19:55.513562: step 15295, loss 0.00773322, acc 1, f1 1\n",
      "2017-11-15T23:19:56.007942: step 15300, loss 0.00895831, acc 0.996855, f1 0.996853\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:19:56.687751: step 15300, loss 1.73789, acc 0.5886, f1 0.588342\n",
      "\n",
      "Current epoch:  850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:19:57.208037: step 15305, loss 0.00475858, acc 1, f1 1\n",
      "2017-11-15T23:19:57.727977: step 15310, loss 0.00789948, acc 0.998047, f1 0.998052\n",
      "2017-11-15T23:19:58.245334: step 15315, loss 0.00505967, acc 0.999023, f1 0.999023\n",
      "Current epoch:  851\n",
      "2017-11-15T23:19:58.730336: step 15320, loss 0.00375958, acc 1, f1 1\n",
      "2017-11-15T23:19:59.245919: step 15325, loss 0.00303459, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:19:59.768375: step 15330, loss 0.0073198, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:20:00.281650: step 15335, loss 0.00477484, acc 0.99707, f1 0.99707\n",
      "Current epoch:  852\n",
      "2017-11-15T23:20:00.758538: step 15340, loss 0.00612134, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:20:01.287550: step 15345, loss 0.00661878, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:20:01.810056: step 15350, loss 0.00241345, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:20:02.467853: step 15350, loss 1.85015, acc 0.587194, f1 0.587066\n",
      "\n",
      "Current epoch:  853\n",
      "2017-11-15T23:20:02.958950: step 15355, loss 0.00438072, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:20:03.474909: step 15360, loss 0.00236043, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:20:03.997878: step 15365, loss 0.00387799, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:20:04.520835: step 15370, loss 0.00358477, acc 0.999023, f1 0.999022\n",
      "Current epoch:  854\n",
      "2017-11-15T23:20:05.012478: step 15375, loss 0.00339702, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:20:05.544175: step 15380, loss 0.00241757, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:20:06.068639: step 15385, loss 0.00281523, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:20:06.564688: step 15390, loss 0.00107008, acc 1, f1 1\n",
      "Current epoch:  855\n",
      "2017-11-15T23:20:07.083140: step 15395, loss 0.00170074, acc 1, f1 1\n",
      "2017-11-15T23:20:07.594584: step 15400, loss 0.00371583, acc 0.998047, f1 0.998046\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:20:08.227773: step 15400, loss 1.94158, acc 0.587679, f1 0.587308\n",
      "\n",
      "2017-11-15T23:20:08.741838: step 15405, loss 0.00512473, acc 0.996094, f1 0.996094\n",
      "Current epoch:  856\n",
      "2017-11-15T23:20:09.223734: step 15410, loss 0.00368931, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:20:09.741878: step 15415, loss 0.00205501, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:20:10.252286: step 15420, loss 0.00342825, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:20:10.777311: step 15425, loss 0.00387815, acc 0.998047, f1 0.998042\n",
      "Current epoch:  857\n",
      "2017-11-15T23:20:11.265778: step 15430, loss 0.00212962, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:20:11.790889: step 15435, loss 0.00191483, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:20:12.312364: step 15440, loss 0.00164278, acc 0.999023, f1 0.999023\n",
      "Current epoch:  858\n",
      "2017-11-15T23:20:12.847581: step 15445, loss 0.0037605, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:20:13.371879: step 15450, loss 0.00293324, acc 0.998047, f1 0.998048\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:20:14.036171: step 15450, loss 2.0439, acc 0.590684, f1 0.588605\n",
      "\n",
      "2017-11-15T23:20:14.557139: step 15455, loss 0.00217772, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:20:15.082684: step 15460, loss 0.00548528, acc 0.99707, f1 0.997069\n",
      "Current epoch:  859\n",
      "2017-11-15T23:20:15.574552: step 15465, loss 0.000361782, acc 1, f1 1\n",
      "2017-11-15T23:20:16.106254: step 15470, loss 0.00549299, acc 0.995117, f1 0.995118\n",
      "2017-11-15T23:20:16.640249: step 15475, loss 0.00295661, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:20:17.133572: step 15480, loss 0.00478854, acc 0.998428, f1 0.99843\n",
      "Current epoch:  860\n",
      "2017-11-15T23:20:17.656060: step 15485, loss 0.00202258, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:20:18.174767: step 15490, loss 0.00194062, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:20:18.698741: step 15495, loss 0.00324036, acc 0.998047, f1 0.998048\n",
      "Current epoch:  861\n",
      "2017-11-15T23:20:19.195222: step 15500, loss 0.00284682, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:20:19.866209: step 15500, loss 2.13076, acc 0.57498, f1 0.576411\n",
      "\n",
      "2017-11-15T23:20:20.379272: step 15505, loss 0.00624029, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:20:20.894538: step 15510, loss 0.00282184, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:20:21.405074: step 15515, loss 0.000321363, acc 1, f1 1\n",
      "Current epoch:  862\n",
      "2017-11-15T23:20:21.917809: step 15520, loss 0.00220006, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:20:22.445190: step 15525, loss 0.010176, acc 0.995117, f1 0.995113\n",
      "2017-11-15T23:20:22.969140: step 15530, loss 0.00326873, acc 0.999023, f1 0.999023\n",
      "Current epoch:  863\n",
      "2017-11-15T23:20:23.466538: step 15535, loss 0.0025352, acc 0.998047, f1 0.998043\n",
      "2017-11-15T23:20:23.988123: step 15540, loss 0.00789553, acc 0.995117, f1 0.995117\n",
      "2017-11-15T23:20:24.511426: step 15545, loss 0.003138, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:20:25.033419: step 15550, loss 0.00271273, acc 0.999023, f1 0.999022\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:20:25.687796: step 15550, loss 2.11918, acc 0.593205, f1 0.592704\n",
      "\n",
      "Current epoch:  864\n",
      "2017-11-15T23:20:26.181236: step 15555, loss 0.00252034, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:20:26.703786: step 15560, loss 0.00544355, acc 0.99707, f1 0.997067\n",
      "2017-11-15T23:20:27.232349: step 15565, loss 0.00815395, acc 0.995117, f1 0.995122\n",
      "2017-11-15T23:20:27.733740: step 15570, loss 0.0102289, acc 0.995283, f1 0.995283\n",
      "Current epoch:  865\n",
      "2017-11-15T23:20:28.260665: step 15575, loss 0.000736784, acc 1, f1 1\n",
      "2017-11-15T23:20:28.779623: step 15580, loss 0.00543613, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:20:29.299660: step 15585, loss 0.00349707, acc 0.998047, f1 0.998051\n",
      "Current epoch:  866\n",
      "2017-11-15T23:20:29.795624: step 15590, loss 0.00515866, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:20:30.313910: step 15595, loss 0.00390934, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:20:30.840661: step 15600, loss 0.00826498, acc 0.995117, f1 0.995117\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:20:31.505925: step 15600, loss 2.26919, acc 0.600523, f1 0.587978\n",
      "\n",
      "2017-11-15T23:20:32.028906: step 15605, loss 0.00501072, acc 0.99707, f1 0.997066\n",
      "Current epoch:  867\n",
      "2017-11-15T23:20:32.528259: step 15610, loss 0.00178187, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:20:33.061256: step 15615, loss 0.00431398, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:20:33.583435: step 15620, loss 0.00619682, acc 0.996094, f1 0.996091\n",
      "Current epoch:  868\n",
      "2017-11-15T23:20:34.079980: step 15625, loss 0.0096095, acc 0.99707, f1 0.997072\n",
      "2017-11-15T23:20:34.604989: step 15630, loss 0.000193571, acc 1, f1 1\n",
      "2017-11-15T23:20:35.124141: step 15635, loss 0.0021666, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:20:35.655206: step 15640, loss 0.00270793, acc 0.999023, f1 0.999023\n",
      "Current epoch:  869\n",
      "2017-11-15T23:20:36.148571: step 15645, loss 0.00275275, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:20:36.669242: step 15650, loss 0.000230091, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:20:37.323038: step 15650, loss 2.17329, acc 0.579682, f1 0.578471\n",
      "\n",
      "2017-11-15T23:20:37.850838: step 15655, loss 0.0040882, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:20:38.366962: step 15660, loss 0.00459921, acc 0.998428, f1 0.998428\n",
      "Current epoch:  870\n",
      "2017-11-15T23:20:38.896448: step 15665, loss 0.00139789, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:20:39.430964: step 15670, loss 0.0131812, acc 0.995117, f1 0.995118\n",
      "2017-11-15T23:20:39.961060: step 15675, loss 0.00414783, acc 0.998047, f1 0.998046\n",
      "Current epoch:  871\n",
      "2017-11-15T23:20:40.450881: step 15680, loss 0.000327236, acc 1, f1 1\n",
      "2017-11-15T23:20:40.964807: step 15685, loss 0.00190923, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:20:41.484481: step 15690, loss 0.0030678, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:20:42.008505: step 15695, loss 0.00408606, acc 0.998047, f1 0.998047\n",
      "Current epoch:  872\n",
      "2017-11-15T23:20:42.506113: step 15700, loss 0.004496, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:20:43.164543: step 15700, loss 2.24885, acc 0.583511, f1 0.578236\n",
      "\n",
      "2017-11-15T23:20:43.737247: step 15705, loss 0.0040501, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:20:44.260725: step 15710, loss 0.00364932, acc 0.999023, f1 0.999025\n",
      "Current epoch:  873\n",
      "2017-11-15T23:20:44.762627: step 15715, loss 0.00206541, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:20:45.282541: step 15720, loss 0.00739518, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:20:45.802676: step 15725, loss 0.00607902, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:20:46.324046: step 15730, loss 0.000313395, acc 1, f1 1\n",
      "Current epoch:  874\n",
      "2017-11-15T23:20:46.825593: step 15735, loss 0.00046827, acc 1, f1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:20:47.347536: step 15740, loss 0.00537268, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:20:47.870274: step 15745, loss 0.00347719, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:20:48.357124: step 15750, loss 0.000342401, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:20:49.014783: step 15750, loss 2.20377, acc 0.56708, f1 0.568304\n",
      "\n",
      "Current epoch:  875\n",
      "2017-11-15T23:20:49.552684: step 15755, loss 0.00430107, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:20:50.079625: step 15760, loss 0.00285561, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:20:50.603942: step 15765, loss 0.00348348, acc 0.998047, f1 0.998046\n",
      "Current epoch:  876\n",
      "2017-11-15T23:20:51.096349: step 15770, loss 0.00126238, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:20:51.615443: step 15775, loss 0.00145381, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:20:52.140093: step 15780, loss 0.000208197, acc 1, f1 1\n",
      "2017-11-15T23:20:52.661055: step 15785, loss 0.00431731, acc 0.998047, f1 0.998047\n",
      "Current epoch:  877\n",
      "2017-11-15T23:20:53.151937: step 15790, loss 0.000337982, acc 1, f1 1\n",
      "2017-11-15T23:20:53.674915: step 15795, loss 0.00716359, acc 0.99707, f1 0.99706\n",
      "2017-11-15T23:20:54.198772: step 15800, loss 0.00452955, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:20:54.875115: step 15800, loss 2.17285, acc 0.597857, f1 0.592255\n",
      "\n",
      "Current epoch:  878\n",
      "2017-11-15T23:20:55.369365: step 15805, loss 0.00414892, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:20:55.890103: step 15810, loss 0.00381914, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:20:56.413225: step 15815, loss 0.010651, acc 0.994141, f1 0.994143\n",
      "2017-11-15T23:20:56.942831: step 15820, loss 0.00227491, acc 0.999023, f1 0.999024\n",
      "Current epoch:  879\n",
      "2017-11-15T23:20:57.435129: step 15825, loss 0.00281963, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:20:57.958564: step 15830, loss 0.00379032, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:20:58.480384: step 15835, loss 0.00214106, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:20:58.971179: step 15840, loss 0.00721419, acc 0.996855, f1 0.996855\n",
      "Current epoch:  880\n",
      "2017-11-15T23:20:59.506053: step 15845, loss 0.00104399, acc 1, f1 1\n",
      "2017-11-15T23:21:00.032498: step 15850, loss 0.00433312, acc 0.998047, f1 0.998046\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:21:00.729689: step 15850, loss 2.2042, acc 0.571345, f1 0.573898\n",
      "\n",
      "2017-11-15T23:21:01.239587: step 15855, loss 0.00191936, acc 0.999023, f1 0.999023\n",
      "Current epoch:  881\n",
      "2017-11-15T23:21:01.731470: step 15860, loss 0.00116031, acc 1, f1 1\n",
      "2017-11-15T23:21:02.253070: step 15865, loss 0.00243345, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:21:02.779215: step 15870, loss 0.00253704, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:21:03.309151: step 15875, loss 0.00984124, acc 0.99707, f1 0.997071\n",
      "Current epoch:  882\n",
      "2017-11-15T23:21:03.808785: step 15880, loss 0.00389657, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:21:04.338086: step 15885, loss 0.00696252, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:21:04.859957: step 15890, loss 0.0043157, acc 0.998047, f1 0.998047\n",
      "Current epoch:  883\n",
      "2017-11-15T23:21:05.352373: step 15895, loss 0.0125957, acc 0.994141, f1 0.99414\n",
      "2017-11-15T23:21:05.894679: step 15900, loss 1.19565, acc 0.666992, f1 0.609217\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:21:06.568441: step 15900, loss 2.41998, acc 0.555157, f1 0.523109\n",
      "\n",
      "2017-11-15T23:21:07.086403: step 15905, loss 0.00483768, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:21:07.609409: step 15910, loss 0.0045336, acc 1, f1 1\n",
      "Current epoch:  884\n",
      "2017-11-15T23:21:08.097679: step 15915, loss 0.00521785, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:21:08.736308: step 15920, loss 0.00407737, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:21:09.256296: step 15925, loss 0.00366697, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:21:09.740179: step 15930, loss 0.00203802, acc 1, f1 1\n",
      "Current epoch:  885\n",
      "2017-11-15T23:21:10.263125: step 15935, loss 0.00513205, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:21:10.784358: step 15940, loss 0.00424537, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:21:11.312412: step 15945, loss 0.00507849, acc 0.998047, f1 0.998047\n",
      "Current epoch:  886\n",
      "2017-11-15T23:21:11.808371: step 15950, loss 0.0039953, acc 0.998047, f1 0.998048\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:21:12.463711: step 15950, loss 1.84021, acc 0.583559, f1 0.583352\n",
      "\n",
      "2017-11-15T23:21:13.000940: step 15955, loss 0.00335636, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:21:13.518866: step 15960, loss 0.00391409, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:21:14.038792: step 15965, loss 0.00605394, acc 0.996094, f1 0.996094\n",
      "Current epoch:  887\n",
      "2017-11-15T23:21:14.533189: step 15970, loss 0.00418272, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:21:15.049358: step 15975, loss 0.00257124, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:21:15.567793: step 15980, loss 0.003521, acc 0.998047, f1 0.998047\n",
      "Current epoch:  888\n",
      "2017-11-15T23:21:16.061953: step 15985, loss 0.00324213, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:21:16.592927: step 15990, loss 0.00147087, acc 1, f1 1\n",
      "2017-11-15T23:21:17.125952: step 15995, loss 0.00784202, acc 0.995117, f1 0.995116\n",
      "2017-11-15T23:21:17.645558: step 16000, loss 0.0028765, acc 0.998047, f1 0.998048\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:21:18.300576: step 16000, loss 1.90626, acc 0.584141, f1 0.584155\n",
      "\n",
      "Current epoch:  889\n",
      "2017-11-15T23:21:18.790014: step 16005, loss 0.00343033, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:21:19.307638: step 16010, loss 0.00256488, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:21:19.826709: step 16015, loss 0.00166899, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:21:20.318812: step 16020, loss 0.0052133, acc 0.995283, f1 0.995273\n",
      "Current epoch:  890\n",
      "2017-11-15T23:21:20.844364: step 16025, loss 0.0037209, acc 0.99707, f1 0.997074\n",
      "2017-11-15T23:21:21.366805: step 16030, loss 0.00155563, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:21:21.893726: step 16035, loss 0.00145356, acc 0.999023, f1 0.999023\n",
      "Current epoch:  891\n",
      "2017-11-15T23:21:22.408393: step 16040, loss 0.00061079, acc 1, f1 1\n",
      "2017-11-15T23:21:22.937345: step 16045, loss 0.00280654, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:21:23.465045: step 16050, loss 0.00298712, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:21:24.118878: step 16050, loss 1.99485, acc 0.582493, f1 0.583557\n",
      "\n",
      "2017-11-15T23:21:24.642323: step 16055, loss 0.00434581, acc 0.99707, f1 0.99707\n",
      "Current epoch:  892\n",
      "2017-11-15T23:21:25.130812: step 16060, loss 0.00526259, acc 0.996094, f1 0.996092\n",
      "2017-11-15T23:21:25.653969: step 16065, loss 0.000337213, acc 1, f1 1\n",
      "2017-11-15T23:21:26.174124: step 16070, loss 0.00127891, acc 0.999023, f1 0.999024\n",
      "Current epoch:  893\n",
      "2017-11-15T23:21:26.672988: step 16075, loss 0.00120143, acc 1, f1 1\n",
      "2017-11-15T23:21:27.192456: step 16080, loss 0.000640069, acc 1, f1 1\n",
      "2017-11-15T23:21:27.734249: step 16085, loss 0.00293181, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:21:28.265218: step 16090, loss 0.00354066, acc 0.99707, f1 0.99707\n",
      "Current epoch:  894\n",
      "2017-11-15T23:21:28.758453: step 16095, loss 0.00241094, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:21:29.278141: step 16100, loss 0.00463947, acc 0.998047, f1 0.998046\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:21:29.943454: step 16100, loss 2.07308, acc 0.586613, f1 0.586442\n",
      "\n",
      "2017-11-15T23:21:30.458863: step 16105, loss 0.00344047, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:21:30.949908: step 16110, loss 0.00311216, acc 0.998428, f1 0.998428\n",
      "Current epoch:  895\n",
      "2017-11-15T23:21:31.469619: step 16115, loss 0.00189598, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:21:31.997593: step 16120, loss 0.00276813, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:21:32.526389: step 16125, loss 0.00617173, acc 0.996094, f1 0.996094\n",
      "Current epoch:  896\n",
      "2017-11-15T23:21:33.027764: step 16130, loss 0.00161217, acc 1, f1 1\n",
      "2017-11-15T23:21:33.560369: step 16135, loss 0.0028008, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:21:34.081240: step 16140, loss 0.00548799, acc 0.99707, f1 0.997075\n",
      "2017-11-15T23:21:34.603181: step 16145, loss 0.00436283, acc 0.998047, f1 0.998046\n",
      "Current epoch:  897\n",
      "2017-11-15T23:21:35.092871: step 16150, loss 0.00356408, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:21:35.748137: step 16150, loss 2.10628, acc 0.586952, f1 0.587738\n",
      "\n",
      "2017-11-15T23:21:36.268126: step 16155, loss 0.00151122, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:21:36.792955: step 16160, loss 0.00844124, acc 0.996094, f1 0.996097\n",
      "Current epoch:  898\n",
      "2017-11-15T23:21:37.288349: step 16165, loss 0.00541638, acc 0.99707, f1 0.99707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:21:37.814329: step 16170, loss 0.00466214, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:21:38.334554: step 16175, loss 0.00545431, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:21:38.879203: step 16180, loss 0.00433444, acc 0.99707, f1 0.997075\n",
      "Current epoch:  899\n",
      "2017-11-15T23:21:39.371055: step 16185, loss 0.00201541, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:21:39.889206: step 16190, loss 0.00193266, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:21:40.414161: step 16195, loss 0.00759429, acc 0.99707, f1 0.997066\n",
      "2017-11-15T23:21:40.903510: step 16200, loss 0.00613536, acc 0.996855, f1 0.996855\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:21:41.557323: step 16200, loss 2.28421, acc 0.56645, f1 0.564166\n",
      "\n",
      "Current epoch:  900\n",
      "2017-11-15T23:21:42.077853: step 16205, loss 0.00240662, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:21:42.659974: step 16210, loss 0.00199552, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:21:43.181007: step 16215, loss 0.00181313, acc 0.999023, f1 0.999023\n",
      "Current epoch:  901\n",
      "2017-11-15T23:21:43.685400: step 16220, loss 0.00145582, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:21:44.215697: step 16225, loss 0.00407165, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:21:44.745226: step 16230, loss 0.00280243, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:21:45.267782: step 16235, loss 0.00444226, acc 0.99707, f1 0.99707\n",
      "Current epoch:  902\n",
      "2017-11-15T23:21:45.768703: step 16240, loss 0.00304369, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:21:46.288675: step 16245, loss 0.00404137, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:21:46.814432: step 16250, loss 0.00605349, acc 0.99707, f1 0.997071\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:21:47.475363: step 16250, loss 2.20042, acc 0.593059, f1 0.58684\n",
      "\n",
      "Current epoch:  903\n",
      "2017-11-15T23:21:47.972792: step 16255, loss 0.000210442, acc 1, f1 1\n",
      "2017-11-15T23:21:48.496251: step 16260, loss 0.00454826, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:21:49.020685: step 16265, loss 0.00183706, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:21:49.549731: step 16270, loss 0.00780134, acc 0.99707, f1 0.997074\n",
      "Current epoch:  904\n",
      "2017-11-15T23:21:50.054674: step 16275, loss 0.00442688, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:21:50.576697: step 16280, loss 0.00305847, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:21:51.098497: step 16285, loss 0.000186717, acc 1, f1 1\n",
      "2017-11-15T23:21:51.591261: step 16290, loss 0.00727015, acc 0.996855, f1 0.996855\n",
      "Current epoch:  905\n",
      "2017-11-15T23:21:52.117358: step 16295, loss 0.000894883, acc 1, f1 1\n",
      "2017-11-15T23:21:52.641981: step 16300, loss 0.00155729, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:21:53.305251: step 16300, loss 2.15654, acc 0.590829, f1 0.58973\n",
      "\n",
      "2017-11-15T23:21:53.829277: step 16305, loss 0.000261574, acc 1, f1 1\n",
      "Current epoch:  906\n",
      "2017-11-15T23:21:54.323759: step 16310, loss 0.000284179, acc 1, f1 1\n",
      "2017-11-15T23:21:54.848876: step 16315, loss 0.00239537, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:21:55.388935: step 16320, loss 0.00566417, acc 0.998047, f1 0.998052\n",
      "2017-11-15T23:21:55.912546: step 16325, loss 0.00815975, acc 0.99707, f1 0.997071\n",
      "Current epoch:  907\n",
      "2017-11-15T23:21:56.405921: step 16330, loss 0.00144132, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:21:56.929363: step 16335, loss 0.00536296, acc 0.99707, f1 0.997073\n",
      "2017-11-15T23:21:57.456925: step 16340, loss 0.00643996, acc 0.99707, f1 0.997067\n",
      "Current epoch:  908\n",
      "2017-11-15T23:21:57.950840: step 16345, loss 0.00356574, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:21:58.473339: step 16350, loss 0.00337836, acc 0.998047, f1 0.998046\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:21:59.129157: step 16350, loss 2.14517, acc 0.583656, f1 0.583734\n",
      "\n",
      "2017-11-15T23:21:59.654456: step 16355, loss 0.00191823, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:22:00.179910: step 16360, loss 0.0134867, acc 0.996094, f1 0.996096\n",
      "Current epoch:  909\n",
      "2017-11-15T23:22:00.679915: step 16365, loss 0.000281849, acc 1, f1 1\n",
      "2017-11-15T23:22:01.200880: step 16370, loss 0.00433891, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:22:01.726862: step 16375, loss 0.00630355, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:22:02.217735: step 16380, loss 0.00809576, acc 0.995283, f1 0.995283\n",
      "Current epoch:  910\n",
      "2017-11-15T23:22:02.745723: step 16385, loss 0.00149353, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:22:03.264038: step 16390, loss 0.00018352, acc 1, f1 1\n",
      "2017-11-15T23:22:03.786021: step 16395, loss 0.00459405, acc 0.998047, f1 0.998046\n",
      "Current epoch:  911\n",
      "2017-11-15T23:22:04.284075: step 16400, loss 0.174979, acc 0.907227, f1 0.893017\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:22:04.942845: step 16400, loss 8.78051, acc 0.279614, f1 0.222062\n",
      "\n",
      "2017-11-15T23:22:05.462192: step 16405, loss 0.00932555, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:22:05.980634: step 16410, loss 0.00972108, acc 0.99707, f1 0.997072\n",
      "2017-11-15T23:22:06.515091: step 16415, loss 0.00590154, acc 0.99707, f1 0.997075\n",
      "Current epoch:  912\n",
      "2017-11-15T23:22:07.007563: step 16420, loss 0.00577473, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:22:07.533696: step 16425, loss 0.00153219, acc 1, f1 1\n",
      "2017-11-15T23:22:08.056137: step 16430, loss 0.00517001, acc 0.998047, f1 0.998047\n",
      "Current epoch:  913\n",
      "2017-11-15T23:22:08.557509: step 16435, loss 0.00492365, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:22:09.076954: step 16440, loss 0.00292563, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:22:09.594412: step 16445, loss 0.0031528, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:22:10.114376: step 16450, loss 0.00122439, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:22:10.766045: step 16450, loss 1.95468, acc 0.590829, f1 0.588147\n",
      "\n",
      "Current epoch:  914\n",
      "2017-11-15T23:22:11.252419: step 16455, loss 0.00282059, acc 1, f1 1\n",
      "2017-11-15T23:22:11.785502: step 16460, loss 0.00186051, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:22:12.305520: step 16465, loss 0.00164931, acc 1, f1 1\n",
      "2017-11-15T23:22:12.825440: step 16470, loss 0.00769251, acc 0.996855, f1 0.996865\n",
      "Current epoch:  915\n",
      "2017-11-15T23:22:13.348373: step 16475, loss 0.00218587, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:22:13.869918: step 16480, loss 0.00150206, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:22:14.389360: step 16485, loss 0.00361912, acc 0.998047, f1 0.99805\n",
      "Current epoch:  916\n",
      "2017-11-15T23:22:14.879196: step 16490, loss 0.00417471, acc 0.99707, f1 0.997072\n",
      "2017-11-15T23:22:15.398786: step 16495, loss 0.00363465, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:22:15.918226: step 16500, loss 0.00466902, acc 0.996094, f1 0.996093\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:22:16.577998: step 16500, loss 1.99891, acc 0.588648, f1 0.587062\n",
      "\n",
      "2017-11-15T23:22:17.104911: step 16505, loss 0.00435681, acc 0.996094, f1 0.996095\n",
      "Current epoch:  917\n",
      "2017-11-15T23:22:17.599100: step 16510, loss 0.00289951, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:22:18.120124: step 16515, loss 0.00440705, acc 0.998047, f1 0.998052\n",
      "2017-11-15T23:22:18.644562: step 16520, loss 0.00268685, acc 0.998047, f1 0.998046\n",
      "Current epoch:  918\n",
      "2017-11-15T23:22:19.141843: step 16525, loss 0.00284849, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:22:19.659571: step 16530, loss 0.00519835, acc 0.996094, f1 0.996095\n",
      "2017-11-15T23:22:20.182503: step 16535, loss 0.0026561, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:22:20.702450: step 16540, loss 0.00672489, acc 0.995117, f1 0.995117\n",
      "Current epoch:  919\n",
      "2017-11-15T23:22:21.190819: step 16545, loss 0.000377019, acc 1, f1 1\n",
      "2017-11-15T23:22:21.706284: step 16550, loss 0.00580249, acc 0.99707, f1 0.99707\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:22:22.366325: step 16550, loss 2.03885, acc 0.593641, f1 0.591687\n",
      "\n",
      "2017-11-15T23:22:22.899894: step 16555, loss 0.00378122, acc 0.998047, f1 0.998042\n",
      "2017-11-15T23:22:23.402745: step 16560, loss 0.00595496, acc 0.995283, f1 0.995284\n",
      "Current epoch:  920\n",
      "2017-11-15T23:22:23.926882: step 16565, loss 0.00351542, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:22:24.449298: step 16570, loss 0.00387986, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:22:24.970321: step 16575, loss 0.000928369, acc 1, f1 1\n",
      "Current epoch:  921\n",
      "2017-11-15T23:22:25.465450: step 16580, loss 0.00220775, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:22:25.983491: step 16585, loss 0.00315153, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:22:26.509937: step 16590, loss 0.00416086, acc 0.998047, f1 0.998042\n",
      "2017-11-15T23:22:27.034891: step 16595, loss 0.00447813, acc 0.998047, f1 0.998046\n",
      "Current epoch:  922\n",
      "2017-11-15T23:22:27.530416: step 16600, loss 0.00233482, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:22:28.194490: step 16600, loss 2.11308, acc 0.58637, f1 0.586332\n",
      "\n",
      "2017-11-15T23:22:28.723435: step 16605, loss 0.000798045, acc 1, f1 1\n",
      "2017-11-15T23:22:29.243088: step 16610, loss 0.00152259, acc 0.999023, f1 0.999024\n",
      "Current epoch:  923\n",
      "2017-11-15T23:22:29.733641: step 16615, loss 0.00290007, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:22:30.250061: step 16620, loss 0.00510389, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:22:30.772006: step 16625, loss 0.0127211, acc 0.994141, f1 0.994143\n",
      "2017-11-15T23:22:31.291795: step 16630, loss 0.00156845, acc 0.999023, f1 0.999024\n",
      "Current epoch:  924\n",
      "2017-11-15T23:22:31.788500: step 16635, loss 0.00277088, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:22:32.311023: step 16640, loss 0.00497832, acc 0.996094, f1 0.996095\n",
      "2017-11-15T23:22:32.835505: step 16645, loss 0.0046217, acc 0.99707, f1 0.997074\n",
      "2017-11-15T23:22:33.322703: step 16650, loss 0.00028278, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:22:34.023068: step 16650, loss 2.15689, acc 0.591072, f1 0.589018\n",
      "\n",
      "Current epoch:  925\n",
      "2017-11-15T23:22:34.550804: step 16655, loss 0.00228457, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:22:35.070552: step 16660, loss 0.00580102, acc 0.996094, f1 0.996095\n",
      "2017-11-15T23:22:35.594240: step 16665, loss 0.00534066, acc 0.998047, f1 0.998048\n",
      "Current epoch:  926\n",
      "2017-11-15T23:22:36.091123: step 16670, loss 0.00112947, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:22:36.615049: step 16675, loss 0.00225074, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:22:37.137066: step 16680, loss 0.00452575, acc 0.998047, f1 0.998043\n",
      "2017-11-15T23:22:37.659200: step 16685, loss 0.008186, acc 0.995117, f1 0.995117\n",
      "Current epoch:  927\n",
      "2017-11-15T23:22:38.151633: step 16690, loss 0.00185275, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:22:38.672081: step 16695, loss 0.00985172, acc 0.995117, f1 0.995108\n",
      "2017-11-15T23:22:39.199745: step 16700, loss 0.000165894, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:22:39.885478: step 16700, loss 2.17691, acc 0.582978, f1 0.582487\n",
      "\n",
      "Current epoch:  928\n",
      "2017-11-15T23:22:40.377937: step 16705, loss 0.00163782, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:22:40.894758: step 16710, loss 0.00193097, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:22:41.421736: step 16715, loss 0.00021354, acc 1, f1 1\n",
      "2017-11-15T23:22:41.944674: step 16720, loss 0.00267846, acc 0.999023, f1 0.999022\n",
      "Current epoch:  929\n",
      "2017-11-15T23:22:42.441290: step 16725, loss 0.00737325, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:22:42.964104: step 16730, loss 0.000285391, acc 1, f1 1\n",
      "2017-11-15T23:22:43.491143: step 16735, loss 0.00356674, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:22:43.981022: step 16740, loss 0.00233204, acc 0.998428, f1 0.998428\n",
      "Current epoch:  930\n",
      "2017-11-15T23:22:44.528373: step 16745, loss 0.000189656, acc 1, f1 1\n",
      "2017-11-15T23:22:45.064362: step 16750, loss 0.000533297, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:22:45.717945: step 16750, loss 2.18321, acc 0.59398, f1 0.590074\n",
      "\n",
      "2017-11-15T23:22:46.274512: step 16755, loss 0.00235472, acc 0.999023, f1 0.999023\n",
      "Current epoch:  931\n",
      "2017-11-15T23:22:46.767736: step 16760, loss 0.00223357, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:22:47.286908: step 16765, loss 0.00418659, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:22:47.808587: step 16770, loss 0.00296054, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:22:48.329519: step 16775, loss 0.0027118, acc 0.999023, f1 0.999023\n",
      "Current epoch:  932\n",
      "2017-11-15T23:22:48.828887: step 16780, loss 0.00313583, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:22:49.357964: step 16785, loss 0.00606459, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:22:49.883837: step 16790, loss 0.00220945, acc 0.999023, f1 0.999022\n",
      "Current epoch:  933\n",
      "2017-11-15T23:22:50.388723: step 16795, loss 0.000696291, acc 1, f1 1\n",
      "2017-11-15T23:22:50.908644: step 16800, loss 0.000432826, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:22:51.568044: step 16800, loss 2.24868, acc 0.573381, f1 0.572808\n",
      "\n",
      "2017-11-15T23:22:52.096068: step 16805, loss 0.0102212, acc 0.996094, f1 0.996095\n",
      "2017-11-15T23:22:52.621155: step 16810, loss 0.00144381, acc 0.999023, f1 0.999022\n",
      "Current epoch:  934\n",
      "2017-11-15T23:22:53.109803: step 16815, loss 0.00543796, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:22:53.640878: step 16820, loss 0.00178763, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:22:54.164836: step 16825, loss 0.00296446, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:22:54.657563: step 16830, loss 0.00266333, acc 0.998428, f1 0.998428\n",
      "Current epoch:  935\n",
      "2017-11-15T23:22:55.175083: step 16835, loss 0.00177012, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:22:55.705709: step 16840, loss 0.00287773, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:22:56.233185: step 16845, loss 0.00377833, acc 0.998047, f1 0.998047\n",
      "Current epoch:  936\n",
      "2017-11-15T23:22:56.732280: step 16850, loss 0.00592209, acc 0.99707, f1 0.997071\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:22:57.395572: step 16850, loss 2.29783, acc 0.582105, f1 0.575862\n",
      "\n",
      "2017-11-15T23:22:57.909978: step 16855, loss 0.00283558, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:22:58.434466: step 16860, loss 0.0020394, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:22:58.958909: step 16865, loss 0.00340197, acc 0.999023, f1 0.999023\n",
      "Current epoch:  937\n",
      "2017-11-15T23:22:59.454806: step 16870, loss 0.00153875, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:22:59.980278: step 16875, loss 0.00211311, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:23:00.507235: step 16880, loss 0.00268164, acc 0.999023, f1 0.999024\n",
      "Current epoch:  938\n",
      "2017-11-15T23:23:01.012331: step 16885, loss 0.00133712, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:23:01.561406: step 16890, loss 0.00702419, acc 0.99707, f1 0.997072\n",
      "2017-11-15T23:23:02.082287: step 16895, loss 0.00146693, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:23:02.610631: step 16900, loss 0.00757348, acc 0.995117, f1 0.995119\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:23:03.263534: step 16900, loss 2.31666, acc 0.581911, f1 0.574547\n",
      "\n",
      "Current epoch:  939\n",
      "2017-11-15T23:23:03.750239: step 16905, loss 0.00414148, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:23:04.271682: step 16910, loss 0.00263562, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:23:04.795501: step 16915, loss 0.00451064, acc 0.998047, f1 0.998042\n",
      "2017-11-15T23:23:05.291267: step 16920, loss 0.00381744, acc 0.996855, f1 0.996856\n",
      "Current epoch:  940\n",
      "2017-11-15T23:23:05.811926: step 16925, loss 0.000729869, acc 1, f1 1\n",
      "2017-11-15T23:23:06.336564: step 16930, loss 0.00829697, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:23:06.877110: step 16935, loss 0.00241975, acc 0.999023, f1 0.999022\n",
      "Current epoch:  941\n",
      "2017-11-15T23:23:07.377819: step 16940, loss 0.00463661, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:23:07.897361: step 16945, loss 0.00587968, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:23:08.425641: step 16950, loss 0.00437917, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:23:09.083473: step 16950, loss 2.19641, acc 0.585934, f1 0.584241\n",
      "\n",
      "2017-11-15T23:23:09.602633: step 16955, loss 0.00263699, acc 0.999023, f1 0.999023\n",
      "Current epoch:  942\n",
      "2017-11-15T23:23:10.090505: step 16960, loss 0.00576126, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:23:10.611996: step 16965, loss 0.00315016, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:23:11.121633: step 16970, loss 0.00308688, acc 0.998047, f1 0.998047\n",
      "Current epoch:  943\n",
      "2017-11-15T23:23:11.608029: step 16975, loss 0.00136607, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:23:12.138780: step 16980, loss 0.000364422, acc 1, f1 1\n",
      "2017-11-15T23:23:12.671247: step 16985, loss 0.00528867, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:23:13.190545: step 16990, loss 0.00712938, acc 0.996094, f1 0.996093\n",
      "Current epoch:  944\n",
      "2017-11-15T23:23:13.691867: step 16995, loss 0.00122251, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:23:14.210782: step 17000, loss 0.00323184, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:23:14.881023: step 17000, loss 2.22402, acc 0.576677, f1 0.580269\n",
      "\n",
      "2017-11-15T23:23:15.402117: step 17005, loss 0.00170909, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:23:15.894509: step 17010, loss 0.0121708, acc 0.995283, f1 0.995282\n",
      "Current epoch:  945\n",
      "2017-11-15T23:23:16.420841: step 17015, loss 0.000150948, acc 1, f1 1\n",
      "2017-11-15T23:23:16.938651: step 17020, loss 0.00497775, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:23:17.457599: step 17025, loss 0.00306004, acc 0.999023, f1 0.999023\n",
      "Current epoch:  946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:23:17.973216: step 17030, loss 0.00217115, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:23:18.501145: step 17035, loss 0.000240805, acc 1, f1 1\n",
      "2017-11-15T23:23:19.022773: step 17040, loss 0.00991169, acc 0.996094, f1 0.996097\n",
      "2017-11-15T23:23:19.548088: step 17045, loss 0.000404683, acc 1, f1 1\n",
      "Current epoch:  947\n",
      "2017-11-15T23:23:20.037970: step 17050, loss 0.00210672, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:23:20.694383: step 17050, loss 2.19879, acc 0.584189, f1 0.583637\n",
      "\n",
      "2017-11-15T23:23:21.207626: step 17055, loss 0.000243022, acc 1, f1 1\n",
      "2017-11-15T23:23:21.726243: step 17060, loss 0.00362078, acc 0.998047, f1 0.998047\n",
      "Current epoch:  948\n",
      "2017-11-15T23:23:22.221510: step 17065, loss 0.000274549, acc 1, f1 1\n",
      "2017-11-15T23:23:22.740524: step 17070, loss 0.00211322, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:23:23.274023: step 17075, loss 0.00657452, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:23:23.803193: step 17080, loss 0.00233359, acc 0.998047, f1 0.998046\n",
      "Current epoch:  949\n",
      "2017-11-15T23:23:24.300656: step 17085, loss 0.00445961, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:23:24.823172: step 17090, loss 0.00331825, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:23:25.348747: step 17095, loss 0.00041106, acc 1, f1 1\n",
      "2017-11-15T23:23:25.838588: step 17100, loss 0.0106094, acc 0.993711, f1 0.993723\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:23:26.495709: step 17100, loss 2.25337, acc 0.589327, f1 0.582576\n",
      "\n",
      "Current epoch:  950\n",
      "2017-11-15T23:23:27.015645: step 17105, loss 0.00169132, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:23:27.541405: step 17110, loss 0.0038453, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:23:28.062493: step 17115, loss 0.00377782, acc 0.999023, f1 0.999023\n",
      "Current epoch:  951\n",
      "2017-11-15T23:23:28.559295: step 17120, loss 0.00221579, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:23:29.088850: step 17125, loss 0.000671938, acc 1, f1 1\n",
      "2017-11-15T23:23:29.603876: step 17130, loss 0.00561741, acc 0.99707, f1 0.997074\n",
      "2017-11-15T23:23:30.121608: step 17135, loss 0.0114151, acc 0.994141, f1 0.99414\n",
      "Current epoch:  952\n",
      "2017-11-15T23:23:30.608216: step 17140, loss 0.00672236, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:23:31.120135: step 17145, loss 0.00331644, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:23:31.639066: step 17150, loss 0.00595522, acc 0.996094, f1 0.996094\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:23:32.298695: step 17150, loss 2.16461, acc 0.58637, f1 0.58595\n",
      "\n",
      "Current epoch:  953\n",
      "2017-11-15T23:23:32.793452: step 17155, loss 0.000587162, acc 1, f1 1\n",
      "2017-11-15T23:23:33.313416: step 17160, loss 0.00374916, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:23:33.841900: step 17165, loss 0.0081511, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:23:34.378805: step 17170, loss 0.0036475, acc 0.99707, f1 0.997071\n",
      "Current epoch:  954\n",
      "2017-11-15T23:23:34.873070: step 17175, loss 0.00199945, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:23:35.396625: step 17180, loss 0.00155094, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:23:35.924876: step 17185, loss 0.00582065, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:23:36.420896: step 17190, loss 0.00692701, acc 0.996855, f1 0.996856\n",
      "Current epoch:  955\n",
      "2017-11-15T23:23:36.951362: step 17195, loss 0.00753968, acc 0.996094, f1 0.996093\n",
      "2017-11-15T23:23:37.476646: step 17200, loss 0.00200301, acc 0.999023, f1 0.999024\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:23:38.135446: step 17200, loss 2.17231, acc 0.585692, f1 0.584737\n",
      "\n",
      "2017-11-15T23:23:38.661591: step 17205, loss 0.00875772, acc 0.996094, f1 0.996091\n",
      "Current epoch:  956\n",
      "2017-11-15T23:23:39.152542: step 17210, loss 0.00274387, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:23:39.682354: step 17215, loss 0.00291005, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:23:40.214585: step 17220, loss 0.00119875, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:23:40.739528: step 17225, loss 0.00143919, acc 0.999023, f1 0.999023\n",
      "Current epoch:  957\n",
      "2017-11-15T23:23:41.235416: step 17230, loss 0.000489355, acc 1, f1 1\n",
      "2017-11-15T23:23:41.762866: step 17235, loss 0.00567699, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:23:42.282292: step 17240, loss 0.00357499, acc 0.998047, f1 0.998047\n",
      "Current epoch:  958\n",
      "2017-11-15T23:23:42.776217: step 17245, loss 0.0017688, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:23:43.304662: step 17250, loss 0.00557394, acc 0.99707, f1 0.997067\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:23:43.969494: step 17250, loss 2.19984, acc 0.580457, f1 0.582348\n",
      "\n",
      "2017-11-15T23:23:44.597941: step 17255, loss 0.00698311, acc 0.99707, f1 0.997072\n",
      "2017-11-15T23:23:45.122506: step 17260, loss 0.00154225, acc 0.999023, f1 0.999023\n",
      "Current epoch:  959\n",
      "2017-11-15T23:23:45.629399: step 17265, loss 0.0023226, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:23:46.148841: step 17270, loss 0.0034641, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:23:46.670791: step 17275, loss 0.000266877, acc 1, f1 1\n",
      "2017-11-15T23:23:47.161162: step 17280, loss 0.00539052, acc 0.996855, f1 0.996855\n",
      "Current epoch:  960\n",
      "2017-11-15T23:23:47.683667: step 17285, loss 0.00259767, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:23:48.209613: step 17290, loss 0.00317368, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:23:48.736593: step 17295, loss 0.00144348, acc 0.999023, f1 0.999022\n",
      "Current epoch:  961\n",
      "2017-11-15T23:23:49.231547: step 17300, loss 0.0019731, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:23:49.872595: step 17300, loss 2.22721, acc 0.57372, f1 0.575311\n",
      "\n",
      "2017-11-15T23:23:50.387712: step 17305, loss 0.00188873, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:23:50.906247: step 17310, loss 0.000188177, acc 1, f1 1\n",
      "2017-11-15T23:23:51.425169: step 17315, loss 0.00387413, acc 0.998047, f1 0.998046\n",
      "Current epoch:  962\n",
      "2017-11-15T23:23:51.918080: step 17320, loss 0.00278307, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:23:52.447047: step 17325, loss 0.00372078, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:23:52.972003: step 17330, loss 0.00288269, acc 0.998047, f1 0.998047\n",
      "Current epoch:  963\n",
      "2017-11-15T23:23:53.463415: step 17335, loss 0.000149165, acc 1, f1 1\n",
      "2017-11-15T23:23:53.985613: step 17340, loss 0.00659362, acc 0.996094, f1 0.996095\n",
      "2017-11-15T23:23:54.506647: step 17345, loss 0.00652822, acc 0.996094, f1 0.996096\n",
      "2017-11-15T23:23:55.032115: step 17350, loss 0.00606014, acc 0.99707, f1 0.997071\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:23:55.686752: step 17350, loss 2.21625, acc 0.584189, f1 0.580557\n",
      "\n",
      "Current epoch:  964\n",
      "2017-11-15T23:23:56.186676: step 17355, loss 0.00215432, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:23:56.710182: step 17360, loss 0.00217604, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:23:57.223809: step 17365, loss 0.00217641, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:23:57.718447: step 17370, loss 0.000206637, acc 1, f1 1\n",
      "Current epoch:  965\n",
      "2017-11-15T23:23:58.237038: step 17375, loss 0.000261773, acc 1, f1 1\n",
      "2017-11-15T23:23:58.758212: step 17380, loss 0.00529364, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:23:59.283182: step 17385, loss 0.00230894, acc 0.999023, f1 0.999023\n",
      "Current epoch:  966\n",
      "2017-11-15T23:23:59.780583: step 17390, loss 0.00043379, acc 1, f1 1\n",
      "2017-11-15T23:24:00.297763: step 17395, loss 0.00270186, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:24:00.820202: step 17400, loss 0.000147529, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:24:01.477029: step 17400, loss 2.20522, acc 0.579003, f1 0.578741\n",
      "\n",
      "2017-11-15T23:24:02.015826: step 17405, loss 0.00424896, acc 0.998047, f1 0.998047\n",
      "Current epoch:  967\n",
      "2017-11-15T23:24:02.509729: step 17410, loss 0.00382812, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:24:03.033648: step 17415, loss 0.00536495, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:24:03.557584: step 17420, loss 0.00885116, acc 0.996094, f1 0.996097\n",
      "Current epoch:  968\n",
      "2017-11-15T23:24:04.052468: step 17425, loss 0.000995596, acc 1, f1 1\n",
      "2017-11-15T23:24:04.573235: step 17430, loss 0.00174119, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:24:05.094865: step 17435, loss 0.00323878, acc 0.998047, f1 0.998043\n",
      "2017-11-15T23:24:05.617266: step 17440, loss 0.000208242, acc 1, f1 1\n",
      "Current epoch:  969\n",
      "2017-11-15T23:24:06.110022: step 17445, loss 0.00243757, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:24:06.634582: step 17450, loss 0.00431712, acc 0.998047, f1 0.998046\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:24:07.300778: step 17450, loss 2.20362, acc 0.580942, f1 0.581889\n",
      "\n",
      "2017-11-15T23:24:07.824533: step 17455, loss 0.00183257, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:24:08.317414: step 17460, loss 0.00597281, acc 0.996855, f1 0.996858\n",
      "Current epoch:  970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:24:08.846464: step 17465, loss 0.00138153, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:24:09.370439: step 17470, loss 0.00604647, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:24:09.892004: step 17475, loss 0.00316042, acc 0.999023, f1 0.999024\n",
      "Current epoch:  971\n",
      "2017-11-15T23:24:10.381402: step 17480, loss 0.00461813, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:24:10.889308: step 17485, loss 0.00191041, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:24:11.407236: step 17490, loss 0.00494704, acc 0.99707, f1 0.997072\n",
      "2017-11-15T23:24:11.924276: step 17495, loss 0.00269568, acc 0.998047, f1 0.998047\n",
      "Current epoch:  972\n",
      "2017-11-15T23:24:12.417144: step 17500, loss 0.000955187, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:24:13.183719: step 17500, loss 2.17954, acc 0.585013, f1 0.585852\n",
      "\n",
      "2017-11-15T23:24:13.758346: step 17505, loss 0.00279642, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:24:14.277708: step 17510, loss 0.00240566, acc 0.998047, f1 0.998047\n",
      "Current epoch:  973\n",
      "2017-11-15T23:24:14.773583: step 17515, loss 0.0026897, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:24:15.290514: step 17520, loss 0.00258667, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:24:15.806012: step 17525, loss 0.000773459, acc 1, f1 1\n",
      "2017-11-15T23:24:16.319992: step 17530, loss 0.00807192, acc 0.996094, f1 0.996094\n",
      "Current epoch:  974\n",
      "2017-11-15T23:24:16.804430: step 17535, loss 0.00351262, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:24:17.316510: step 17540, loss 0.00173942, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:24:17.830883: step 17545, loss 0.00692566, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:24:18.325732: step 17550, loss 0.00351265, acc 0.998428, f1 0.998431\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:24:18.974877: step 17550, loss 2.23118, acc 0.58511, f1 0.582115\n",
      "\n",
      "Current epoch:  975\n",
      "2017-11-15T23:24:19.492350: step 17555, loss 0.00321518, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:24:20.001258: step 17560, loss 0.00133293, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:24:20.525205: step 17565, loss 0.0020874, acc 0.999023, f1 0.999023\n",
      "Current epoch:  976\n",
      "2017-11-15T23:24:21.010596: step 17570, loss 0.00275411, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:24:21.532794: step 17575, loss 0.00574085, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:24:22.043741: step 17580, loss 0.00530813, acc 0.998047, f1 0.998042\n",
      "2017-11-15T23:24:22.563187: step 17585, loss 0.00809385, acc 0.99707, f1 0.997069\n",
      "Current epoch:  977\n",
      "2017-11-15T23:24:23.054685: step 17590, loss 0.00119077, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:24:23.577726: step 17595, loss 0.00288279, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:24:24.101853: step 17600, loss 0.00276975, acc 0.999023, f1 0.999022\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:24:24.731964: step 17600, loss 2.19706, acc 0.57847, f1 0.581574\n",
      "\n",
      "Current epoch:  978\n",
      "2017-11-15T23:24:25.217025: step 17605, loss 0.00280243, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:24:25.734479: step 17610, loss 0.00456673, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:24:26.251456: step 17615, loss 0.00244921, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:24:26.766097: step 17620, loss 0.00611153, acc 0.998047, f1 0.998047\n",
      "Current epoch:  979\n",
      "2017-11-15T23:24:27.257986: step 17625, loss 0.00173372, acc 1, f1 1\n",
      "2017-11-15T23:24:27.769911: step 17630, loss 0.00199365, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:24:28.285822: step 17635, loss 0.00153682, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:24:28.769235: step 17640, loss 0.00526929, acc 0.996855, f1 0.996867\n",
      "Current epoch:  980\n",
      "2017-11-15T23:24:29.289665: step 17645, loss 0.000181679, acc 1, f1 1\n",
      "2017-11-15T23:24:29.808112: step 17650, loss 0.00474972, acc 0.998047, f1 0.998048\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:24:30.450258: step 17650, loss 2.18885, acc 0.59083, f1 0.588845\n",
      "\n",
      "2017-11-15T23:24:30.962594: step 17655, loss 0.00900688, acc 0.994141, f1 0.994142\n",
      "Current epoch:  981\n",
      "2017-11-15T23:24:31.449078: step 17660, loss 0.000225679, acc 1, f1 1\n",
      "2017-11-15T23:24:31.960499: step 17665, loss 0.00357851, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:24:32.474495: step 17670, loss 0.00443104, acc 0.99707, f1 0.997072\n",
      "2017-11-15T23:24:32.995849: step 17675, loss 0.00267604, acc 0.998047, f1 0.998047\n",
      "Current epoch:  982\n",
      "2017-11-15T23:24:33.497264: step 17680, loss 0.00165971, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:24:34.022203: step 17685, loss 0.00364925, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:24:34.548419: step 17690, loss 0.00222577, acc 0.999023, f1 0.999023\n",
      "Current epoch:  983\n",
      "2017-11-15T23:24:35.056661: step 17695, loss 0.00226086, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:24:35.574344: step 17700, loss 0.00391231, acc 0.998047, f1 0.998046\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:24:36.228130: step 17700, loss 2.17244, acc 0.585498, f1 0.586578\n",
      "\n",
      "2017-11-15T23:24:36.752584: step 17705, loss 0.00632158, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:24:37.263819: step 17710, loss 0.00104309, acc 0.999023, f1 0.999023\n",
      "Current epoch:  984\n",
      "2017-11-15T23:24:37.751182: step 17715, loss 0.00470657, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:24:38.269035: step 17720, loss 0.00218811, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:24:38.778962: step 17725, loss 0.0074545, acc 0.995117, f1 0.99512\n",
      "2017-11-15T23:24:39.258788: step 17730, loss 0.000180799, acc 1, f1 1\n",
      "Current epoch:  985\n",
      "2017-11-15T23:24:39.776841: step 17735, loss 0.00502188, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:24:40.293726: step 17740, loss 0.000165286, acc 1, f1 1\n",
      "2017-11-15T23:24:40.815768: step 17745, loss 0.00245171, acc 0.998047, f1 0.998047\n",
      "Current epoch:  986\n",
      "2017-11-15T23:24:41.299091: step 17750, loss 0.000257723, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:24:41.939385: step 17750, loss 2.19204, acc 0.579536, f1 0.582261\n",
      "\n",
      "2017-11-15T23:24:42.448136: step 17755, loss 0.00222245, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:24:42.968593: step 17760, loss 0.00805876, acc 0.996094, f1 0.996095\n",
      "2017-11-15T23:24:43.494148: step 17765, loss 0.00231188, acc 0.999023, f1 0.999025\n",
      "Current epoch:  987\n",
      "2017-11-15T23:24:43.991366: step 17770, loss 0.00216156, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:24:44.514105: step 17775, loss 0.00725242, acc 0.99707, f1 0.997068\n",
      "2017-11-15T23:24:45.036264: step 17780, loss 0.000768976, acc 1, f1 1\n",
      "Current epoch:  988\n",
      "2017-11-15T23:24:45.533300: step 17785, loss 0.00257528, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:24:46.076980: step 17790, loss 0.00236563, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:24:46.596272: step 17795, loss 0.000223561, acc 1, f1 1\n",
      "2017-11-15T23:24:47.110411: step 17800, loss 0.00918559, acc 0.996094, f1 0.996096\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:24:47.743675: step 17800, loss 2.23169, acc 0.581427, f1 0.580032\n",
      "\n",
      "Current epoch:  989\n",
      "2017-11-15T23:24:48.261632: step 17805, loss 0.00359802, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:24:48.774081: step 17810, loss 0.00809515, acc 0.996094, f1 0.996093\n",
      "2017-11-15T23:24:49.294643: step 17815, loss 0.00714802, acc 0.996094, f1 0.996095\n",
      "2017-11-15T23:24:49.788542: step 17820, loss 0.0035395, acc 0.996855, f1 0.996855\n",
      "Current epoch:  990\n",
      "2017-11-15T23:24:50.311078: step 17825, loss 0.00174276, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:24:50.832557: step 17830, loss 0.00322875, acc 0.99707, f1 0.997072\n",
      "2017-11-15T23:24:51.363649: step 17835, loss 0.00264915, acc 0.999023, f1 0.999022\n",
      "Current epoch:  991\n",
      "2017-11-15T23:24:51.867372: step 17840, loss 0.00342919, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:24:52.392646: step 17845, loss 0.00947425, acc 0.996094, f1 0.996096\n",
      "2017-11-15T23:24:52.916338: step 17850, loss 0.0045397, acc 0.99707, f1 0.997075\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:24:53.576414: step 17850, loss 2.24784, acc 0.582784, f1 0.579087\n",
      "\n",
      "2017-11-15T23:24:54.096505: step 17855, loss 0.00444042, acc 0.99707, f1 0.997065\n",
      "Current epoch:  992\n",
      "2017-11-15T23:24:54.587097: step 17860, loss 0.00303686, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:24:55.111178: step 17865, loss 0.00990883, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:24:55.627421: step 17870, loss 0.00355655, acc 0.998047, f1 0.998047\n",
      "Current epoch:  993\n",
      "2017-11-15T23:24:56.121545: step 17875, loss 0.000441028, acc 1, f1 1\n",
      "2017-11-15T23:24:56.650345: step 17880, loss 0.00288414, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:24:57.182058: step 17885, loss 0.000938441, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:24:57.704268: step 17890, loss 0.00345904, acc 0.998047, f1 0.998051\n",
      "Current epoch:  994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:24:58.196659: step 17895, loss 0.00652732, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:24:58.715385: step 17900, loss 0.00588953, acc 0.99707, f1 0.99707\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:24:59.351348: step 17900, loss 2.25765, acc 0.58007, f1 0.578297\n",
      "\n",
      "2017-11-15T23:24:59.866783: step 17905, loss 0.00665814, acc 0.998047, f1 0.99805\n",
      "2017-11-15T23:25:00.348639: step 17910, loss 0.00232484, acc 0.998428, f1 0.998428\n",
      "Current epoch:  995\n",
      "2017-11-15T23:25:00.871720: step 17915, loss 0.00170904, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:25:01.390641: step 17920, loss 0.00417381, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:25:01.900596: step 17925, loss 0.000142941, acc 1, f1 1\n",
      "Current epoch:  996\n",
      "2017-11-15T23:25:02.402092: step 17930, loss 0.00223277, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:25:02.920768: step 17935, loss 0.00693591, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:25:03.443281: step 17940, loss 0.00353132, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:25:03.964334: step 17945, loss 0.00721145, acc 0.99707, f1 0.997074\n",
      "Current epoch:  997\n",
      "2017-11-15T23:25:04.463742: step 17950, loss 0.00158578, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:25:05.122054: step 17950, loss 2.1621, acc 0.591314, f1 0.590747\n",
      "\n",
      "2017-11-15T23:25:05.643490: step 17955, loss 0.00339985, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:25:06.159301: step 17960, loss 0.000817654, acc 1, f1 1\n",
      "Current epoch:  998\n",
      "2017-11-15T23:25:06.656060: step 17965, loss 0.00220051, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:25:07.181007: step 17970, loss 0.00060995, acc 1, f1 1\n",
      "2017-11-15T23:25:07.708592: step 17975, loss 0.00328701, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:25:08.239604: step 17980, loss 0.00507156, acc 0.996094, f1 0.99609\n",
      "Current epoch:  999\n",
      "2017-11-15T23:25:08.734484: step 17985, loss 0.00358958, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:25:09.252451: step 17990, loss 0.00592234, acc 0.99707, f1 0.997072\n",
      "2017-11-15T23:25:09.763445: step 17995, loss 0.0028682, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:25:10.250338: step 18000, loss 0.00367635, acc 0.998428, f1 0.998428\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:25:10.896312: step 18000, loss 2.43831, acc 0.549147, f1 0.544975\n",
      "\n",
      "Current epoch:  1000\n",
      "2017-11-15T23:25:11.414416: step 18005, loss 0.00321878, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:25:11.930019: step 18010, loss 0.00463633, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:25:12.455563: step 18015, loss 0.00426244, acc 0.998047, f1 0.998046\n",
      "Current epoch:  1001\n",
      "2017-11-15T23:25:12.948339: step 18020, loss 0.00325992, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:25:13.488903: step 18025, loss 0.000840956, acc 1, f1 1\n",
      "2017-11-15T23:25:14.018846: step 18030, loss 0.00998597, acc 0.995117, f1 0.99511\n",
      "2017-11-15T23:25:14.540025: step 18035, loss 0.00164374, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1002\n",
      "2017-11-15T23:25:15.028287: step 18040, loss 0.00174376, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:25:15.552331: step 18045, loss 0.000544171, acc 1, f1 1\n",
      "2017-11-15T23:25:16.073290: step 18050, loss 0.00376712, acc 0.99707, f1 0.99707\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:25:16.728827: step 18050, loss 2.34101, acc 0.563299, f1 0.562265\n",
      "\n",
      "Current epoch:  1003\n",
      "2017-11-15T23:25:17.223178: step 18055, loss 0.00263119, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:25:17.749116: step 18060, loss 0.000161545, acc 1, f1 1\n",
      "2017-11-15T23:25:18.271094: step 18065, loss 0.000192889, acc 1, f1 1\n",
      "2017-11-15T23:25:18.800444: step 18070, loss 0.00274086, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1004\n",
      "2017-11-15T23:25:19.305696: step 18075, loss 0.00384722, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:25:19.830946: step 18080, loss 0.00059114, acc 1, f1 1\n",
      "2017-11-15T23:25:20.353388: step 18085, loss 0.00287458, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:25:20.843639: step 18090, loss 0.00258907, acc 0.998428, f1 0.998431\n",
      "Current epoch:  1005\n",
      "2017-11-15T23:25:21.369078: step 18095, loss 0.000127358, acc 1, f1 1\n",
      "2017-11-15T23:25:21.885018: step 18100, loss 0.000698655, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:25:22.546260: step 18100, loss 2.19033, acc 0.583268, f1 0.583453\n",
      "\n",
      "2017-11-15T23:25:23.065723: step 18105, loss 0.00337051, acc 0.99707, f1 0.997066\n",
      "Current epoch:  1006\n",
      "2017-11-15T23:25:23.563174: step 18110, loss 0.000213936, acc 1, f1 1\n",
      "2017-11-15T23:25:24.087637: step 18115, loss 0.00174835, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:25:24.627157: step 18120, loss 0.00123911, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:25:25.152112: step 18125, loss 0.00634941, acc 0.996094, f1 0.996094\n",
      "Current epoch:  1007\n",
      "2017-11-15T23:25:25.642996: step 18130, loss 0.00336915, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:25:26.162763: step 18135, loss 0.00517128, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:25:26.681172: step 18140, loss 0.00285295, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1008\n",
      "2017-11-15T23:25:27.172100: step 18145, loss 0.00318949, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:25:27.687561: step 18150, loss 0.00194428, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:25:28.345939: step 18150, loss 2.1764, acc 0.588261, f1 0.586149\n",
      "\n",
      "2017-11-15T23:25:28.870679: step 18155, loss 0.00375577, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:25:29.398637: step 18160, loss 0.00679397, acc 0.996094, f1 0.996105\n",
      "Current epoch:  1009\n",
      "2017-11-15T23:25:29.891518: step 18165, loss 0.00183757, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:25:30.410160: step 18170, loss 0.0025236, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:25:30.918167: step 18175, loss 0.00614603, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:25:31.403711: step 18180, loss 0.00888538, acc 0.995283, f1 0.995271\n",
      "Current epoch:  1010\n",
      "2017-11-15T23:25:31.917660: step 18185, loss 0.000125691, acc 1, f1 1\n",
      "2017-11-15T23:25:32.437204: step 18190, loss 0.00047936, acc 1, f1 1\n",
      "2017-11-15T23:25:32.963286: step 18195, loss 0.00323422, acc 0.998047, f1 0.998048\n",
      "Current epoch:  1011\n",
      "2017-11-15T23:25:33.456313: step 18200, loss 0.0040364, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:25:34.117171: step 18200, loss 2.25443, acc 0.575611, f1 0.575076\n",
      "\n",
      "2017-11-15T23:25:34.638028: step 18205, loss 0.00304517, acc 0.998047, f1 0.998043\n",
      "2017-11-15T23:25:35.167041: step 18210, loss 0.00180104, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:25:35.698928: step 18215, loss 0.0074356, acc 0.995117, f1 0.995117\n",
      "Current epoch:  1012\n",
      "2017-11-15T23:25:36.192810: step 18220, loss 0.00400709, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:25:36.711929: step 18225, loss 0.00346626, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:25:37.230878: step 18230, loss 0.00808975, acc 0.99707, f1 0.997067\n",
      "Current epoch:  1013\n",
      "2017-11-15T23:25:37.725750: step 18235, loss 0.00479981, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:25:38.249208: step 18240, loss 0.00362412, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:25:38.769654: step 18245, loss 0.00486214, acc 0.99707, f1 0.997074\n",
      "2017-11-15T23:25:39.297318: step 18250, loss 0.00520688, acc 0.99707, f1 0.99707\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:25:39.959713: step 18250, loss 2.21378, acc 0.577404, f1 0.57816\n",
      "\n",
      "Current epoch:  1014\n",
      "2017-11-15T23:25:40.451569: step 18255, loss 0.000744711, acc 1, f1 1\n",
      "2017-11-15T23:25:40.985634: step 18260, loss 0.00124423, acc 1, f1 1\n",
      "2017-11-15T23:25:41.510633: step 18265, loss 0.00615519, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:25:42.005019: step 18270, loss 0.0119536, acc 0.992138, f1 0.992142\n",
      "Current epoch:  1015\n",
      "2017-11-15T23:25:42.536678: step 18275, loss 0.000139394, acc 1, f1 1\n",
      "2017-11-15T23:25:43.058121: step 18280, loss 0.00272985, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:25:43.579815: step 18285, loss 0.00265334, acc 0.998047, f1 0.998046\n",
      "Current epoch:  1016\n",
      "2017-11-15T23:25:44.073995: step 18290, loss 0.0021417, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:25:44.592554: step 18295, loss 0.000566946, acc 1, f1 1\n",
      "2017-11-15T23:25:45.113515: step 18300, loss 0.00385907, acc 0.998047, f1 0.998046\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:25:45.770266: step 18300, loss 2.25468, acc 0.576725, f1 0.575718\n",
      "\n",
      "2017-11-15T23:25:46.299864: step 18305, loss 0.0112902, acc 0.994141, f1 0.994138\n",
      "Current epoch:  1017\n",
      "2017-11-15T23:25:46.803989: step 18310, loss 0.000213346, acc 1, f1 1\n",
      "2017-11-15T23:25:47.321161: step 18315, loss 0.00082237, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:25:47.846599: step 18320, loss 0.00551755, acc 0.996094, f1 0.996094\n",
      "Current epoch:  1018\n",
      "2017-11-15T23:25:48.345013: step 18325, loss 0.00167293, acc 0.999023, f1 0.999024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:25:48.871541: step 18330, loss 0.00192435, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:25:49.396193: step 18335, loss 0.000653831, acc 1, f1 1\n",
      "2017-11-15T23:25:49.927162: step 18340, loss 0.00434232, acc 0.99707, f1 0.997078\n",
      "Current epoch:  1019\n",
      "2017-11-15T23:25:50.426493: step 18345, loss 0.00622126, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:25:50.947935: step 18350, loss 0.00423873, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:25:51.606808: step 18350, loss 2.22883, acc 0.579246, f1 0.579678\n",
      "\n",
      "2017-11-15T23:25:52.127723: step 18355, loss 0.00146137, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:25:52.616109: step 18360, loss 0.00379, acc 0.998428, f1 0.998429\n",
      "Current epoch:  1020\n",
      "2017-11-15T23:25:53.136605: step 18365, loss 0.000442515, acc 1, f1 1\n",
      "2017-11-15T23:25:53.651073: step 18370, loss 0.000210113, acc 1, f1 1\n",
      "2017-11-15T23:25:54.172464: step 18375, loss 0.00525879, acc 0.996094, f1 0.996093\n",
      "Current epoch:  1021\n",
      "2017-11-15T23:25:54.655399: step 18380, loss 0.000967885, acc 1, f1 1\n",
      "2017-11-15T23:25:55.170829: step 18385, loss 0.00263467, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:25:55.685270: step 18390, loss 0.00682338, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:25:56.197726: step 18395, loss 0.00275192, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1022\n",
      "2017-11-15T23:25:56.684137: step 18400, loss 0.00185164, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:25:57.333080: step 18400, loss 2.19902, acc 0.580409, f1 0.581292\n",
      "\n",
      "2017-11-15T23:25:57.845532: step 18405, loss 0.00290863, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:25:58.358621: step 18410, loss 0.00734498, acc 0.995117, f1 0.995117\n",
      "Current epoch:  1023\n",
      "2017-11-15T23:25:58.841469: step 18415, loss 0.00303497, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:25:59.353886: step 18420, loss 0.0064321, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:25:59.870804: step 18425, loss 0.00397175, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:26:00.387735: step 18430, loss 0.00561909, acc 0.996094, f1 0.996096\n",
      "Current epoch:  1024\n",
      "2017-11-15T23:26:00.869472: step 18435, loss 0.0105577, acc 0.992188, f1 0.992189\n",
      "2017-11-15T23:26:01.386367: step 18440, loss 0.00118187, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:26:01.899093: step 18445, loss 0.00368338, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:26:02.382963: step 18450, loss 0.00407709, acc 0.996855, f1 0.996852\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:26:03.044122: step 18450, loss 2.17882, acc 0.594416, f1 0.591907\n",
      "\n",
      "Current epoch:  1025\n",
      "2017-11-15T23:26:03.560164: step 18455, loss 0.00684088, acc 0.996094, f1 0.996092\n",
      "2017-11-15T23:26:04.080098: step 18460, loss 0.00329483, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:26:04.592030: step 18465, loss 0.00237783, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1026\n",
      "2017-11-15T23:26:05.074555: step 18470, loss 0.000229613, acc 1, f1 1\n",
      "2017-11-15T23:26:05.590475: step 18475, loss 0.0017189, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:26:06.103172: step 18480, loss 0.00258913, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:26:06.631659: step 18485, loss 0.0055674, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1027\n",
      "2017-11-15T23:26:07.164811: step 18490, loss 0.00186225, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:26:07.674228: step 18495, loss 0.00240844, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:26:08.197206: step 18500, loss 0.000208252, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:26:08.862483: step 18500, loss 2.26582, acc 0.580845, f1 0.577409\n",
      "\n",
      "Current epoch:  1028\n",
      "2017-11-15T23:26:09.343965: step 18505, loss 0.00279669, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:26:09.865460: step 18510, loss 0.00624492, acc 0.99707, f1 0.997073\n",
      "2017-11-15T23:26:10.385017: step 18515, loss 0.00231036, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:26:10.904102: step 18520, loss 0.000124035, acc 1, f1 1\n",
      "Current epoch:  1029\n",
      "2017-11-15T23:26:11.393008: step 18525, loss 0.00252174, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:26:11.922948: step 18530, loss 0.00268003, acc 0.998047, f1 0.998051\n",
      "2017-11-15T23:26:12.448887: step 18535, loss 0.00327538, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:26:12.937441: step 18540, loss 0.000127208, acc 1, f1 1\n",
      "Current epoch:  1030\n",
      "2017-11-15T23:26:13.458423: step 18545, loss 0.00213022, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:26:13.988110: step 18550, loss 0.00158347, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:26:14.650649: step 18550, loss 2.20969, acc 0.5791, f1 0.580242\n",
      "\n",
      "2017-11-15T23:26:15.171572: step 18555, loss 0.0045989, acc 0.99707, f1 0.997071\n",
      "Current epoch:  1031\n",
      "2017-11-15T23:26:15.654135: step 18560, loss 0.00031091, acc 1, f1 1\n",
      "2017-11-15T23:26:16.169020: step 18565, loss 0.00299321, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:26:16.681251: step 18570, loss 0.000152486, acc 1, f1 1\n",
      "2017-11-15T23:26:17.194260: step 18575, loss 0.00226546, acc 0.999023, f1 0.999022\n",
      "Current epoch:  1032\n",
      "2017-11-15T23:26:17.677624: step 18580, loss 0.00280443, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:26:18.191561: step 18585, loss 0.0072337, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:26:18.809665: step 18590, loss 0.000238551, acc 1, f1 1\n",
      "Current epoch:  1033\n",
      "2017-11-15T23:26:19.302556: step 18595, loss 0.00456902, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:26:19.828490: step 18600, loss 0.00166352, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:26:20.463487: step 18600, loss 2.15904, acc 0.588115, f1 0.588348\n",
      "\n",
      "2017-11-15T23:26:20.975922: step 18605, loss 0.000594558, acc 1, f1 1\n",
      "2017-11-15T23:26:21.493287: step 18610, loss 0.010538, acc 0.995117, f1 0.995122\n",
      "Current epoch:  1034\n",
      "2017-11-15T23:26:21.980588: step 18615, loss 0.00170318, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:26:22.496087: step 18620, loss 0.00353649, acc 0.998047, f1 0.998041\n",
      "2017-11-15T23:26:23.007010: step 18625, loss 0.00209892, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:26:23.489887: step 18630, loss 0.00273894, acc 0.998428, f1 0.998428\n",
      "Current epoch:  1035\n",
      "2017-11-15T23:26:24.007480: step 18635, loss 0.00325081, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:26:24.535192: step 18640, loss 0.00234552, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:26:25.073407: step 18645, loss 0.00401807, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1036\n",
      "2017-11-15T23:26:25.569304: step 18650, loss 0.000316335, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:26:26.226612: step 18650, loss 2.22465, acc 0.588842, f1 0.586159\n",
      "\n",
      "2017-11-15T23:26:26.749112: step 18655, loss 0.000679334, acc 1, f1 1\n",
      "2017-11-15T23:26:27.273397: step 18660, loss 0.00552031, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:26:27.793403: step 18665, loss 0.00999534, acc 0.993164, f1 0.993163\n",
      "Current epoch:  1037\n",
      "2017-11-15T23:26:28.274399: step 18670, loss 0.00569005, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:26:28.788303: step 18675, loss 0.00655883, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:26:29.299733: step 18680, loss 0.00241517, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1038\n",
      "2017-11-15T23:26:29.789121: step 18685, loss 0.004207, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:26:30.308172: step 18690, loss 0.00289149, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:26:30.833994: step 18695, loss 0.00299946, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:26:31.352911: step 18700, loss 0.00381203, acc 0.998047, f1 0.998046\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:26:32.009047: step 18700, loss 2.24498, acc 0.58007, f1 0.579975\n",
      "\n",
      "Current epoch:  1039\n",
      "2017-11-15T23:26:32.498844: step 18705, loss 0.00646422, acc 0.996094, f1 0.996097\n",
      "2017-11-15T23:26:33.023278: step 18710, loss 0.00364905, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:26:33.544535: step 18715, loss 0.00471441, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:26:34.037643: step 18720, loss 0.00801951, acc 0.995283, f1 0.995294\n",
      "Current epoch:  1040\n",
      "2017-11-15T23:26:34.562391: step 18725, loss 0.00115141, acc 1, f1 1\n",
      "2017-11-15T23:26:35.086313: step 18730, loss 0.00571393, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:26:35.612181: step 18735, loss 0.00216099, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1041\n",
      "2017-11-15T23:26:36.118590: step 18740, loss 0.00444224, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:26:36.642541: step 18745, loss 0.00254142, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:26:37.166054: step 18750, loss 0.00266455, acc 0.99707, f1 0.997072\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:26:37.822218: step 18750, loss 2.17012, acc 0.594222, f1 0.590967\n",
      "\n",
      "2017-11-15T23:26:38.332423: step 18755, loss 0.00262822, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:26:38.853360: step 18760, loss 0.000822996, acc 1, f1 1\n",
      "2017-11-15T23:26:39.366260: step 18765, loss 0.00381184, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:26:39.877221: step 18770, loss 0.00463563, acc 0.99707, f1 0.997074\n",
      "Current epoch:  1043\n",
      "2017-11-15T23:26:40.361093: step 18775, loss 0.000927732, acc 1, f1 1\n",
      "2017-11-15T23:26:40.879010: step 18780, loss 0.0027566, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:26:41.405543: step 18785, loss 0.0031944, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:26:41.921725: step 18790, loss 0.00283777, acc 0.999023, f1 0.999025\n",
      "Current epoch:  1044\n",
      "2017-11-15T23:26:42.411806: step 18795, loss 0.00489741, acc 0.99707, f1 0.997074\n",
      "2017-11-15T23:26:42.926739: step 18800, loss 0.00126621, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:26:43.560384: step 18800, loss 2.17661, acc 0.591072, f1 0.587972\n",
      "\n",
      "2017-11-15T23:26:44.073323: step 18805, loss 0.00329689, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:26:44.565723: step 18810, loss 0.00562768, acc 0.996855, f1 0.996853\n",
      "Current epoch:  1045\n",
      "2017-11-15T23:26:45.086161: step 18815, loss 0.00332027, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:26:45.611103: step 18820, loss 0.00542182, acc 0.99707, f1 0.997072\n",
      "2017-11-15T23:26:46.130960: step 18825, loss 0.00275739, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1046\n",
      "2017-11-15T23:26:46.630548: step 18830, loss 0.00253416, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:26:47.168013: step 18835, loss 0.00197838, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:26:47.687244: step 18840, loss 0.00210966, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:26:48.206247: step 18845, loss 0.00501427, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1047\n",
      "2017-11-15T23:26:48.699019: step 18850, loss 0.00511404, acc 0.998047, f1 0.998042\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:26:49.350710: step 18850, loss 2.20295, acc 0.577016, f1 0.577996\n",
      "\n",
      "2017-11-15T23:26:49.919901: step 18855, loss 0.000148738, acc 1, f1 1\n",
      "2017-11-15T23:26:50.439836: step 18860, loss 0.00894033, acc 0.996094, f1 0.996097\n",
      "Current epoch:  1048\n",
      "2017-11-15T23:26:50.936737: step 18865, loss 0.00243704, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:26:51.459671: step 18870, loss 0.00224045, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:26:51.979753: step 18875, loss 0.00243594, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:26:52.522700: step 18880, loss 0.00519538, acc 0.99707, f1 0.997071\n",
      "Current epoch:  1049\n",
      "2017-11-15T23:26:53.009541: step 18885, loss 0.00301569, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:26:53.539674: step 18890, loss 0.00498534, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:26:54.061943: step 18895, loss 0.00183342, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:26:54.553291: step 18900, loss 0.00236131, acc 0.998428, f1 0.998428\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:26:55.215733: step 18900, loss 2.27149, acc 0.579536, f1 0.576592\n",
      "\n",
      "Current epoch:  1050\n",
      "2017-11-15T23:26:55.739666: step 18905, loss 0.00255698, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:26:56.272163: step 18910, loss 0.00182239, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:26:56.793811: step 18915, loss 0.00315018, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1051\n",
      "2017-11-15T23:26:57.292201: step 18920, loss 0.000163884, acc 1, f1 1\n",
      "2017-11-15T23:26:57.824018: step 18925, loss 0.00140928, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:26:58.355002: step 18930, loss 0.00405479, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:26:58.877927: step 18935, loss 0.00373147, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1052\n",
      "2017-11-15T23:26:59.374343: step 18940, loss 0.000876655, acc 1, f1 1\n",
      "2017-11-15T23:26:59.892003: step 18945, loss 0.000521079, acc 1, f1 1\n",
      "2017-11-15T23:27:00.413987: step 18950, loss 0.00323884, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:27:01.058282: step 18950, loss 2.19293, acc 0.578761, f1 0.581405\n",
      "\n",
      "Current epoch:  1053\n",
      "2017-11-15T23:27:01.537153: step 18955, loss 0.000391498, acc 1, f1 1\n",
      "2017-11-15T23:27:02.054067: step 18960, loss 0.000143361, acc 1, f1 1\n",
      "2017-11-15T23:27:02.572541: step 18965, loss 0.00312172, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:27:03.090967: step 18970, loss 0.00442576, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1054\n",
      "2017-11-15T23:27:03.584248: step 18975, loss 0.00466232, acc 0.99707, f1 0.997073\n",
      "2017-11-15T23:27:04.094858: step 18980, loss 0.00579561, acc 0.99707, f1 0.997073\n",
      "2017-11-15T23:27:04.611772: step 18985, loss 0.00313313, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:27:05.091635: step 18990, loss 0.0024108, acc 0.998428, f1 0.998425\n",
      "Current epoch:  1055\n",
      "2017-11-15T23:27:05.611545: step 18995, loss 0.00099835, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:27:06.118474: step 19000, loss 0.00534683, acc 0.99707, f1 0.997075\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:27:06.757608: step 19000, loss 2.21344, acc 0.583075, f1 0.581878\n",
      "\n",
      "2017-11-15T23:27:07.266537: step 19005, loss 0.00215305, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1056\n",
      "2017-11-15T23:27:07.750912: step 19010, loss 0.00110614, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:27:08.263356: step 19015, loss 0.00172282, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:27:08.785811: step 19020, loss 0.00340225, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:27:09.305735: step 19025, loss 0.00553191, acc 0.996094, f1 0.996097\n",
      "Current epoch:  1057\n",
      "2017-11-15T23:27:09.791298: step 19030, loss 0.00379683, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:27:10.300201: step 19035, loss 0.00553551, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:27:10.812123: step 19040, loss 0.00330371, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1058\n",
      "2017-11-15T23:27:11.293993: step 19045, loss 0.00184776, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:27:11.813504: step 19050, loss 0.000844794, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:27:12.450506: step 19050, loss 2.20945, acc 0.57847, f1 0.580303\n",
      "\n",
      "2017-11-15T23:27:12.969148: step 19055, loss 0.00487272, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:27:13.488078: step 19060, loss 0.00360212, acc 0.998047, f1 0.998043\n",
      "Current epoch:  1059\n",
      "2017-11-15T23:27:13.976944: step 19065, loss 0.00373702, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:27:14.507450: step 19070, loss 0.00291383, acc 0.998047, f1 0.998052\n",
      "2017-11-15T23:27:15.024918: step 19075, loss 0.0018658, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:27:15.512866: step 19080, loss 0.0022411, acc 0.998428, f1 0.998428\n",
      "Current epoch:  1060\n",
      "2017-11-15T23:27:16.031314: step 19085, loss 0.00375053, acc 0.99707, f1 0.997066\n",
      "2017-11-15T23:27:16.547516: step 19090, loss 0.00270677, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:27:17.069974: step 19095, loss 0.0052658, acc 0.99707, f1 0.997069\n",
      "Current epoch:  1061\n",
      "2017-11-15T23:27:17.553324: step 19100, loss 0.000987744, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:27:18.191746: step 19100, loss 2.25285, acc 0.578712, f1 0.576652\n",
      "\n",
      "2017-11-15T23:27:18.700715: step 19105, loss 0.000544862, acc 1, f1 1\n",
      "2017-11-15T23:27:19.211644: step 19110, loss 0.00110963, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:27:19.736591: step 19115, loss 0.00432942, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1062\n",
      "2017-11-15T23:27:20.229486: step 19120, loss 0.00142521, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:27:20.745411: step 19125, loss 0.00485647, acc 0.99707, f1 0.997072\n",
      "2017-11-15T23:27:21.261355: step 19130, loss 0.00326563, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1063\n",
      "2017-11-15T23:27:21.752387: step 19135, loss 0.00107773, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:27:22.262795: step 19140, loss 0.005418, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:27:22.773718: step 19145, loss 0.00378451, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:27:23.289287: step 19150, loss 0.00118014, acc 0.999023, f1 0.999024\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:27:23.929910: step 19150, loss 2.34867, acc 0.564705, f1 0.562712\n",
      "\n",
      "Current epoch:  1064\n",
      "2017-11-15T23:27:24.420044: step 19155, loss 0.00503714, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:27:24.931089: step 19160, loss 0.00148522, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:27:25.468855: step 19165, loss 0.00146168, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:27:25.949721: step 19170, loss 0.00365991, acc 0.998428, f1 0.998428\n",
      "Current epoch:  1065\n",
      "2017-11-15T23:27:26.462350: step 19175, loss 0.00012658, acc 1, f1 1\n",
      "2017-11-15T23:27:26.977209: step 19180, loss 0.00283767, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:27:27.493158: step 19185, loss 0.00545046, acc 0.99707, f1 0.997074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch:  1066\n",
      "2017-11-15T23:27:27.980026: step 19190, loss 0.00228715, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:27:28.492952: step 19195, loss 0.00386233, acc 0.998047, f1 0.998052\n",
      "2017-11-15T23:27:29.007868: step 19200, loss 0.00441333, acc 0.99707, f1 0.997069\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:27:29.644140: step 19200, loss 2.2175, acc 0.59621, f1 0.591625\n",
      "\n",
      "2017-11-15T23:27:30.158057: step 19205, loss 0.00206164, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1067\n",
      "2017-11-15T23:27:30.657080: step 19210, loss 0.00203935, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:27:31.183013: step 19215, loss 0.00156807, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:27:31.700925: step 19220, loss 0.00192357, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1068\n",
      "2017-11-15T23:27:32.187792: step 19225, loss 0.0017432, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:27:32.707236: step 19230, loss 0.0021743, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:27:33.216665: step 19235, loss 0.00188574, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:27:33.738512: step 19240, loss 0.00171433, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1069\n",
      "2017-11-15T23:27:34.223423: step 19245, loss 0.00213726, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:27:34.743027: step 19250, loss 0.00578123, acc 0.996094, f1 0.99609\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:27:35.400263: step 19250, loss 2.24438, acc 0.577404, f1 0.577624\n",
      "\n",
      "2017-11-15T23:27:35.927224: step 19255, loss 0.00302169, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:27:36.421916: step 19260, loss 0.00971739, acc 0.993711, f1 0.993701\n",
      "Current epoch:  1070\n",
      "2017-11-15T23:27:36.944875: step 19265, loss 0.00226812, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:27:37.456924: step 19270, loss 0.00120638, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:27:37.968844: step 19275, loss 0.00716588, acc 0.99707, f1 0.997066\n",
      "Current epoch:  1071\n",
      "2017-11-15T23:27:38.452688: step 19280, loss 0.00260129, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:27:38.967618: step 19285, loss 0.000364616, acc 1, f1 1\n",
      "2017-11-15T23:27:39.478533: step 19290, loss 0.000838394, acc 1, f1 1\n",
      "2017-11-15T23:27:39.995531: step 19295, loss 0.00487498, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1072\n",
      "2017-11-15T23:27:40.477893: step 19300, loss 0.00577668, acc 0.996094, f1 0.996094\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:27:41.107724: step 19300, loss 2.1868, acc 0.586419, f1 0.586542\n",
      "\n",
      "2017-11-15T23:27:41.621052: step 19305, loss 0.00327431, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:27:42.144224: step 19310, loss 0.00248108, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1073\n",
      "2017-11-15T23:27:42.628556: step 19315, loss 0.00541118, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:27:43.137488: step 19320, loss 0.000135366, acc 1, f1 1\n",
      "2017-11-15T23:27:43.657409: step 19325, loss 0.00570982, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:27:44.182344: step 19330, loss 0.00456272, acc 0.99707, f1 0.997069\n",
      "Current epoch:  1074\n",
      "2017-11-15T23:27:44.671222: step 19335, loss 0.0036633, acc 0.998047, f1 0.998053\n",
      "2017-11-15T23:27:45.188151: step 19340, loss 0.00410936, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:27:45.704102: step 19345, loss 0.00113193, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:27:46.195038: step 19350, loss 0.00217294, acc 0.998428, f1 0.998428\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:27:46.865285: step 19350, loss 2.22103, acc 0.578082, f1 0.578324\n",
      "\n",
      "Current epoch:  1075\n",
      "2017-11-15T23:27:47.397376: step 19355, loss 0.00071748, acc 1, f1 1\n",
      "2017-11-15T23:27:47.924343: step 19360, loss 0.00140692, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:27:48.448281: step 19365, loss 0.00503508, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1076\n",
      "2017-11-15T23:27:48.934392: step 19370, loss 0.00146707, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:27:49.445794: step 19375, loss 0.00155915, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:27:49.957720: step 19380, loss 0.00443693, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:27:50.466516: step 19385, loss 0.00480855, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1077\n",
      "2017-11-15T23:27:50.955410: step 19390, loss 0.000174653, acc 1, f1 1\n",
      "2017-11-15T23:27:51.473435: step 19395, loss 0.00218868, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:27:52.009391: step 19400, loss 0.00728746, acc 0.996094, f1 0.99609\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:27:52.651511: step 19400, loss 2.25126, acc 0.569794, f1 0.572424\n",
      "\n",
      "Current epoch:  1078\n",
      "2017-11-15T23:27:53.146309: step 19405, loss 0.00249661, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:27:53.662236: step 19410, loss 0.00244162, acc 0.998047, f1 0.998042\n",
      "2017-11-15T23:27:54.173218: step 19415, loss 0.00169345, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:27:54.694880: step 19420, loss 0.00238475, acc 0.998047, f1 0.998046\n",
      "Current epoch:  1079\n",
      "2017-11-15T23:27:55.187119: step 19425, loss 0.00291864, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:27:55.706991: step 19430, loss 0.000789502, acc 1, f1 1\n",
      "2017-11-15T23:27:56.232920: step 19435, loss 0.000148042, acc 1, f1 1\n",
      "2017-11-15T23:27:56.724825: step 19440, loss 0.0100312, acc 0.993711, f1 0.99371\n",
      "Current epoch:  1080\n",
      "2017-11-15T23:27:57.247773: step 19445, loss 0.00330175, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:27:57.773218: step 19450, loss 0.000937027, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:27:58.441487: step 19450, loss 2.19826, acc 0.593883, f1 0.591423\n",
      "\n",
      "2017-11-15T23:27:58.958641: step 19455, loss 0.00123924, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1081\n",
      "2017-11-15T23:27:59.443495: step 19460, loss 0.00373818, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:27:59.956930: step 19465, loss 0.00235946, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:28:00.472339: step 19470, loss 0.00306635, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:28:00.993247: step 19475, loss 0.00186318, acc 0.999023, f1 0.999022\n",
      "Current epoch:  1082\n",
      "2017-11-15T23:28:01.484125: step 19480, loss 0.00150973, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:28:02.015065: step 19485, loss 0.00279903, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:28:02.536980: step 19490, loss 0.00660643, acc 0.996094, f1 0.996083\n",
      "Current epoch:  1083\n",
      "2017-11-15T23:28:03.032850: step 19495, loss 0.000992248, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:28:03.558617: step 19500, loss 0.000163476, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:28:04.308717: step 19500, loss 2.24308, acc 0.578664, f1 0.577503\n",
      "\n",
      "2017-11-15T23:28:04.838489: step 19505, loss 0.00203327, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:28:05.350076: step 19510, loss 0.00555929, acc 0.99707, f1 0.997074\n",
      "Current epoch:  1084\n",
      "2017-11-15T23:28:05.834943: step 19515, loss 0.00120766, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:28:06.349783: step 19520, loss 0.00171304, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:28:06.869707: step 19525, loss 0.00117264, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:28:07.352614: step 19530, loss 0.00685176, acc 0.995283, f1 0.99528\n",
      "Current epoch:  1085\n",
      "2017-11-15T23:28:07.866518: step 19535, loss 0.000642844, acc 1, f1 1\n",
      "2017-11-15T23:28:08.378412: step 19540, loss 0.0011318, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:28:08.890354: step 19545, loss 0.000187577, acc 1, f1 1\n",
      "Current epoch:  1086\n",
      "2017-11-15T23:28:09.384258: step 19550, loss 0.0035254, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:28:10.037922: step 19550, loss 2.22687, acc 0.579343, f1 0.578334\n",
      "\n",
      "2017-11-15T23:28:10.542836: step 19555, loss 0.00296318, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:28:11.055302: step 19560, loss 0.00434169, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:28:11.571433: step 19565, loss 0.00165583, acc 0.999023, f1 0.999024\n",
      "Current epoch:  1087\n",
      "2017-11-15T23:28:12.056686: step 19570, loss 0.00184917, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:28:12.574608: step 19575, loss 0.00371083, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:28:13.087555: step 19580, loss 0.00407897, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1088\n",
      "2017-11-15T23:28:13.589452: step 19585, loss 0.000388643, acc 1, f1 1\n",
      "2017-11-15T23:28:14.100365: step 19590, loss 0.00194502, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:28:14.617288: step 19595, loss 0.00191561, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:28:15.153253: step 19600, loss 0.000758942, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:28:15.804168: step 19600, loss 2.26112, acc 0.569843, f1 0.570056\n",
      "\n",
      "Current epoch:  1089\n",
      "2017-11-15T23:28:16.297732: step 19605, loss 0.00087665, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:28:16.821673: step 19610, loss 0.00646355, acc 0.995117, f1 0.995119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:28:17.343158: step 19615, loss 0.00758456, acc 0.996094, f1 0.996092\n",
      "2017-11-15T23:28:17.832602: step 19620, loss 0.000155256, acc 1, f1 1\n",
      "Current epoch:  1090\n",
      "2017-11-15T23:28:18.380254: step 19625, loss 0.00277928, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:28:18.894935: step 19630, loss 0.0038633, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:28:19.407336: step 19635, loss 0.00505748, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1091\n",
      "2017-11-15T23:28:19.893388: step 19640, loss 0.00114514, acc 1, f1 1\n",
      "2017-11-15T23:28:20.419629: step 19645, loss 0.00226565, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:28:20.938355: step 19650, loss 0.00138957, acc 0.999023, f1 0.999022\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:28:21.576622: step 19650, loss 2.17733, acc 0.585062, f1 0.58618\n",
      "\n",
      "2017-11-15T23:28:22.094765: step 19655, loss 0.00394673, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1092\n",
      "2017-11-15T23:28:22.606738: step 19660, loss 0.00399841, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:28:23.121746: step 19665, loss 0.00547988, acc 0.996094, f1 0.99609\n",
      "2017-11-15T23:28:23.647274: step 19670, loss 0.00349832, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1093\n",
      "2017-11-15T23:28:24.145259: step 19675, loss 0.00259532, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:28:24.654044: step 19680, loss 0.00132773, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:28:25.169119: step 19685, loss 0.00282913, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:28:25.684319: step 19690, loss 0.00292707, acc 0.999023, f1 0.999022\n",
      "Current epoch:  1094\n",
      "2017-11-15T23:28:26.178459: step 19695, loss 0.000870797, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:28:26.696881: step 19700, loss 0.00250458, acc 0.998047, f1 0.998048\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:28:27.355159: step 19700, loss 2.25224, acc 0.588552, f1 0.583103\n",
      "\n",
      "2017-11-15T23:28:27.874839: step 19705, loss 0.00524326, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:28:28.367730: step 19710, loss 0.00249357, acc 0.998428, f1 0.998431\n",
      "Current epoch:  1095\n",
      "2017-11-15T23:28:28.893188: step 19715, loss 0.000478229, acc 1, f1 1\n",
      "2017-11-15T23:28:29.418614: step 19720, loss 0.00403025, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:28:29.939557: step 19725, loss 0.00424149, acc 0.99707, f1 0.997075\n",
      "Current epoch:  1096\n",
      "2017-11-15T23:28:30.432465: step 19730, loss 0.000736476, acc 1, f1 1\n",
      "2017-11-15T23:28:30.950372: step 19735, loss 0.0028505, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:28:31.469990: step 19740, loss 0.00179222, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:28:31.986443: step 19745, loss 0.00338796, acc 0.998047, f1 0.998046\n",
      "Current epoch:  1097\n",
      "2017-11-15T23:28:32.467011: step 19750, loss 0.00247148, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:28:33.100631: step 19750, loss 2.25011, acc 0.600184, f1 0.593414\n",
      "\n",
      "2017-11-15T23:28:33.608562: step 19755, loss 0.00356558, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:28:34.120510: step 19760, loss 0.00246992, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1098\n",
      "2017-11-15T23:28:34.611011: step 19765, loss 0.00097043, acc 1, f1 1\n",
      "2017-11-15T23:28:35.128456: step 19770, loss 0.00270553, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:28:35.650135: step 19775, loss 0.00639659, acc 0.995117, f1 0.995118\n",
      "2017-11-15T23:28:36.172926: step 19780, loss 0.00246791, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1099\n",
      "2017-11-15T23:28:36.673186: step 19785, loss 0.00259093, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:28:37.205643: step 19790, loss 0.000248476, acc 1, f1 1\n",
      "2017-11-15T23:28:37.731334: step 19795, loss 0.00750777, acc 0.995117, f1 0.995119\n",
      "2017-11-15T23:28:38.220208: step 19800, loss 0.00142788, acc 0.998428, f1 0.998428\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:28:38.881811: step 19800, loss 2.24472, acc 0.581184, f1 0.5787\n",
      "\n",
      "Current epoch:  1100\n",
      "2017-11-15T23:28:39.398910: step 19805, loss 0.00406403, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:28:39.921573: step 19810, loss 0.00394899, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:28:40.452562: step 19815, loss 0.0086407, acc 0.995117, f1 0.995119\n",
      "Current epoch:  1101\n",
      "2017-11-15T23:28:40.945424: step 19820, loss 0.00252011, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:28:41.471380: step 19825, loss 0.00378126, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:28:41.995032: step 19830, loss 0.00147721, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:28:42.535078: step 19835, loss 0.00463569, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1102\n",
      "2017-11-15T23:28:43.025964: step 19840, loss 0.00166791, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:28:43.539473: step 19845, loss 0.00205251, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:28:44.048753: step 19850, loss 0.00143801, acc 0.999023, f1 0.999022\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:28:44.679296: step 19850, loss 2.28604, acc 0.565626, f1 0.566545\n",
      "\n",
      "Current epoch:  1103\n",
      "2017-11-15T23:28:45.161180: step 19855, loss 0.000762778, acc 1, f1 1\n",
      "2017-11-15T23:28:45.673312: step 19860, loss 0.00115963, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:28:46.191063: step 19865, loss 0.00493774, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:28:46.712995: step 19870, loss 0.00508349, acc 0.99707, f1 0.997067\n",
      "Current epoch:  1104\n",
      "2017-11-15T23:28:47.202327: step 19875, loss 0.00269531, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:28:47.738982: step 19880, loss 0.00118452, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:28:48.272011: step 19885, loss 0.00398286, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:28:48.765375: step 19890, loss 0.00196752, acc 0.998428, f1 0.998428\n",
      "Current epoch:  1105\n",
      "2017-11-15T23:28:49.287212: step 19895, loss 0.00283001, acc 0.998047, f1 0.998042\n",
      "2017-11-15T23:28:49.808240: step 19900, loss 0.0085813, acc 0.994141, f1 0.994145\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:28:50.465981: step 19900, loss 2.28081, acc 0.574108, f1 0.572965\n",
      "\n",
      "2017-11-15T23:28:51.022501: step 19905, loss 0.00612583, acc 0.996094, f1 0.996094\n",
      "Current epoch:  1106\n",
      "2017-11-15T23:28:51.510450: step 19910, loss 0.00443749, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:28:52.025068: step 19915, loss 0.00197404, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:28:52.542001: step 19920, loss 0.000975645, acc 1, f1 1\n",
      "2017-11-15T23:28:53.157732: step 19925, loss 0.00323006, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1107\n",
      "2017-11-15T23:28:53.650797: step 19930, loss 0.000770985, acc 1, f1 1\n",
      "2017-11-15T23:28:54.162946: step 19935, loss 0.00287154, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:28:54.680327: step 19940, loss 0.00551576, acc 0.99707, f1 0.997074\n",
      "Current epoch:  1108\n",
      "2017-11-15T23:28:55.179222: step 19945, loss 0.000872939, acc 1, f1 1\n",
      "2017-11-15T23:28:55.704148: step 19950, loss 0.00111024, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:28:56.356819: step 19950, loss 2.19067, acc 0.588018, f1 0.58784\n",
      "\n",
      "2017-11-15T23:28:56.880863: step 19955, loss 0.000178241, acc 1, f1 1\n",
      "2017-11-15T23:28:57.404828: step 19960, loss 0.00215875, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1109\n",
      "2017-11-15T23:28:57.896773: step 19965, loss 0.00146906, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:28:58.421704: step 19970, loss 0.00571393, acc 0.996094, f1 0.996084\n",
      "2017-11-15T23:28:58.949314: step 19975, loss 0.00488269, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:28:59.445206: step 19980, loss 0.0053457, acc 0.995283, f1 0.995282\n",
      "Current epoch:  1110\n",
      "2017-11-15T23:28:59.969665: step 19985, loss 0.000294999, acc 1, f1 1\n",
      "2017-11-15T23:29:00.487110: step 19990, loss 0.0019231, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:29:01.008027: step 19995, loss 0.00127432, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1111\n",
      "2017-11-15T23:29:01.505000: step 20000, loss 0.00125167, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:29:02.170287: step 20000, loss 2.27851, acc 0.577985, f1 0.576335\n",
      "\n",
      "2017-11-15T23:29:02.689010: step 20005, loss 0.00269594, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:29:03.214088: step 20010, loss 0.00741929, acc 0.995117, f1 0.995115\n",
      "2017-11-15T23:29:03.737080: step 20015, loss 0.00126653, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1112\n",
      "2017-11-15T23:29:04.237983: step 20020, loss 0.000121635, acc 1, f1 1\n",
      "2017-11-15T23:29:04.762861: step 20025, loss 0.00367091, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:29:05.290101: step 20030, loss 0.00279432, acc 0.998047, f1 0.998052\n",
      "Current epoch:  1113\n",
      "2017-11-15T23:29:05.786218: step 20035, loss 0.000350844, acc 1, f1 1\n",
      "2017-11-15T23:29:06.304660: step 20040, loss 0.00207151, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:29:06.829567: step 20045, loss 0.00356363, acc 0.998047, f1 0.998047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:29:07.350522: step 20050, loss 0.0020785, acc 0.999023, f1 0.999024\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:29:08.012685: step 20050, loss 2.22691, acc 0.576822, f1 0.577871\n",
      "\n",
      "Current epoch:  1114\n",
      "2017-11-15T23:29:08.510360: step 20055, loss 0.00127147, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:29:09.034831: step 20060, loss 0.00315415, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:29:09.558567: step 20065, loss 0.00202997, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:29:10.066479: step 20070, loss 0.00209418, acc 0.998428, f1 0.998425\n",
      "Current epoch:  1115\n",
      "2017-11-15T23:29:10.594428: step 20075, loss 0.00222898, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:29:11.112358: step 20080, loss 0.00340722, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:29:11.638294: step 20085, loss 0.00218394, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1116\n",
      "2017-11-15T23:29:12.122229: step 20090, loss 0.000268326, acc 1, f1 1\n",
      "2017-11-15T23:29:12.641171: step 20095, loss 0.000125295, acc 1, f1 1\n",
      "2017-11-15T23:29:13.158060: step 20100, loss 0.00165025, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:29:13.791333: step 20100, loss 2.30123, acc 0.56834, f1 0.568445\n",
      "\n",
      "2017-11-15T23:29:14.299752: step 20105, loss 0.00398977, acc 0.99707, f1 0.997071\n",
      "Current epoch:  1117\n",
      "2017-11-15T23:29:14.795154: step 20110, loss 0.00225453, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:29:15.323429: step 20115, loss 0.00242914, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:29:15.861838: step 20120, loss 0.00250752, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1118\n",
      "2017-11-15T23:29:16.350576: step 20125, loss 0.00390553, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:29:16.870602: step 20130, loss 0.00162024, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:29:17.399641: step 20135, loss 0.00150237, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:29:17.919721: step 20140, loss 0.00216592, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1119\n",
      "2017-11-15T23:29:18.416146: step 20145, loss 0.00112555, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:29:18.936599: step 20150, loss 0.00212088, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:29:19.593413: step 20150, loss 2.20999, acc 0.584819, f1 0.584284\n",
      "\n",
      "2017-11-15T23:29:20.113715: step 20155, loss 0.00396628, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:29:20.609631: step 20160, loss 0.00505436, acc 0.996855, f1 0.996855\n",
      "Current epoch:  1120\n",
      "2017-11-15T23:29:21.144515: step 20165, loss 0.00386978, acc 0.99707, f1 0.997072\n",
      "2017-11-15T23:29:21.672319: step 20170, loss 0.000137118, acc 1, f1 1\n",
      "2017-11-15T23:29:22.193259: step 20175, loss 0.00116619, acc 0.999023, f1 0.999024\n",
      "Current epoch:  1121\n",
      "2017-11-15T23:29:22.688146: step 20180, loss 0.00314871, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:29:23.203060: step 20185, loss 0.00472119, acc 0.99707, f1 0.997072\n",
      "2017-11-15T23:29:23.720857: step 20190, loss 0.0030228, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:29:24.230778: step 20195, loss 0.00297844, acc 0.998047, f1 0.998048\n",
      "Current epoch:  1122\n",
      "2017-11-15T23:29:24.716775: step 20200, loss 0.00314387, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:29:25.349395: step 20200, loss 2.24572, acc 0.589472, f1 0.585173\n",
      "\n",
      "2017-11-15T23:29:25.861831: step 20205, loss 0.000139823, acc 1, f1 1\n",
      "2017-11-15T23:29:26.384459: step 20210, loss 0.00581298, acc 0.99707, f1 0.997069\n",
      "Current epoch:  1123\n",
      "2017-11-15T23:29:26.877124: step 20215, loss 0.00118135, acc 1, f1 1\n",
      "2017-11-15T23:29:27.400660: step 20220, loss 0.000887929, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:29:27.923700: step 20225, loss 0.00592737, acc 0.996094, f1 0.996085\n",
      "2017-11-15T23:29:28.451040: step 20230, loss 0.00530931, acc 0.99707, f1 0.997071\n",
      "Current epoch:  1124\n",
      "2017-11-15T23:29:28.949484: step 20235, loss 0.0033672, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:29:29.472178: step 20240, loss 0.00177342, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:29:29.995726: step 20245, loss 0.00704211, acc 0.995117, f1 0.995125\n",
      "2017-11-15T23:29:30.483438: step 20250, loss 0.000191732, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:29:31.140607: step 20250, loss 2.21044, acc 0.595967, f1 0.591666\n",
      "\n",
      "Current epoch:  1125\n",
      "2017-11-15T23:29:31.665879: step 20255, loss 0.000465572, acc 1, f1 1\n",
      "2017-11-15T23:29:32.193837: step 20260, loss 0.00398726, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:29:32.719480: step 20265, loss 0.00654735, acc 0.996094, f1 0.996092\n",
      "Current epoch:  1126\n",
      "2017-11-15T23:29:33.218377: step 20270, loss 0.00127605, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:29:33.751329: step 20275, loss 0.000259498, acc 1, f1 1\n",
      "2017-11-15T23:29:34.274272: step 20280, loss 0.00695064, acc 0.996094, f1 0.996093\n",
      "2017-11-15T23:29:34.800731: step 20285, loss 0.00166675, acc 0.999023, f1 0.999024\n",
      "Current epoch:  1127\n",
      "2017-11-15T23:29:35.291888: step 20290, loss 0.00205941, acc 1, f1 1\n",
      "2017-11-15T23:29:35.807116: step 20295, loss 0.00112017, acc 1, f1 1\n",
      "2017-11-15T23:29:36.324647: step 20300, loss 0.00702818, acc 0.995117, f1 0.995116\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:29:36.988260: step 20300, loss 2.27945, acc 0.574108, f1 0.572689\n",
      "\n",
      "Current epoch:  1128\n",
      "2017-11-15T23:29:37.490592: step 20305, loss 0.00290775, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:29:38.010207: step 20310, loss 0.00227949, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:29:38.539741: step 20315, loss 0.000155747, acc 1, f1 1\n",
      "2017-11-15T23:29:39.065686: step 20320, loss 0.00254798, acc 0.998047, f1 0.998046\n",
      "Current epoch:  1129\n",
      "2017-11-15T23:29:39.562537: step 20325, loss 0.00251526, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:29:40.086554: step 20330, loss 0.000719663, acc 1, f1 1\n",
      "2017-11-15T23:29:40.613799: step 20335, loss 0.00368727, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:29:41.104904: step 20340, loss 0.00012965, acc 1, f1 1\n",
      "Current epoch:  1130\n",
      "2017-11-15T23:29:41.633833: step 20345, loss 0.00491884, acc 0.996094, f1 0.996105\n",
      "2017-11-15T23:29:42.155563: step 20350, loss 0.000162501, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:29:42.815694: step 20350, loss 2.19605, acc 0.584335, f1 0.585984\n",
      "\n",
      "2017-11-15T23:29:43.341367: step 20355, loss 0.00370569, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1131\n",
      "2017-11-15T23:29:43.833775: step 20360, loss 0.0021471, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:29:44.358445: step 20365, loss 0.000236518, acc 1, f1 1\n",
      "2017-11-15T23:29:44.881917: step 20370, loss 0.00113376, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:29:45.399274: step 20375, loss 0.00232039, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1132\n",
      "2017-11-15T23:29:45.892246: step 20380, loss 0.00666563, acc 0.996094, f1 0.996095\n",
      "2017-11-15T23:29:46.414958: step 20385, loss 0.00159308, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:29:46.939527: step 20390, loss 0.00242667, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1133\n",
      "2017-11-15T23:29:47.434298: step 20395, loss 0.00209758, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:29:47.955342: step 20400, loss 0.0018552, acc 0.999023, f1 0.999022\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:29:48.631391: step 20400, loss 2.2667, acc 0.572121, f1 0.573633\n",
      "\n",
      "2017-11-15T23:29:49.156368: step 20405, loss 0.000859617, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:29:49.676189: step 20410, loss 0.00307703, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1134\n",
      "2017-11-15T23:29:50.169538: step 20415, loss 0.00418437, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:29:50.687346: step 20420, loss 0.00322637, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:29:51.204267: step 20425, loss 0.00293753, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:29:51.702566: step 20430, loss 0.00824655, acc 0.995283, f1 0.995286\n",
      "Current epoch:  1135\n",
      "2017-11-15T23:29:52.222692: step 20435, loss 0.00128102, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:29:52.750227: step 20440, loss 0.00207307, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:29:53.271692: step 20445, loss 0.00360703, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1136\n",
      "2017-11-15T23:29:53.770556: step 20450, loss 0.00185093, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:29:54.431300: step 20450, loss 2.29219, acc 0.567322, f1 0.569289\n",
      "\n",
      "2017-11-15T23:29:54.950225: step 20455, loss 0.000456398, acc 1, f1 1\n",
      "2017-11-15T23:29:55.470670: step 20460, loss 0.00266889, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:29:55.997591: step 20465, loss 0.0092459, acc 0.994141, f1 0.99414\n",
      "Current epoch:  1137\n",
      "2017-11-15T23:29:56.488885: step 20470, loss 0.0018464, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:29:57.007913: step 20475, loss 0.00370536, acc 0.998047, f1 0.998051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:29:57.535820: step 20480, loss 0.00463061, acc 0.99707, f1 0.997074\n",
      "Current epoch:  1138\n",
      "2017-11-15T23:29:58.029710: step 20485, loss 0.0040522, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:29:58.552636: step 20490, loss 0.000849644, acc 1, f1 1\n",
      "2017-11-15T23:29:59.071330: step 20495, loss 0.00259986, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:29:59.612817: step 20500, loss 0.000121265, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:30:00.268831: step 20500, loss 2.21863, acc 0.583365, f1 0.582563\n",
      "\n",
      "Current epoch:  1139\n",
      "2017-11-15T23:30:00.762080: step 20505, loss 0.00016953, acc 1, f1 1\n",
      "2017-11-15T23:30:01.284822: step 20510, loss 0.00346755, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:30:01.812200: step 20515, loss 0.00123435, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:30:02.297452: step 20520, loss 0.00423851, acc 0.996855, f1 0.996855\n",
      "Current epoch:  1140\n",
      "2017-11-15T23:30:02.821416: step 20525, loss 0.00049353, acc 1, f1 1\n",
      "2017-11-15T23:30:03.345325: step 20530, loss 0.00361049, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:30:03.871215: step 20535, loss 0.00395748, acc 0.99707, f1 0.997071\n",
      "Current epoch:  1141\n",
      "2017-11-15T23:30:04.358732: step 20540, loss 0.00198802, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:30:04.880065: step 20545, loss 0.00316602, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:30:05.399994: step 20550, loss 0.00101553, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:30:06.031622: step 20550, loss 2.33844, acc 0.573817, f1 0.571646\n",
      "\n",
      "2017-11-15T23:30:06.563611: step 20555, loss 0.00330414, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1142\n",
      "2017-11-15T23:30:07.059924: step 20560, loss 0.00325922, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:30:07.579057: step 20565, loss 0.00133713, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:30:08.105257: step 20570, loss 0.00465908, acc 0.99707, f1 0.997069\n",
      "Current epoch:  1143\n",
      "2017-11-15T23:30:08.605150: step 20575, loss 0.00381286, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:30:09.128601: step 20580, loss 0.000131216, acc 1, f1 1\n",
      "2017-11-15T23:30:09.652047: step 20585, loss 0.0054678, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:30:10.185039: step 20590, loss 0.00269529, acc 0.998047, f1 0.998048\n",
      "Current epoch:  1144\n",
      "2017-11-15T23:30:10.692625: step 20595, loss 0.00140211, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:30:11.219060: step 20600, loss 0.000394667, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:30:11.885876: step 20600, loss 2.1966, acc 0.590345, f1 0.590127\n",
      "\n",
      "2017-11-15T23:30:12.407330: step 20605, loss 0.00267993, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:30:12.898172: step 20610, loss 0.00601831, acc 0.995283, f1 0.995282\n",
      "Current epoch:  1145\n",
      "2017-11-15T23:30:13.421145: step 20615, loss 0.00118319, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:30:13.983200: step 20620, loss 0.00218049, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:30:14.502166: step 20625, loss 0.00228422, acc 0.998047, f1 0.998046\n",
      "Current epoch:  1146\n",
      "2017-11-15T23:30:15.000019: step 20630, loss 0.00161571, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:30:15.526972: step 20635, loss 0.00378363, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:30:16.058755: step 20640, loss 0.00223979, acc 0.998047, f1 0.998044\n",
      "2017-11-15T23:30:16.579233: step 20645, loss 0.00729392, acc 0.996094, f1 0.996094\n",
      "Current epoch:  1147\n",
      "2017-11-15T23:30:17.082941: step 20650, loss 0.000104364, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:30:17.751234: step 20650, loss 2.23002, acc 0.579827, f1 0.580075\n",
      "\n",
      "2017-11-15T23:30:18.272365: step 20655, loss 0.00419013, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:30:18.797326: step 20660, loss 0.00356008, acc 0.99707, f1 0.997075\n",
      "Current epoch:  1148\n",
      "2017-11-15T23:30:19.291215: step 20665, loss 0.000598226, acc 1, f1 1\n",
      "2017-11-15T23:30:19.815778: step 20670, loss 0.00413533, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:30:20.333693: step 20675, loss 0.00360175, acc 0.99707, f1 0.997072\n",
      "2017-11-15T23:30:20.855778: step 20680, loss 0.00188709, acc 0.999023, f1 0.999022\n",
      "Current epoch:  1149\n",
      "2017-11-15T23:30:21.354647: step 20685, loss 0.00109413, acc 1, f1 1\n",
      "2017-11-15T23:30:21.881225: step 20690, loss 0.00649301, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:30:22.406164: step 20695, loss 0.00866592, acc 0.994141, f1 0.994144\n",
      "2017-11-15T23:30:22.896063: step 20700, loss 0.00193904, acc 0.998428, f1 0.998428\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:30:23.553542: step 20700, loss 2.18996, acc 0.588648, f1 0.58803\n",
      "\n",
      "Current epoch:  1150\n",
      "2017-11-15T23:30:24.072961: step 20705, loss 0.00308895, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:30:24.587986: step 20710, loss 0.00399909, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:30:25.104419: step 20715, loss 0.00232831, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1151\n",
      "2017-11-15T23:30:25.598813: step 20720, loss 0.00120102, acc 1, f1 1\n",
      "2017-11-15T23:30:26.112565: step 20725, loss 0.00254605, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:30:26.636485: step 20730, loss 0.00550069, acc 0.996094, f1 0.996092\n",
      "2017-11-15T23:30:27.161111: step 20735, loss 0.00229826, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1152\n",
      "2017-11-15T23:30:27.660476: step 20740, loss 0.00101073, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:30:28.181717: step 20745, loss 0.000553655, acc 1, f1 1\n",
      "2017-11-15T23:30:28.712077: step 20750, loss 0.00413199, acc 0.998047, f1 0.998042\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:30:29.368279: step 20750, loss 2.22456, acc 0.576725, f1 0.579432\n",
      "\n",
      "Current epoch:  1153\n",
      "2017-11-15T23:30:29.860772: step 20755, loss 0.00172071, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:30:30.382221: step 20760, loss 0.00634236, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:30:30.908241: step 20765, loss 0.00246133, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:30:31.435668: step 20770, loss 0.00411921, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1154\n",
      "2017-11-15T23:30:31.933500: step 20775, loss 0.00341225, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:30:32.467738: step 20780, loss 0.00389558, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:30:32.995678: step 20785, loss 0.000683378, acc 1, f1 1\n",
      "2017-11-15T23:30:33.493714: step 20790, loss 0.0121542, acc 0.992138, f1 0.992151\n",
      "Current epoch:  1155\n",
      "2017-11-15T23:30:34.020198: step 20795, loss 0.00135326, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:30:34.545241: step 20800, loss 0.00267276, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:30:35.201697: step 20800, loss 2.20762, acc 0.58797, f1 0.58612\n",
      "\n",
      "2017-11-15T23:30:35.727643: step 20805, loss 0.00275846, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1156\n",
      "2017-11-15T23:30:36.219514: step 20810, loss 0.00165385, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:30:36.750452: step 20815, loss 0.0054818, acc 0.996094, f1 0.996097\n",
      "2017-11-15T23:30:37.276456: step 20820, loss 0.0054581, acc 0.996094, f1 0.996095\n",
      "2017-11-15T23:30:37.807916: step 20825, loss 0.00148259, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1157\n",
      "2017-11-15T23:30:38.311333: step 20830, loss 0.000867162, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:30:38.835650: step 20835, loss 0.0010851, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:30:39.362935: step 20840, loss 0.00204347, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1158\n",
      "2017-11-15T23:30:39.858817: step 20845, loss 0.000197135, acc 1, f1 1\n",
      "2017-11-15T23:30:40.380538: step 20850, loss 0.00341892, acc 0.998047, f1 0.998042\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:30:41.036227: step 20850, loss 2.2662, acc 0.570376, f1 0.571836\n",
      "\n",
      "2017-11-15T23:30:41.561372: step 20855, loss 0.000117662, acc 1, f1 1\n",
      "2017-11-15T23:30:42.086318: step 20860, loss 0.00694684, acc 0.995117, f1 0.99512\n",
      "Current epoch:  1159\n",
      "2017-11-15T23:30:42.581191: step 20865, loss 0.0019759, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:30:43.099201: step 20870, loss 0.00208324, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:30:43.640878: step 20875, loss 0.00168617, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:30:44.127926: step 20880, loss 0.00603913, acc 0.995283, f1 0.995283\n",
      "Current epoch:  1160\n",
      "2017-11-15T23:30:44.651071: step 20885, loss 0.00169042, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:30:45.176623: step 20890, loss 0.0109746, acc 0.993164, f1 0.993164\n",
      "2017-11-15T23:30:45.702222: step 20895, loss 0.00143746, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1161\n",
      "2017-11-15T23:30:46.194712: step 20900, loss 0.00287878, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:30:46.857102: step 20900, loss 2.21931, acc 0.578034, f1 0.580934\n",
      "\n",
      "2017-11-15T23:30:47.368291: step 20905, loss 0.00525063, acc 0.99707, f1 0.99707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:30:47.887207: step 20910, loss 0.00283784, acc 0.99707, f1 0.997072\n",
      "2017-11-15T23:30:48.412369: step 20915, loss 0.00224786, acc 0.998047, f1 0.998048\n",
      "Current epoch:  1162\n",
      "2017-11-15T23:30:48.916286: step 20920, loss 0.00234437, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:30:49.441864: step 20925, loss 0.000129426, acc 1, f1 1\n",
      "2017-11-15T23:30:49.967885: step 20930, loss 0.00336152, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1163\n",
      "2017-11-15T23:30:50.460372: step 20935, loss 0.00200862, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:30:50.978424: step 20940, loss 0.00187472, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:30:51.499703: step 20945, loss 0.00218004, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:30:52.022666: step 20950, loss 0.00568834, acc 0.99707, f1 0.99707\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:30:52.681860: step 20950, loss 2.21016, acc 0.584383, f1 0.583998\n",
      "\n",
      "Current epoch:  1164\n",
      "2017-11-15T23:30:53.204790: step 20955, loss 0.000117717, acc 1, f1 1\n",
      "2017-11-15T23:30:53.727263: step 20960, loss 0.00299212, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:30:54.256474: step 20965, loss 0.0061661, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:30:54.762441: step 20970, loss 0.000130586, acc 1, f1 1\n",
      "Current epoch:  1165\n",
      "2017-11-15T23:30:55.290043: step 20975, loss 0.00440392, acc 0.99707, f1 0.997074\n",
      "2017-11-15T23:30:55.816749: step 20980, loss 0.00394911, acc 0.99707, f1 0.997075\n",
      "2017-11-15T23:30:56.340078: step 20985, loss 0.00478425, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1166\n",
      "2017-11-15T23:30:56.836596: step 20990, loss 0.00284975, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:30:57.356130: step 20995, loss 0.00094482, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:30:57.879087: step 21000, loss 0.00462909, acc 0.99707, f1 0.997074\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:30:58.540827: step 21000, loss 2.26734, acc 0.585837, f1 0.581\n",
      "\n",
      "2017-11-15T23:30:59.065789: step 21005, loss 0.000141449, acc 1, f1 1\n",
      "Current epoch:  1167\n",
      "2017-11-15T23:30:59.560280: step 21010, loss 9.41301e-05, acc 1, f1 1\n",
      "2017-11-15T23:31:00.098736: step 21015, loss 0.000283443, acc 1, f1 1\n",
      "2017-11-15T23:31:00.624654: step 21020, loss 0.00226723, acc 0.998047, f1 0.998048\n",
      "Current epoch:  1168\n",
      "2017-11-15T23:31:01.114530: step 21025, loss 0.00132758, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:31:01.643979: step 21030, loss 0.00128178, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:31:02.168024: step 21035, loss 0.00062779, acc 1, f1 1\n",
      "2017-11-15T23:31:02.693484: step 21040, loss 0.00320193, acc 0.998047, f1 0.998046\n",
      "Current epoch:  1169\n",
      "2017-11-15T23:31:03.192376: step 21045, loss 0.00207092, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:31:03.712846: step 21050, loss 0.00311499, acc 0.999023, f1 0.999024\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:31:04.356503: step 21050, loss 2.20682, acc 0.589909, f1 0.588848\n",
      "\n",
      "2017-11-15T23:31:04.875525: step 21055, loss 0.000976091, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:31:05.365174: step 21060, loss 0.00709942, acc 0.995283, f1 0.995283\n",
      "Current epoch:  1170\n",
      "2017-11-15T23:31:05.893778: step 21065, loss 0.00429258, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:31:06.408963: step 21070, loss 0.00679591, acc 0.996094, f1 0.996097\n",
      "2017-11-15T23:31:06.924674: step 21075, loss 0.000192151, acc 1, f1 1\n",
      "Current epoch:  1171\n",
      "2017-11-15T23:31:07.412540: step 21080, loss 0.000906388, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:31:07.919444: step 21085, loss 0.00262861, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:31:08.439318: step 21090, loss 0.00670322, acc 0.995117, f1 0.995119\n",
      "2017-11-15T23:31:08.949231: step 21095, loss 0.00512529, acc 0.996094, f1 0.996095\n",
      "Current epoch:  1172\n",
      "2017-11-15T23:31:09.436124: step 21100, loss 0.000479399, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:31:10.065429: step 21100, loss 2.19475, acc 0.588842, f1 0.58789\n",
      "\n",
      "2017-11-15T23:31:10.578367: step 21105, loss 0.00408018, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:31:11.099291: step 21110, loss 0.00150708, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1173\n",
      "2017-11-15T23:31:11.589828: step 21115, loss 0.0056818, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:31:12.104749: step 21120, loss 0.00180222, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:31:12.618676: step 21125, loss 0.00202053, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:31:13.130594: step 21130, loss 0.00939513, acc 0.993164, f1 0.993165\n",
      "Current epoch:  1174\n",
      "2017-11-15T23:31:13.630660: step 21135, loss 0.00234409, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:31:14.150294: step 21140, loss 0.00189817, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:31:14.666777: step 21145, loss 0.00479459, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:31:15.151152: step 21150, loss 0.00756793, acc 0.995283, f1 0.995287\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:31:15.779282: step 21150, loss 2.49679, acc 0.550019, f1 0.54532\n",
      "\n",
      "Current epoch:  1175\n",
      "2017-11-15T23:31:16.297020: step 21155, loss 0.00171579, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:31:16.826307: step 21160, loss 0.00300452, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:31:17.343921: step 21165, loss 0.00203289, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1176\n",
      "2017-11-15T23:31:17.832887: step 21170, loss 0.00205347, acc 0.998047, f1 0.998052\n",
      "2017-11-15T23:31:18.345785: step 21175, loss 0.00207767, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:31:18.864785: step 21180, loss 0.000996295, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:31:19.381721: step 21185, loss 0.00605199, acc 0.99707, f1 0.997073\n",
      "Current epoch:  1177\n",
      "2017-11-15T23:31:19.869328: step 21190, loss 0.00323944, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:31:20.383752: step 21195, loss 0.00096121, acc 1, f1 1\n",
      "2017-11-15T23:31:20.894176: step 21200, loss 0.0034891, acc 0.99707, f1 0.997072\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:31:21.538581: step 21200, loss 2.2948, acc 0.577646, f1 0.575654\n",
      "\n",
      "Current epoch:  1178\n",
      "2017-11-15T23:31:22.036109: step 21205, loss 0.00106934, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:31:22.553541: step 21210, loss 0.00216015, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:31:23.070464: step 21215, loss 0.00117244, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:31:23.591387: step 21220, loss 0.00570034, acc 0.995117, f1 0.995116\n",
      "Current epoch:  1179\n",
      "2017-11-15T23:31:24.074343: step 21225, loss 0.000160135, acc 1, f1 1\n",
      "2017-11-15T23:31:24.584294: step 21230, loss 0.00207703, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:31:25.093218: step 21235, loss 0.00132381, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:31:25.580796: step 21240, loss 0.00251266, acc 0.998428, f1 0.998425\n",
      "Current epoch:  1180\n",
      "2017-11-15T23:31:26.088708: step 21245, loss 0.00206637, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:31:26.608627: step 21250, loss 0.0026349, acc 0.998047, f1 0.998048\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:31:27.272467: step 21250, loss 2.26816, acc 0.584238, f1 0.579417\n",
      "\n",
      "2017-11-15T23:31:27.790501: step 21255, loss 0.00600547, acc 0.996094, f1 0.996093\n",
      "Current epoch:  1181\n",
      "2017-11-15T23:31:28.370959: step 21260, loss 0.00076073, acc 1, f1 1\n",
      "2017-11-15T23:31:28.884393: step 21265, loss 0.00323733, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:31:29.409018: step 21270, loss 0.00433345, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:31:29.925838: step 21275, loss 0.00317865, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1182\n",
      "2017-11-15T23:31:30.413907: step 21280, loss 0.00384153, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:31:30.924975: step 21285, loss 0.00241928, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:31:31.443371: step 21290, loss 0.00694618, acc 0.995117, f1 0.995119\n",
      "Current epoch:  1183\n",
      "2017-11-15T23:31:31.930933: step 21295, loss 0.000150873, acc 1, f1 1\n",
      "2017-11-15T23:31:32.447606: step 21300, loss 0.000837754, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:31:33.097680: step 21300, loss 2.21634, acc 0.585207, f1 0.58662\n",
      "\n",
      "2017-11-15T23:31:33.611716: step 21305, loss 0.002675, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:31:34.124641: step 21310, loss 0.00208046, acc 0.998047, f1 0.998048\n",
      "Current epoch:  1184\n",
      "2017-11-15T23:31:34.610062: step 21315, loss 0.00423593, acc 0.998047, f1 0.998041\n",
      "2017-11-15T23:31:35.121672: step 21320, loss 0.000146921, acc 1, f1 1\n",
      "2017-11-15T23:31:35.638509: step 21325, loss 0.000123401, acc 1, f1 1\n",
      "2017-11-15T23:31:36.117374: step 21330, loss 0.00256835, acc 0.998428, f1 0.998428\n",
      "Current epoch:  1185\n",
      "2017-11-15T23:31:36.642307: step 21335, loss 0.00223206, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:31:37.156860: step 21340, loss 0.000946497, acc 0.999023, f1 0.999023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:31:37.671708: step 21345, loss 0.00228016, acc 0.999023, f1 0.999022\n",
      "Current epoch:  1186\n",
      "2017-11-15T23:31:38.164187: step 21350, loss 0.00397088, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:31:38.828918: step 21350, loss 2.2013, acc 0.581087, f1 0.582997\n",
      "\n",
      "2017-11-15T23:31:39.343546: step 21355, loss 0.00226644, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:31:39.861201: step 21360, loss 0.00225619, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:31:40.377669: step 21365, loss 0.00308029, acc 0.998047, f1 0.998042\n",
      "Current epoch:  1187\n",
      "2017-11-15T23:31:40.859555: step 21370, loss 0.000968342, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:31:41.372456: step 21375, loss 0.00041724, acc 1, f1 1\n",
      "2017-11-15T23:31:41.889354: step 21380, loss 0.00434264, acc 0.996094, f1 0.996092\n",
      "Current epoch:  1188\n",
      "2017-11-15T23:31:42.372798: step 21385, loss 0.000446089, acc 1, f1 1\n",
      "2017-11-15T23:31:42.888351: step 21390, loss 0.00123171, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:31:43.416065: step 21395, loss 0.00115076, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:31:43.936553: step 21400, loss 0.00535935, acc 0.996094, f1 0.996094\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:31:44.583720: step 21400, loss 2.22885, acc 0.579682, f1 0.581323\n",
      "\n",
      "Current epoch:  1189\n",
      "2017-11-15T23:31:45.063602: step 21405, loss 0.00189792, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:31:45.576239: step 21410, loss 0.00355784, acc 0.998047, f1 0.998042\n",
      "2017-11-15T23:31:46.086399: step 21415, loss 0.00320982, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:31:46.569259: step 21420, loss 0.00434319, acc 0.996855, f1 0.996854\n",
      "Current epoch:  1190\n",
      "2017-11-15T23:31:47.084954: step 21425, loss 0.00744265, acc 0.995117, f1 0.995115\n",
      "2017-11-15T23:31:47.601021: step 21430, loss 0.00374021, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:31:48.113938: step 21435, loss 0.00350676, acc 0.998047, f1 0.998052\n",
      "Current epoch:  1191\n",
      "2017-11-15T23:31:48.603808: step 21440, loss 0.00238621, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:31:49.112748: step 21445, loss 0.00536464, acc 0.99707, f1 0.997074\n",
      "2017-11-15T23:31:49.648261: step 21450, loss 0.00172529, acc 0.999023, f1 0.999022\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:31:50.286453: step 21450, loss 2.24316, acc 0.580748, f1 0.581677\n",
      "\n",
      "2017-11-15T23:31:50.796577: step 21455, loss 0.00338068, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1192\n",
      "2017-11-15T23:31:51.279426: step 21460, loss 0.00208718, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:31:51.791837: step 21465, loss 0.00344949, acc 0.998047, f1 0.998043\n",
      "2017-11-15T23:31:52.300671: step 21470, loss 0.0033987, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1193\n",
      "2017-11-15T23:31:52.795423: step 21475, loss 0.00345099, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:31:53.306177: step 21480, loss 0.00462361, acc 0.996094, f1 0.996095\n",
      "2017-11-15T23:31:53.821835: step 21485, loss 0.00564325, acc 0.99707, f1 0.997073\n",
      "2017-11-15T23:31:54.335274: step 21490, loss 0.00176792, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1194\n",
      "2017-11-15T23:31:54.835213: step 21495, loss 0.00107728, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:31:55.353128: step 21500, loss 0.000540062, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:31:55.982927: step 21500, loss 2.24375, acc 0.58259, f1 0.581788\n",
      "\n",
      "2017-11-15T23:31:56.495854: step 21505, loss 0.00699761, acc 0.995117, f1 0.995127\n",
      "2017-11-15T23:31:56.978327: step 21510, loss 0.000110036, acc 1, f1 1\n",
      "Current epoch:  1195\n",
      "2017-11-15T23:31:57.494307: step 21515, loss 0.000887102, acc 1, f1 1\n",
      "2017-11-15T23:31:58.004737: step 21520, loss 0.00020527, acc 1, f1 1\n",
      "2017-11-15T23:31:58.519724: step 21525, loss 0.00102414, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1196\n",
      "2017-11-15T23:31:59.005522: step 21530, loss 0.00240802, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:31:59.518914: step 21535, loss 0.00241683, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:32:00.028716: step 21540, loss 0.00401546, acc 0.996094, f1 0.996096\n",
      "2017-11-15T23:32:00.556571: step 21545, loss 0.00113492, acc 0.999023, f1 0.999022\n",
      "Current epoch:  1197\n",
      "2017-11-15T23:32:01.041142: step 21550, loss 0.00206104, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:32:01.680411: step 21550, loss 2.28329, acc 0.572654, f1 0.573273\n",
      "\n",
      "2017-11-15T23:32:02.194563: step 21555, loss 0.00545026, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:32:02.715515: step 21560, loss 0.00357879, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1198\n",
      "2017-11-15T23:32:03.208383: step 21565, loss 0.00253161, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:32:03.722306: step 21570, loss 0.00151861, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:32:04.239944: step 21575, loss 0.000107274, acc 1, f1 1\n",
      "2017-11-15T23:32:04.753877: step 21580, loss 0.00176745, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1199\n",
      "2017-11-15T23:32:05.236737: step 21585, loss 0.00243356, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:32:05.756456: step 21590, loss 0.00325104, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:32:06.279430: step 21595, loss 0.00331974, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:32:06.765311: step 21600, loss 0.0033008, acc 0.998428, f1 0.99843\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:32:07.399742: step 21600, loss 2.19603, acc 0.595531, f1 0.593711\n",
      "\n",
      "Current epoch:  1200\n",
      "2017-11-15T23:32:07.909554: step 21605, loss 0.000106317, acc 1, f1 1\n",
      "2017-11-15T23:32:08.426185: step 21610, loss 0.00300662, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:32:08.942835: step 21615, loss 0.00188491, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1201\n",
      "2017-11-15T23:32:09.438550: step 21620, loss 0.00185148, acc 1, f1 1\n",
      "2017-11-15T23:32:09.950977: step 21625, loss 0.00357259, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:32:10.462407: step 21630, loss 0.00330991, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:32:10.975349: step 21635, loss 0.00243158, acc 0.999023, f1 0.999025\n",
      "Current epoch:  1202\n",
      "2017-11-15T23:32:11.465257: step 21640, loss 0.00138594, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:32:11.983188: step 21645, loss 0.00246713, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:32:12.497873: step 21650, loss 0.00160319, acc 0.999023, f1 0.999024\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:32:13.140155: step 21650, loss 2.27515, acc 0.575029, f1 0.574803\n",
      "\n",
      "Current epoch:  1203\n",
      "2017-11-15T23:32:13.652111: step 21655, loss 0.00203207, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:32:14.167019: step 21660, loss 0.00133977, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:32:14.685961: step 21665, loss 0.00114877, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:32:15.208882: step 21670, loss 0.00795127, acc 0.994141, f1 0.994141\n",
      "Current epoch:  1204\n",
      "2017-11-15T23:32:15.699493: step 21675, loss 0.000183217, acc 1, f1 1\n",
      "2017-11-15T23:32:16.209539: step 21680, loss 0.00306404, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:32:16.729405: step 21685, loss 0.00874943, acc 0.993164, f1 0.993166\n",
      "2017-11-15T23:32:17.221109: step 21690, loss 0.00420041, acc 0.996855, f1 0.996856\n",
      "Current epoch:  1205\n",
      "2017-11-15T23:32:17.738903: step 21695, loss 0.000602516, acc 1, f1 1\n",
      "2017-11-15T23:32:18.250832: step 21700, loss 0.00803387, acc 0.994141, f1 0.99414\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:32:18.889560: step 21700, loss 2.32603, acc 0.567468, f1 0.567493\n",
      "\n",
      "2017-11-15T23:32:19.405654: step 21705, loss 0.00518951, acc 0.99707, f1 0.997072\n",
      "Current epoch:  1206\n",
      "2017-11-15T23:32:19.900704: step 21710, loss 0.00121508, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:32:20.419252: step 21715, loss 0.00368464, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:32:20.944317: step 21720, loss 9.45252e-05, acc 1, f1 1\n",
      "2017-11-15T23:32:21.458990: step 21725, loss 0.00447236, acc 0.99707, f1 0.997075\n",
      "Current epoch:  1207\n",
      "2017-11-15T23:32:21.944570: step 21730, loss 0.00293964, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:32:22.468349: step 21735, loss 0.000101705, acc 1, f1 1\n",
      "2017-11-15T23:32:22.985198: step 21740, loss 0.00591241, acc 0.996094, f1 0.996094\n",
      "Current epoch:  1208\n",
      "2017-11-15T23:32:23.471884: step 21745, loss 0.00930893, acc 0.993164, f1 0.993165\n",
      "2017-11-15T23:32:23.985218: step 21750, loss 9.50503e-05, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:32:24.621405: step 21750, loss 2.2093, acc 0.587534, f1 0.586033\n",
      "\n",
      "2017-11-15T23:32:25.128975: step 21755, loss 0.000802507, acc 1, f1 1\n",
      "2017-11-15T23:32:25.640400: step 21760, loss 0.00605556, acc 0.996094, f1 0.996094\n",
      "Current epoch:  1209\n",
      "2017-11-15T23:32:26.122765: step 21765, loss 0.00269114, acc 0.998047, f1 0.998047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:32:26.636640: step 21770, loss 0.00297934, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:32:27.147551: step 21775, loss 0.000125525, acc 1, f1 1\n",
      "2017-11-15T23:32:27.634438: step 21780, loss 0.00214653, acc 0.998428, f1 0.998425\n",
      "Current epoch:  1210\n",
      "2017-11-15T23:32:28.161371: step 21785, loss 0.00251348, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:32:28.674293: step 21790, loss 0.00489195, acc 0.996094, f1 0.996095\n",
      "2017-11-15T23:32:29.188229: step 21795, loss 0.00118962, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1211\n",
      "2017-11-15T23:32:29.671104: step 21800, loss 0.00157041, acc 0.999023, f1 0.999024\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:32:30.300642: step 21800, loss 2.21576, acc 0.596549, f1 0.592639\n",
      "\n",
      "2017-11-15T23:32:30.810716: step 21805, loss 0.00266323, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:32:31.321636: step 21810, loss 0.00218496, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:32:31.844570: step 21815, loss 0.00380998, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1212\n",
      "2017-11-15T23:32:32.329443: step 21820, loss 0.00143894, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:32:32.842367: step 21825, loss 0.00118813, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:32:33.357548: step 21830, loss 0.00201365, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1213\n",
      "2017-11-15T23:32:33.851951: step 21835, loss 0.000164779, acc 1, f1 1\n",
      "2017-11-15T23:32:34.360871: step 21840, loss 0.00181982, acc 1, f1 1\n",
      "2017-11-15T23:32:34.874818: step 21845, loss 0.00181469, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:32:35.387701: step 21850, loss 0.00245776, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:32:36.017447: step 21850, loss 2.20266, acc 0.583123, f1 0.584496\n",
      "\n",
      "Current epoch:  1214\n",
      "2017-11-15T23:32:36.500823: step 21855, loss 0.000110393, acc 1, f1 1\n",
      "2017-11-15T23:32:37.013731: step 21860, loss 0.0041262, acc 0.99707, f1 0.997074\n",
      "2017-11-15T23:32:37.526691: step 21865, loss 0.00927775, acc 0.993164, f1 0.993165\n",
      "2017-11-15T23:32:38.011554: step 21870, loss 0.00349792, acc 0.996855, f1 0.996853\n",
      "Current epoch:  1215\n",
      "2017-11-15T23:32:38.535471: step 21875, loss 0.00356815, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:32:39.058804: step 21880, loss 0.000102569, acc 1, f1 1\n",
      "2017-11-15T23:32:39.576245: step 21885, loss 0.00687722, acc 0.995117, f1 0.995115\n",
      "Current epoch:  1216\n",
      "2017-11-15T23:32:40.071012: step 21890, loss 0.000586091, acc 1, f1 1\n",
      "2017-11-15T23:32:40.588946: step 21895, loss 0.0013425, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:32:41.099793: step 21900, loss 0.00230926, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:32:41.734425: step 21900, loss 2.22084, acc 0.589085, f1 0.587025\n",
      "\n",
      "2017-11-15T23:32:42.249428: step 21905, loss 0.00667965, acc 0.995117, f1 0.995116\n",
      "Current epoch:  1217\n",
      "2017-11-15T23:32:42.736605: step 21910, loss 0.0035187, acc 0.998047, f1 0.998052\n",
      "2017-11-15T23:32:43.254107: step 21915, loss 0.00505805, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:32:43.766073: step 21920, loss 0.00412407, acc 0.99707, f1 0.997075\n",
      "Current epoch:  1218\n",
      "2017-11-15T23:32:44.248978: step 21925, loss 0.00100889, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:32:44.777834: step 21930, loss 0.00603271, acc 0.996094, f1 0.996097\n",
      "2017-11-15T23:32:45.294798: step 21935, loss 0.00417194, acc 0.996094, f1 0.996092\n",
      "2017-11-15T23:32:45.810776: step 21940, loss 0.00113618, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1219\n",
      "2017-11-15T23:32:46.298634: step 21945, loss 0.00095479, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:32:46.815156: step 21950, loss 0.00728948, acc 0.995117, f1 0.99512\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:32:47.457687: step 21950, loss 2.19782, acc 0.589133, f1 0.587646\n",
      "\n",
      "2017-11-15T23:32:47.972117: step 21955, loss 0.00430508, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:32:48.457483: step 21960, loss 0.00156612, acc 0.998428, f1 0.998428\n",
      "Current epoch:  1220\n",
      "2017-11-15T23:32:48.971928: step 21965, loss 0.000696542, acc 1, f1 1\n",
      "2017-11-15T23:32:49.478755: step 21970, loss 0.00593852, acc 0.996094, f1 0.996096\n",
      "2017-11-15T23:32:50.011434: step 21975, loss 0.00261412, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1221\n",
      "2017-11-15T23:32:50.506313: step 21980, loss 0.00411319, acc 0.99707, f1 0.997083\n",
      "2017-11-15T23:32:51.017419: step 21985, loss 0.00124276, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:32:51.532822: step 21990, loss 0.00203979, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:32:52.044759: step 21995, loss 0.0024494, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1222\n",
      "2017-11-15T23:32:52.539626: step 22000, loss 0.00237255, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:32:53.176759: step 22000, loss 2.22981, acc 0.579197, f1 0.580264\n",
      "\n",
      "2017-11-15T23:32:53.727244: step 22005, loss 0.00110281, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:32:54.240144: step 22010, loss 0.00262796, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1223\n",
      "2017-11-15T23:32:54.741788: step 22015, loss 0.00116097, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:32:55.267600: step 22020, loss 0.000934773, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:32:55.798202: step 22025, loss 0.000910026, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:32:56.312820: step 22030, loss 0.00258252, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1224\n",
      "2017-11-15T23:32:56.801271: step 22035, loss 0.000798768, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:32:57.317838: step 22040, loss 0.00417685, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:32:57.839771: step 22045, loss 0.0014168, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:32:58.319747: step 22050, loss 0.000103304, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:32:58.969405: step 22050, loss 2.25985, acc 0.576483, f1 0.576686\n",
      "\n",
      "Current epoch:  1225\n",
      "2017-11-15T23:32:59.479651: step 22055, loss 0.000767808, acc 1, f1 1\n",
      "2017-11-15T23:32:59.991084: step 22060, loss 0.00271614, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:33:00.509664: step 22065, loss 0.00493083, acc 0.99707, f1 0.997069\n",
      "Current epoch:  1226\n",
      "2017-11-15T23:33:01.007493: step 22070, loss 0.00437933, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:33:01.523434: step 22075, loss 0.00289517, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:33:02.034339: step 22080, loss 0.00249376, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:33:02.547979: step 22085, loss 0.00474169, acc 0.996094, f1 0.996094\n",
      "Current epoch:  1227\n",
      "2017-11-15T23:33:03.033962: step 22090, loss 0.000819512, acc 1, f1 1\n",
      "2017-11-15T23:33:03.547879: step 22095, loss 0.00167581, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:33:04.066810: step 22100, loss 0.00193234, acc 0.999023, f1 0.999024\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:33:04.704788: step 22100, loss 2.28235, acc 0.583656, f1 0.580526\n",
      "\n",
      "Current epoch:  1228\n",
      "2017-11-15T23:33:05.181155: step 22105, loss 0.00192323, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:33:05.697107: step 22110, loss 0.00205606, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:33:06.211536: step 22115, loss 0.00421049, acc 0.996094, f1 0.996092\n",
      "2017-11-15T23:33:06.745023: step 22120, loss 0.00330046, acc 0.998047, f1 0.998046\n",
      "Current epoch:  1229\n",
      "2017-11-15T23:33:07.227902: step 22125, loss 0.000965599, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:33:07.747832: step 22130, loss 0.0048579, acc 0.996094, f1 0.996099\n",
      "2017-11-15T23:33:08.260743: step 22135, loss 0.0015431, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:33:08.748132: step 22140, loss 0.00396966, acc 0.996855, f1 0.996844\n",
      "Current epoch:  1230\n",
      "2017-11-15T23:33:09.265404: step 22145, loss 0.00191378, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:33:09.779322: step 22150, loss 0.00235865, acc 0.998047, f1 0.998048\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:33:10.420457: step 22150, loss 2.25557, acc 0.584868, f1 0.581907\n",
      "\n",
      "2017-11-15T23:33:10.935036: step 22155, loss 0.000174137, acc 1, f1 1\n",
      "Current epoch:  1231\n",
      "2017-11-15T23:33:11.421289: step 22160, loss 0.00193026, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:33:11.955261: step 22165, loss 0.00446613, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:33:12.472520: step 22170, loss 0.00238874, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:33:12.987970: step 22175, loss 0.00291156, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1232\n",
      "2017-11-15T23:33:13.479330: step 22180, loss 0.000118049, acc 1, f1 1\n",
      "2017-11-15T23:33:13.992884: step 22185, loss 0.00200507, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:33:14.511853: step 22190, loss 0.00216279, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1233\n",
      "2017-11-15T23:33:14.994731: step 22195, loss 0.00404096, acc 0.998047, f1 0.998047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:33:15.509661: step 22200, loss 0.0016672, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:33:16.140909: step 22200, loss 2.24032, acc 0.580118, f1 0.580799\n",
      "\n",
      "2017-11-15T23:33:16.655827: step 22205, loss 0.00362979, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:33:17.163888: step 22210, loss 0.00487187, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1234\n",
      "2017-11-15T23:33:17.670807: step 22215, loss 0.000108057, acc 1, f1 1\n",
      "2017-11-15T23:33:18.183366: step 22220, loss 0.00306189, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:33:18.698327: step 22225, loss 0.00200114, acc 0.998047, f1 0.998043\n",
      "2017-11-15T23:33:19.183223: step 22230, loss 0.00612788, acc 0.995283, f1 0.995283\n",
      "Current epoch:  1235\n",
      "2017-11-15T23:33:19.699660: step 22235, loss 0.00125553, acc 1, f1 1\n",
      "2017-11-15T23:33:20.211189: step 22240, loss 0.00188918, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:33:20.726135: step 22245, loss 0.00308532, acc 0.998047, f1 0.998046\n",
      "Current epoch:  1236\n",
      "2017-11-15T23:33:21.208020: step 22250, loss 0.000145932, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:33:21.845186: step 22250, loss 2.21785, acc 0.583608, f1 0.584031\n",
      "\n",
      "2017-11-15T23:33:22.353101: step 22255, loss 0.00359836, acc 0.996094, f1 0.996096\n",
      "2017-11-15T23:33:22.879407: step 22260, loss 0.00138893, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:33:23.410345: step 22265, loss 0.00521842, acc 0.996094, f1 0.996096\n",
      "Current epoch:  1237\n",
      "2017-11-15T23:33:23.894234: step 22270, loss 0.00141266, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:33:24.405821: step 22275, loss 0.00137847, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:33:24.916750: step 22280, loss 0.00100567, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1238\n",
      "2017-11-15T23:33:25.407875: step 22285, loss 0.00287475, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:33:25.920336: step 22290, loss 0.00504117, acc 0.996094, f1 0.996096\n",
      "2017-11-15T23:33:26.434138: step 22295, loss 0.00010537, acc 1, f1 1\n",
      "2017-11-15T23:33:26.948079: step 22300, loss 0.00459388, acc 0.99707, f1 0.99707\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:33:27.582438: step 22300, loss 2.20493, acc 0.58511, f1 0.585533\n",
      "\n",
      "Current epoch:  1239\n",
      "2017-11-15T23:33:28.065337: step 22305, loss 0.00219516, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:33:28.595763: step 22310, loss 0.00127468, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:33:29.117956: step 22315, loss 0.00380789, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:33:29.614962: step 22320, loss 0.00486069, acc 0.996855, f1 0.996852\n",
      "Current epoch:  1240\n",
      "2017-11-15T23:33:30.128865: step 22325, loss 0.00577842, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:33:30.640778: step 22330, loss 8.68173e-05, acc 1, f1 1\n",
      "2017-11-15T23:33:31.150654: step 22335, loss 0.00358678, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1241\n",
      "2017-11-15T23:33:31.634999: step 22340, loss 0.00164395, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:33:32.146917: step 22345, loss 0.00228459, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:33:32.663949: step 22350, loss 0.00343504, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:33:33.291930: step 22350, loss 2.22172, acc 0.596113, f1 0.593015\n",
      "\n",
      "2017-11-15T23:33:33.807374: step 22355, loss 0.00364832, acc 0.99707, f1 0.997066\n",
      "Current epoch:  1242\n",
      "2017-11-15T23:33:34.299915: step 22360, loss 0.00252437, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:33:34.822530: step 22365, loss 0.00216196, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:33:35.340447: step 22370, loss 8.82409e-05, acc 1, f1 1\n",
      "Current epoch:  1243\n",
      "2017-11-15T23:33:35.822027: step 22375, loss 0.00418787, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:33:36.333940: step 22380, loss 0.00443113, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:33:36.845679: step 22385, loss 0.00238751, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:33:37.364916: step 22390, loss 0.00429372, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1244\n",
      "2017-11-15T23:33:37.860351: step 22395, loss 0.00108566, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:33:38.386832: step 22400, loss 0.00307462, acc 0.998047, f1 0.998048\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:33:39.044670: step 22400, loss 2.24784, acc 0.592526, f1 0.588233\n",
      "\n",
      "2017-11-15T23:33:39.575935: step 22405, loss 0.00581889, acc 0.996094, f1 0.996096\n",
      "2017-11-15T23:33:40.066419: step 22410, loss 0.000164973, acc 1, f1 1\n",
      "Current epoch:  1245\n",
      "2017-11-15T23:33:40.579477: step 22415, loss 0.00225383, acc 1, f1 1\n",
      "2017-11-15T23:33:41.101711: step 22420, loss 0.00381924, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:33:41.628691: step 22425, loss 0.000963115, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1246\n",
      "2017-11-15T23:33:42.119552: step 22430, loss 0.000770057, acc 1, f1 1\n",
      "2017-11-15T23:33:42.642490: step 22435, loss 0.00104281, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:33:43.160454: step 22440, loss 0.00220251, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:33:43.688024: step 22445, loss 0.00420165, acc 0.998047, f1 0.998043\n",
      "Current epoch:  1247\n",
      "2017-11-15T23:33:44.170070: step 22450, loss 0.000996219, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:33:44.814366: step 22450, loss 2.22738, acc 0.584141, f1 0.583466\n",
      "\n",
      "2017-11-15T23:33:45.329740: step 22455, loss 0.00328668, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:33:45.848676: step 22460, loss 0.00204294, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1248\n",
      "2017-11-15T23:33:46.329536: step 22465, loss 0.00154638, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:33:46.846447: step 22470, loss 0.00144166, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:33:47.355367: step 22475, loss 0.00589048, acc 0.996094, f1 0.996096\n",
      "2017-11-15T23:33:47.871217: step 22480, loss 0.00280097, acc 0.998047, f1 0.998046\n",
      "Current epoch:  1249\n",
      "2017-11-15T23:33:48.357187: step 22485, loss 0.000108035, acc 1, f1 1\n",
      "2017-11-15T23:33:48.867168: step 22490, loss 0.00274067, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:33:49.387433: step 22495, loss 0.00341132, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:33:49.866165: step 22500, loss 0.000323318, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:33:50.512113: step 22500, loss 2.32118, acc 0.566498, f1 0.568032\n",
      "\n",
      "Current epoch:  1250\n",
      "2017-11-15T23:33:51.028937: step 22505, loss 0.000211193, acc 1, f1 1\n",
      "2017-11-15T23:33:51.545397: step 22510, loss 0.00343871, acc 0.99707, f1 0.997072\n",
      "2017-11-15T23:33:52.066714: step 22515, loss 0.00240243, acc 0.998047, f1 0.998046\n",
      "Current epoch:  1251\n",
      "2017-11-15T23:33:52.556599: step 22520, loss 0.00355484, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:33:53.071556: step 22525, loss 0.00425516, acc 0.99707, f1 0.997072\n",
      "2017-11-15T23:33:53.583992: step 22530, loss 0.00122102, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:33:54.094745: step 22535, loss 0.000177701, acc 1, f1 1\n",
      "Current epoch:  1252\n",
      "2017-11-15T23:33:54.578497: step 22540, loss 0.00162569, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:33:55.091437: step 22545, loss 0.00362112, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:33:55.608382: step 22550, loss 0.0012032, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:33:56.282811: step 22550, loss 2.21238, acc 0.58734, f1 0.588259\n",
      "\n",
      "Current epoch:  1253\n",
      "2017-11-15T23:33:56.772803: step 22555, loss 0.00111913, acc 1, f1 1\n",
      "2017-11-15T23:33:57.288744: step 22560, loss 0.00237081, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:33:57.802685: step 22565, loss 0.00451698, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:33:58.320400: step 22570, loss 0.00344094, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1254\n",
      "2017-11-15T23:33:58.804418: step 22575, loss 0.00159661, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:33:59.317839: step 22580, loss 0.00142816, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:33:59.839477: step 22585, loss 0.00300034, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:34:00.323986: step 22590, loss 0.00212788, acc 0.998428, f1 0.998428\n",
      "Current epoch:  1255\n",
      "2017-11-15T23:34:00.944751: step 22595, loss 0.00310158, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:34:01.467675: step 22600, loss 0.00145801, acc 0.999023, f1 0.999024\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:34:02.114864: step 22600, loss 2.21421, acc 0.59461, f1 0.591071\n",
      "\n",
      "2017-11-15T23:34:02.630263: step 22605, loss 0.000819294, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1256\n",
      "2017-11-15T23:34:03.114609: step 22610, loss 0.00184254, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:34:03.634525: step 22615, loss 0.00340707, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:34:04.144436: step 22620, loss 0.00368887, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:34:04.660182: step 22625, loss 0.00372327, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:34:05.144032: step 22630, loss 0.00185564, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:34:05.654707: step 22635, loss 0.00255688, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:34:06.177633: step 22640, loss 0.00365153, acc 0.99707, f1 0.997073\n",
      "Current epoch:  1258\n",
      "2017-11-15T23:34:06.670538: step 22645, loss 0.00160175, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:34:07.194493: step 22650, loss 0.000102825, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:34:07.835640: step 22650, loss 2.20776, acc 0.590684, f1 0.589503\n",
      "\n",
      "2017-11-15T23:34:08.354584: step 22655, loss 0.00123884, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:34:08.872289: step 22660, loss 0.00322167, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1259\n",
      "2017-11-15T23:34:09.357686: step 22665, loss 0.000784152, acc 1, f1 1\n",
      "2017-11-15T23:34:09.875120: step 22670, loss 0.00427285, acc 0.99707, f1 0.997074\n",
      "2017-11-15T23:34:10.389488: step 22675, loss 0.00355952, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:34:10.872263: step 22680, loss 0.000102842, acc 1, f1 1\n",
      "Current epoch:  1260\n",
      "2017-11-15T23:34:11.386693: step 22685, loss 0.00197143, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:34:11.901158: step 22690, loss 0.00376552, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:34:12.433127: step 22695, loss 0.00135534, acc 0.999023, f1 0.999024\n",
      "Current epoch:  1261\n",
      "2017-11-15T23:34:12.922157: step 22700, loss 0.00273792, acc 0.998047, f1 0.998042\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:34:13.561914: step 22700, loss 2.31787, acc 0.568389, f1 0.569269\n",
      "\n",
      "2017-11-15T23:34:14.079106: step 22705, loss 0.00206216, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:34:14.594027: step 22710, loss 0.00498164, acc 0.99707, f1 0.997074\n",
      "2017-11-15T23:34:15.104936: step 22715, loss 0.00318514, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1262\n",
      "2017-11-15T23:34:15.591810: step 22720, loss 0.00123537, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:34:16.109691: step 22725, loss 0.00255037, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:34:16.628609: step 22730, loss 0.00314293, acc 0.998047, f1 0.998052\n",
      "Current epoch:  1263\n",
      "2017-11-15T23:34:17.114038: step 22735, loss 0.00112493, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:34:17.628384: step 22740, loss 0.00494171, acc 0.996094, f1 0.996093\n",
      "2017-11-15T23:34:18.147411: step 22745, loss 0.00198693, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:34:18.659376: step 22750, loss 0.00288735, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:34:19.289631: step 22750, loss 2.35583, acc 0.571103, f1 0.569136\n",
      "\n",
      "Current epoch:  1264\n",
      "2017-11-15T23:34:19.772052: step 22755, loss 0.000419829, acc 1, f1 1\n",
      "2017-11-15T23:34:20.285484: step 22760, loss 0.00402832, acc 0.996094, f1 0.996093\n",
      "2017-11-15T23:34:20.802446: step 22765, loss 0.00268568, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:34:21.289290: step 22770, loss 8.93001e-05, acc 1, f1 1\n",
      "Current epoch:  1265\n",
      "2017-11-15T23:34:21.806243: step 22775, loss 0.000933704, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:34:22.328158: step 22780, loss 0.00340632, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:34:22.858647: step 22785, loss 0.00104363, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1266\n",
      "2017-11-15T23:34:23.363245: step 22790, loss 0.00333865, acc 0.99707, f1 0.997072\n",
      "2017-11-15T23:34:23.891219: step 22795, loss 0.00241584, acc 0.998047, f1 0.998052\n",
      "2017-11-15T23:34:24.404151: step 22800, loss 0.00124355, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:34:25.042962: step 22800, loss 2.23534, acc 0.581087, f1 0.58053\n",
      "\n",
      "2017-11-15T23:34:25.551431: step 22805, loss 0.00222624, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1267\n",
      "2017-11-15T23:34:26.035941: step 22810, loss 0.00299307, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:34:26.547379: step 22815, loss 0.00314184, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:34:27.063820: step 22820, loss 0.00493079, acc 0.996094, f1 0.996094\n",
      "Current epoch:  1268\n",
      "2017-11-15T23:34:27.551178: step 22825, loss 0.00113635, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:34:28.066677: step 22830, loss 0.0005161, acc 1, f1 1\n",
      "2017-11-15T23:34:28.578593: step 22835, loss 0.00247842, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:34:29.110548: step 22840, loss 9.92265e-05, acc 1, f1 1\n",
      "Current epoch:  1269\n",
      "2017-11-15T23:34:29.600430: step 22845, loss 7.80023e-05, acc 1, f1 1\n",
      "2017-11-15T23:34:30.108344: step 22850, loss 0.00368488, acc 0.99707, f1 0.99707\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:34:30.743485: step 22850, loss 2.23044, acc 0.593738, f1 0.590507\n",
      "\n",
      "2017-11-15T23:34:31.254406: step 22855, loss 0.00299753, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:34:31.740862: step 22860, loss 0.00318687, acc 0.998428, f1 0.998428\n",
      "Current epoch:  1270\n",
      "2017-11-15T23:34:32.253787: step 22865, loss 0.00327212, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:34:32.764214: step 22870, loss 0.00308607, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:34:33.277744: step 22875, loss 0.00376419, acc 0.99707, f1 0.997069\n",
      "Current epoch:  1271\n",
      "2017-11-15T23:34:33.762049: step 22880, loss 0.000831288, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:34:34.276962: step 22885, loss 0.00481806, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:34:34.805214: step 22890, loss 0.00386018, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:34:35.317141: step 22895, loss 0.00645015, acc 0.995117, f1 0.99512\n",
      "Current epoch:  1272\n",
      "2017-11-15T23:34:35.806027: step 22900, loss 8.17819e-05, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:34:36.437269: step 22900, loss 2.23154, acc 0.584189, f1 0.583334\n",
      "\n",
      "2017-11-15T23:34:36.948875: step 22905, loss 0.00244735, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:34:37.466880: step 22910, loss 0.0018054, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1273\n",
      "2017-11-15T23:34:37.956872: step 22915, loss 0.00104143, acc 1, f1 1\n",
      "2017-11-15T23:34:38.489619: step 22920, loss 0.000365362, acc 1, f1 1\n",
      "2017-11-15T23:34:39.008757: step 22925, loss 8.61239e-05, acc 1, f1 1\n",
      "2017-11-15T23:34:39.527019: step 22930, loss 0.00564596, acc 0.996094, f1 0.996094\n",
      "Current epoch:  1274\n",
      "2017-11-15T23:34:40.019055: step 22935, loss 0.00382492, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:34:40.540730: step 22940, loss 0.00536181, acc 0.996094, f1 0.996096\n",
      "2017-11-15T23:34:41.059654: step 22945, loss 0.0013803, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:34:41.553414: step 22950, loss 0.000301686, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:34:42.208969: step 22950, loss 2.31807, acc 0.568776, f1 0.570214\n",
      "\n",
      "Current epoch:  1275\n",
      "2017-11-15T23:34:42.733957: step 22955, loss 0.0035673, acc 0.99707, f1 0.997074\n",
      "2017-11-15T23:34:43.261930: step 22960, loss 0.00174405, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:34:43.781990: step 22965, loss 0.00396663, acc 0.99707, f1 0.997074\n",
      "Current epoch:  1276\n",
      "2017-11-15T23:34:44.272179: step 22970, loss 0.00180481, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:34:44.784630: step 22975, loss 0.00140279, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:34:45.299503: step 22980, loss 0.00010616, acc 1, f1 1\n",
      "2017-11-15T23:34:45.823385: step 22985, loss 0.000112258, acc 1, f1 1\n",
      "Current epoch:  1277\n",
      "2017-11-15T23:34:46.301231: step 22990, loss 0.00399582, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:34:46.815163: step 22995, loss 0.00194791, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:34:47.329064: step 23000, loss 0.00273424, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:34:47.970932: step 23000, loss 2.31647, acc 0.567032, f1 0.568508\n",
      "\n",
      "Current epoch:  1278\n",
      "2017-11-15T23:34:48.454310: step 23005, loss 0.00336207, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:34:48.969364: step 23010, loss 0.00625229, acc 0.99707, f1 0.997072\n",
      "2017-11-15T23:34:49.479990: step 23015, loss 0.00270821, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:34:49.998500: step 23020, loss 0.0017144, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1279\n",
      "2017-11-15T23:34:50.481353: step 23025, loss 0.00146664, acc 1, f1 1\n",
      "2017-11-15T23:34:51.000292: step 23030, loss 0.00218986, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:34:51.521205: step 23035, loss 0.00106868, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:34:52.010270: step 23040, loss 0.00658812, acc 0.995283, f1 0.99529\n",
      "Current epoch:  1280\n",
      "2017-11-15T23:34:52.530220: step 23045, loss 0.0029191, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:34:53.043242: step 23050, loss 0.00173185, acc 0.999023, f1 0.999022\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:34:53.678895: step 23050, loss 2.24955, acc 0.579052, f1 0.579199\n",
      "\n",
      "2017-11-15T23:34:54.265456: step 23055, loss 0.00119792, acc 0.999023, f1 0.999025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch:  1281\n",
      "2017-11-15T23:34:54.760865: step 23060, loss 0.00236845, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:34:55.279330: step 23065, loss 0.0008286, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:34:55.798114: step 23070, loss 9.51493e-05, acc 1, f1 1\n",
      "2017-11-15T23:34:56.316064: step 23075, loss 0.00313682, acc 0.998047, f1 0.998046\n",
      "Current epoch:  1282\n",
      "2017-11-15T23:34:56.816970: step 23080, loss 0.00245051, acc 0.998047, f1 0.998051\n",
      "2017-11-15T23:34:57.331496: step 23085, loss 0.002873, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:34:57.849743: step 23090, loss 0.00362686, acc 0.99707, f1 0.997071\n",
      "Current epoch:  1283\n",
      "2017-11-15T23:34:58.338404: step 23095, loss 0.00444298, acc 0.99707, f1 0.997073\n",
      "2017-11-15T23:34:58.850941: step 23100, loss 0.000808514, acc 0.999023, f1 0.999022\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:34:59.486619: step 23100, loss 2.21908, acc 0.584286, f1 0.586333\n",
      "\n",
      "2017-11-15T23:34:59.989037: step 23105, loss 0.00360703, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:35:00.506991: step 23110, loss 0.00442447, acc 0.996094, f1 0.996093\n",
      "Current epoch:  1284\n",
      "2017-11-15T23:35:00.986335: step 23115, loss 0.00277415, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:35:01.499472: step 23120, loss 0.00255791, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:35:02.020230: step 23125, loss 0.00165598, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:35:02.508852: step 23130, loss 0.00520542, acc 0.996855, f1 0.996855\n",
      "Current epoch:  1285\n",
      "2017-11-15T23:35:03.021779: step 23135, loss 0.00264373, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:35:03.531799: step 23140, loss 0.00495428, acc 0.996094, f1 0.996084\n",
      "2017-11-15T23:35:04.043710: step 23145, loss 0.00015179, acc 1, f1 1\n",
      "Current epoch:  1286\n",
      "2017-11-15T23:35:04.528588: step 23150, loss 0.00464159, acc 0.996094, f1 0.996085\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:35:05.161770: step 23150, loss 2.3222, acc 0.567759, f1 0.569951\n",
      "\n",
      "2017-11-15T23:35:05.667696: step 23155, loss 0.00306398, acc 0.998047, f1 0.998051\n",
      "2017-11-15T23:35:06.178621: step 23160, loss 0.00110438, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:35:06.694532: step 23165, loss 0.00374148, acc 0.99707, f1 0.997075\n",
      "Current epoch:  1287\n",
      "2017-11-15T23:35:07.180165: step 23170, loss 0.00120481, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:35:07.702655: step 23175, loss 0.00286533, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:35:08.211081: step 23180, loss 0.00155228, acc 0.999023, f1 0.999024\n",
      "Current epoch:  1288\n",
      "2017-11-15T23:35:08.700957: step 23185, loss 0.00144152, acc 1, f1 1\n",
      "2017-11-15T23:35:09.212878: step 23190, loss 0.00448586, acc 0.99707, f1 0.997068\n",
      "2017-11-15T23:35:09.730810: step 23195, loss 0.00164486, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:35:10.237436: step 23200, loss 0.00117741, acc 0.999023, f1 0.999022\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:35:10.877205: step 23200, loss 2.24033, acc 0.589812, f1 0.590483\n",
      "\n",
      "Current epoch:  1289\n",
      "2017-11-15T23:35:11.354564: step 23205, loss 0.00118299, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:35:11.868503: step 23210, loss 0.00299005, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:35:12.378936: step 23215, loss 0.00491028, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:35:12.862392: step 23220, loss 0.00202514, acc 0.998428, f1 0.998431\n",
      "Current epoch:  1290\n",
      "2017-11-15T23:35:13.392854: step 23225, loss 0.001307, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:35:13.921083: step 23230, loss 0.000599858, acc 1, f1 1\n",
      "2017-11-15T23:35:14.444239: step 23235, loss 0.00491784, acc 0.995117, f1 0.995119\n",
      "Current epoch:  1291\n",
      "2017-11-15T23:35:14.943645: step 23240, loss 0.0025021, acc 0.998047, f1 0.998041\n",
      "2017-11-15T23:35:15.460831: step 23245, loss 0.00338564, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:35:15.973729: step 23250, loss 0.000495313, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:35:16.610140: step 23250, loss 2.22469, acc 0.588697, f1 0.588143\n",
      "\n",
      "2017-11-15T23:35:17.115044: step 23255, loss 0.0035318, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1292\n",
      "2017-11-15T23:35:17.597911: step 23260, loss 0.00485624, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:35:18.111841: step 23265, loss 0.002449, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:35:18.641918: step 23270, loss 0.00259758, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1293\n",
      "2017-11-15T23:35:19.133804: step 23275, loss 0.00078154, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:35:19.654613: step 23280, loss 7.5575e-05, acc 1, f1 1\n",
      "2017-11-15T23:35:20.172038: step 23285, loss 0.00610654, acc 0.996094, f1 0.99609\n",
      "2017-11-15T23:35:20.683910: step 23290, loss 0.00592248, acc 0.995117, f1 0.995116\n",
      "Current epoch:  1294\n",
      "2017-11-15T23:35:21.166910: step 23295, loss 0.000923165, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:35:21.679782: step 23300, loss 0.000871772, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:35:22.331932: step 23300, loss 2.2496, acc 0.577161, f1 0.578445\n",
      "\n",
      "2017-11-15T23:35:22.855398: step 23305, loss 0.00362218, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:35:23.350353: step 23310, loss 0.00021773, acc 1, f1 1\n",
      "Current epoch:  1295\n",
      "2017-11-15T23:35:23.881935: step 23315, loss 0.00222334, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:35:24.414498: step 23320, loss 0.000556431, acc 1, f1 1\n",
      "2017-11-15T23:35:24.935097: step 23325, loss 0.00505555, acc 0.996094, f1 0.996094\n",
      "Current epoch:  1296\n",
      "2017-11-15T23:35:25.429991: step 23330, loss 0.00330951, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:35:25.944906: step 23335, loss 0.00222069, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:35:26.457827: step 23340, loss 0.00269411, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:35:26.978258: step 23345, loss 0.00195734, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1297\n",
      "2017-11-15T23:35:27.460606: step 23350, loss 0.00321543, acc 0.998047, f1 0.998051\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:35:28.095149: step 23350, loss 2.21685, acc 0.589957, f1 0.587992\n",
      "\n",
      "2017-11-15T23:35:28.604151: step 23355, loss 0.000126607, acc 1, f1 1\n",
      "2017-11-15T23:35:29.113076: step 23360, loss 0.00166568, acc 0.999023, f1 0.999022\n",
      "Current epoch:  1298\n",
      "2017-11-15T23:35:29.622838: step 23365, loss 8.54297e-05, acc 1, f1 1\n",
      "2017-11-15T23:35:30.144749: step 23370, loss 0.00149035, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:35:30.663683: step 23375, loss 0.00371304, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:35:31.187633: step 23380, loss 0.00690141, acc 0.995117, f1 0.995109\n",
      "Current epoch:  1299\n",
      "2017-11-15T23:35:31.680521: step 23385, loss 0.00289108, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:35:32.202658: step 23390, loss 0.00377097, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:35:32.722915: step 23395, loss 0.00237998, acc 0.998047, f1 0.998052\n",
      "2017-11-15T23:35:33.206252: step 23400, loss 9.66627e-05, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:35:33.876215: step 23400, loss 2.24679, acc 0.590054, f1 0.586977\n",
      "\n",
      "Current epoch:  1300\n",
      "2017-11-15T23:35:34.394588: step 23405, loss 0.000795939, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:35:34.927872: step 23410, loss 0.00478684, acc 0.99707, f1 0.997075\n",
      "2017-11-15T23:35:35.451802: step 23415, loss 0.00184563, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1301\n",
      "2017-11-15T23:35:35.941711: step 23420, loss 0.00165601, acc 1, f1 1\n",
      "2017-11-15T23:35:36.452510: step 23425, loss 0.00182752, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:35:36.965941: step 23430, loss 0.00132934, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:35:37.480377: step 23435, loss 0.000874921, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1302\n",
      "2017-11-15T23:35:37.966170: step 23440, loss 0.00155731, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:35:38.483080: step 23445, loss 0.00385711, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:35:38.998671: step 23450, loss 0.00397646, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:35:39.627147: step 23450, loss 2.29135, acc 0.580263, f1 0.578912\n",
      "\n",
      "Current epoch:  1303\n",
      "2017-11-15T23:35:40.108019: step 23455, loss 0.00192255, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:35:40.643759: step 23460, loss 0.00182427, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:35:41.169724: step 23465, loss 0.002976, acc 0.99707, f1 0.997066\n",
      "2017-11-15T23:35:41.682124: step 23470, loss 0.00212785, acc 0.999023, f1 0.999024\n",
      "Current epoch:  1304\n",
      "2017-11-15T23:35:42.166515: step 23475, loss 0.000805397, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:35:42.685646: step 23480, loss 0.00112708, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:35:43.202560: step 23485, loss 0.00129149, acc 0.999023, f1 0.999023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:35:43.698097: step 23490, loss 0.00937752, acc 0.993711, f1 0.993711\n",
      "Current epoch:  1305\n",
      "2017-11-15T23:35:44.224549: step 23495, loss 0.00282539, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:35:44.753848: step 23500, loss 0.00374728, acc 0.99707, f1 0.997072\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:35:45.410142: step 23500, loss 2.31092, acc 0.589715, f1 0.583326\n",
      "\n",
      "2017-11-15T23:35:45.930159: step 23505, loss 0.000126524, acc 1, f1 1\n",
      "Current epoch:  1306\n",
      "2017-11-15T23:35:46.433056: step 23510, loss 0.00146991, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:35:46.952976: step 23515, loss 0.00270236, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:35:47.470950: step 23520, loss 0.00128121, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:35:47.979859: step 23525, loss 0.00319012, acc 0.99707, f1 0.997066\n",
      "Current epoch:  1307\n",
      "2017-11-15T23:35:48.463208: step 23530, loss 9.8713e-05, acc 1, f1 1\n",
      "2017-11-15T23:35:48.979411: step 23535, loss 0.00314604, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:35:49.496450: step 23540, loss 0.00368439, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1308\n",
      "2017-11-15T23:35:49.978319: step 23545, loss 0.00196204, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:35:50.496000: step 23550, loss 0.00536288, acc 0.996094, f1 0.996092\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:35:51.135384: step 23550, loss 2.36771, acc 0.563493, f1 0.565476\n",
      "\n",
      "2017-11-15T23:35:51.658332: step 23555, loss 0.00557911, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:35:52.170291: step 23560, loss 0.00231384, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1309\n",
      "2017-11-15T23:35:52.657185: step 23565, loss 0.000825285, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:35:53.171666: step 23570, loss 0.00300345, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:35:53.690589: step 23575, loss 0.00152674, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:35:54.182475: step 23580, loss 0.00614151, acc 0.996855, f1 0.996853\n",
      "Current epoch:  1310\n",
      "2017-11-15T23:35:54.707416: step 23585, loss 0.00077244, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:35:55.223329: step 23590, loss 0.00290535, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:35:55.749777: step 23595, loss 0.00234082, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1311\n",
      "2017-11-15T23:35:56.244891: step 23600, loss 0.00287041, acc 0.99707, f1 0.99707\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:35:56.915152: step 23600, loss 2.21558, acc 0.596937, f1 0.593085\n",
      "\n",
      "2017-11-15T23:35:57.434130: step 23605, loss 0.00103949, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:35:57.950202: step 23610, loss 8.8906e-05, acc 1, f1 1\n",
      "2017-11-15T23:35:58.464585: step 23615, loss 0.00188479, acc 0.998047, f1 0.998046\n",
      "Current epoch:  1312\n",
      "2017-11-15T23:35:58.950064: step 23620, loss 9.80256e-05, acc 1, f1 1\n",
      "2017-11-15T23:35:59.463997: step 23625, loss 0.00388373, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:35:59.973930: step 23630, loss 0.0017139, acc 0.999023, f1 0.999024\n",
      "Current epoch:  1313\n",
      "2017-11-15T23:36:00.457384: step 23635, loss 0.00429223, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:36:00.970791: step 23640, loss 0.0014536, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:36:01.486725: step 23645, loss 0.00309209, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:36:01.998407: step 23650, loss 0.00435268, acc 0.99707, f1 0.997069\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:36:02.644243: step 23650, loss 2.23132, acc 0.585013, f1 0.584089\n",
      "\n",
      "Current epoch:  1314\n",
      "2017-11-15T23:36:03.133123: step 23655, loss 0.000881061, acc 1, f1 1\n",
      "2017-11-15T23:36:03.650970: step 23660, loss 0.00155498, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:36:04.176957: step 23665, loss 0.000835342, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:36:04.673955: step 23670, loss 0.0057798, acc 0.995283, f1 0.995292\n",
      "Current epoch:  1315\n",
      "2017-11-15T23:36:05.197911: step 23675, loss 0.000524424, acc 1, f1 1\n",
      "2017-11-15T23:36:05.710806: step 23680, loss 0.00327581, acc 0.998047, f1 0.99805\n",
      "2017-11-15T23:36:06.220052: step 23685, loss 0.00310981, acc 0.99707, f1 0.997072\n",
      "Current epoch:  1316\n",
      "2017-11-15T23:36:06.704519: step 23690, loss 0.00172099, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:36:07.213934: step 23695, loss 0.000506476, acc 1, f1 1\n",
      "2017-11-15T23:36:07.729279: step 23700, loss 0.00697868, acc 0.994141, f1 0.994156\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:36:08.390297: step 23700, loss 2.48309, acc 0.568437, f1 0.560968\n",
      "\n",
      "2017-11-15T23:36:08.898874: step 23705, loss 0.0040174, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1317\n",
      "2017-11-15T23:36:09.382723: step 23710, loss 0.00395112, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:36:09.909067: step 23715, loss 0.00345911, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:36:10.431370: step 23720, loss 0.00155191, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1318\n",
      "2017-11-15T23:36:10.919149: step 23725, loss 0.00435546, acc 0.99707, f1 0.997072\n",
      "2017-11-15T23:36:11.431881: step 23730, loss 0.00242753, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:36:11.948670: step 23735, loss 0.00482692, acc 0.996094, f1 0.996096\n",
      "2017-11-15T23:36:12.462578: step 23740, loss 0.00279221, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1319\n",
      "2017-11-15T23:36:12.948479: step 23745, loss 0.00159304, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:36:13.466529: step 23750, loss 0.00175341, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:36:14.122479: step 23750, loss 2.31402, acc 0.571297, f1 0.572379\n",
      "\n",
      "2017-11-15T23:36:14.655448: step 23755, loss 8.30191e-05, acc 1, f1 1\n",
      "2017-11-15T23:36:15.143318: step 23760, loss 0.00408795, acc 0.996855, f1 0.996855\n",
      "Current epoch:  1320\n",
      "2017-11-15T23:36:15.666267: step 23765, loss 0.00206221, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:36:16.185216: step 23770, loss 0.00169797, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:36:16.703039: step 23775, loss 0.00469321, acc 0.99707, f1 0.997072\n",
      "Current epoch:  1321\n",
      "2017-11-15T23:36:17.188384: step 23780, loss 0.00240327, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:36:17.700782: step 23785, loss 0.0015334, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:36:18.213702: step 23790, loss 0.00280149, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:36:18.732630: step 23795, loss 0.00218959, acc 0.999023, f1 0.999022\n",
      "Current epoch:  1322\n",
      "2017-11-15T23:36:19.229547: step 23800, loss 0.00370932, acc 0.99707, f1 0.997072\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:36:19.885093: step 23800, loss 2.2257, acc 0.588212, f1 0.58788\n",
      "\n",
      "2017-11-15T23:36:20.404897: step 23805, loss 0.00228883, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:36:20.919306: step 23810, loss 0.00415662, acc 0.996094, f1 0.996094\n",
      "Current epoch:  1323\n",
      "2017-11-15T23:36:21.406051: step 23815, loss 0.00226281, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:36:21.921976: step 23820, loss 0.00132075, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:36:22.440896: step 23825, loss 0.00518438, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:36:22.956300: step 23830, loss 0.00310967, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1324\n",
      "2017-11-15T23:36:23.444058: step 23835, loss 0.000580816, acc 1, f1 1\n",
      "2017-11-15T23:36:23.958491: step 23840, loss 0.000925274, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:36:24.497452: step 23845, loss 0.00709933, acc 0.994141, f1 0.994141\n",
      "2017-11-15T23:36:24.995911: step 23850, loss 0.00404548, acc 0.996855, f1 0.996856\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:36:25.657562: step 23850, loss 2.34443, acc 0.572799, f1 0.571766\n",
      "\n",
      "Current epoch:  1325\n",
      "2017-11-15T23:36:26.178502: step 23855, loss 0.00290924, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:36:26.701439: step 23860, loss 9.16472e-05, acc 1, f1 1\n",
      "2017-11-15T23:36:27.221872: step 23865, loss 0.00320758, acc 0.998047, f1 0.998048\n",
      "Current epoch:  1326\n",
      "2017-11-15T23:36:27.716783: step 23870, loss 0.000118349, acc 1, f1 1\n",
      "2017-11-15T23:36:28.239226: step 23875, loss 0.00103423, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:36:28.760001: step 23880, loss 9.01747e-05, acc 1, f1 1\n",
      "2017-11-15T23:36:29.282644: step 23885, loss 0.00575259, acc 0.995117, f1 0.995118\n",
      "Current epoch:  1327\n",
      "2017-11-15T23:36:29.781049: step 23890, loss 0.00401358, acc 0.998047, f1 0.998042\n",
      "2017-11-15T23:36:30.314021: step 23895, loss 0.00502175, acc 0.996094, f1 0.996105\n",
      "2017-11-15T23:36:30.840962: step 23900, loss 0.000964154, acc 0.999023, f1 0.999025\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:36:31.499686: step 23900, loss 2.2835, acc 0.583802, f1 0.581124\n",
      "\n",
      "Current epoch:  1328\n",
      "2017-11-15T23:36:31.987191: step 23905, loss 0.00270516, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:36:32.502102: step 23910, loss 9.42352e-05, acc 1, f1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:36:33.016024: step 23915, loss 0.00374432, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:36:33.533939: step 23920, loss 0.00198787, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1329\n",
      "2017-11-15T23:36:34.016564: step 23925, loss 0.00116567, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:36:34.643977: step 23930, loss 0.000123148, acc 1, f1 1\n",
      "2017-11-15T23:36:35.163405: step 23935, loss 7.15513e-05, acc 1, f1 1\n",
      "2017-11-15T23:36:35.667111: step 23940, loss 0.00269221, acc 0.998428, f1 0.998428\n",
      "Current epoch:  1330\n",
      "2017-11-15T23:36:36.186119: step 23945, loss 0.00335275, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:36:36.702642: step 23950, loss 8.33454e-05, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:36:37.333946: step 23950, loss 2.27462, acc 0.581766, f1 0.580721\n",
      "\n",
      "2017-11-15T23:36:37.852741: step 23955, loss 0.0051079, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1331\n",
      "2017-11-15T23:36:38.336621: step 23960, loss 0.00179014, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:36:38.850611: step 23965, loss 8.59751e-05, acc 1, f1 1\n",
      "2017-11-15T23:36:39.360537: step 23970, loss 0.00262957, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:36:39.872464: step 23975, loss 0.00200211, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1332\n",
      "2017-11-15T23:36:40.354325: step 23980, loss 0.00112466, acc 1, f1 1\n",
      "2017-11-15T23:36:40.871981: step 23985, loss 0.00454595, acc 0.99707, f1 0.997073\n",
      "2017-11-15T23:36:41.396717: step 23990, loss 0.00118815, acc 0.999023, f1 0.999025\n",
      "Current epoch:  1333\n",
      "2017-11-15T23:36:41.886591: step 23995, loss 0.000625689, acc 1, f1 1\n",
      "2017-11-15T23:36:42.405013: step 24000, loss 0.00236606, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:36:43.062662: step 24000, loss 2.34441, acc 0.578858, f1 0.5748\n",
      "\n",
      "2017-11-15T23:36:43.576970: step 24005, loss 0.00321939, acc 0.99707, f1 0.997066\n",
      "2017-11-15T23:36:44.088383: step 24010, loss 0.00134993, acc 0.999023, f1 0.999022\n",
      "Current epoch:  1334\n",
      "2017-11-15T23:36:44.576299: step 24015, loss 0.00270393, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:36:45.099114: step 24020, loss 0.00311527, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:36:45.620042: step 24025, loss 9.79248e-05, acc 1, f1 1\n",
      "2017-11-15T23:36:46.103660: step 24030, loss 8.87442e-05, acc 1, f1 1\n",
      "Current epoch:  1335\n",
      "2017-11-15T23:36:46.627466: step 24035, loss 0.00205149, acc 1, f1 1\n",
      "2017-11-15T23:36:47.144371: step 24040, loss 0.000751109, acc 1, f1 1\n",
      "2017-11-15T23:36:47.658020: step 24045, loss 0.00721143, acc 0.995117, f1 0.995117\n",
      "Current epoch:  1336\n",
      "2017-11-15T23:36:48.143893: step 24050, loss 0.000901697, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:36:48.778789: step 24050, loss 2.25959, acc 0.600135, f1 0.594118\n",
      "\n",
      "2017-11-15T23:36:49.286835: step 24055, loss 0.00143417, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:36:49.806466: step 24060, loss 0.00124768, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:36:50.325905: step 24065, loss 0.00255255, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1337\n",
      "2017-11-15T23:36:50.824081: step 24070, loss 0.00208251, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:36:51.346509: step 24075, loss 0.00157402, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:36:51.866737: step 24080, loss 0.00436997, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1338\n",
      "2017-11-15T23:36:52.357608: step 24085, loss 0.000114191, acc 1, f1 1\n",
      "2017-11-15T23:36:52.870067: step 24090, loss 0.00465493, acc 0.99707, f1 0.997081\n",
      "2017-11-15T23:36:53.381481: step 24095, loss 0.00327593, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:36:53.905136: step 24100, loss 0.00712103, acc 0.995117, f1 0.99512\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:36:54.547400: step 24100, loss 2.45067, acc 0.553315, f1 0.554337\n",
      "\n",
      "Current epoch:  1339\n",
      "2017-11-15T23:36:55.102515: step 24105, loss 0.00206892, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:36:55.626220: step 24110, loss 0.00367867, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:36:56.149190: step 24115, loss 0.0030833, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:36:56.639068: step 24120, loss 0.00820071, acc 0.995283, f1 0.995283\n",
      "Current epoch:  1340\n",
      "2017-11-15T23:36:57.160510: step 24125, loss 0.00459765, acc 0.998047, f1 0.998041\n",
      "2017-11-15T23:36:57.688865: step 24130, loss 0.00659128, acc 0.996094, f1 0.996093\n",
      "2017-11-15T23:36:58.205789: step 24135, loss 9.44038e-05, acc 1, f1 1\n",
      "Current epoch:  1341\n",
      "2017-11-15T23:36:58.686673: step 24140, loss 0.00238717, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:36:59.194595: step 24145, loss 0.00202629, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:36:59.711514: step 24150, loss 0.00207957, acc 0.999023, f1 0.999024\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:37:00.348007: step 24150, loss 2.32743, acc 0.576386, f1 0.574888\n",
      "\n",
      "2017-11-15T23:37:00.858946: step 24155, loss 0.0041256, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1342\n",
      "2017-11-15T23:37:01.342800: step 24160, loss 0.00527686, acc 0.996094, f1 0.996098\n",
      "2017-11-15T23:37:01.858249: step 24165, loss 0.00402703, acc 0.996094, f1 0.996092\n",
      "2017-11-15T23:37:02.381705: step 24170, loss 0.000578963, acc 1, f1 1\n",
      "Current epoch:  1343\n",
      "2017-11-15T23:37:02.868379: step 24175, loss 0.000917742, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:37:03.392302: step 24180, loss 0.00046191, acc 1, f1 1\n",
      "2017-11-15T23:37:03.914124: step 24185, loss 0.00329112, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:37:04.429804: step 24190, loss 8.93612e-05, acc 1, f1 1\n",
      "Current epoch:  1344\n",
      "2017-11-15T23:37:04.911365: step 24195, loss 0.000484784, acc 1, f1 1\n",
      "2017-11-15T23:37:05.428251: step 24200, loss 0.00202275, acc 0.999023, f1 0.999024\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:37:06.069019: step 24200, loss 2.23337, acc 0.591217, f1 0.589071\n",
      "\n",
      "2017-11-15T23:37:06.582468: step 24205, loss 0.00322075, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:37:07.070354: step 24210, loss 0.00863792, acc 0.993711, f1 0.993686\n",
      "Current epoch:  1345\n",
      "2017-11-15T23:37:07.580848: step 24215, loss 0.00447446, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:37:08.097974: step 24220, loss 0.00432202, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:37:08.620077: step 24225, loss 0.00282365, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1346\n",
      "2017-11-15T23:37:09.114488: step 24230, loss 0.000713193, acc 1, f1 1\n",
      "2017-11-15T23:37:09.633404: step 24235, loss 0.00210779, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:37:10.149384: step 24240, loss 0.00174218, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:37:10.661120: step 24245, loss 0.00204148, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1347\n",
      "2017-11-15T23:37:11.152520: step 24250, loss 0.00447617, acc 0.996094, f1 0.996092\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:37:11.817442: step 24250, loss 2.24887, acc 0.59524, f1 0.590769\n",
      "\n",
      "2017-11-15T23:37:12.332397: step 24255, loss 0.00185243, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:37:12.849862: step 24260, loss 0.00214063, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1348\n",
      "2017-11-15T23:37:13.342240: step 24265, loss 0.0021032, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:37:13.858437: step 24270, loss 0.00279177, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:37:14.378371: step 24275, loss 0.00117314, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:37:14.896282: step 24280, loss 0.00184058, acc 0.998047, f1 0.998042\n",
      "Current epoch:  1349\n",
      "2017-11-15T23:37:15.394177: step 24285, loss 0.0011571, acc 1, f1 1\n",
      "2017-11-15T23:37:15.920064: step 24290, loss 0.00460422, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:37:16.445225: step 24295, loss 0.00401008, acc 0.99707, f1 0.997072\n",
      "2017-11-15T23:37:16.933228: step 24300, loss 9.12447e-05, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:37:17.569078: step 24300, loss 2.22539, acc 0.588115, f1 0.586867\n",
      "\n",
      "Current epoch:  1350\n",
      "2017-11-15T23:37:18.079849: step 24305, loss 0.00263304, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:37:18.597293: step 24310, loss 0.00177995, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:37:19.115218: step 24315, loss 0.000116822, acc 1, f1 1\n",
      "Current epoch:  1351\n",
      "2017-11-15T23:37:19.615637: step 24320, loss 0.00137603, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:37:20.134166: step 24325, loss 0.0023858, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:37:20.652262: step 24330, loss 0.00589308, acc 0.996094, f1 0.99609\n",
      "2017-11-15T23:37:21.171476: step 24335, loss 0.00370057, acc 0.99707, f1 0.997071\n",
      "Current epoch:  1352\n",
      "2017-11-15T23:37:21.672369: step 24340, loss 0.00257173, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:37:22.192290: step 24345, loss 8.12276e-05, acc 1, f1 1\n",
      "2017-11-15T23:37:22.717224: step 24350, loss 0.0035563, acc 0.99707, f1 0.99707\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:37:23.374927: step 24350, loss 2.2506, acc 0.577598, f1 0.578365\n",
      "\n",
      "Current epoch:  1353\n",
      "2017-11-15T23:37:23.858194: step 24355, loss 0.00236893, acc 1, f1 1\n",
      "2017-11-15T23:37:24.372626: step 24360, loss 0.0016584, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:37:24.886839: step 24365, loss 0.00528308, acc 0.996094, f1 0.996096\n",
      "2017-11-15T23:37:25.421366: step 24370, loss 0.00219474, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1354\n",
      "2017-11-15T23:37:25.911231: step 24375, loss 0.00249999, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:37:26.435179: step 24380, loss 0.0037454, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:37:26.956621: step 24385, loss 0.00488849, acc 0.996094, f1 0.996085\n",
      "2017-11-15T23:37:27.445720: step 24390, loss 0.00485625, acc 0.996855, f1 0.996855\n",
      "Current epoch:  1355\n",
      "2017-11-15T23:37:27.970765: step 24395, loss 0.00343546, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:37:28.496738: step 24400, loss 0.00269638, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:37:29.151530: step 24400, loss 2.24233, acc 0.583802, f1 0.582859\n",
      "\n",
      "2017-11-15T23:37:29.671361: step 24405, loss 0.0036947, acc 0.99707, f1 0.997074\n",
      "Current epoch:  1356\n",
      "2017-11-15T23:37:30.165580: step 24410, loss 0.00367209, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:37:30.687611: step 24415, loss 0.00129316, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:37:31.197527: step 24420, loss 0.00228618, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:37:31.710043: step 24425, loss 0.00212536, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1357\n",
      "2017-11-15T23:37:32.191395: step 24430, loss 0.00115095, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:37:32.711309: step 24435, loss 0.00138477, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:37:33.224223: step 24440, loss 0.00332557, acc 0.99707, f1 0.997071\n",
      "Current epoch:  1358\n",
      "2017-11-15T23:37:33.711108: step 24445, loss 0.00171468, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:37:34.222145: step 24450, loss 0.00481239, acc 0.996094, f1 0.996094\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:37:34.885857: step 24450, loss 2.28479, acc 0.58099, f1 0.580154\n",
      "\n",
      "2017-11-15T23:37:35.405412: step 24455, loss 0.0037702, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:37:35.934323: step 24460, loss 0.00213111, acc 0.999023, f1 0.999024\n",
      "Current epoch:  1359\n",
      "2017-11-15T23:37:36.445228: step 24465, loss 0.00119547, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:37:36.967265: step 24470, loss 0.0011016, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:37:37.487204: step 24475, loss 0.00402387, acc 0.998047, f1 0.998042\n",
      "2017-11-15T23:37:37.976582: step 24480, loss 0.00473756, acc 0.996855, f1 0.996855\n",
      "Current epoch:  1360\n",
      "2017-11-15T23:37:38.502028: step 24485, loss 9.41193e-05, acc 1, f1 1\n",
      "2017-11-15T23:37:39.019811: step 24490, loss 0.00119492, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:37:39.542255: step 24495, loss 0.00129256, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1361\n",
      "2017-11-15T23:37:40.039746: step 24500, loss 0.00196277, acc 0.998047, f1 0.998053\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:37:40.698974: step 24500, loss 2.28146, acc 0.578131, f1 0.577663\n",
      "\n",
      "2017-11-15T23:37:41.214383: step 24505, loss 0.00265775, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:37:41.764154: step 24510, loss 0.00161659, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:37:42.286221: step 24515, loss 0.00327118, acc 0.998047, f1 0.998048\n",
      "Current epoch:  1362\n",
      "2017-11-15T23:37:42.775168: step 24520, loss 0.000747866, acc 1, f1 1\n",
      "2017-11-15T23:37:43.304009: step 24525, loss 0.00359662, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:37:43.827091: step 24530, loss 0.0009968, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1363\n",
      "2017-11-15T23:37:44.313595: step 24535, loss 0.00396015, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:37:44.827513: step 24540, loss 0.00330444, acc 0.99707, f1 0.997066\n",
      "2017-11-15T23:37:45.346489: step 24545, loss 7.5156e-05, acc 1, f1 1\n",
      "2017-11-15T23:37:45.871456: step 24550, loss 0.00232791, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:37:46.525640: step 24550, loss 2.24683, acc 0.584335, f1 0.58256\n",
      "\n",
      "Current epoch:  1364\n",
      "2017-11-15T23:37:47.021525: step 24555, loss 0.00173373, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:37:47.555393: step 24560, loss 0.00115291, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:37:48.079562: step 24565, loss 0.000649707, acc 1, f1 1\n",
      "2017-11-15T23:37:48.573412: step 24570, loss 0.00335826, acc 0.996855, f1 0.996843\n",
      "Current epoch:  1365\n",
      "2017-11-15T23:37:49.098539: step 24575, loss 0.00133216, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:37:49.617487: step 24580, loss 0.00268246, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:37:50.132033: step 24585, loss 0.00164229, acc 0.999023, f1 0.999024\n",
      "Current epoch:  1366\n",
      "2017-11-15T23:37:50.622411: step 24590, loss 7.78548e-05, acc 1, f1 1\n",
      "2017-11-15T23:37:51.136342: step 24595, loss 0.000104427, acc 1, f1 1\n",
      "2017-11-15T23:37:51.654262: step 24600, loss 0.00145146, acc 0.999023, f1 0.999022\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:37:52.300891: step 24600, loss 2.27166, acc 0.573526, f1 0.575751\n",
      "\n",
      "2017-11-15T23:37:52.821873: step 24605, loss 0.0009737, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1367\n",
      "2017-11-15T23:37:53.301868: step 24610, loss 0.00157843, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:37:53.816848: step 24615, loss 0.00162603, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:37:54.331321: step 24620, loss 0.000100251, acc 1, f1 1\n",
      "Current epoch:  1368\n",
      "2017-11-15T23:37:54.819703: step 24625, loss 0.00262111, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:37:55.347832: step 24630, loss 0.00168676, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:37:55.870460: step 24635, loss 0.00103181, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:37:56.392936: step 24640, loss 0.00288961, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1369\n",
      "2017-11-15T23:37:56.884293: step 24645, loss 0.00399271, acc 0.996094, f1 0.996093\n",
      "2017-11-15T23:37:57.403522: step 24650, loss 0.00333776, acc 0.99707, f1 0.99707\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:37:58.068293: step 24650, loss 2.34295, acc 0.571975, f1 0.570162\n",
      "\n",
      "2017-11-15T23:37:58.589230: step 24655, loss 0.00356627, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:37:59.070106: step 24660, loss 0.00146693, acc 0.998428, f1 0.998428\n",
      "Current epoch:  1370\n",
      "2017-11-15T23:37:59.585146: step 24665, loss 0.00262731, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:38:00.102072: step 24670, loss 0.00467419, acc 0.99707, f1 0.997066\n",
      "2017-11-15T23:38:00.618969: step 24675, loss 9.58261e-05, acc 1, f1 1\n",
      "Current epoch:  1371\n",
      "2017-11-15T23:38:01.129234: step 24680, loss 0.00047947, acc 1, f1 1\n",
      "2017-11-15T23:38:01.646665: step 24685, loss 0.00116074, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:38:02.158347: step 24690, loss 0.00291095, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:38:02.673291: step 24695, loss 0.00126149, acc 0.999023, f1 0.999024\n",
      "Current epoch:  1372\n",
      "2017-11-15T23:38:03.162178: step 24700, loss 0.000824132, acc 0.999023, f1 0.999024\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:38:03.827045: step 24700, loss 2.25921, acc 0.597809, f1 0.592777\n",
      "\n",
      "2017-11-15T23:38:04.338688: step 24705, loss 0.00159014, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:38:04.848237: step 24710, loss 0.00381767, acc 0.99707, f1 0.997071\n",
      "Current epoch:  1373\n",
      "2017-11-15T23:38:05.326088: step 24715, loss 0.00217386, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:38:05.839676: step 24720, loss 0.00140296, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:38:06.360612: step 24725, loss 0.00260875, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:38:06.879562: step 24730, loss 0.0011576, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1374\n",
      "2017-11-15T23:38:07.363546: step 24735, loss 0.00237714, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:38:07.882035: step 24740, loss 0.00251998, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:38:08.401783: step 24745, loss 0.00398627, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:38:08.886257: step 24750, loss 0.000103769, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:38:09.539504: step 24750, loss 2.29854, acc 0.578616, f1 0.577375\n",
      "\n",
      "Current epoch:  1375\n",
      "2017-11-15T23:38:10.054481: step 24755, loss 0.00232908, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:38:10.564403: step 24760, loss 0.00291921, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:38:11.091259: step 24765, loss 0.00527559, acc 0.996094, f1 0.996094\n",
      "Current epoch:  1376\n",
      "2017-11-15T23:38:11.583429: step 24770, loss 0.00204734, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:38:12.096855: step 24775, loss 0.00292125, acc 0.998047, f1 0.998047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:38:12.613769: step 24780, loss 0.00277427, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:38:13.131686: step 24785, loss 7.43283e-05, acc 1, f1 1\n",
      "Current epoch:  1377\n",
      "2017-11-15T23:38:13.620257: step 24790, loss 0.006163, acc 0.994141, f1 0.994137\n",
      "2017-11-15T23:38:14.140694: step 24795, loss 0.00153298, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:38:14.750313: step 24800, loss 0.00036796, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:38:15.404245: step 24800, loss 2.26585, acc 0.590103, f1 0.586728\n",
      "\n",
      "Current epoch:  1378\n",
      "2017-11-15T23:38:15.900363: step 24805, loss 0.000761521, acc 1, f1 1\n",
      "2017-11-15T23:38:16.427232: step 24810, loss 0.000906542, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:38:16.948024: step 24815, loss 0.00123615, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:38:17.465092: step 24820, loss 0.00188335, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1379\n",
      "2017-11-15T23:38:17.947965: step 24825, loss 0.00180311, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:38:18.458934: step 24830, loss 0.00136635, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:38:18.967834: step 24835, loss 0.00402628, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:38:19.448615: step 24840, loss 0.00373626, acc 0.996855, f1 0.996855\n",
      "Current epoch:  1380\n",
      "2017-11-15T23:38:19.964506: step 24845, loss 0.002378, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:38:20.482671: step 24850, loss 0.00575273, acc 0.995117, f1 0.99512\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:38:21.122716: step 24850, loss 2.24199, acc 0.586904, f1 0.586055\n",
      "\n",
      "2017-11-15T23:38:21.651444: step 24855, loss 7.18073e-05, acc 1, f1 1\n",
      "Current epoch:  1381\n",
      "2017-11-15T23:38:22.145839: step 24860, loss 0.000417515, acc 1, f1 1\n",
      "2017-11-15T23:38:22.663768: step 24865, loss 0.00446413, acc 0.99707, f1 0.997072\n",
      "2017-11-15T23:38:23.173691: step 24870, loss 0.00269006, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:38:23.689726: step 24875, loss 0.00562775, acc 0.996094, f1 0.996094\n",
      "Current epoch:  1382\n",
      "2017-11-15T23:38:24.173603: step 24880, loss 0.00213256, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:38:24.685520: step 24885, loss 0.0013705, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:38:25.199038: step 24890, loss 0.00110615, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1383\n",
      "2017-11-15T23:38:25.699765: step 24895, loss 0.00242027, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:38:26.222202: step 24900, loss 0.00157482, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:38:26.888231: step 24900, loss 2.28475, acc 0.577113, f1 0.577632\n",
      "\n",
      "2017-11-15T23:38:27.408121: step 24905, loss 0.0009533, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:38:27.935899: step 24910, loss 0.00134449, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1384\n",
      "2017-11-15T23:38:28.430899: step 24915, loss 0.00436164, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:38:28.950888: step 24920, loss 0.00160722, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:38:29.468401: step 24925, loss 0.00317451, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:38:29.946768: step 24930, loss 0.00390204, acc 0.996855, f1 0.996858\n",
      "Current epoch:  1385\n",
      "2017-11-15T23:38:30.462623: step 24935, loss 0.00354951, acc 0.998047, f1 0.998052\n",
      "2017-11-15T23:38:31.013626: step 24940, loss 0.00278519, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:38:31.530357: step 24945, loss 8.90045e-05, acc 1, f1 1\n",
      "Current epoch:  1386\n",
      "2017-11-15T23:38:32.018242: step 24950, loss 0.00172089, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:38:32.651152: step 24950, loss 2.28927, acc 0.578373, f1 0.577431\n",
      "\n",
      "2017-11-15T23:38:33.156339: step 24955, loss 0.00393733, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:38:33.669267: step 24960, loss 0.0043673, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:38:34.181194: step 24965, loss 0.00120813, acc 0.999023, f1 0.999024\n",
      "Current epoch:  1387\n",
      "2017-11-15T23:38:34.667819: step 24970, loss 0.00295387, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:38:35.192731: step 24975, loss 0.00285385, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:38:35.716710: step 24980, loss 0.00354127, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1388\n",
      "2017-11-15T23:38:36.214882: step 24985, loss 0.00204187, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:38:36.759240: step 24990, loss 0.00236578, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:38:37.277657: step 24995, loss 0.00230271, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:38:37.800085: step 25000, loss 7.46839e-05, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:38:38.454312: step 25000, loss 2.26523, acc 0.584529, f1 0.58312\n",
      "\n",
      "Current epoch:  1389\n",
      "2017-11-15T23:38:38.946059: step 25005, loss 0.00181145, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:38:39.465817: step 25010, loss 0.00290703, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:38:39.987751: step 25015, loss 0.00462484, acc 0.996094, f1 0.996093\n",
      "2017-11-15T23:38:40.476620: step 25020, loss 0.00357685, acc 0.996855, f1 0.99687\n",
      "Current epoch:  1390\n",
      "2017-11-15T23:38:40.989536: step 25025, loss 0.00175026, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:38:41.513480: step 25030, loss 0.000515966, acc 1, f1 1\n",
      "2017-11-15T23:38:42.046165: step 25035, loss 0.0106743, acc 0.993164, f1 0.993158\n",
      "Current epoch:  1391\n",
      "2017-11-15T23:38:42.551075: step 25040, loss 0.00138574, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:38:43.075029: step 25045, loss 0.00388825, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:38:43.597665: step 25050, loss 0.00193584, acc 0.998047, f1 0.998051\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:38:44.254484: step 25050, loss 2.25883, acc 0.588745, f1 0.585565\n",
      "\n",
      "2017-11-15T23:38:44.773206: step 25055, loss 0.00164637, acc 0.999023, f1 0.999024\n",
      "Current epoch:  1392\n",
      "2017-11-15T23:38:45.270192: step 25060, loss 0.00293381, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:38:45.790126: step 25065, loss 0.00130799, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:38:46.311035: step 25070, loss 0.00321896, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1393\n",
      "2017-11-15T23:38:46.803909: step 25075, loss 0.00325943, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:38:47.327598: step 25080, loss 0.00296564, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:38:47.865628: step 25085, loss 0.00249328, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:38:48.389059: step 25090, loss 0.00283482, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1394\n",
      "2017-11-15T23:38:48.884313: step 25095, loss 0.00209603, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:38:49.404381: step 25100, loss 0.00176359, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:38:50.064929: step 25100, loss 2.25379, acc 0.580457, f1 0.583358\n",
      "\n",
      "2017-11-15T23:38:50.581881: step 25105, loss 0.00103673, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:38:51.073759: step 25110, loss 8.62508e-05, acc 1, f1 1\n",
      "Current epoch:  1395\n",
      "2017-11-15T23:38:51.600514: step 25115, loss 0.00117165, acc 1, f1 1\n",
      "2017-11-15T23:38:52.122142: step 25120, loss 0.00542654, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:38:52.651358: step 25125, loss 0.00109407, acc 0.999023, f1 0.999025\n",
      "Current epoch:  1396\n",
      "2017-11-15T23:38:53.150912: step 25130, loss 0.00162013, acc 1, f1 1\n",
      "2017-11-15T23:38:53.672264: step 25135, loss 0.00165817, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:38:54.184187: step 25140, loss 0.00236873, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:38:54.700810: step 25145, loss 0.00493614, acc 0.996094, f1 0.996096\n",
      "Current epoch:  1397\n",
      "2017-11-15T23:38:55.185151: step 25150, loss 0.00431982, acc 0.99707, f1 0.997068\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:38:55.815986: step 25150, loss 2.31809, acc 0.574883, f1 0.575325\n",
      "\n",
      "2017-11-15T23:38:56.333467: step 25155, loss 0.00150522, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:38:56.860376: step 25160, loss 8.76277e-05, acc 1, f1 1\n",
      "Current epoch:  1398\n",
      "2017-11-15T23:38:57.346134: step 25165, loss 0.000854895, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:38:57.857053: step 25170, loss 0.000985344, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:38:58.372972: step 25175, loss 0.00309661, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:38:58.895635: step 25180, loss 0.00203034, acc 0.998047, f1 0.998046\n",
      "Current epoch:  1399\n",
      "2017-11-15T23:38:59.384595: step 25185, loss 0.00324076, acc 0.99707, f1 0.997074\n",
      "2017-11-15T23:38:59.893650: step 25190, loss 0.00437287, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:39:00.407596: step 25195, loss 0.00314272, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:39:00.888473: step 25200, loss 0.00280154, acc 0.996855, f1 0.996843\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:39:01.520289: step 25200, loss 2.42154, acc 0.557386, f1 0.559732\n",
      "\n",
      "Current epoch:  1400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:39:02.035730: step 25205, loss 0.00312672, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:39:02.547456: step 25210, loss 0.000583455, acc 1, f1 1\n",
      "2017-11-15T23:39:03.062377: step 25215, loss 0.00246281, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1401\n",
      "2017-11-15T23:39:03.550158: step 25220, loss 0.00174825, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:39:04.069882: step 25225, loss 0.0041446, acc 0.996094, f1 0.996093\n",
      "2017-11-15T23:39:04.588587: step 25230, loss 0.00436366, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:39:05.104495: step 25235, loss 0.00184401, acc 0.998047, f1 0.998046\n",
      "Current epoch:  1402\n",
      "2017-11-15T23:39:05.591120: step 25240, loss 0.000444915, acc 1, f1 1\n",
      "2017-11-15T23:39:06.105035: step 25245, loss 0.00287351, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:39:06.620005: step 25250, loss 0.00229966, acc 0.998047, f1 0.998048\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:39:07.257151: step 25250, loss 2.24777, acc 0.592526, f1 0.588649\n",
      "\n",
      "Current epoch:  1403\n",
      "2017-11-15T23:39:07.741541: step 25255, loss 9.6595e-05, acc 1, f1 1\n",
      "2017-11-15T23:39:08.255469: step 25260, loss 0.00260029, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:39:08.868400: step 25265, loss 0.000799017, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:39:09.386905: step 25270, loss 0.00276634, acc 0.99707, f1 0.997071\n",
      "Current epoch:  1404\n",
      "2017-11-15T23:39:09.884460: step 25275, loss 0.00149365, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:39:10.400909: step 25280, loss 0.000924245, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:39:10.915457: step 25285, loss 0.00286792, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:39:11.403053: step 25290, loss 0.000112793, acc 1, f1 1\n",
      "Current epoch:  1405\n",
      "2017-11-15T23:39:11.927781: step 25295, loss 0.000944446, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:39:12.443701: step 25300, loss 0.0018504, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:39:13.075800: step 25300, loss 2.34736, acc 0.579343, f1 0.575364\n",
      "\n",
      "2017-11-15T23:39:13.587737: step 25305, loss 0.00493733, acc 0.996094, f1 0.996083\n",
      "Current epoch:  1406\n",
      "2017-11-15T23:39:14.077598: step 25310, loss 0.00167111, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:39:14.589817: step 25315, loss 0.00453559, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:39:15.116252: step 25320, loss 0.00670467, acc 0.995117, f1 0.995109\n",
      "2017-11-15T23:39:15.635025: step 25325, loss 0.00285719, acc 0.998047, f1 0.998048\n",
      "Current epoch:  1407\n",
      "2017-11-15T23:39:16.113900: step 25330, loss 0.000924574, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:39:16.629840: step 25335, loss 0.00384108, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:39:17.143580: step 25340, loss 0.00472007, acc 0.996094, f1 0.996095\n",
      "Current epoch:  1408\n",
      "2017-11-15T23:39:17.634977: step 25345, loss 8.66547e-05, acc 1, f1 1\n",
      "2017-11-15T23:39:18.149400: step 25350, loss 0.000900809, acc 0.999023, f1 0.999024\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:39:18.797267: step 25350, loss 2.31556, acc 0.577258, f1 0.57642\n",
      "\n",
      "2017-11-15T23:39:19.312882: step 25355, loss 0.00498606, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:39:19.835752: step 25360, loss 0.00571918, acc 0.995117, f1 0.995116\n",
      "Current epoch:  1409\n",
      "2017-11-15T23:39:20.327727: step 25365, loss 0.000978899, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:39:20.863243: step 25370, loss 0.00245467, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:39:21.378557: step 25375, loss 0.00240203, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:39:21.861458: step 25380, loss 0.0047546, acc 0.996855, f1 0.996855\n",
      "Current epoch:  1410\n",
      "2017-11-15T23:39:22.376383: step 25385, loss 0.00170964, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:39:22.890915: step 25390, loss 0.000518605, acc 1, f1 1\n",
      "2017-11-15T23:39:23.410127: step 25395, loss 0.00277954, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1411\n",
      "2017-11-15T23:39:23.896065: step 25400, loss 9.29953e-05, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:39:24.546112: step 25400, loss 2.30279, acc 0.580942, f1 0.580553\n",
      "\n",
      "2017-11-15T23:39:25.058936: step 25405, loss 0.000809031, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:39:25.573747: step 25410, loss 0.00202856, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:39:26.106226: step 25415, loss 0.00438581, acc 0.998047, f1 0.998043\n",
      "Current epoch:  1412\n",
      "2017-11-15T23:39:26.600125: step 25420, loss 0.00359396, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:39:27.110442: step 25425, loss 0.00176569, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:39:27.626092: step 25430, loss 0.00149407, acc 0.999023, f1 0.999022\n",
      "Current epoch:  1413\n",
      "2017-11-15T23:39:28.115952: step 25435, loss 0.00102425, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:39:28.639881: step 25440, loss 0.0043436, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:39:29.151815: step 25445, loss 0.00249018, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:39:29.664738: step 25450, loss 0.0019345, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:39:30.298040: step 25450, loss 2.23939, acc 0.59049, f1 0.59063\n",
      "\n",
      "Current epoch:  1414\n",
      "2017-11-15T23:39:30.783924: step 25455, loss 0.00241162, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:39:31.295351: step 25460, loss 0.0043968, acc 0.995117, f1 0.995118\n",
      "2017-11-15T23:39:31.825828: step 25465, loss 0.000121341, acc 1, f1 1\n",
      "2017-11-15T23:39:32.313720: step 25470, loss 0.00433137, acc 0.995283, f1 0.99528\n",
      "Current epoch:  1415\n",
      "2017-11-15T23:39:32.830154: step 25475, loss 0.00132784, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:39:33.343824: step 25480, loss 0.00595226, acc 0.996094, f1 0.996093\n",
      "2017-11-15T23:39:33.855731: step 25485, loss 0.00221124, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1416\n",
      "2017-11-15T23:39:34.338591: step 25490, loss 0.00177018, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:39:34.857513: step 25495, loss 0.00288839, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:39:35.386472: step 25500, loss 8.53149e-05, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:39:36.042181: step 25500, loss 2.26428, acc 0.581475, f1 0.582158\n",
      "\n",
      "2017-11-15T23:39:36.561100: step 25505, loss 0.00432016, acc 0.996094, f1 0.996095\n",
      "Current epoch:  1417\n",
      "2017-11-15T23:39:37.063084: step 25510, loss 0.00433686, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:39:37.583942: step 25515, loss 0.00261947, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:39:38.105305: step 25520, loss 0.00234389, acc 0.998047, f1 0.998051\n",
      "Current epoch:  1418\n",
      "2017-11-15T23:39:38.607868: step 25525, loss 0.00265152, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:39:39.126701: step 25530, loss 0.00121178, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:39:39.656015: step 25535, loss 0.000687942, acc 1, f1 1\n",
      "2017-11-15T23:39:40.174789: step 25540, loss 0.00443804, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1419\n",
      "2017-11-15T23:39:40.659664: step 25545, loss 0.00251402, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:39:41.174328: step 25550, loss 0.000369393, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:39:41.821831: step 25550, loss 2.31439, acc 0.578034, f1 0.577435\n",
      "\n",
      "2017-11-15T23:39:42.348364: step 25555, loss 0.00295843, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:39:42.840070: step 25560, loss 8.52533e-05, acc 1, f1 1\n",
      "Current epoch:  1420\n",
      "2017-11-15T23:39:43.352056: step 25565, loss 0.00280226, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:39:43.862963: step 25570, loss 0.00486375, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:39:44.372476: step 25575, loss 0.00392121, acc 0.99707, f1 0.997075\n",
      "Current epoch:  1421\n",
      "2017-11-15T23:39:44.856855: step 25580, loss 0.00160507, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:39:45.366752: step 25585, loss 0.00317027, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:39:45.876654: step 25590, loss 0.00283754, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:39:46.391088: step 25595, loss 0.00332894, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1422\n",
      "2017-11-15T23:39:46.874443: step 25600, loss 0.00287515, acc 0.999023, f1 0.999025\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:39:47.507231: step 25600, loss 2.24984, acc 0.58637, f1 0.585352\n",
      "\n",
      "2017-11-15T23:39:48.018651: step 25605, loss 0.00384548, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:39:48.544891: step 25610, loss 0.00511285, acc 0.995117, f1 0.995115\n",
      "Current epoch:  1423\n",
      "2017-11-15T23:39:49.032267: step 25615, loss 6.46507e-05, acc 1, f1 1\n",
      "2017-11-15T23:39:49.546202: step 25620, loss 0.00378573, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:39:50.060111: step 25625, loss 0.00262504, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:39:50.570037: step 25630, loss 0.00210642, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:39:51.061927: step 25635, loss 0.00315742, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:39:51.583710: step 25640, loss 0.00276324, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:39:52.096657: step 25645, loss 0.00520758, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:39:52.582562: step 25650, loss 0.001371, acc 0.998428, f1 0.998428\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:39:53.225318: step 25650, loss 2.2491, acc 0.590393, f1 0.588149\n",
      "\n",
      "Current epoch:  1425\n",
      "2017-11-15T23:39:53.761432: step 25655, loss 0.00495785, acc 0.995117, f1 0.995126\n",
      "2017-11-15T23:39:54.274329: step 25660, loss 0.000500515, acc 1, f1 1\n",
      "2017-11-15T23:39:54.788093: step 25665, loss 0.00294257, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1426\n",
      "2017-11-15T23:39:55.269976: step 25670, loss 0.00174939, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:39:55.783560: step 25675, loss 7.16673e-05, acc 1, f1 1\n",
      "2017-11-15T23:39:56.299602: step 25680, loss 0.00167164, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:39:56.810603: step 25685, loss 0.00157341, acc 0.999023, f1 0.999024\n",
      "Current epoch:  1427\n",
      "2017-11-15T23:39:57.302395: step 25690, loss 0.00275013, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:39:57.820340: step 25695, loss 0.00342565, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:39:58.337623: step 25700, loss 0.00352241, acc 0.99707, f1 0.997071\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:39:58.974263: step 25700, loss 2.2935, acc 0.581572, f1 0.580224\n",
      "\n",
      "Current epoch:  1428\n",
      "2017-11-15T23:39:59.468210: step 25705, loss 0.00277918, acc 0.99707, f1 0.997072\n",
      "2017-11-15T23:39:59.978830: step 25710, loss 0.00152407, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:40:00.500825: step 25715, loss 0.00461739, acc 0.99707, f1 0.997074\n",
      "2017-11-15T23:40:01.016753: step 25720, loss 0.00422507, acc 0.996094, f1 0.996093\n",
      "Current epoch:  1429\n",
      "2017-11-15T23:40:01.500627: step 25725, loss 0.000779348, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:40:02.019570: step 25730, loss 0.0029128, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:40:02.532413: step 25735, loss 0.00226954, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:40:03.012295: step 25740, loss 0.00656196, acc 0.995283, f1 0.995288\n",
      "Current epoch:  1430\n",
      "2017-11-15T23:40:03.526562: step 25745, loss 0.00206533, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:40:04.042079: step 25750, loss 7.47366e-05, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:40:04.683746: step 25750, loss 2.26714, acc 0.583559, f1 0.582074\n",
      "\n",
      "2017-11-15T23:40:05.192657: step 25755, loss 0.00866172, acc 0.994141, f1 0.994137\n",
      "Current epoch:  1431\n",
      "2017-11-15T23:40:05.679533: step 25760, loss 0.000427362, acc 1, f1 1\n",
      "2017-11-15T23:40:06.193445: step 25765, loss 0.00313088, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:40:06.709894: step 25770, loss 0.00494603, acc 0.995117, f1 0.995118\n",
      "2017-11-15T23:40:07.223817: step 25775, loss 0.00377667, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1432\n",
      "2017-11-15T23:40:07.720809: step 25780, loss 0.000681783, acc 1, f1 1\n",
      "2017-11-15T23:40:08.236960: step 25785, loss 0.00462376, acc 0.996094, f1 0.996093\n",
      "2017-11-15T23:40:08.748885: step 25790, loss 0.00166616, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1433\n",
      "2017-11-15T23:40:09.235251: step 25795, loss 0.00217692, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:40:09.756621: step 25800, loss 0.00202539, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:40:10.427487: step 25800, loss 2.28833, acc 0.601977, f1 0.595\n",
      "\n",
      "2017-11-15T23:40:10.944996: step 25805, loss 0.00125831, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:40:11.455884: step 25810, loss 0.00354335, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1434\n",
      "2017-11-15T23:40:11.945757: step 25815, loss 0.00137498, acc 1, f1 1\n",
      "2017-11-15T23:40:12.464902: step 25820, loss 0.00310882, acc 0.99707, f1 0.997072\n",
      "2017-11-15T23:40:12.990369: step 25825, loss 0.0035013, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:40:13.476741: step 25830, loss 0.00332136, acc 0.996855, f1 0.996855\n",
      "Current epoch:  1435\n",
      "2017-11-15T23:40:13.995647: step 25835, loss 0.0032681, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:40:14.581407: step 25840, loss 0.0025904, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:40:15.100271: step 25845, loss 0.000600077, acc 1, f1 1\n",
      "Current epoch:  1436\n",
      "2017-11-15T23:40:15.601167: step 25850, loss 0.0035264, acc 0.99707, f1 0.997071\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:40:16.278572: step 25850, loss 2.25026, acc 0.584383, f1 0.584285\n",
      "\n",
      "2017-11-15T23:40:16.792157: step 25855, loss 0.00390168, acc 0.99707, f1 0.997072\n",
      "2017-11-15T23:40:17.312607: step 25860, loss 0.00184236, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:40:17.835373: step 25865, loss 0.00174388, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1437\n",
      "2017-11-15T23:40:18.329249: step 25870, loss 0.00322086, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:40:18.856178: step 25875, loss 0.00158254, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:40:19.377121: step 25880, loss 0.00220529, acc 0.998047, f1 0.998043\n",
      "Current epoch:  1438\n",
      "2017-11-15T23:40:19.866999: step 25885, loss 0.00149691, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:40:20.393937: step 25890, loss 0.00182367, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:40:20.914863: step 25895, loss 0.00325934, acc 0.998047, f1 0.998041\n",
      "2017-11-15T23:40:21.443725: step 25900, loss 0.00372858, acc 0.99707, f1 0.997073\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:40:22.081638: step 25900, loss 2.25987, acc 0.579246, f1 0.579747\n",
      "\n",
      "Current epoch:  1439\n",
      "2017-11-15T23:40:22.564494: step 25905, loss 9.00998e-05, acc 1, f1 1\n",
      "2017-11-15T23:40:23.073419: step 25910, loss 0.00441552, acc 0.996094, f1 0.996091\n",
      "2017-11-15T23:40:23.584347: step 25915, loss 0.00254813, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:40:24.064219: step 25920, loss 0.0036567, acc 0.996855, f1 0.996857\n",
      "Current epoch:  1440\n",
      "2017-11-15T23:40:24.579140: step 25925, loss 0.00118122, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:40:25.091488: step 25930, loss 0.00562196, acc 0.995117, f1 0.995117\n",
      "2017-11-15T23:40:25.608653: step 25935, loss 0.0049986, acc 0.996094, f1 0.996096\n",
      "Current epoch:  1441\n",
      "2017-11-15T23:40:26.091043: step 25940, loss 0.00221206, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:40:26.618048: step 25945, loss 0.00254855, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:40:27.144648: step 25950, loss 0.000613855, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:40:27.809044: step 25950, loss 2.27447, acc 0.589036, f1 0.586358\n",
      "\n",
      "2017-11-15T23:40:28.331878: step 25955, loss 0.00233315, acc 0.99707, f1 0.997071\n",
      "Current epoch:  1442\n",
      "2017-11-15T23:40:28.837192: step 25960, loss 0.000804224, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:40:29.363751: step 25965, loss 0.00209164, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:40:29.889265: step 25970, loss 7.52058e-05, acc 1, f1 1\n",
      "Current epoch:  1443\n",
      "2017-11-15T23:40:30.383115: step 25975, loss 0.00195912, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:40:30.903078: step 25980, loss 0.00293759, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:40:31.418642: step 25985, loss 0.00071537, acc 1, f1 1\n",
      "2017-11-15T23:40:31.940178: step 25990, loss 0.00325973, acc 0.998047, f1 0.998046\n",
      "Current epoch:  1444\n",
      "2017-11-15T23:40:32.435053: step 25995, loss 0.000692005, acc 1, f1 1\n",
      "2017-11-15T23:40:32.971313: step 26000, loss 0.00267343, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:40:33.629760: step 26000, loss 2.34257, acc 0.568825, f1 0.569883\n",
      "\n",
      "2017-11-15T23:40:34.146701: step 26005, loss 0.00114342, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:40:34.631054: step 26010, loss 0.00382075, acc 0.996855, f1 0.996855\n",
      "Current epoch:  1445\n",
      "2017-11-15T23:40:35.154008: step 26015, loss 0.00135147, acc 1, f1 1\n",
      "2017-11-15T23:40:35.693630: step 26020, loss 0.00400167, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:40:36.221574: step 26025, loss 0.00347405, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1446\n",
      "2017-11-15T23:40:36.725521: step 26030, loss 0.00369419, acc 0.99707, f1 0.997074\n",
      "2017-11-15T23:40:37.236410: step 26035, loss 0.00131272, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:40:37.765122: step 26040, loss 0.00285826, acc 0.99707, f1 0.997068\n",
      "2017-11-15T23:40:38.289313: step 26045, loss 0.0046782, acc 0.996094, f1 0.996095\n",
      "Current epoch:  1447\n",
      "2017-11-15T23:40:38.770092: step 26050, loss 0.0026637, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:40:39.412557: step 26050, loss 2.25333, acc 0.584868, f1 0.584703\n",
      "\n",
      "2017-11-15T23:40:39.922975: step 26055, loss 0.00264113, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:40:40.440373: step 26060, loss 0.00144784, acc 0.999023, f1 0.999023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch:  1448\n",
      "2017-11-15T23:40:40.923713: step 26065, loss 0.00240314, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:40:41.445133: step 26070, loss 0.000808313, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:40:41.959581: step 26075, loss 0.00741278, acc 0.995117, f1 0.995116\n",
      "2017-11-15T23:40:42.470937: step 26080, loss 0.00288749, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1449\n",
      "2017-11-15T23:40:42.973645: step 26085, loss 0.00123072, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:40:43.497523: step 26090, loss 0.00294011, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:40:44.009180: step 26095, loss 7.7083e-05, acc 1, f1 1\n",
      "2017-11-15T23:40:44.493695: step 26100, loss 0.00429178, acc 0.996855, f1 0.996858\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:40:45.122021: step 26100, loss 2.31649, acc 0.601832, f1 0.593006\n",
      "\n",
      "Current epoch:  1450\n",
      "2017-11-15T23:40:45.631934: step 26105, loss 0.00434168, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:40:46.147857: step 26110, loss 0.00132433, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:40:46.664055: step 26115, loss 0.00450463, acc 0.996094, f1 0.996094\n",
      "Current epoch:  1451\n",
      "2017-11-15T23:40:47.154934: step 26120, loss 0.000622896, acc 1, f1 1\n",
      "2017-11-15T23:40:47.668890: step 26125, loss 0.00227439, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:40:48.185698: step 26130, loss 0.00481291, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:40:48.725075: step 26135, loss 0.00245973, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1452\n",
      "2017-11-15T23:40:49.218555: step 26140, loss 0.00344574, acc 0.99707, f1 0.997066\n",
      "2017-11-15T23:40:49.744290: step 26145, loss 0.00206665, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:40:50.257707: step 26150, loss 0.00193084, acc 0.999023, f1 0.999022\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:40:50.898486: step 26150, loss 2.25217, acc 0.585595, f1 0.58705\n",
      "\n",
      "Current epoch:  1453\n",
      "2017-11-15T23:40:51.384585: step 26155, loss 7.23044e-05, acc 1, f1 1\n",
      "2017-11-15T23:40:51.895489: step 26160, loss 0.00167472, acc 1, f1 1\n",
      "2017-11-15T23:40:52.408272: step 26165, loss 0.00367332, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:40:52.925241: step 26170, loss 0.00296886, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1454\n",
      "2017-11-15T23:40:53.421840: step 26175, loss 0.00107427, acc 1, f1 1\n",
      "2017-11-15T23:40:53.942884: step 26180, loss 0.0015286, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:40:54.470294: step 26185, loss 0.0034254, acc 0.99707, f1 0.997067\n",
      "2017-11-15T23:40:54.956782: step 26190, loss 0.00124306, acc 0.998428, f1 0.998427\n",
      "Current epoch:  1455\n",
      "2017-11-15T23:40:55.479390: step 26195, loss 0.00365633, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:40:56.004009: step 26200, loss 0.00188225, acc 0.998047, f1 0.998048\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:40:56.662309: step 26200, loss 2.34448, acc 0.576338, f1 0.574541\n",
      "\n",
      "2017-11-15T23:40:57.226507: step 26205, loss 0.00096368, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1456\n",
      "2017-11-15T23:40:57.713362: step 26210, loss 0.00277871, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:40:58.231361: step 26215, loss 0.000119683, acc 1, f1 1\n",
      "2017-11-15T23:40:58.757431: step 26220, loss 0.00208545, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:40:59.282424: step 26225, loss 0.00213201, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1457\n",
      "2017-11-15T23:40:59.789955: step 26230, loss 0.00150113, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:41:00.311921: step 26235, loss 0.00231372, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:41:00.833165: step 26240, loss 0.00438704, acc 0.996094, f1 0.996094\n",
      "Current epoch:  1458\n",
      "2017-11-15T23:41:01.320529: step 26245, loss 0.00324385, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:41:01.842847: step 26250, loss 0.00247611, acc 0.998047, f1 0.998046\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:41:02.485111: step 26250, loss 2.26868, acc 0.582929, f1 0.584993\n",
      "\n",
      "2017-11-15T23:41:03.004033: step 26255, loss 0.00261936, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:41:03.518948: step 26260, loss 0.00209007, acc 0.998047, f1 0.998046\n",
      "Current epoch:  1459\n",
      "2017-11-15T23:41:04.004824: step 26265, loss 0.00106852, acc 1, f1 1\n",
      "2017-11-15T23:41:04.518460: step 26270, loss 0.0015298, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:41:05.031394: step 26275, loss 0.00489811, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:41:05.519255: step 26280, loss 0.00155617, acc 0.998428, f1 0.998428\n",
      "Current epoch:  1460\n",
      "2017-11-15T23:41:06.032177: step 26285, loss 0.00169168, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:41:06.544076: step 26290, loss 0.00291804, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:41:07.059011: step 26295, loss 5.71182e-05, acc 1, f1 1\n",
      "Current epoch:  1461\n",
      "2017-11-15T23:41:07.544892: step 26300, loss 0.000107027, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:41:08.175731: step 26300, loss 2.25333, acc 0.593059, f1 0.590056\n",
      "\n",
      "2017-11-15T23:41:08.698436: step 26305, loss 0.00490847, acc 0.996094, f1 0.996095\n",
      "2017-11-15T23:41:09.219386: step 26310, loss 0.000107782, acc 1, f1 1\n",
      "2017-11-15T23:41:09.745135: step 26315, loss 0.00204798, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1462\n",
      "2017-11-15T23:41:10.235991: step 26320, loss 7.40466e-05, acc 1, f1 1\n",
      "2017-11-15T23:41:10.764382: step 26325, loss 0.00193017, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:41:11.288368: step 26330, loss 0.00160078, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1463\n",
      "2017-11-15T23:41:11.786265: step 26335, loss 0.000689813, acc 1, f1 1\n",
      "2017-11-15T23:41:12.301682: step 26340, loss 0.00317475, acc 0.99707, f1 0.997067\n",
      "2017-11-15T23:41:12.821829: step 26345, loss 0.00177713, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:41:13.348806: step 26350, loss 0.00193682, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:41:14.003974: step 26350, loss 2.45357, acc 0.567565, f1 0.563662\n",
      "\n",
      "Current epoch:  1464\n",
      "2017-11-15T23:41:14.487216: step 26355, loss 0.00176619, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:41:15.010866: step 26360, loss 0.0036085, acc 0.99707, f1 0.997067\n",
      "2017-11-15T23:41:15.534573: step 26365, loss 0.00251549, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:41:16.034036: step 26370, loss 0.00270476, acc 0.998428, f1 0.998428\n",
      "Current epoch:  1465\n",
      "2017-11-15T23:41:16.563410: step 26375, loss 0.000680304, acc 1, f1 1\n",
      "2017-11-15T23:41:17.090579: step 26380, loss 0.00244351, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:41:17.612728: step 26385, loss 0.00722315, acc 0.995117, f1 0.995118\n",
      "Current epoch:  1466\n",
      "2017-11-15T23:41:18.107298: step 26390, loss 0.00141958, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:41:18.632727: step 26395, loss 0.00253993, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:41:19.151852: step 26400, loss 0.00368219, acc 0.99707, f1 0.997081\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:41:19.807215: step 26400, loss 2.34817, acc 0.581281, f1 0.576723\n",
      "\n",
      "2017-11-15T23:41:20.326341: step 26405, loss 0.00415458, acc 0.996094, f1 0.996094\n",
      "Current epoch:  1467\n",
      "2017-11-15T23:41:20.827044: step 26410, loss 0.00149799, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:41:21.350412: step 26415, loss 0.00538215, acc 0.996094, f1 0.996095\n",
      "2017-11-15T23:41:21.881443: step 26420, loss 0.00150045, acc 0.999023, f1 0.999025\n",
      "Current epoch:  1468\n",
      "2017-11-15T23:41:22.369281: step 26425, loss 0.00158607, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:41:22.884184: step 26430, loss 0.00363159, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:41:23.396841: step 26435, loss 0.000921442, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:41:23.911759: step 26440, loss 0.00262897, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1469\n",
      "2017-11-15T23:41:24.394615: step 26445, loss 0.00255343, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:41:24.903384: step 26450, loss 0.00444253, acc 0.996094, f1 0.996092\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:41:25.548898: step 26450, loss 2.30831, acc 0.577695, f1 0.577891\n",
      "\n",
      "2017-11-15T23:41:26.063823: step 26455, loss 0.00220208, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:41:26.549167: step 26460, loss 0.00186174, acc 0.998428, f1 0.998428\n",
      "Current epoch:  1470\n",
      "2017-11-15T23:41:27.072627: step 26465, loss 0.00458095, acc 0.99707, f1 0.99708\n",
      "2017-11-15T23:41:27.601340: step 26470, loss 0.00320556, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:41:28.126461: step 26475, loss 9.04511e-05, acc 1, f1 1\n",
      "Current epoch:  1471\n",
      "2017-11-15T23:41:28.621989: step 26480, loss 0.0029753, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:41:29.140578: step 26485, loss 0.00122921, acc 1, f1 1\n",
      "2017-11-15T23:41:29.669621: step 26490, loss 0.00149143, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:41:30.193052: step 26495, loss 0.00444664, acc 0.996094, f1 0.996095\n",
      "Current epoch:  1472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:41:30.682959: step 26500, loss 0.00374304, acc 0.99707, f1 0.997066\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:41:31.338477: step 26500, loss 2.31657, acc 0.577016, f1 0.576737\n",
      "\n",
      "2017-11-15T23:41:31.861277: step 26505, loss 0.000927757, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:41:32.390264: step 26510, loss 0.000929787, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1473\n",
      "2017-11-15T23:41:32.901649: step 26515, loss 0.000851172, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:41:33.427083: step 26520, loss 7.09339e-05, acc 1, f1 1\n",
      "2017-11-15T23:41:33.949570: step 26525, loss 0.00224499, acc 0.998047, f1 0.99805\n",
      "2017-11-15T23:41:34.477411: step 26530, loss 0.00253828, acc 0.998047, f1 0.998048\n",
      "Current epoch:  1474\n",
      "2017-11-15T23:41:34.969289: step 26535, loss 0.00143672, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:41:35.495678: step 26540, loss 0.000760222, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:41:36.016855: step 26545, loss 0.00198577, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:41:36.506616: step 26550, loss 0.00629221, acc 0.995283, f1 0.99528\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:41:37.162286: step 26550, loss 2.24174, acc 0.588261, f1 0.589208\n",
      "\n",
      "Current epoch:  1475\n",
      "2017-11-15T23:41:37.674382: step 26555, loss 0.00207586, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:41:38.208406: step 26560, loss 7.71637e-05, acc 1, f1 1\n",
      "2017-11-15T23:41:38.745883: step 26565, loss 0.00269215, acc 0.998047, f1 0.998052\n",
      "Current epoch:  1476\n",
      "2017-11-15T23:41:39.239422: step 26570, loss 8.79541e-05, acc 1, f1 1\n",
      "2017-11-15T23:41:39.762799: step 26575, loss 0.00301572, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:41:40.284987: step 26580, loss 0.00439261, acc 0.996094, f1 0.996096\n",
      "2017-11-15T23:41:40.807905: step 26585, loss 0.00204081, acc 0.998047, f1 0.998046\n",
      "Current epoch:  1477\n",
      "2017-11-15T23:41:41.297334: step 26590, loss 0.000993843, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:41:41.821908: step 26595, loss 0.00264467, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:41:42.452067: step 26600, loss 0.00235655, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:41:43.105961: step 26600, loss 2.48442, acc 0.55506, f1 0.553125\n",
      "\n",
      "Current epoch:  1478\n",
      "2017-11-15T23:41:43.604737: step 26605, loss 0.00177917, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:41:44.129132: step 26610, loss 0.0029789, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:41:44.660067: step 26615, loss 0.00114853, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:41:45.186096: step 26620, loss 0.00682704, acc 0.995117, f1 0.995118\n",
      "Current epoch:  1479\n",
      "2017-11-15T23:41:45.679995: step 26625, loss 0.00156667, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:41:46.203423: step 26630, loss 0.00202088, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:41:46.727003: step 26635, loss 0.00269125, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:41:47.223358: step 26640, loss 0.00191863, acc 0.998428, f1 0.998428\n",
      "Current epoch:  1480\n",
      "2017-11-15T23:41:47.756407: step 26645, loss 0.00241568, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:41:48.274339: step 26650, loss 0.00238771, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:41:48.934092: step 26650, loss 2.24952, acc 0.587001, f1 0.586711\n",
      "\n",
      "2017-11-15T23:41:49.464538: step 26655, loss 0.00298047, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1481\n",
      "2017-11-15T23:41:49.958519: step 26660, loss 0.00222491, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:41:50.484195: step 26665, loss 0.00385668, acc 0.996094, f1 0.996104\n",
      "2017-11-15T23:41:51.005634: step 26670, loss 0.00377382, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:41:51.532574: step 26675, loss 0.00137655, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1482\n",
      "2017-11-15T23:41:52.025465: step 26680, loss 0.00454969, acc 0.995117, f1 0.995116\n",
      "2017-11-15T23:41:52.550083: step 26685, loss 0.00345732, acc 0.99707, f1 0.997072\n",
      "2017-11-15T23:41:53.070668: step 26690, loss 0.00186946, acc 0.998047, f1 0.998048\n",
      "Current epoch:  1483\n",
      "2017-11-15T23:41:53.562532: step 26695, loss 0.000989923, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:41:54.078166: step 26700, loss 0.00162759, acc 0.999023, f1 0.999022\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:41:54.720122: step 26700, loss 2.31281, acc 0.575707, f1 0.576247\n",
      "\n",
      "2017-11-15T23:41:55.237330: step 26705, loss 0.0012576, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:41:55.764793: step 26710, loss 0.000966481, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1484\n",
      "2017-11-15T23:41:56.255634: step 26715, loss 0.00347123, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:41:56.773355: step 26720, loss 0.00487033, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:41:57.297262: step 26725, loss 0.00333302, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:41:57.788155: step 26730, loss 5.85701e-05, acc 1, f1 1\n",
      "Current epoch:  1485\n",
      "2017-11-15T23:41:58.310082: step 26735, loss 0.00402928, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:41:58.835703: step 26740, loss 0.00205693, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:41:59.355771: step 26745, loss 8.62452e-05, acc 1, f1 1\n",
      "Current epoch:  1486\n",
      "2017-11-15T23:41:59.845647: step 26750, loss 0.00140827, acc 0.999023, f1 0.999022\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:42:00.542722: step 26750, loss 2.24706, acc 0.586128, f1 0.587276\n",
      "\n",
      "2017-11-15T23:42:01.061689: step 26755, loss 0.0036639, acc 0.99707, f1 0.997074\n",
      "2017-11-15T23:42:01.584149: step 26760, loss 0.00322705, acc 0.998047, f1 0.998052\n",
      "2017-11-15T23:42:02.103580: step 26765, loss 0.000914537, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1487\n",
      "2017-11-15T23:42:02.598948: step 26770, loss 0.00181574, acc 1, f1 1\n",
      "2017-11-15T23:42:03.119885: step 26775, loss 0.00136143, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:42:03.642965: step 26780, loss 6.53536e-05, acc 1, f1 1\n",
      "Current epoch:  1488\n",
      "2017-11-15T23:42:04.140220: step 26785, loss 9.43462e-05, acc 1, f1 1\n",
      "2017-11-15T23:42:04.653854: step 26790, loss 0.00458518, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:42:05.164739: step 26795, loss 0.00477437, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:42:05.689244: step 26800, loss 7.05161e-05, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:42:06.343131: step 26800, loss 2.25229, acc 0.585643, f1 0.585919\n",
      "\n",
      "Current epoch:  1489\n",
      "2017-11-15T23:42:06.826582: step 26805, loss 0.00132924, acc 1, f1 1\n",
      "2017-11-15T23:42:07.339343: step 26810, loss 7.78381e-05, acc 1, f1 1\n",
      "2017-11-15T23:42:07.855391: step 26815, loss 0.000676004, acc 1, f1 1\n",
      "2017-11-15T23:42:08.334224: step 26820, loss 5.94961e-05, acc 1, f1 1\n",
      "Current epoch:  1490\n",
      "2017-11-15T23:42:08.860182: step 26825, loss 0.00130156, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:42:09.386087: step 26830, loss 0.00063704, acc 1, f1 1\n",
      "2017-11-15T23:42:09.910191: step 26835, loss 0.00195752, acc 0.998047, f1 0.998046\n",
      "Current epoch:  1491\n",
      "2017-11-15T23:42:10.406003: step 26840, loss 0.000738858, acc 1, f1 1\n",
      "2017-11-15T23:42:10.933521: step 26845, loss 0.00165922, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:42:11.462506: step 26850, loss 0.00632572, acc 0.995117, f1 0.995118\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:42:12.118535: step 26850, loss 2.28907, acc 0.579536, f1 0.581174\n",
      "\n",
      "2017-11-15T23:42:12.643744: step 26855, loss 0.00240628, acc 0.998047, f1 0.998046\n",
      "Current epoch:  1492\n",
      "2017-11-15T23:42:13.141619: step 26860, loss 9.21072e-05, acc 1, f1 1\n",
      "2017-11-15T23:42:13.661557: step 26865, loss 0.00289606, acc 0.99707, f1 0.997074\n",
      "2017-11-15T23:42:14.179562: step 26870, loss 0.00357379, acc 0.998047, f1 0.998048\n",
      "Current epoch:  1493\n",
      "2017-11-15T23:42:14.680455: step 26875, loss 0.0005197, acc 1, f1 1\n",
      "2017-11-15T23:42:15.204371: step 26880, loss 0.00421397, acc 0.998047, f1 0.998053\n",
      "2017-11-15T23:42:15.729378: step 26885, loss 6.50477e-05, acc 1, f1 1\n",
      "2017-11-15T23:42:16.248842: step 26890, loss 0.00200335, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1494\n",
      "2017-11-15T23:42:16.750116: step 26895, loss 0.0011461, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:42:17.274860: step 26900, loss 0.000908191, acc 0.999023, f1 0.999022\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:42:17.979013: step 26900, loss 2.24664, acc 0.594368, f1 0.59279\n",
      "\n",
      "2017-11-15T23:42:18.501489: step 26905, loss 0.00306851, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:42:18.998530: step 26910, loss 6.53954e-05, acc 1, f1 1\n",
      "Current epoch:  1495\n",
      "2017-11-15T23:42:19.523529: step 26915, loss 0.00069507, acc 1, f1 1\n",
      "2017-11-15T23:42:20.046467: step 26920, loss 0.00359612, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:42:20.563393: step 26925, loss 0.00196838, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:42:21.058171: step 26930, loss 0.000576486, acc 1, f1 1\n",
      "2017-11-15T23:42:21.577169: step 26935, loss 0.00287609, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:42:22.111117: step 26940, loss 0.00217488, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:42:22.644734: step 26945, loss 0.00109326, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1497\n",
      "2017-11-15T23:42:23.139613: step 26950, loss 0.00110845, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:42:23.800927: step 26950, loss 2.26741, acc 0.584723, f1 0.584426\n",
      "\n",
      "2017-11-15T23:42:24.316848: step 26955, loss 7.5028e-05, acc 1, f1 1\n",
      "2017-11-15T23:42:24.846815: step 26960, loss 0.00286973, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1498\n",
      "2017-11-15T23:42:25.337550: step 26965, loss 0.00252285, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:42:25.866557: step 26970, loss 0.000978355, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:42:26.385013: step 26975, loss 0.00251714, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:42:26.910960: step 26980, loss 0.00014475, acc 1, f1 1\n",
      "Current epoch:  1499\n",
      "2017-11-15T23:42:27.403838: step 26985, loss 0.00298333, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:42:27.939999: step 26990, loss 0.00228189, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:42:28.458920: step 26995, loss 0.00217696, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:42:28.947946: step 27000, loss 0.00207725, acc 0.998428, f1 0.998428\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:42:29.604963: step 27000, loss 2.26243, acc 0.592671, f1 0.589269\n",
      "\n",
      "Current epoch:  1500\n",
      "2017-11-15T23:42:30.128582: step 27005, loss 0.00131371, acc 1, f1 1\n",
      "2017-11-15T23:42:30.654533: step 27010, loss 0.00106109, acc 1, f1 1\n",
      "2017-11-15T23:42:31.173505: step 27015, loss 0.0022195, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1501\n",
      "2017-11-15T23:42:31.667222: step 27020, loss 0.00145291, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:42:32.192169: step 27025, loss 0.00235647, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:42:32.717299: step 27030, loss 0.00206953, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:42:33.253282: step 27035, loss 0.00163506, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1502\n",
      "2017-11-15T23:42:33.757310: step 27040, loss 0.00214705, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:42:34.279932: step 27045, loss 0.00180499, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:42:34.805381: step 27050, loss 9.65103e-05, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:42:35.464062: step 27050, loss 2.3261, acc 0.576919, f1 0.577018\n",
      "\n",
      "Current epoch:  1503\n",
      "2017-11-15T23:42:35.956308: step 27055, loss 0.00379859, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:42:36.481675: step 27060, loss 0.00305448, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:42:37.004173: step 27065, loss 0.00172304, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:42:37.531437: step 27070, loss 0.00266583, acc 0.99707, f1 0.997071\n",
      "Current epoch:  1504\n",
      "2017-11-15T23:42:38.028383: step 27075, loss 0.00313349, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:42:38.558961: step 27080, loss 0.00259321, acc 0.998047, f1 0.99805\n",
      "2017-11-15T23:42:39.090210: step 27085, loss 0.00109915, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:42:39.584154: step 27090, loss 0.00173294, acc 0.998428, f1 0.998428\n",
      "Current epoch:  1505\n",
      "2017-11-15T23:42:40.112130: step 27095, loss 0.00104196, acc 1, f1 1\n",
      "2017-11-15T23:42:40.644772: step 27100, loss 0.00263153, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:42:41.304048: step 27100, loss 2.25664, acc 0.582396, f1 0.582743\n",
      "\n",
      "2017-11-15T23:42:41.832096: step 27105, loss 0.00522711, acc 0.996094, f1 0.996097\n",
      "Current epoch:  1506\n",
      "2017-11-15T23:42:42.323455: step 27110, loss 0.0012426, acc 1, f1 1\n",
      "2017-11-15T23:42:42.849912: step 27115, loss 0.00147192, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:42:43.373947: step 27120, loss 0.00405911, acc 0.996094, f1 0.996097\n",
      "2017-11-15T23:42:43.897008: step 27125, loss 0.00360471, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1507\n",
      "2017-11-15T23:42:44.406650: step 27130, loss 0.00088651, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:42:44.927598: step 27135, loss 0.00295467, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:42:45.448582: step 27140, loss 0.00378519, acc 0.99707, f1 0.997071\n",
      "Current epoch:  1508\n",
      "2017-11-15T23:42:45.947483: step 27145, loss 0.00201857, acc 0.998047, f1 0.998043\n",
      "2017-11-15T23:42:46.470498: step 27150, loss 0.00182319, acc 0.998047, f1 0.998046\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:42:47.127483: step 27150, loss 2.27156, acc 0.580215, f1 0.581405\n",
      "\n",
      "2017-11-15T23:42:47.649037: step 27155, loss 0.00214098, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:42:48.170184: step 27160, loss 0.00102351, acc 0.999023, f1 0.999025\n",
      "Current epoch:  1509\n",
      "2017-11-15T23:42:48.667081: step 27165, loss 0.00166138, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:42:49.189790: step 27170, loss 0.00178044, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:42:49.723748: step 27175, loss 0.00141126, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:42:50.218478: step 27180, loss 6.66049e-05, acc 1, f1 1\n",
      "Current epoch:  1510\n",
      "2017-11-15T23:42:50.745399: step 27185, loss 0.00159206, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:42:51.264404: step 27190, loss 0.00237043, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:42:51.784700: step 27195, loss 0.00237532, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1511\n",
      "2017-11-15T23:42:52.277288: step 27200, loss 0.00234578, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:42:52.943635: step 27200, loss 2.26054, acc 0.584916, f1 0.585658\n",
      "\n",
      "2017-11-15T23:42:53.463094: step 27205, loss 0.000569554, acc 1, f1 1\n",
      "2017-11-15T23:42:53.984744: step 27210, loss 8.92146e-05, acc 1, f1 1\n",
      "2017-11-15T23:42:54.509356: step 27215, loss 0.00329415, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1512\n",
      "2017-11-15T23:42:55.004181: step 27220, loss 0.000667562, acc 1, f1 1\n",
      "2017-11-15T23:42:55.545950: step 27225, loss 0.00116195, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:42:56.068043: step 27230, loss 0.00328644, acc 0.998047, f1 0.998043\n",
      "Current epoch:  1513\n",
      "2017-11-15T23:42:56.569059: step 27235, loss 0.00172949, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:42:57.088980: step 27240, loss 0.0022869, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:42:57.610292: step 27245, loss 7.59821e-05, acc 1, f1 1\n",
      "2017-11-15T23:42:58.127215: step 27250, loss 0.00495228, acc 0.996094, f1 0.996091\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:42:58.781526: step 27250, loss 2.4159, acc 0.564996, f1 0.564198\n",
      "\n",
      "Current epoch:  1514\n",
      "2017-11-15T23:42:59.340265: step 27255, loss 8.16377e-05, acc 1, f1 1\n",
      "2017-11-15T23:42:59.863456: step 27260, loss 0.00153059, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:43:00.384959: step 27265, loss 5.61955e-05, acc 1, f1 1\n",
      "2017-11-15T23:43:00.888383: step 27270, loss 0.00598058, acc 0.995283, f1 0.995282\n",
      "Current epoch:  1515\n",
      "2017-11-15T23:43:01.416304: step 27275, loss 0.00287698, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:43:01.962267: step 27280, loss 0.000129258, acc 1, f1 1\n",
      "2017-11-15T23:43:02.501163: step 27285, loss 0.000957054, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1516\n",
      "2017-11-15T23:43:03.013074: step 27290, loss 0.00100568, acc 1, f1 1\n",
      "2017-11-15T23:43:03.541063: step 27295, loss 0.00238652, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:43:04.064996: step 27300, loss 0.00236469, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:43:04.699196: step 27300, loss 2.36545, acc 0.570667, f1 0.570748\n",
      "\n",
      "2017-11-15T23:43:05.211650: step 27305, loss 0.00308972, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1517\n",
      "2017-11-15T23:43:05.707534: step 27310, loss 0.00205364, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:43:06.233519: step 27315, loss 0.00481762, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:43:06.760347: step 27320, loss 0.00318003, acc 0.99707, f1 0.997067\n",
      "Current epoch:  1518\n",
      "2017-11-15T23:43:07.248220: step 27325, loss 0.000693379, acc 1, f1 1\n",
      "2017-11-15T23:43:07.764135: step 27330, loss 0.0017314, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:43:08.279045: step 27335, loss 0.00153975, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:43:08.793203: step 27340, loss 0.00506076, acc 0.995117, f1 0.995119\n",
      "Current epoch:  1519\n",
      "2017-11-15T23:43:09.275573: step 27345, loss 0.00286512, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:43:09.785246: step 27350, loss 0.00477292, acc 0.996094, f1 0.996094\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:43:10.420449: step 27350, loss 2.24349, acc 0.588697, f1 0.588239\n",
      "\n",
      "2017-11-15T23:43:10.932859: step 27355, loss 0.0047206, acc 0.996094, f1 0.996094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:43:11.419766: step 27360, loss 0.00348864, acc 0.996855, f1 0.996852\n",
      "Current epoch:  1520\n",
      "2017-11-15T23:43:11.940259: step 27365, loss 0.00145678, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:43:12.458188: step 27370, loss 0.00126055, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:43:12.972219: step 27375, loss 0.0036591, acc 0.99707, f1 0.997074\n",
      "Current epoch:  1521\n",
      "2017-11-15T23:43:13.456052: step 27380, loss 0.00191984, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:43:13.980553: step 27385, loss 0.000109907, acc 1, f1 1\n",
      "2017-11-15T23:43:14.494472: step 27390, loss 0.000739372, acc 1, f1 1\n",
      "2017-11-15T23:43:15.010433: step 27395, loss 0.00271221, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1522\n",
      "2017-11-15T23:43:15.505023: step 27400, loss 0.00135726, acc 0.999023, f1 0.999022\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:43:16.171231: step 27400, loss 2.25063, acc 0.590927, f1 0.588404\n",
      "\n",
      "2017-11-15T23:43:16.685637: step 27405, loss 0.00206267, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:43:17.204554: step 27410, loss 0.00148722, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1523\n",
      "2017-11-15T23:43:17.692434: step 27415, loss 0.00398717, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:43:18.207354: step 27420, loss 0.000807125, acc 1, f1 1\n",
      "2017-11-15T23:43:18.726254: step 27425, loss 0.0029687, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:43:19.238156: step 27430, loss 0.000997576, acc 0.999023, f1 0.999022\n",
      "Current epoch:  1524\n",
      "2017-11-15T23:43:19.733041: step 27435, loss 0.00114533, acc 1, f1 1\n",
      "2017-11-15T23:43:20.255303: step 27440, loss 0.00304894, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:43:20.773219: step 27445, loss 0.00429527, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:43:21.257950: step 27450, loss 0.00171332, acc 0.998428, f1 0.998431\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:43:21.898049: step 27450, loss 2.4384, acc 0.570715, f1 0.566609\n",
      "\n",
      "Current epoch:  1525\n",
      "2017-11-15T23:43:22.416673: step 27455, loss 7.04334e-05, acc 1, f1 1\n",
      "2017-11-15T23:43:22.936087: step 27460, loss 0.00321742, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:43:23.451099: step 27465, loss 0.00109052, acc 0.999023, f1 0.999025\n",
      "Current epoch:  1526\n",
      "2017-11-15T23:43:23.936824: step 27470, loss 0.000500323, acc 1, f1 1\n",
      "2017-11-15T23:43:24.487249: step 27475, loss 0.00212922, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:43:25.011712: step 27480, loss 6.49229e-05, acc 1, f1 1\n",
      "2017-11-15T23:43:25.535238: step 27485, loss 0.00481868, acc 0.996094, f1 0.996091\n",
      "Current epoch:  1527\n",
      "2017-11-15T23:43:26.051407: step 27490, loss 0.000961831, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:43:26.618445: step 27495, loss 0.00223372, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:43:27.171499: step 27500, loss 7.61757e-05, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:43:27.983276: step 27500, loss 2.31952, acc 0.575417, f1 0.57599\n",
      "\n",
      "Current epoch:  1528\n",
      "2017-11-15T23:43:28.537827: step 27505, loss 0.000983979, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:43:29.084763: step 27510, loss 0.00265556, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:43:29.616371: step 27515, loss 6.61705e-05, acc 1, f1 1\n",
      "2017-11-15T23:43:30.154515: step 27520, loss 0.00268194, acc 0.99707, f1 0.997071\n",
      "Current epoch:  1529\n",
      "2017-11-15T23:43:30.649518: step 27525, loss 7.17641e-05, acc 1, f1 1\n",
      "2017-11-15T23:43:31.168165: step 27530, loss 0.000795073, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:43:31.685073: step 27535, loss 0.00252021, acc 0.998047, f1 0.998051\n",
      "2017-11-15T23:43:32.195478: step 27540, loss 0.0029049, acc 0.998428, f1 0.998426\n",
      "Current epoch:  1530\n",
      "2017-11-15T23:43:32.723993: step 27545, loss 0.000899545, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:43:33.255385: step 27550, loss 0.0022404, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:43:33.922208: step 27550, loss 2.26171, acc 0.582444, f1 0.583614\n",
      "\n",
      "2017-11-15T23:43:34.453717: step 27555, loss 0.00211314, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1531\n",
      "2017-11-15T23:43:34.951315: step 27560, loss 0.000655532, acc 1, f1 1\n",
      "2017-11-15T23:43:35.465254: step 27565, loss 0.00210181, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:43:35.990302: step 27570, loss 0.00578066, acc 0.995117, f1 0.995118\n",
      "2017-11-15T23:43:36.507233: step 27575, loss 0.00100895, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1532\n",
      "2017-11-15T23:43:36.992834: step 27580, loss 0.000699992, acc 1, f1 1\n",
      "2017-11-15T23:43:37.526602: step 27585, loss 0.000965149, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:43:38.109650: step 27590, loss 0.00241553, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1533\n",
      "2017-11-15T23:43:38.667698: step 27595, loss 0.00113359, acc 1, f1 1\n",
      "2017-11-15T23:43:39.241936: step 27600, loss 0.00112014, acc 0.999023, f1 0.999022\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:43:39.995305: step 27600, loss 2.26287, acc 0.582541, f1 0.583038\n",
      "\n",
      "2017-11-15T23:43:40.580188: step 27605, loss 0.000493388, acc 1, f1 1\n",
      "2017-11-15T23:43:41.146825: step 27610, loss 0.00369834, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1534\n",
      "2017-11-15T23:43:41.715155: step 27615, loss 0.00233788, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:43:42.285405: step 27620, loss 0.0035266, acc 0.99707, f1 0.997072\n",
      "2017-11-15T23:43:42.891095: step 27625, loss 0.000995411, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:43:43.452131: step 27630, loss 0.0063846, acc 0.995283, f1 0.995283\n",
      "Current epoch:  1535\n",
      "2017-11-15T23:43:44.053225: step 27635, loss 0.00278233, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:43:44.621512: step 27640, loss 0.00193006, acc 0.998047, f1 0.998051\n",
      "2017-11-15T23:43:45.190795: step 27645, loss 0.00887318, acc 0.994141, f1 0.994145\n",
      "Current epoch:  1536\n",
      "2017-11-15T23:43:45.767353: step 27650, loss 6.99644e-05, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:43:46.538957: step 27650, loss 2.26507, acc 0.590733, f1 0.588314\n",
      "\n",
      "2017-11-15T23:43:47.187357: step 27655, loss 0.0040007, acc 0.996094, f1 0.996092\n",
      "2017-11-15T23:43:47.824402: step 27660, loss 8.05649e-05, acc 1, f1 1\n",
      "2017-11-15T23:43:48.512952: step 27665, loss 0.00269699, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1537\n",
      "2017-11-15T23:43:49.126092: step 27670, loss 0.000553128, acc 1, f1 1\n",
      "2017-11-15T23:43:49.699123: step 27675, loss 0.0012945, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:43:50.443224: step 27680, loss 0.00128672, acc 0.999023, f1 0.999024\n",
      "Current epoch:  1538\n",
      "2017-11-15T23:43:51.092487: step 27685, loss 0.0027629, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:43:51.792664: step 27690, loss 7.92883e-05, acc 1, f1 1\n",
      "2017-11-15T23:43:52.473481: step 27695, loss 0.00376684, acc 0.99707, f1 0.997073\n",
      "2017-11-15T23:43:53.080082: step 27700, loss 0.00206473, acc 0.998047, f1 0.998048\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:43:53.978474: step 27700, loss 2.27685, acc 0.586855, f1 0.585818\n",
      "\n",
      "Current epoch:  1539\n",
      "2017-11-15T23:43:54.613417: step 27705, loss 0.00158488, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:43:55.298906: step 27710, loss 0.00268722, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:43:56.043269: step 27715, loss 0.00100371, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:43:56.646571: step 27720, loss 8.47293e-05, acc 1, f1 1\n",
      "Current epoch:  1540\n",
      "2017-11-15T23:43:57.244892: step 27725, loss 0.000755104, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:43:57.821303: step 27730, loss 0.00290765, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:43:58.422128: step 27735, loss 0.00321957, acc 0.99707, f1 0.997071\n",
      "Current epoch:  1541\n",
      "2017-11-15T23:43:58.997301: step 27740, loss 0.00130599, acc 1, f1 1\n",
      "2017-11-15T23:43:59.541174: step 27745, loss 0.00343613, acc 0.99707, f1 0.997059\n",
      "2017-11-15T23:44:00.061123: step 27750, loss 0.002948, acc 0.99707, f1 0.99707\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:44:00.708341: step 27750, loss 2.26619, acc 0.595337, f1 0.592469\n",
      "\n",
      "2017-11-15T23:44:01.276334: step 27755, loss 0.00272744, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1542\n",
      "2017-11-15T23:44:01.857386: step 27760, loss 0.00231819, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:44:02.491091: step 27765, loss 0.00202802, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:44:03.091306: step 27770, loss 0.00144279, acc 0.999023, f1 0.999025\n",
      "Current epoch:  1543\n",
      "2017-11-15T23:44:03.706320: step 27775, loss 0.00660624, acc 0.995117, f1 0.995118\n",
      "2017-11-15T23:44:04.331253: step 27780, loss 0.00368286, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:44:04.933356: step 27785, loss 0.00155037, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:44:05.592789: step 27790, loss 0.00227595, acc 0.998047, f1 0.998052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch:  1544\n",
      "2017-11-15T23:44:06.140965: step 27795, loss 0.000815335, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:44:06.683980: step 27800, loss 0.00224603, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:44:07.394014: step 27800, loss 2.25096, acc 0.590054, f1 0.588589\n",
      "\n",
      "2017-11-15T23:44:07.947010: step 27805, loss 0.00508549, acc 0.995117, f1 0.995122\n",
      "2017-11-15T23:44:08.446899: step 27810, loss 0.00545572, acc 0.995283, f1 0.995276\n",
      "Current epoch:  1545\n",
      "2017-11-15T23:44:08.973901: step 27815, loss 0.00169079, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:44:09.502134: step 27820, loss 0.00111834, acc 1, f1 1\n",
      "2017-11-15T23:44:10.023133: step 27825, loss 0.00559433, acc 0.995117, f1 0.995117\n",
      "Current epoch:  1546\n",
      "2017-11-15T23:44:10.521547: step 27830, loss 0.000777822, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:44:11.094017: step 27835, loss 0.00130433, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:44:11.658763: step 27840, loss 0.00370254, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:44:12.206294: step 27845, loss 0.00562325, acc 0.995117, f1 0.995117\n",
      "Current epoch:  1547\n",
      "2017-11-15T23:44:12.704968: step 27850, loss 0.00143442, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:44:13.354620: step 27850, loss 2.28501, acc 0.595773, f1 0.590846\n",
      "\n",
      "2017-11-15T23:44:13.880298: step 27855, loss 0.000532822, acc 1, f1 1\n",
      "2017-11-15T23:44:14.428004: step 27860, loss 0.00182829, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1548\n",
      "2017-11-15T23:44:15.118678: step 27865, loss 0.000774766, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:44:15.666006: step 27870, loss 0.00203448, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:44:16.188741: step 27875, loss 0.00230933, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:44:16.724183: step 27880, loss 0.00198955, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1549\n",
      "2017-11-15T23:44:17.244107: step 27885, loss 0.00318369, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:44:17.781166: step 27890, loss 0.002616, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:44:18.322549: step 27895, loss 0.00114945, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:44:18.848567: step 27900, loss 0.00438364, acc 0.996855, f1 0.996856\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:44:19.584971: step 27900, loss 2.25975, acc 0.589812, f1 0.58711\n",
      "\n",
      "Current epoch:  1550\n",
      "2017-11-15T23:44:20.148113: step 27905, loss 0.00439495, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:44:20.671637: step 27910, loss 0.00377313, acc 0.996094, f1 0.996095\n",
      "2017-11-15T23:44:21.192369: step 27915, loss 0.00248044, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1551\n",
      "2017-11-15T23:44:21.712447: step 27920, loss 0.00183573, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:44:22.256087: step 27925, loss 0.00143382, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:44:22.828571: step 27930, loss 0.00253353, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:44:23.539854: step 27935, loss 0.00204649, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1552\n",
      "2017-11-15T23:44:24.031402: step 27940, loss 0.00229257, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:44:24.548859: step 27945, loss 0.00404738, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:44:25.097336: step 27950, loss 0.000940772, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:44:25.737952: step 27950, loss 2.25454, acc 0.589666, f1 0.588963\n",
      "\n",
      "Current epoch:  1553\n",
      "2017-11-15T23:44:26.221863: step 27955, loss 0.00413921, acc 0.996094, f1 0.996096\n",
      "2017-11-15T23:44:26.755883: step 27960, loss 0.00229069, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:44:27.263398: step 27965, loss 0.00182946, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:44:27.799338: step 27970, loss 0.00226125, acc 0.998047, f1 0.998046\n",
      "Current epoch:  1554\n",
      "2017-11-15T23:44:28.300320: step 27975, loss 0.00107503, acc 1, f1 1\n",
      "2017-11-15T23:44:28.870154: step 27980, loss 6.30984e-05, acc 1, f1 1\n",
      "2017-11-15T23:44:29.438853: step 27985, loss 0.00561751, acc 0.996094, f1 0.996093\n",
      "2017-11-15T23:44:29.972388: step 27990, loss 0.00341978, acc 0.996855, f1 0.996858\n",
      "Current epoch:  1555\n",
      "2017-11-15T23:44:30.544504: step 27995, loss 0.000580542, acc 1, f1 1\n",
      "2017-11-15T23:44:31.135573: step 28000, loss 0.00222656, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:44:31.912718: step 28000, loss 2.29598, acc 0.576628, f1 0.579076\n",
      "\n",
      "2017-11-15T23:44:32.438410: step 28005, loss 0.00735798, acc 0.993164, f1 0.993164\n",
      "Current epoch:  1556\n",
      "2017-11-15T23:44:32.931288: step 28010, loss 0.00167318, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:44:33.457471: step 28015, loss 0.00220265, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:44:33.983096: step 28020, loss 0.000921831, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:44:34.530179: step 28025, loss 0.00140055, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1557\n",
      "2017-11-15T23:44:35.042253: step 28030, loss 0.00191211, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:44:35.575193: step 28035, loss 0.00155211, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:44:36.106690: step 28040, loss 0.00111321, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1558\n",
      "2017-11-15T23:44:36.635732: step 28045, loss 0.00128787, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:44:37.182638: step 28050, loss 0.00203966, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:44:37.860371: step 28050, loss 2.26258, acc 0.596064, f1 0.592503\n",
      "\n",
      "2017-11-15T23:44:38.378120: step 28055, loss 0.00140762, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:44:38.895805: step 28060, loss 0.00218321, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1559\n",
      "2017-11-15T23:44:39.394916: step 28065, loss 0.00190067, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:44:39.980302: step 28070, loss 0.00142315, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:44:40.539578: step 28075, loss 0.00511609, acc 0.996094, f1 0.996097\n",
      "2017-11-15T23:44:41.027805: step 28080, loss 0.00150961, acc 0.998428, f1 0.998428\n",
      "Current epoch:  1560\n",
      "2017-11-15T23:44:41.561487: step 28085, loss 0.0019298, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:44:42.099572: step 28090, loss 0.00214195, acc 0.998047, f1 0.998052\n",
      "2017-11-15T23:44:42.623155: step 28095, loss 0.00460158, acc 0.996094, f1 0.996094\n",
      "Current epoch:  1561\n",
      "2017-11-15T23:44:43.108289: step 28100, loss 0.00296446, acc 0.998047, f1 0.998048\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:44:43.786786: step 28100, loss 2.37702, acc 0.569794, f1 0.569054\n",
      "\n",
      "2017-11-15T23:44:44.311979: step 28105, loss 0.00284126, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:44:44.829421: step 28110, loss 0.00232311, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:44:45.362411: step 28115, loss 0.000835454, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1562\n",
      "2017-11-15T23:44:45.847289: step 28120, loss 0.000400428, acc 1, f1 1\n",
      "2017-11-15T23:44:46.366235: step 28125, loss 0.00197531, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:44:46.885279: step 28130, loss 0.00177621, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1563\n",
      "2017-11-15T23:44:47.373159: step 28135, loss 0.00246066, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:44:47.889793: step 28140, loss 0.00229583, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:44:48.409323: step 28145, loss 0.00288826, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:44:48.930946: step 28150, loss 0.00250274, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:44:49.585950: step 28150, loss 2.25742, acc 0.593932, f1 0.593125\n",
      "\n",
      "Current epoch:  1564\n",
      "2017-11-15T23:44:50.073947: step 28155, loss 0.000583405, acc 1, f1 1\n",
      "2017-11-15T23:44:50.595895: step 28160, loss 0.000782331, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:44:51.123373: step 28165, loss 0.000801603, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:44:51.690957: step 28170, loss 0.00169727, acc 0.998428, f1 0.998428\n",
      "Current epoch:  1565\n",
      "2017-11-15T23:44:52.274510: step 28175, loss 0.003198, acc 0.99707, f1 0.997066\n",
      "2017-11-15T23:44:52.843454: step 28180, loss 0.00175086, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:44:53.422494: step 28185, loss 0.00260151, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1566\n",
      "2017-11-15T23:44:53.974762: step 28190, loss 0.000783809, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:44:54.583077: step 28195, loss 0.000715067, acc 1, f1 1\n",
      "2017-11-15T23:44:55.108388: step 28200, loss 8.00408e-05, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:44:55.752265: step 28200, loss 2.2861, acc 0.578567, f1 0.580554\n",
      "\n",
      "2017-11-15T23:44:56.275419: step 28205, loss 0.00220709, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1567\n",
      "2017-11-15T23:44:56.778523: step 28210, loss 0.000737953, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:44:57.319041: step 28215, loss 0.00437134, acc 0.99707, f1 0.997074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:44:57.876623: step 28220, loss 0.00408196, acc 0.996094, f1 0.99609\n",
      "Current epoch:  1568\n",
      "2017-11-15T23:44:58.362035: step 28225, loss 0.00356103, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:44:58.879607: step 28230, loss 0.000769548, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:44:59.399593: step 28235, loss 0.00142902, acc 1, f1 1\n",
      "2017-11-15T23:44:59.920234: step 28240, loss 0.00287693, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1569\n",
      "2017-11-15T23:45:00.442704: step 28245, loss 0.00191892, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:45:00.977690: step 28250, loss 0.00151672, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:45:01.674934: step 28250, loss 2.27738, acc 0.589327, f1 0.587414\n",
      "\n",
      "2017-11-15T23:45:02.229183: step 28255, loss 0.00299201, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:45:02.709038: step 28260, loss 0.00192059, acc 0.998428, f1 0.998428\n",
      "Current epoch:  1570\n",
      "2017-11-15T23:45:03.293852: step 28265, loss 6.47381e-05, acc 1, f1 1\n",
      "2017-11-15T23:45:03.863381: step 28270, loss 0.00368611, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:45:04.407008: step 28275, loss 0.000854746, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1571\n",
      "2017-11-15T23:45:04.935871: step 28280, loss 0.00230923, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:45:05.486952: step 28285, loss 0.000969429, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:45:06.028248: step 28290, loss 0.00135999, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:45:06.566135: step 28295, loss 5.74798e-05, acc 1, f1 1\n",
      "Current epoch:  1572\n",
      "2017-11-15T23:45:07.076708: step 28300, loss 0.00330484, acc 0.99707, f1 0.99707\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:45:07.806946: step 28300, loss 2.26581, acc 0.581572, f1 0.582457\n",
      "\n",
      "2017-11-15T23:45:08.329674: step 28305, loss 0.00207236, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:45:08.854677: step 28310, loss 0.00264991, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1573\n",
      "2017-11-15T23:45:09.348071: step 28315, loss 0.00121401, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:45:09.865661: step 28320, loss 0.00269354, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:45:10.386582: step 28325, loss 0.0023236, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:45:10.911437: step 28330, loss 6.14413e-05, acc 1, f1 1\n",
      "Current epoch:  1574\n",
      "2017-11-15T23:45:11.394141: step 28335, loss 0.00169174, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:45:11.917867: step 28340, loss 0.00140413, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:45:12.436435: step 28345, loss 0.0031642, acc 0.99707, f1 0.997068\n",
      "2017-11-15T23:45:12.944342: step 28350, loss 8.28068e-05, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:45:13.596031: step 28350, loss 2.33066, acc 0.58133, f1 0.579028\n",
      "\n",
      "Current epoch:  1575\n",
      "2017-11-15T23:45:14.126936: step 28355, loss 0.00217706, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:45:14.647144: step 28360, loss 0.00235627, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:45:15.167310: step 28365, loss 0.00351694, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1576\n",
      "2017-11-15T23:45:15.653276: step 28370, loss 0.00253062, acc 0.998047, f1 0.998041\n",
      "2017-11-15T23:45:16.178250: step 28375, loss 0.00412086, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:45:16.708180: step 28380, loss 0.00200292, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:45:17.227105: step 28385, loss 0.00292612, acc 0.99707, f1 0.997071\n",
      "Current epoch:  1577\n",
      "2017-11-15T23:45:17.717996: step 28390, loss 0.00278496, acc 0.998047, f1 0.998042\n",
      "2017-11-15T23:45:18.304076: step 28395, loss 6.39665e-05, acc 1, f1 1\n",
      "2017-11-15T23:45:18.895558: step 28400, loss 0.00313026, acc 0.99707, f1 0.99707\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:45:19.622505: step 28400, loss 2.30534, acc 0.590878, f1 0.587381\n",
      "\n",
      "Current epoch:  1578\n",
      "2017-11-15T23:45:20.137388: step 28405, loss 0.00117842, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:45:20.693242: step 28410, loss 0.00295854, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:45:21.247106: step 28415, loss 0.00299322, acc 0.99707, f1 0.997075\n",
      "2017-11-15T23:45:21.804433: step 28420, loss 0.00500873, acc 0.995117, f1 0.995115\n",
      "Current epoch:  1579\n",
      "2017-11-15T23:45:22.322677: step 28425, loss 0.00131763, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:45:22.866191: step 28430, loss 0.00311737, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:45:23.417205: step 28435, loss 0.00426628, acc 0.99707, f1 0.997072\n",
      "2017-11-15T23:45:23.928986: step 28440, loss 0.00156068, acc 0.998428, f1 0.998426\n",
      "Current epoch:  1580\n",
      "2017-11-15T23:45:24.508012: step 28445, loss 0.00470593, acc 0.995117, f1 0.995115\n",
      "2017-11-15T23:45:25.066626: step 28450, loss 0.00127348, acc 0.999023, f1 0.999024\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:45:25.785591: step 28450, loss 2.31365, acc 0.576144, f1 0.57687\n",
      "\n",
      "2017-11-15T23:45:26.310527: step 28455, loss 0.00240535, acc 0.998047, f1 0.998048\n",
      "Current epoch:  1581\n",
      "2017-11-15T23:45:26.806882: step 28460, loss 0.000741086, acc 1, f1 1\n",
      "2017-11-15T23:45:27.357323: step 28465, loss 0.00355571, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:45:27.910310: step 28470, loss 0.0020272, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:45:28.458321: step 28475, loss 0.0038603, acc 0.99707, f1 0.997075\n",
      "Current epoch:  1582\n",
      "2017-11-15T23:45:28.964299: step 28480, loss 0.000778971, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:45:29.498421: step 28485, loss 6.84229e-05, acc 1, f1 1\n",
      "2017-11-15T23:45:30.019084: step 28490, loss 0.00186115, acc 0.998047, f1 0.998046\n",
      "Current epoch:  1583\n",
      "2017-11-15T23:45:30.528229: step 28495, loss 5.75579e-05, acc 1, f1 1\n",
      "2017-11-15T23:45:31.044213: step 28500, loss 0.00190865, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:45:31.688697: step 28500, loss 2.35537, acc 0.576919, f1 0.57579\n",
      "\n",
      "2017-11-15T23:45:32.201546: step 28505, loss 0.00237864, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:45:32.727501: step 28510, loss 0.00514139, acc 0.995117, f1 0.995116\n",
      "Current epoch:  1584\n",
      "2017-11-15T23:45:33.220390: step 28515, loss 0.00186069, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:45:33.741908: step 28520, loss 0.00131273, acc 1, f1 1\n",
      "2017-11-15T23:45:34.280560: step 28525, loss 0.000677292, acc 1, f1 1\n",
      "2017-11-15T23:45:34.797966: step 28530, loss 0.00597459, acc 0.995283, f1 0.995283\n",
      "Current epoch:  1585\n",
      "2017-11-15T23:45:35.323248: step 28535, loss 0.0021253, acc 0.998047, f1 0.99805\n",
      "2017-11-15T23:45:35.869785: step 28540, loss 0.000440664, acc 1, f1 1\n",
      "2017-11-15T23:45:36.398528: step 28545, loss 0.00312805, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1586\n",
      "2017-11-15T23:45:36.930649: step 28550, loss 0.0029347, acc 0.999023, f1 0.999022\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:45:37.657135: step 28550, loss 2.38781, acc 0.567662, f1 0.568958\n",
      "\n",
      "2017-11-15T23:45:38.215051: step 28555, loss 0.00268336, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:45:38.736560: step 28560, loss 0.00207633, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:45:39.270585: step 28565, loss 0.00207476, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1587\n",
      "2017-11-15T23:45:39.755762: step 28570, loss 0.00161885, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:45:40.278750: step 28575, loss 0.00240245, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:45:40.804689: step 28580, loss 0.000911108, acc 0.999023, f1 0.999025\n",
      "Current epoch:  1588\n",
      "2017-11-15T23:45:41.296546: step 28585, loss 0.00309054, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:45:41.844226: step 28590, loss 0.000110087, acc 1, f1 1\n",
      "2017-11-15T23:45:42.371350: step 28595, loss 0.00292157, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:45:42.887616: step 28600, loss 0.00123927, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:45:43.532316: step 28600, loss 2.28145, acc 0.587485, f1 0.585842\n",
      "\n",
      "Current epoch:  1589\n",
      "2017-11-15T23:45:44.032200: step 28605, loss 0.00157542, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:45:44.546638: step 28610, loss 0.00150107, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:45:45.066079: step 28615, loss 0.0060529, acc 0.995117, f1 0.995114\n",
      "2017-11-15T23:45:45.573885: step 28620, loss 0.00281666, acc 0.996855, f1 0.996855\n",
      "Current epoch:  1590\n",
      "2017-11-15T23:45:46.104420: step 28625, loss 5.71904e-05, acc 1, f1 1\n",
      "2017-11-15T23:45:46.622121: step 28630, loss 0.00139927, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:45:47.139378: step 28635, loss 0.00618918, acc 0.995117, f1 0.995109\n",
      "Current epoch:  1591\n",
      "2017-11-15T23:45:47.633881: step 28640, loss 0.00168679, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:45:48.148606: step 28645, loss 0.00421988, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:45:48.671447: step 28650, loss 0.00124458, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:45:49.310977: step 28650, loss 2.2659, acc 0.591896, f1 0.590577\n",
      "\n",
      "2017-11-15T23:45:49.825484: step 28655, loss 0.00386814, acc 0.996094, f1 0.996094\n",
      "Current epoch:  1592\n",
      "2017-11-15T23:45:50.310022: step 28660, loss 0.00153759, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:45:50.822744: step 28665, loss 0.00258661, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:45:51.352169: step 28670, loss 0.00137868, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1593\n",
      "2017-11-15T23:45:51.842602: step 28675, loss 0.000639613, acc 1, f1 1\n",
      "2017-11-15T23:45:52.360281: step 28680, loss 0.00294478, acc 0.99707, f1 0.997082\n",
      "2017-11-15T23:45:52.875169: step 28685, loss 0.00427626, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:45:53.398795: step 28690, loss 0.00130105, acc 0.999023, f1 0.999022\n",
      "Current epoch:  1594\n",
      "2017-11-15T23:45:53.891669: step 28695, loss 0.00191334, acc 0.999023, f1 0.999024\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "num_checkpoints = 5\n",
    "num_epochs = 2000\n",
    "print_train_every = 5\n",
    "evaluate_every = 50\n",
    "checkpoint_every = 10000000\n",
    "allow_soft_placement=True\n",
    "log_device_placement=False\n",
    "\n",
    "with graph.as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=allow_soft_placement,\n",
    "      log_device_placement=log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        \n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdadeltaOptimizer(1.0)\n",
    "        grads_and_vars = optimizer.compute_gradients(loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=num_checkpoints)\n",
    "\n",
    "#         # Write vocabulary\n",
    "#         vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              input_x: x_batch,\n",
    "              input_y: y_batch,\n",
    "            }\n",
    "            _, step, summaries, cur_loss, cur_accuracy, y_pred = sess.run(\n",
    "                [train_op, global_step, train_summary_op, loss, accuracy, predictions],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "#             print(y_pred)\n",
    "#             print(y_batch)\n",
    "            if step % print_train_every == 0:\n",
    "                f1 = f1_score(y_batch, y_pred, average = 'weighted')\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}, f1 {:g}\".format(time_str, step, cur_loss, cur_accuracy,\n",
    "                                                                     f1))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              input_x: x_batch,\n",
    "              input_y: y_batch,\n",
    "            }\n",
    "            step, summaries, cur_loss, cur_accuracy, y_pred = sess.run(\n",
    "                [global_step, dev_summary_op, loss, accuracy, predictions],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            f1 = f1_score(y_batch, y_pred, average = 'weighted')\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}, f1 {:g}\".format(time_str, step, cur_loss, cur_accuracy,\n",
    "                                                                    f1))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        \n",
    "        sess.run(embedding_init, feed_dict={embedding_placeholder: final_embeddings})\n",
    "        # Generate batches\n",
    "        batches = batch_iter(\n",
    "            list(zip(x_train, y_train)), batch_size, num_epochs)\n",
    "        \n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            \n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_test, y_test, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
