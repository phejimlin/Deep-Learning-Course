{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "\n",
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from tensorflow.contrib import learn\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from IPython.display import clear_output, Image, display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### Do not modify here ###### \n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = \"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = graph_def\n",
    "    #strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))\n",
    "###### Do not modify  here ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Train/test file separated, use in Supervised Phase\n",
    "    \"\"\"\n",
    "def load_data_and_labels(train_data_file, test_data_file):\n",
    "    \"\"\"\n",
    "    Loads MR polarity data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "    # Load data from files\n",
    "    train_data = pd.read_csv(train_data_file, sep='\\t', quoting=csv.QUOTE_NONE, header=None, names=['id', 'language', 'label', 'text'])\n",
    "    test_data = pd.read_csv(test_data_file, sep='\\t', quoting=csv.QUOTE_NONE, header=None, names=['id', 'language', 'label', 'text'])\n",
    "\n",
    "    x_train = train_data['text'].tolist()\n",
    "    y_train = train_data['label'].tolist()\n",
    "\n",
    "    x_test = test_data['text'].tolist()\n",
    "    y_test = test_data['label'].tolist()\n",
    "    \n",
    "    x_train = [s.strip() for s in x_train]\n",
    "    x_test = [s.strip() for s in x_test]\n",
    "    \n",
    "    label_encoding = {'neutral':0, 'positive':1, 'negative':2}\n",
    "    \n",
    "    y_train_encoding = [label_encoding[label] for label in y_train]    \n",
    "    y_test_encoding = [label_encoding[label] for label in y_test]\n",
    "\n",
    "    return [x_train, y_train_encoding, x_test, y_test_encoding]\n",
    "\n",
    "\"\"\"Load file without using pandas\n",
    "\"\"\"\n",
    "def load_without_pandas(paths, numbers={}):\n",
    "    label_encoding = {'neutral':0, 'positive':1, 'negative':2}\n",
    "    i=0\n",
    "    X = []\n",
    "    y = []\n",
    "    for path in paths:\n",
    "        try:\n",
    "            n = numbers[path]\n",
    "        except KeyError:\n",
    "            n = 0\n",
    "        i=0\n",
    "        with open(path, 'r', encoding='utf8') as f:\n",
    "            for line in f:\n",
    "                if n and i >= n:\n",
    "                    break\n",
    "                i += 1\n",
    "                splits = line.split('\\t')\n",
    "                y.append(label_encoding[splits[2]])\n",
    "                X.append(splits[3].rstrip())\n",
    "    y = np.array(y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "    return [X_train, y_train, X_test, y_test]\n",
    "\n",
    "\"\"\"\n",
    "    One single data, use in Distance-supervised Phase\n",
    "    \"\"\"\n",
    "def transform_data_and_labels(data):\n",
    "    \"\"\"\n",
    "    Loads MR polarity data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    x = np.array(data['text'].tolist())\n",
    "    y = data['label'].tolist()\n",
    "    \n",
    "    # encoding label\n",
    "    label_encoding = {'neutral':0, 'positive':1, 'negative':2}\n",
    "    y = [label_encoding[label] for label in y]    \n",
    "    \n",
    "    \n",
    "    y = np.array(y)\n",
    "    \n",
    "    # Randomly shuffle data\n",
    "    np.random.seed(10)\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "    x_shuffled = x[shuffle_indices]\n",
    "    y_shuffled = y[shuffle_indices]\n",
    "\n",
    "    # Split train/test set\n",
    "    # maybe we can use cross-validation to improve\n",
    "    dev_sample_index = -1 * int(0.1 * float(len(y)))\n",
    "    x_train, x_test = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "    y_train, y_test = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "    print(\"Train/Test split: {:d}/{:d}\".format(len(y_train), len(y_test)))\n",
    "    \n",
    "    return [x_train, y_train, x_test, y_test]\n",
    "\n",
    "\"\"\"Tokenize train and test tweets, find the maximum length of tweetm,\n",
    "    and find all tokens seen in the word dict\"\"\"\n",
    "def tokenize_tweet(train_tweets, test_tweets, word_dict):\n",
    "    all_tokens = {}\n",
    "    dropped = 0\n",
    "    # max_document_length = max([len(x.split(\" \")) for x in x_train_sentence])\n",
    "    ppl_re = re.compile(r'@\\S*')\n",
    "    url_re = re.compile(r'http\\S+')\n",
    "    esc_re = re.compile(r'\\\\u')\n",
    "    tknzr = TweetTokenizer()\n",
    "    # tknzr = TweetTokenizer(reduce_len=True)\n",
    "    \n",
    "    tokenized_tweets_all = []\n",
    "    max_document_length = 0\n",
    "    dropped = 0\n",
    "    for tweets in [train_tweets, test_tweets]:\n",
    "        tweets = [url_re.sub('URLTOK', ppl_re.sub('USRTOK', tweet.lower())) for tweet in tweets]\n",
    "        tokenized_tweets = []\n",
    "        for tweet in tweets:\n",
    "            if len(esc_re.findall(tweet)) > 6:\n",
    "                dropped += 1\n",
    "                continue\n",
    "            tokenized_tweet = tknzr.tokenize(tweet)\n",
    "            if len(tokenized_tweet) > 65:\n",
    "                dropped += 1\n",
    "                continue\n",
    "            for token in tokenized_tweet:\n",
    "                if token in word_dict:\n",
    "                    all_tokens[token] = True\n",
    "            tokenized_tweets. append(tokenized_tweet)     \n",
    "        tokenized_tweets_all.append(tokenized_tweets)\n",
    "        max_document_length = max(max_document_length, max([len(tweet) for tweet in tokenized_tweets]))\n",
    "    print(max_document_length)\n",
    "    print(\"dropped \", dropped)\n",
    "    return tokenized_tweets_all[0], tokenized_tweets_all[1], all_tokens, max_document_length\n",
    "\n",
    "\"\"\"\n",
    "    This function assumes that the last word in the word embedding is a zero vector, and will use it as UNKNOWN WORDS.\n",
    "    Padding will be num_voc, and unknown words will be num_voc-1.\n",
    "    The input 'num_voc' equals to the shape[0] of the word embedding.\n",
    "    Also returns all seen tokens for reducing word embedding.\n",
    "\"\"\"\n",
    "def process_tweet(train_tweets, test_tweets, word_dict, max_document_length):\n",
    "    x = []\n",
    "    num_voc = len(word_dict)\n",
    "    for tokenized_tweets in [train_tweets, test_tweets]:\n",
    "        x_curr = []\n",
    "        for tokenized_tweet in tokenized_tweets:\n",
    "#             if len(tokenized_tweet) == max_document_length:\n",
    "#                 print(tokenized_tweet)\n",
    "            \"\"\"Not sure if original paper does this, but since index 0 means USRTOK, padding should be a number\n",
    "            higher than total word count, so tf.nn.embedding_lookup will return a tensor of 0 insted of USRTOK.\"\"\"\n",
    "        #     temp = np.zeros(max_document_length, dtype=np.int).tolist()\n",
    "            temp = (np.ones(max_document_length, dtype=np.int)*(num_voc)).tolist()\n",
    "\n",
    "            for index, word in enumerate(tokenized_tweet):\n",
    "                if word in word_dict:\n",
    "#                     temp[index] = word_dict[word][0]\n",
    "                    temp[index] = word_dict[word]\n",
    "                else:\n",
    "                    temp[index] = num_voc-1\n",
    "            x_curr.append(temp)\n",
    "        x_curr = np.array(x_curr)\n",
    "        x.append(x_curr)\n",
    "    \n",
    "    return x[0], x[1]\n",
    "\n",
    "\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Current epoch: \", epoch)\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            \n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            if batch_num == num_batches_per_epoch-1:\n",
    "                yield shuffled_data[start_index:end_index], True\n",
    "            else:\n",
    "                yield shuffled_data[start_index:end_index], False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pre-train word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_embeddings = np.load('./data/embed_tweets_en_590M_52D_data/en_word2vec_52_paper.npy')\n",
    "original_word_dict = {}\n",
    "with open('./data/embed_tweets_en_590M_52D_data/vocabulary_dict_52_paper.pickle', 'rb') as myfile:\n",
    "    original_word_dict = pickle.load(myfile)\n",
    "original_embeddings = np.load('./data/embed_tweets_en_590M_52D_data/en_word2vec_52_paper.npy.npy')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(original_word_dict.items()))\n",
    "print(original_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distant Supervision phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = ['./data/distant_data/sad_processed',\n",
    "        './data/distant_data/smile_processed']\n",
    "nums = {'./data/distant_data/smile_processed': 660000}\n",
    "x_train_distance, y_train, x_test_distance, y_test = load_without_pandas(files, nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "dropped  14\n"
     ]
    }
   ],
   "source": [
    "# Tokenize, get all seen words, get max length\n",
    "x_train_distance, x_test_distance, all_words, max_length = tokenize_tweet(x_train_distance, x_test_distance, original_word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transform tokens into indices in word embedding\n",
    "x_train_distance, x_test_distance = process_tweet(x_train_distance, x_test_distance,\n",
    "                                                  original_word_dict, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x_train_distance))\n",
    "print(len(x_test_distance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 65)\n",
      "(?, 65, 52, 1)\n",
      "CNN filter (4, 52, 1, 200)\n",
      "CNN filter (3, 1, 200, 200)\n",
      "(?, 28, 1, 200)\n",
      "h_pool_flat (?, 200)\n",
      "scores (?, 3)\n",
      "predictions (?,)\n",
      "losses (?,)\n"
     ]
    }
   ],
   "source": [
    "# put Word2Vec on 590 million English Tweets using 52 dimensions.\n",
    "vocabulary_size = original_embeddings.shape[0]\n",
    "embedding_size = original_embeddings.shape[1]  # Dimension of the embedding vector.\n",
    "graph = tf.Graph()\n",
    "\n",
    "sequence_length=x_train_distance.shape[1]\n",
    "num_classes=3\n",
    "\n",
    "# filter_sizes: The number of words we want our convolutional filters to cover. \n",
    "# We will have num_filters for each size specified here. \n",
    "# For example, [3, 4, 5] means that we will have filters that slide over 3, 4 and 5 words respectively, for a total of 3 * num_filters filters.\n",
    "first_filter_sizes = [4]\n",
    "first_pool_window_sizes = [4]\n",
    "first_pool_strides = [2]\n",
    "\n",
    "\n",
    "second_filter_window_sizes = [3]\n",
    "num_filters = 200\n",
    "\n",
    "# No L2 norm\n",
    "l2_reg_lambda=0.0\n",
    "\n",
    "with graph.as_default():\n",
    "    input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "    input_y = tf.placeholder(tf.int64, [None], name=\"input_y\")\n",
    "#     with tf.device('/cpu:0'):\n",
    "    embeddings_words = tf.Variable(tf.constant(0.0, shape=[vocabulary_size, embedding_size]),\n",
    "                     name=\"embedding_words\")\n",
    "    embedding_padding = tf.Variable(tf.constant(0.0, shape=[1, embedding_size]),\n",
    "                     trainable = False, name=\"embedding_padding\")\n",
    "\n",
    "    embedding_placeholder = tf.placeholder(tf.float32, [vocabulary_size, embedding_size], name='word_embedding_placeholder')\n",
    "    embedding_init = embeddings_words.assign(embedding_placeholder)  # assign exist word embeddings\n",
    "\n",
    "    embeddings = tf.concat([embeddings_words, embedding_padding], 0, name = 'embedding')\n",
    "\n",
    "    embedded_chars = tf.nn.embedding_lookup(embeddings, input_x)\n",
    "    embedded_chars_expanded = tf.expand_dims(embedded_chars, -1)   \n",
    "    print(input_x.shape)\n",
    "    print(embedded_chars_expanded.shape)\n",
    "    # Keeping track of l2 regularization loss (optional)\n",
    "    l2_loss = tf.constant(0.0)\n",
    "    \n",
    "     # Add variable initializer.\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    \n",
    "    # Create first cnn : a convolution + maxpool layer for each filter size    \n",
    "    # 1st Convolution Layer\n",
    "    for i, first_filter_size in enumerate(first_filter_sizes):\n",
    "        with tf.name_scope(\"conv-maxpool-1\"):\n",
    "            # Convolution Layer\n",
    "            filter_shape = [first_filter_size, embedding_size, 1, num_filters]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "            print(\"CNN filter\", W.shape)\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "            conv = tf.nn.conv2d(\n",
    "                embedded_chars_expanded,\n",
    "                W,\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding=\"VALID\",\n",
    "                name=\"conv\")\n",
    "            # Apply nonlinearity\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "\n",
    "            pooled = tf.nn.max_pool(\n",
    "                h,\n",
    "                ksize=[1, first_pool_window_sizes[i], 1, 1],\n",
    "                strides=[1, first_pool_strides[i], 1, 1],\n",
    "                padding='VALID',\n",
    "                name=\"pool\")\n",
    "\n",
    "    for i, second_filter_size in enumerate(second_filter_window_sizes):\n",
    "        with tf.name_scope(\"conv-maxpool-2\"):\n",
    "            # Convolution Layer\n",
    "            filter_shape = [second_filter_size, 1, num_filters, num_filters]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "            print(\"CNN filter\", W.shape)\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "            conv = tf.nn.conv2d(\n",
    "                pooled,\n",
    "                W,\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding=\"VALID\",\n",
    "                name=\"conv\")\n",
    "            # Apply nonlinearity\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "            print(h.shape)\n",
    "            # Maxpooling over the outputs and transform for next layer, so the \"channel\" of convolution \n",
    "            # will become \"input_width\" for next layer\n",
    "            pooled = tf.nn.max_pool(\n",
    "                h,\n",
    "                ksize=[1, h.shape[1], 1, 1],\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding='VALID',\n",
    "                name=\"pool\")\n",
    " \n",
    "\n",
    "    h_pool_flat = tf.reshape(pooled, [-1, num_filters])  # flatten pooling layers\n",
    "    print(\"h_pool_flat\", h_pool_flat.shape)\n",
    "    \n",
    "    # Add dropout\n",
    "#     with tf.name_scope(\"dropout\"):\n",
    "#         self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "    # Final (unnormalized) scores and predictions\n",
    "    \n",
    "    # Fully connected hidden layer\n",
    "    with tf.name_scope(\"hidden\"):\n",
    "        with tf.variable_scope(\"hidden\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters, num_filters],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            out = tf.nn.relu(tf.nn.xw_plus_b(h_pool_flat, W, b))\n",
    "        \n",
    "    \n",
    "    with tf.name_scope(\"output\"):\n",
    "        with tf.variable_scope(\"output\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            scores = tf.nn.xw_plus_b(out, W, b, name=\"scores\")\n",
    "            print(\"scores\", scores.shape)\n",
    "            predictions = tf.argmax(scores, 1, name=\"predictions\")\n",
    "            print(\"predictions\", predictions.shape)\n",
    "\n",
    "\n",
    "    # Calculate mean cross-entropy loss\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=scores, labels=input_y)\n",
    "        print(\"losses\", losses.shape)\n",
    "        loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "    # Accuracy\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        correct_predictions = tf.equal(predictions, input_y)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.12049593762675714&quot;).pbtxt = 'node {\\n  name: &quot;input_x&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: -1\\n        }\\n        dim {\\n          size: 65\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;input_y&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: -1\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 6140853\\n          }\\n          dim {\\n            size: 52\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;embedding_words&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 6140853\\n        }\\n        dim {\\n          size: 52\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;embedding_words/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;embedding_words&quot;\\n  input: &quot;Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding_words&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;embedding_words/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;embedding_words&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding_words&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Const_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n          dim {\\n            size: 52\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;embedding_padding&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 1\\n        }\\n        dim {\\n          size: 52\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;embedding_padding/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;embedding_padding&quot;\\n  input: &quot;Const_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding_padding&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;embedding_padding/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;embedding_padding&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding_padding&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;word_embedding_placeholder&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 6140853\\n        }\\n        dim {\\n          size: 52\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;embedding_words&quot;\\n  input: &quot;word_embedding_placeholder&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding_words&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;embedding/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;embedding&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;embedding_words/read&quot;\\n  input: &quot;embedding_padding/read&quot;\\n  input: &quot;embedding/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;embedding_lookup&quot;\\n  op: &quot;Gather&quot;\\n  input: &quot;embedding&quot;\\n  input: &quot;input_x&quot;\\n  attr {\\n    key: &quot;Tindices&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tparams&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_indices&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ExpandDims/dim&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: -1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ExpandDims&quot;\\n  op: &quot;ExpandDims&quot;\\n  input: &quot;embedding_lookup&quot;\\n  input: &quot;ExpandDims/dim&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tdim&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Const_2&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;init&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^embedding_words/Assign&quot;\\n  input: &quot;^embedding_padding/Assign&quot;\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/truncated_normal/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\004\\\\000\\\\000\\\\0004\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\310\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/truncated_normal/mean&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/truncated_normal/stddev&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.10000000149011612\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/truncated_normal/TruncatedNormal&quot;\\n  op: &quot;TruncatedNormal&quot;\\n  input: &quot;conv-maxpool-1/truncated_normal/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/truncated_normal/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;conv-maxpool-1/truncated_normal/TruncatedNormal&quot;\\n  input: &quot;conv-maxpool-1/truncated_normal/stddev&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/truncated_normal&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;conv-maxpool-1/truncated_normal/mul&quot;\\n  input: &quot;conv-maxpool-1/truncated_normal/mean&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/W&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 4\\n        }\\n        dim {\\n          size: 52\\n        }\\n        dim {\\n          size: 1\\n        }\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/W/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;conv-maxpool-1/W&quot;\\n  input: &quot;conv-maxpool-1/truncated_normal&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-1/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/W/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;conv-maxpool-1/W&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-1/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 200\\n          }\\n        }\\n        float_val: 0.10000000149011612\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/b&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/b/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;conv-maxpool-1/b&quot;\\n  input: &quot;conv-maxpool-1/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-1/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/b/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;conv-maxpool-1/b&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-1/b&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/conv&quot;\\n  op: &quot;Conv2D&quot;\\n  input: &quot;ExpandDims&quot;\\n  input: &quot;conv-maxpool-1/W/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;VALID&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;conv-maxpool-1/conv&quot;\\n  input: &quot;conv-maxpool-1/b/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;conv-maxpool-1/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/pool&quot;\\n  op: &quot;MaxPool&quot;\\n  input: &quot;conv-maxpool-1/relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;ksize&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 4\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;VALID&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 2\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/truncated_normal/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\003\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\310\\\\000\\\\000\\\\000\\\\310\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/truncated_normal/mean&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/truncated_normal/stddev&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.10000000149011612\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/truncated_normal/TruncatedNormal&quot;\\n  op: &quot;TruncatedNormal&quot;\\n  input: &quot;conv-maxpool-2/truncated_normal/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/truncated_normal/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;conv-maxpool-2/truncated_normal/TruncatedNormal&quot;\\n  input: &quot;conv-maxpool-2/truncated_normal/stddev&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/truncated_normal&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;conv-maxpool-2/truncated_normal/mul&quot;\\n  input: &quot;conv-maxpool-2/truncated_normal/mean&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/W&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 3\\n        }\\n        dim {\\n          size: 1\\n        }\\n        dim {\\n          size: 200\\n        }\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/W/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;conv-maxpool-2/W&quot;\\n  input: &quot;conv-maxpool-2/truncated_normal&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-2/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/W/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;conv-maxpool-2/W&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-2/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 200\\n          }\\n        }\\n        float_val: 0.10000000149011612\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/b&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/b/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;conv-maxpool-2/b&quot;\\n  input: &quot;conv-maxpool-2/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-2/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/b/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;conv-maxpool-2/b&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-2/b&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/conv&quot;\\n  op: &quot;Conv2D&quot;\\n  input: &quot;conv-maxpool-1/pool&quot;\\n  input: &quot;conv-maxpool-2/W/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;VALID&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;conv-maxpool-2/conv&quot;\\n  input: &quot;conv-maxpool-2/b/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;conv-maxpool-2/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/pool&quot;\\n  op: &quot;MaxPool&quot;\\n  input: &quot;conv-maxpool-2/relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;ksize&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 28\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;VALID&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\310\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;conv-maxpool-2/pool&quot;\\n  input: &quot;Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\310\\\\000\\\\000\\\\000\\\\310\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.12247448414564133\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.12247448414564133\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform/max&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform/mul&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 200\\n        }\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden/W&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden/W&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 200\\n          }\\n        }\\n        float_val: 0.10000000149011612\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/b&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/b/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden/hidden/b&quot;\\n  input: &quot;hidden/hidden/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/hidden/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/b/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden/hidden/b&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/hidden/b&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/L2Loss&quot;\\n  op: &quot;L2Loss&quot;\\n  input: &quot;hidden/W/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Const_2&quot;\\n  input: &quot;hidden/hidden/L2Loss&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/L2Loss_1&quot;\\n  op: &quot;L2Loss&quot;\\n  input: &quot;hidden/hidden/b/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/add_1&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden/hidden/add&quot;\\n  input: &quot;hidden/hidden/L2Loss_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/xw_plus_b/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Reshape&quot;\\n  input: &quot;hidden/W/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/xw_plus_b&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;hidden/hidden/xw_plus_b/MatMul&quot;\\n  input: &quot;hidden/hidden/b/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;hidden/hidden/xw_plus_b&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\310\\\\000\\\\000\\\\000\\\\003\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.17192047834396362\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.17192047834396362\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;output/W/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;output/W/Initializer/random_uniform/max&quot;\\n  input: &quot;output/W/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;output/W/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;output/W/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;output/W/Initializer/random_uniform/mul&quot;\\n  input: &quot;output/W/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 200\\n        }\\n        dim {\\n          size: 3\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;output/W&quot;\\n  input: &quot;output/W/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;output/W&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        float_val: 0.10000000149011612\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/b&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 3\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/b/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;output/output/b&quot;\\n  input: &quot;output/output/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/output/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/b/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;output/output/b&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/output/b&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/L2Loss&quot;\\n  op: &quot;L2Loss&quot;\\n  input: &quot;output/W/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden/hidden/add_1&quot;\\n  input: &quot;output/output/L2Loss&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/L2Loss_1&quot;\\n  op: &quot;L2Loss&quot;\\n  input: &quot;output/output/b/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/add_1&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;output/output/add&quot;\\n  input: &quot;output/output/L2Loss_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/scores/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;hidden/hidden/Relu&quot;\\n  input: &quot;output/W/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/scores&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;output/output/scores/MatMul&quot;\\n  input: &quot;output/output/b/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/predictions/dimension&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/predictions&quot;\\n  op: &quot;ArgMax&quot;\\n  input: &quot;output/output/scores&quot;\\n  input: &quot;output/output/predictions/dimension&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;output_type&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;input_y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  op: &quot;SparseSoftmaxCrossEntropyWithLogits&quot;\\n  input: &quot;output/output/scores&quot;\\n  input: &quot;input_y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tlabels&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/Mean&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  input: &quot;loss/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/mul/x&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;loss/mul/x&quot;\\n  input: &quot;output/output/add_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;loss/Mean&quot;\\n  input: &quot;loss/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;accuracy/Equal&quot;\\n  op: &quot;Equal&quot;\\n  input: &quot;output/output/predictions&quot;\\n  input: &quot;input_y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;accuracy/Cast&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;accuracy/Equal&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_BOOL\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;accuracy/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;accuracy/accuracy&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;accuracy/Cast&quot;\\n  input: &quot;accuracy/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nversions {\\n  producer: 24\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.12049593762675714&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_graph(graph.as_graph_def())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /home/phejimlin/Documents/Machine-learning/Milestone2/runs/1511536236\n",
      "\n",
      "Current epoch:  0\n",
      "Current epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phejimlin/anaconda3/envs/tensorflow_3.5/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-24T23:13:57.817809: step 5, loss 0.945578, acc 0.551758, f1 0.392377\n",
      "2017-11-24T23:13:58.860637: step 10, loss 0.857497, acc 0.536133, f1 0.374236\n",
      "2017-11-24T23:13:59.883633: step 15, loss 0.79628, acc 0.582031, f1 0.42826\n",
      "2017-11-24T23:14:00.932715: step 20, loss 0.77936, acc 0.546875, f1 0.386679\n",
      "2017-11-24T23:14:01.925324: step 25, loss 0.759641, acc 0.549805, f1 0.390095\n",
      "2017-11-24T23:14:02.928441: step 30, loss 0.745595, acc 0.552734, f1 0.393519\n",
      "2017-11-24T23:14:03.938161: step 35, loss 0.736969, acc 0.549805, f1 0.390095\n",
      "2017-11-24T23:14:04.953323: step 40, loss 0.731581, acc 0.539062, f1 0.377617\n",
      "2017-11-24T23:14:05.943063: step 45, loss 0.722626, acc 0.550781, f1 0.391235\n",
      "2017-11-24T23:14:06.990418: step 50, loss 0.718983, acc 0.547852, f1 0.387817\n",
      "2017-11-24T23:14:08.018860: step 55, loss 0.716914, acc 0.539062, f1 0.377617\n",
      "2017-11-24T23:14:09.014740: step 60, loss 0.712025, acc 0.545898, f1 0.385543\n",
      "2017-11-24T23:14:10.057073: step 65, loss 0.699907, acc 0.585938, f1 0.432959\n",
      "2017-11-24T23:14:11.055889: step 70, loss 0.708683, acc 0.541016, f1 0.379877\n",
      "2017-11-24T23:14:12.103950: step 75, loss 0.698914, acc 0.570312, f1 0.414257\n",
      "2017-11-24T23:14:13.122974: step 80, loss 0.702481, acc 0.549805, f1 0.390095\n",
      "2017-11-24T23:14:14.147752: step 85, loss 0.702336, acc 0.541992, f1 0.381008\n",
      "2017-11-24T23:14:15.180622: step 90, loss 0.695143, acc 0.570312, f1 0.414257\n",
      "2017-11-24T23:14:16.184895: step 95, loss 0.696262, acc 0.55957, f1 0.401545\n",
      "2017-11-24T23:14:17.215987: step 100, loss 0.695299, acc 0.55957, f1 0.401545\n",
      "2017-11-24T23:14:18.221453: step 105, loss 0.694442, acc 0.5625, f1 0.405\n",
      "2017-11-24T23:14:19.223152: step 110, loss 0.697332, acc 0.544922, f1 0.384408\n",
      "2017-11-24T23:14:20.252998: step 115, loss 0.701871, acc 0.527344, f1 0.36415\n",
      "2017-11-24T23:14:21.233172: step 120, loss 0.69389, acc 0.552734, f1 0.393519\n",
      "2017-11-24T23:14:22.252030: step 125, loss 0.69032, acc 0.56543, f1 0.408464\n",
      "2017-11-24T23:14:23.297560: step 130, loss 0.688637, acc 0.571289, f1 0.415418\n",
      "2017-11-24T23:14:24.324377: step 135, loss 0.693475, acc 0.550781, f1 0.391235\n",
      "2017-11-24T23:14:25.317470: step 140, loss 0.696771, acc 0.537109, f1 0.375362\n",
      "2017-11-24T23:14:26.322701: step 145, loss 0.693788, acc 0.541992, f1 0.381008\n",
      "2017-11-24T23:14:27.332998: step 150, loss 0.702952, acc 0.511719, f1 0.346435\n",
      "2017-11-24T23:14:28.319678: step 155, loss 0.684939, acc 0.584961, f1 0.431783\n",
      "2017-11-24T23:14:29.353911: step 160, loss 0.693487, acc 0.541016, f1 0.379877\n",
      "2017-11-24T23:14:30.347601: step 165, loss 0.69152, acc 0.549805, f1 0.390095\n",
      "2017-11-24T23:14:31.345136: step 170, loss 0.693984, acc 0.536133, f1 0.374236\n",
      "2017-11-24T23:14:32.357988: step 175, loss 0.690734, acc 0.549805, f1 0.390095\n",
      "2017-11-24T23:14:33.362637: step 180, loss 0.689955, acc 0.552734, f1 0.393519\n",
      "2017-11-24T23:14:34.392355: step 185, loss 0.690545, acc 0.549805, f1 0.390095\n",
      "2017-11-24T23:14:35.380630: step 190, loss 0.688941, acc 0.557617, f1 0.399247\n",
      "2017-11-24T23:14:36.346889: step 195, loss 0.689921, acc 0.550781, f1 0.391235\n",
      "2017-11-24T23:14:37.317064: step 200, loss 0.693508, acc 0.540039, f1 0.378746\n",
      "2017-11-24T23:14:38.283329: step 205, loss 0.693223, acc 0.53418, f1 0.371988\n",
      "2017-11-24T23:14:39.286069: step 210, loss 0.683466, acc 0.579102, f1 0.424746\n",
      "2017-11-24T23:14:40.265579: step 215, loss 0.691912, acc 0.537109, f1 0.375362\n",
      "2017-11-24T23:14:41.220440: step 220, loss 0.69284, acc 0.544922, f1 0.384408\n",
      "2017-11-24T23:14:42.228337: step 225, loss 0.691449, acc 0.552734, f1 0.393519\n",
      "2017-11-24T23:14:43.214762: step 230, loss 0.687993, acc 0.556641, f1 0.398099\n",
      "2017-11-24T23:14:44.226233: step 235, loss 0.686483, acc 0.569336, f1 0.413096\n",
      "2017-11-24T23:14:45.237201: step 240, loss 0.690703, acc 0.545898, f1 0.385543\n",
      "2017-11-24T23:14:46.263615: step 245, loss 0.692388, acc 0.530273, f1 0.367503\n",
      "2017-11-24T23:14:47.266682: step 250, loss 0.683364, acc 0.573242, f1 0.417744\n",
      "2017-11-24T23:14:48.272416: step 255, loss 0.69031, acc 0.550781, f1 0.391235\n",
      "2017-11-24T23:14:49.269446: step 260, loss 0.690833, acc 0.546875, f1 0.386679\n",
      "2017-11-24T23:14:50.248186: step 265, loss 0.689076, acc 0.55957, f1 0.401545\n",
      "2017-11-24T23:14:51.258564: step 270, loss 0.684855, acc 0.568359, f1 0.411937\n",
      "2017-11-24T23:14:52.301716: step 275, loss 0.689306, acc 0.569336, f1 0.413096\n",
      "2017-11-24T23:14:53.331213: step 280, loss 0.691939, acc 0.543945, f1 0.383273\n",
      "2017-11-24T23:14:54.336905: step 285, loss 0.685068, acc 0.568359, f1 0.411937\n",
      "2017-11-24T23:14:55.359128: step 290, loss 0.681859, acc 0.577148, f1 0.422408\n",
      "2017-11-24T23:14:56.398051: step 295, loss 0.690338, acc 0.544922, f1 0.384408\n",
      "2017-11-24T23:14:57.422862: step 300, loss 0.687755, acc 0.561523, f1 0.403847\n",
      "2017-11-24T23:14:58.417514: step 305, loss 0.691642, acc 0.53418, f1 0.371988\n",
      "2017-11-24T23:14:59.403486: step 310, loss 0.69458, acc 0.532227, f1 0.369743\n",
      "2017-11-24T23:15:00.381106: step 315, loss 0.687911, acc 0.556641, f1 0.398099\n",
      "2017-11-24T23:15:01.388965: step 320, loss 0.691095, acc 0.541016, f1 0.379877\n",
      "2017-11-24T23:15:02.406501: step 325, loss 0.687669, acc 0.554688, f1 0.395807\n",
      "2017-11-24T23:15:03.432300: step 330, loss 0.686467, acc 0.561523, f1 0.403847\n",
      "2017-11-24T23:15:04.451708: step 335, loss 0.671441, acc 0.612305, f1 0.46507\n",
      "2017-11-24T23:15:05.446970: step 340, loss 0.685756, acc 0.5625, f1 0.405\n",
      "2017-11-24T23:15:06.478259: step 345, loss 0.694378, acc 0.541992, f1 0.381008\n",
      "2017-11-24T23:15:07.495915: step 350, loss 0.689503, acc 0.552734, f1 0.393519\n",
      "2017-11-24T23:15:08.525576: step 355, loss 0.686182, acc 0.561523, f1 0.403847\n",
      "2017-11-24T23:15:09.572089: step 360, loss 0.684177, acc 0.568359, f1 0.411937\n",
      "2017-11-24T23:15:10.596195: step 365, loss 0.692892, acc 0.546875, f1 0.386679\n",
      "2017-11-24T23:15:11.635752: step 370, loss 0.671071, acc 0.605469, f1 0.45668\n",
      "2017-11-24T23:15:12.659756: step 375, loss 0.681506, acc 0.581055, f1 0.427088\n",
      "2017-11-24T23:15:13.684859: step 380, loss 0.683796, acc 0.579102, f1 0.424746\n",
      "2017-11-24T23:15:14.724576: step 385, loss 0.686354, acc 0.566406, f1 0.40962\n",
      "2017-11-24T23:15:15.731227: step 390, loss 0.691181, acc 0.545898, f1 0.385543\n",
      "2017-11-24T23:15:16.740983: step 395, loss 0.690094, acc 0.541016, f1 0.379877\n",
      "2017-11-24T23:15:17.763188: step 400, loss 0.684917, acc 0.56543, f1 0.408464\n",
      "2017-11-24T23:15:18.775991: step 405, loss 0.688173, acc 0.552734, f1 0.393519\n",
      "2017-11-24T23:15:19.730236: step 410, loss 0.689865, acc 0.544922, f1 0.384408\n",
      "2017-11-24T23:15:20.759289: step 415, loss 0.682889, acc 0.591797, f1 0.440035\n",
      "2017-11-24T23:15:21.786167: step 420, loss 0.689832, acc 0.550781, f1 0.391235\n",
      "2017-11-24T23:15:22.774309: step 425, loss 0.687183, acc 0.555664, f1 0.396953\n",
      "2017-11-24T23:15:23.785948: step 430, loss 0.689149, acc 0.551758, f1 0.392377\n",
      "2017-11-24T23:15:24.815189: step 435, loss 0.678842, acc 0.595703, f1 0.444772\n",
      "2017-11-24T23:15:25.812303: step 440, loss 0.68882, acc 0.547852, f1 0.387817\n",
      "2017-11-24T23:15:26.831648: step 445, loss 0.690634, acc 0.537109, f1 0.375362\n",
      "2017-11-24T23:15:27.852212: step 450, loss 0.691448, acc 0.536133, f1 0.374236\n",
      "2017-11-24T23:15:28.882678: step 455, loss 0.690114, acc 0.541992, f1 0.381008\n",
      "2017-11-24T23:15:29.888889: step 460, loss 0.682898, acc 0.572266, f1 0.416581\n",
      "2017-11-24T23:15:30.918450: step 465, loss 0.687616, acc 0.55957, f1 0.401545\n",
      "2017-11-24T23:15:31.919675: step 470, loss 0.689513, acc 0.546875, f1 0.386679\n",
      "2017-11-24T23:15:32.929740: step 475, loss 0.685104, acc 0.56543, f1 0.408464\n",
      "2017-11-24T23:15:33.944984: step 480, loss 0.69026, acc 0.540039, f1 0.378746\n",
      "2017-11-24T23:15:34.947271: step 485, loss 0.691297, acc 0.549805, f1 0.390095\n",
      "2017-11-24T23:15:35.950907: step 490, loss 0.690534, acc 0.543945, f1 0.383273\n",
      "2017-11-24T23:15:36.960062: step 495, loss 0.688745, acc 0.549805, f1 0.390095\n",
      "2017-11-24T23:15:37.941431: step 500, loss 0.692453, acc 0.536133, f1 0.374236\n",
      "2017-11-24T23:15:38.950256: step 505, loss 0.689502, acc 0.544922, f1 0.384408\n",
      "2017-11-24T23:15:39.959989: step 510, loss 0.693957, acc 0.520508, f1 0.356366\n",
      "2017-11-24T23:15:40.951606: step 515, loss 0.692773, acc 0.529297, f1 0.366384\n",
      "2017-11-24T23:15:41.966234: step 520, loss 0.690262, acc 0.541992, f1 0.381008\n",
      "2017-11-24T23:15:42.966097: step 525, loss 0.686036, acc 0.561523, f1 0.403847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-24T23:15:44.002134: step 530, loss 0.683335, acc 0.575195, f1 0.420074\n",
      "2017-11-24T23:15:45.036995: step 535, loss 0.686567, acc 0.558594, f1 0.400396\n",
      "2017-11-24T23:15:46.088265: step 540, loss 0.691514, acc 0.542969, f1 0.38214\n",
      "2017-11-24T23:15:47.098133: step 545, loss 0.685988, acc 0.560547, f1 0.402696\n",
      "2017-11-24T23:15:48.110067: step 550, loss 0.686262, acc 0.560547, f1 0.402696\n",
      "2017-11-24T23:15:49.119290: step 555, loss 0.683241, acc 0.575195, f1 0.420074\n",
      "2017-11-24T23:15:50.135382: step 560, loss 0.688331, acc 0.549805, f1 0.390095\n",
      "2017-11-24T23:15:51.131846: step 565, loss 0.685112, acc 0.569336, f1 0.413096\n",
      "2017-11-24T23:15:52.123795: step 570, loss 0.687511, acc 0.564453, f1 0.407308\n",
      "2017-11-24T23:15:53.126233: step 575, loss 0.688664, acc 0.548828, f1 0.388955\n",
      "2017-11-24T23:15:54.087114: step 580, loss 0.684223, acc 0.567383, f1 0.410778\n",
      "2017-11-24T23:15:55.087711: step 585, loss 0.684052, acc 0.569336, f1 0.413096\n",
      "2017-11-24T23:15:56.119511: step 590, loss 0.689133, acc 0.547852, f1 0.387817\n",
      "2017-11-24T23:15:57.109298: step 595, loss 0.704285, acc 0.521484, f1 0.357475\n",
      "2017-11-24T23:15:58.121539: step 600, loss 0.689021, acc 0.546875, f1 0.386679\n",
      "2017-11-24T23:15:59.140120: step 605, loss 0.686645, acc 0.557617, f1 0.399247\n",
      "2017-11-24T23:16:00.172197: step 610, loss 0.680611, acc 0.583984, f1 0.430607\n",
      "2017-11-24T23:16:01.190723: step 615, loss 0.69091, acc 0.542969, f1 0.38214\n",
      "2017-11-24T23:16:02.205655: step 620, loss 0.684251, acc 0.567383, f1 0.410778\n",
      "2017-11-24T23:16:03.254320: step 625, loss 0.685291, acc 0.56543, f1 0.408464\n",
      "2017-11-24T23:16:04.265928: step 630, loss 0.687682, acc 0.552734, f1 0.393519\n",
      "2017-11-24T23:16:05.271811: step 635, loss 0.688801, acc 0.555664, f1 0.396953\n",
      "2017-11-24T23:16:06.263368: step 640, loss 0.686125, acc 0.560547, f1 0.402696\n",
      "2017-11-24T23:16:07.289077: step 645, loss 0.688417, acc 0.549805, f1 0.390095\n",
      "2017-11-24T23:16:08.305560: step 650, loss 0.686595, acc 0.557617, f1 0.399247\n",
      "2017-11-24T23:16:09.276441: step 655, loss 0.683876, acc 0.568359, f1 0.411937\n",
      "2017-11-24T23:16:10.255721: step 660, loss 0.68786, acc 0.553711, f1 0.394663\n",
      "2017-11-24T23:16:11.273892: step 665, loss 0.689019, acc 0.547852, f1 0.387817\n",
      "2017-11-24T23:16:12.278871: step 670, loss 0.68605, acc 0.560547, f1 0.402696\n",
      "2017-11-24T23:16:13.286606: step 675, loss 0.693878, acc 0.529297, f1 0.366384\n",
      "2017-11-24T23:16:14.284601: step 680, loss 0.687949, acc 0.555664, f1 0.396953\n",
      "2017-11-24T23:16:15.294951: step 685, loss 0.687352, acc 0.556641, f1 0.398099\n",
      "2017-11-24T23:16:16.270026: step 690, loss 0.677572, acc 0.588867, f1 0.436493\n",
      "2017-11-24T23:16:17.235917: step 695, loss 0.686017, acc 0.570312, f1 0.414257\n",
      "2017-11-24T23:16:18.259525: step 700, loss 0.689332, acc 0.550781, f1 0.391235\n",
      "2017-11-24T23:16:19.244300: step 705, loss 0.68333, acc 0.570312, f1 0.414257\n",
      "2017-11-24T23:16:20.266176: step 710, loss 0.685652, acc 0.561523, f1 0.403847\n",
      "2017-11-24T23:16:21.264264: step 715, loss 0.689132, acc 0.549805, f1 0.390095\n",
      "2017-11-24T23:16:22.248419: step 720, loss 0.690321, acc 0.547852, f1 0.387817\n",
      "2017-11-24T23:16:23.216189: step 725, loss 0.687659, acc 0.560547, f1 0.402696\n",
      "2017-11-24T23:16:24.236711: step 730, loss 0.687521, acc 0.576172, f1 0.421241\n",
      "2017-11-24T23:16:25.250734: step 735, loss 0.685344, acc 0.564453, f1 0.407308\n",
      "2017-11-24T23:16:26.220700: step 740, loss 0.686346, acc 0.574219, f1 0.418909\n",
      "2017-11-24T23:16:27.232963: step 745, loss 0.686654, acc 0.557617, f1 0.399247\n",
      "2017-11-24T23:16:28.185262: step 750, loss 0.689799, acc 0.545898, f1 0.385543\n",
      "2017-11-24T23:16:29.204636: step 755, loss 0.692446, acc 0.520508, f1 0.356366\n",
      "2017-11-24T23:16:30.241611: step 760, loss 0.692321, acc 0.540039, f1 0.378746\n",
      "2017-11-24T23:16:31.269350: step 765, loss 0.688478, acc 0.548828, f1 0.388955\n",
      "2017-11-24T23:16:32.312233: step 770, loss 0.68452, acc 0.569336, f1 0.413096\n",
      "2017-11-24T23:16:33.300618: step 775, loss 0.686578, acc 0.557617, f1 0.399247\n",
      "2017-11-24T23:16:34.311455: step 780, loss 0.683883, acc 0.569336, f1 0.413096\n",
      "2017-11-24T23:16:35.309493: step 785, loss 0.684132, acc 0.574219, f1 0.418909\n",
      "2017-11-24T23:16:36.323893: step 790, loss 0.685783, acc 0.561523, f1 0.403847\n",
      "2017-11-24T23:16:37.357962: step 795, loss 0.687833, acc 0.553711, f1 0.394663\n",
      "2017-11-24T23:16:38.369606: step 800, loss 0.690718, acc 0.543945, f1 0.383273\n",
      "2017-11-24T23:16:39.375936: step 805, loss 0.691595, acc 0.536133, f1 0.374236\n",
      "2017-11-24T23:16:40.382495: step 810, loss 0.687482, acc 0.553711, f1 0.394663\n",
      "2017-11-24T23:16:41.357214: step 815, loss 0.68857, acc 0.548828, f1 0.388955\n",
      "2017-11-24T23:16:42.369969: step 820, loss 0.689608, acc 0.542969, f1 0.38214\n",
      "2017-11-24T23:16:43.388393: step 825, loss 0.6905, acc 0.540039, f1 0.378746\n",
      "2017-11-24T23:16:44.427438: step 830, loss 0.68703, acc 0.555664, f1 0.396953\n",
      "2017-11-24T23:16:45.445121: step 835, loss 0.688116, acc 0.553711, f1 0.394663\n",
      "2017-11-24T23:16:46.456832: step 840, loss 0.688928, acc 0.549805, f1 0.390095\n",
      "2017-11-24T23:16:47.441344: step 845, loss 0.689715, acc 0.543945, f1 0.383273\n",
      "2017-11-24T23:16:48.414427: step 850, loss 0.695256, acc 0.527344, f1 0.36415\n",
      "2017-11-24T23:16:49.451740: step 855, loss 0.685181, acc 0.563477, f1 0.406154\n",
      "2017-11-24T23:16:50.484472: step 860, loss 0.678179, acc 0.586914, f1 0.434136\n",
      "2017-11-24T23:16:51.475883: step 865, loss 0.686106, acc 0.55957, f1 0.401545\n",
      "2017-11-24T23:16:52.493892: step 870, loss 0.689197, acc 0.547852, f1 0.387817\n",
      "2017-11-24T23:16:53.524343: step 875, loss 0.690313, acc 0.538086, f1 0.376489\n",
      "2017-11-24T23:16:54.522583: step 880, loss 0.689543, acc 0.542969, f1 0.38214\n",
      "2017-11-24T23:16:55.487971: step 885, loss 0.694627, acc 0.537109, f1 0.375362\n",
      "2017-11-24T23:16:56.522557: step 890, loss 0.69348, acc 0.525391, f1 0.361921\n",
      "2017-11-24T23:16:57.493281: step 895, loss 0.684058, acc 0.569336, f1 0.413096\n",
      "2017-11-24T23:16:58.472163: step 900, loss 0.689079, acc 0.547852, f1 0.387817\n",
      "2017-11-24T23:16:59.456417: step 905, loss 0.687694, acc 0.552734, f1 0.393519\n",
      "2017-11-24T23:17:00.425674: step 910, loss 0.683124, acc 0.571289, f1 0.415418\n",
      "2017-11-24T23:17:01.425917: step 915, loss 0.68463, acc 0.56543, f1 0.408464\n",
      "2017-11-24T23:17:02.450762: step 920, loss 0.692162, acc 0.540039, f1 0.378746\n",
      "2017-11-24T23:17:03.438053: step 925, loss 0.689815, acc 0.541992, f1 0.381008\n",
      "2017-11-24T23:17:04.410178: step 930, loss 0.687689, acc 0.571289, f1 0.415418\n",
      "2017-11-24T23:17:05.463492: step 935, loss 0.686355, acc 0.558594, f1 0.400396\n",
      "2017-11-24T23:17:06.445649: step 940, loss 0.689751, acc 0.541992, f1 0.381008\n",
      "2017-11-24T23:17:07.443755: step 945, loss 0.68905, acc 0.551758, f1 0.392377\n",
      "2017-11-24T23:17:08.413367: step 950, loss 0.684598, acc 0.567383, f1 0.410778\n",
      "2017-11-24T23:17:09.462979: step 955, loss 0.685676, acc 0.5625, f1 0.405\n",
      "2017-11-24T23:17:10.439694: step 960, loss 0.690503, acc 0.537109, f1 0.375362\n",
      "2017-11-24T23:17:11.432918: step 965, loss 0.690586, acc 0.541016, f1 0.379877\n",
      "2017-11-24T23:17:12.465941: step 970, loss 0.688585, acc 0.549805, f1 0.390095\n",
      "2017-11-24T23:17:13.471341: step 975, loss 0.682379, acc 0.577148, f1 0.422408\n",
      "2017-11-24T23:17:14.461898: step 980, loss 0.686608, acc 0.557617, f1 0.399247\n",
      "2017-11-24T23:17:15.481382: step 985, loss 0.696115, acc 0.507812, f1 0.34205\n",
      "2017-11-24T23:17:16.459852: step 990, loss 0.688509, acc 0.551758, f1 0.392377\n",
      "2017-11-24T23:17:17.466133: step 995, loss 0.692133, acc 0.542969, f1 0.38214\n",
      "2017-11-24T23:17:18.467104: step 1000, loss 0.690418, acc 0.540039, f1 0.378746\n",
      "2017-11-24T23:17:19.477434: step 1005, loss 0.688016, acc 0.561523, f1 0.403847\n",
      "2017-11-24T23:17:20.475194: step 1010, loss 0.687042, acc 0.570312, f1 0.414257\n",
      "2017-11-24T23:17:21.467892: step 1015, loss 0.678034, acc 0.586914, f1 0.434136\n",
      "2017-11-24T23:17:22.421610: step 1020, loss 0.689651, acc 0.543945, f1 0.383273\n",
      "2017-11-24T23:17:23.400273: step 1025, loss 0.68449, acc 0.566406, f1 0.40962\n",
      "2017-11-24T23:17:24.405128: step 1030, loss 0.689772, acc 0.56543, f1 0.408464\n",
      "2017-11-24T23:17:25.408433: step 1035, loss 0.689585, acc 0.543945, f1 0.383273\n",
      "2017-11-24T23:17:26.403180: step 1040, loss 0.697219, acc 0.527344, f1 0.36415\n",
      "\n",
      "\n",
      "Current epoch:  1\n",
      "2017-11-24T23:17:27.941560: step 1045, loss 0.688987, acc 0.557617, f1 0.399247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-24T23:17:28.949553: step 1050, loss 0.692675, acc 0.522461, f1 0.358584\n",
      "2017-11-24T23:17:29.932113: step 1055, loss 0.68784, acc 0.551758, f1 0.392377\n",
      "2017-11-24T23:17:30.947214: step 1060, loss 0.684924, acc 0.566406, f1 0.40962\n",
      "2017-11-24T23:17:31.958546: step 1065, loss 0.679269, acc 0.59082, f1 0.438854\n",
      "2017-11-24T23:17:32.944868: step 1070, loss 0.687877, acc 0.551758, f1 0.392377\n",
      "2017-11-24T23:17:33.938864: step 1075, loss 0.69225, acc 0.536133, f1 0.374236\n",
      "2017-11-24T23:17:34.934703: step 1080, loss 0.688766, acc 0.552734, f1 0.393519\n",
      "2017-11-24T23:17:35.922550: step 1085, loss 0.683476, acc 0.576172, f1 0.421241\n",
      "2017-11-24T23:17:36.944628: step 1090, loss 0.688991, acc 0.545898, f1 0.385543\n",
      "2017-11-24T23:17:37.954736: step 1095, loss 0.691846, acc 0.52832, f1 0.365267\n",
      "2017-11-24T23:17:38.962156: step 1100, loss 0.689806, acc 0.546875, f1 0.386679\n",
      "2017-11-24T23:17:39.973259: step 1105, loss 0.688758, acc 0.550781, f1 0.391235\n",
      "2017-11-24T23:17:40.995556: step 1110, loss 0.6791, acc 0.583984, f1 0.430607\n",
      "2017-11-24T23:17:42.004539: step 1115, loss 0.684977, acc 0.56543, f1 0.408464\n",
      "2017-11-24T23:17:42.986505: step 1120, loss 0.681979, acc 0.575195, f1 0.420074\n",
      "2017-11-24T23:17:44.008971: step 1125, loss 0.679682, acc 0.585938, f1 0.432959\n",
      "2017-11-24T23:17:44.995075: step 1130, loss 0.687314, acc 0.554688, f1 0.395807\n",
      "2017-11-24T23:17:46.006305: step 1135, loss 0.684713, acc 0.575195, f1 0.420074\n",
      "2017-11-24T23:17:47.016067: step 1140, loss 0.690557, acc 0.539062, f1 0.377617\n",
      "2017-11-24T23:17:47.989975: step 1145, loss 0.686583, acc 0.558594, f1 0.400396\n",
      "2017-11-24T23:17:48.976508: step 1150, loss 0.700102, acc 0.525391, f1 0.361921\n",
      "2017-11-24T23:17:50.010730: step 1155, loss 0.691306, acc 0.553711, f1 0.394663\n",
      "2017-11-24T23:17:51.023130: step 1160, loss 0.688624, acc 0.547852, f1 0.387817\n",
      "2017-11-24T23:17:52.056564: step 1165, loss 0.688274, acc 0.549805, f1 0.390095\n",
      "2017-11-24T23:17:53.034028: step 1170, loss 0.68613, acc 0.561523, f1 0.403847\n",
      "2017-11-24T23:17:54.038904: step 1175, loss 0.691179, acc 0.547852, f1 0.387817\n",
      "2017-11-24T23:17:55.008460: step 1180, loss 0.692345, acc 0.525391, f1 0.361921\n",
      "2017-11-24T23:17:56.008154: step 1185, loss 0.687917, acc 0.551758, f1 0.392377\n",
      "2017-11-24T23:17:57.026132: step 1190, loss 0.686415, acc 0.561523, f1 0.403847\n",
      "2017-11-24T23:17:58.001145: step 1195, loss 0.691567, acc 0.543945, f1 0.383273\n",
      "2017-11-24T23:17:58.987145: step 1200, loss 0.688079, acc 0.552734, f1 0.393519\n",
      "2017-11-24T23:18:00.008618: step 1205, loss 0.687759, acc 0.552734, f1 0.393519\n",
      "2017-11-24T23:18:01.002191: step 1210, loss 0.68869, acc 0.547852, f1 0.387817\n",
      "2017-11-24T23:18:01.982782: step 1215, loss 0.690674, acc 0.548828, f1 0.388955\n",
      "2017-11-24T23:18:03.004890: step 1220, loss 0.687239, acc 0.554688, f1 0.395807\n",
      "2017-11-24T23:18:03.995397: step 1225, loss 0.682787, acc 0.572266, f1 0.416581\n",
      "2017-11-24T23:18:04.956529: step 1230, loss 0.685429, acc 0.567383, f1 0.410778\n",
      "2017-11-24T23:18:05.926577: step 1235, loss 0.698473, acc 0.521484, f1 0.357475\n",
      "2017-11-24T23:18:06.914307: step 1240, loss 0.693505, acc 0.539062, f1 0.377617\n",
      "2017-11-24T23:18:07.900694: step 1245, loss 0.687428, acc 0.560547, f1 0.402696\n",
      "2017-11-24T23:18:08.906894: step 1250, loss 0.691443, acc 0.539062, f1 0.377617\n",
      "2017-11-24T23:18:09.895861: step 1255, loss 0.68573, acc 0.561523, f1 0.403847\n",
      "2017-11-24T23:18:10.860171: step 1260, loss 0.686834, acc 0.560547, f1 0.402696\n",
      "2017-11-24T23:18:11.839817: step 1265, loss 0.685381, acc 0.5625, f1 0.405\n",
      "2017-11-24T23:18:12.863970: step 1270, loss 0.68601, acc 0.564453, f1 0.407308\n",
      "2017-11-24T23:18:13.866512: step 1275, loss 0.689244, acc 0.553711, f1 0.394663\n",
      "2017-11-24T23:18:14.839750: step 1280, loss 0.686304, acc 0.560547, f1 0.402696\n",
      "2017-11-24T23:18:15.869692: step 1285, loss 0.692046, acc 0.547852, f1 0.387817\n",
      "2017-11-24T23:18:16.878650: step 1290, loss 0.688259, acc 0.551758, f1 0.392377\n",
      "2017-11-24T23:18:17.845664: step 1295, loss 0.684399, acc 0.574219, f1 0.418909\n",
      "2017-11-24T23:18:18.855035: step 1300, loss 0.69043, acc 0.541992, f1 0.381008\n",
      "2017-11-24T23:18:19.830750: step 1305, loss 0.681347, acc 0.577148, f1 0.422408\n",
      "2017-11-24T23:18:20.847901: step 1310, loss 0.691256, acc 0.543945, f1 0.383273\n",
      "2017-11-24T23:18:21.882229: step 1315, loss 0.686988, acc 0.555664, f1 0.396953\n",
      "2017-11-24T23:18:22.879093: step 1320, loss 0.682206, acc 0.574219, f1 0.418909\n",
      "2017-11-24T23:18:23.854672: step 1325, loss 0.681965, acc 0.576172, f1 0.421241\n",
      "2017-11-24T23:18:24.840450: step 1330, loss 0.693875, acc 0.53125, f1 0.368622\n",
      "2017-11-24T23:18:25.824279: step 1335, loss 0.68275, acc 0.574219, f1 0.418909\n",
      "2017-11-24T23:18:26.820710: step 1340, loss 0.68355, acc 0.569336, f1 0.413096\n",
      "2017-11-24T23:18:27.836152: step 1345, loss 0.685847, acc 0.560547, f1 0.402696\n",
      "2017-11-24T23:18:28.831528: step 1350, loss 0.686767, acc 0.556641, f1 0.398099\n",
      "2017-11-24T23:18:29.816418: step 1355, loss 0.680022, acc 0.581055, f1 0.427088\n",
      "2017-11-24T23:18:30.814426: step 1360, loss 0.68438, acc 0.566406, f1 0.40962\n",
      "2017-11-24T23:18:31.796539: step 1365, loss 0.681878, acc 0.575195, f1 0.420074\n",
      "2017-11-24T23:18:32.757636: step 1370, loss 0.681481, acc 0.577148, f1 0.422408\n",
      "2017-11-24T23:18:33.725918: step 1375, loss 0.689236, acc 0.547852, f1 0.387817\n",
      "2017-11-24T23:18:34.710108: step 1380, loss 0.689375, acc 0.547852, f1 0.387817\n",
      "2017-11-24T23:18:35.681998: step 1385, loss 0.688685, acc 0.553711, f1 0.394663\n",
      "2017-11-24T23:18:36.618093: step 1390, loss 0.682666, acc 0.575195, f1 0.420074\n",
      "2017-11-24T23:18:37.578513: step 1395, loss 0.688105, acc 0.550781, f1 0.391235\n",
      "2017-11-24T23:18:38.554738: step 1400, loss 0.686099, acc 0.561523, f1 0.403847\n",
      "2017-11-24T23:18:39.518632: step 1405, loss 0.684296, acc 0.575195, f1 0.420074\n",
      "2017-11-24T23:18:40.487809: step 1410, loss 0.686865, acc 0.557617, f1 0.399247\n",
      "2017-11-24T23:18:41.460702: step 1415, loss 0.686651, acc 0.55957, f1 0.401545\n",
      "2017-11-24T23:18:42.423259: step 1420, loss 0.68824, acc 0.551758, f1 0.392377\n",
      "2017-11-24T23:18:43.371945: step 1425, loss 0.687494, acc 0.556641, f1 0.398099\n",
      "2017-11-24T23:18:44.345723: step 1430, loss 0.693122, acc 0.543945, f1 0.383273\n",
      "2017-11-24T23:18:45.293266: step 1435, loss 0.688643, acc 0.547852, f1 0.387817\n",
      "2017-11-24T23:18:46.254611: step 1440, loss 0.685609, acc 0.564453, f1 0.407308\n",
      "2017-11-24T23:18:47.227947: step 1445, loss 0.692931, acc 0.537109, f1 0.375362\n",
      "2017-11-24T23:18:48.209310: step 1450, loss 0.685107, acc 0.566406, f1 0.40962\n",
      "2017-11-24T23:18:49.181036: step 1455, loss 0.689356, acc 0.547852, f1 0.387817\n",
      "2017-11-24T23:18:50.125297: step 1460, loss 0.686411, acc 0.561523, f1 0.403847\n",
      "2017-11-24T23:18:51.113872: step 1465, loss 0.68649, acc 0.566406, f1 0.40962\n",
      "2017-11-24T23:18:52.075188: step 1470, loss 0.691313, acc 0.533203, f1 0.370865\n",
      "2017-11-24T23:18:53.029123: step 1475, loss 0.683052, acc 0.571289, f1 0.415418\n",
      "2017-11-24T23:18:53.966079: step 1480, loss 0.682255, acc 0.574219, f1 0.418909\n",
      "2017-11-24T23:18:54.889640: step 1485, loss 0.68428, acc 0.573242, f1 0.417744\n",
      "2017-11-24T23:18:55.855086: step 1490, loss 0.690939, acc 0.53418, f1 0.371988\n",
      "2017-11-24T23:18:56.812965: step 1495, loss 0.68817, acc 0.550781, f1 0.391235\n",
      "2017-11-24T23:18:57.783405: step 1500, loss 0.68677, acc 0.556641, f1 0.398099\n",
      "2017-11-24T23:18:58.749966: step 1505, loss 0.68683, acc 0.5625, f1 0.405\n",
      "2017-11-24T23:18:59.733221: step 1510, loss 0.687194, acc 0.555664, f1 0.396953\n",
      "2017-11-24T23:19:00.715391: step 1515, loss 0.682095, acc 0.576172, f1 0.421241\n",
      "2017-11-24T23:19:01.682200: step 1520, loss 0.689544, acc 0.544922, f1 0.384408\n",
      "2017-11-24T23:19:02.633650: step 1525, loss 0.689685, acc 0.544922, f1 0.384408\n",
      "2017-11-24T23:19:03.615021: step 1530, loss 0.689386, acc 0.555664, f1 0.396953\n",
      "2017-11-24T23:19:04.556313: step 1535, loss 0.687663, acc 0.5625, f1 0.405\n",
      "2017-11-24T23:19:05.497048: step 1540, loss 0.688204, acc 0.550781, f1 0.391235\n",
      "2017-11-24T23:19:06.475901: step 1545, loss 0.683967, acc 0.568359, f1 0.411937\n",
      "2017-11-24T23:19:07.414542: step 1550, loss 0.697343, acc 0.539062, f1 0.377617\n",
      "2017-11-24T23:19:08.359377: step 1555, loss 0.685544, acc 0.5625, f1 0.405\n",
      "2017-11-24T23:19:09.322936: step 1560, loss 0.687867, acc 0.555664, f1 0.396953\n",
      "2017-11-24T23:19:10.290161: step 1565, loss 0.684192, acc 0.568359, f1 0.411937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-24T23:19:11.244613: step 1570, loss 0.689324, acc 0.55957, f1 0.401545\n",
      "2017-11-24T23:19:12.191118: step 1575, loss 0.690896, acc 0.541992, f1 0.381008\n",
      "2017-11-24T23:19:13.158229: step 1580, loss 0.688412, acc 0.550781, f1 0.391235\n",
      "2017-11-24T23:19:14.087520: step 1585, loss 0.685302, acc 0.563477, f1 0.406154\n",
      "2017-11-24T23:19:15.013641: step 1590, loss 0.694084, acc 0.526367, f1 0.363035\n",
      "2017-11-24T23:19:15.954919: step 1595, loss 0.68218, acc 0.577148, f1 0.422408\n",
      "2017-11-24T23:19:16.881143: step 1600, loss 0.690367, acc 0.538086, f1 0.376489\n",
      "2017-11-24T23:19:17.830239: step 1605, loss 0.684209, acc 0.567383, f1 0.410778\n",
      "2017-11-24T23:19:18.792827: step 1610, loss 0.687981, acc 0.568359, f1 0.411937\n",
      "2017-11-24T23:19:19.786144: step 1615, loss 0.700465, acc 0.525391, f1 0.361921\n",
      "2017-11-24T23:19:20.732813: step 1620, loss 0.689044, acc 0.556641, f1 0.398099\n",
      "2017-11-24T23:19:21.711459: step 1625, loss 0.688741, acc 0.550781, f1 0.391235\n",
      "2017-11-24T23:19:22.671975: step 1630, loss 0.687831, acc 0.555664, f1 0.396953\n",
      "2017-11-24T23:19:23.603386: step 1635, loss 0.6855, acc 0.568359, f1 0.411937\n",
      "2017-11-24T23:19:24.541902: step 1640, loss 0.692462, acc 0.530273, f1 0.367503\n",
      "2017-11-24T23:19:25.520260: step 1645, loss 0.686882, acc 0.556641, f1 0.398099\n",
      "2017-11-24T23:19:26.474683: step 1650, loss 0.687127, acc 0.55957, f1 0.401545\n",
      "2017-11-24T23:19:27.442594: step 1655, loss 0.683048, acc 0.574219, f1 0.418909\n",
      "2017-11-24T23:19:28.413379: step 1660, loss 0.685746, acc 0.576172, f1 0.421241\n",
      "2017-11-24T23:19:29.382099: step 1665, loss 0.686677, acc 0.558594, f1 0.400396\n",
      "2017-11-24T23:19:30.349326: step 1670, loss 0.681542, acc 0.576172, f1 0.421241\n",
      "2017-11-24T23:19:31.304450: step 1675, loss 0.686172, acc 0.571289, f1 0.415418\n",
      "2017-11-24T23:19:32.270700: step 1680, loss 0.678556, acc 0.587891, f1 0.435314\n",
      "2017-11-24T23:19:33.210073: step 1685, loss 0.685255, acc 0.566406, f1 0.40962\n",
      "2017-11-24T23:19:34.170109: step 1690, loss 0.684372, acc 0.568359, f1 0.411937\n",
      "2017-11-24T23:19:35.132311: step 1695, loss 0.687164, acc 0.555664, f1 0.396953\n",
      "2017-11-24T23:19:36.102122: step 1700, loss 0.687228, acc 0.555664, f1 0.396953\n",
      "2017-11-24T23:19:37.045343: step 1705, loss 0.685927, acc 0.560547, f1 0.402696\n",
      "2017-11-24T23:19:38.013327: step 1710, loss 0.69797, acc 0.518555, f1 0.354151\n",
      "2017-11-24T23:19:38.975471: step 1715, loss 0.692233, acc 0.530273, f1 0.367503\n",
      "2017-11-24T23:19:39.925421: step 1720, loss 0.686363, acc 0.561523, f1 0.403847\n",
      "2017-11-24T23:19:40.899488: step 1725, loss 0.686847, acc 0.561523, f1 0.403847\n",
      "2017-11-24T23:19:41.888975: step 1730, loss 0.691681, acc 0.52832, f1 0.365267\n",
      "2017-11-24T23:19:42.830235: step 1735, loss 0.694064, acc 0.519531, f1 0.355258\n",
      "2017-11-24T23:19:43.762753: step 1740, loss 0.687948, acc 0.556641, f1 0.398099\n",
      "2017-11-24T23:19:44.706370: step 1745, loss 0.689159, acc 0.545898, f1 0.385543\n",
      "2017-11-24T23:19:45.652466: step 1750, loss 0.688081, acc 0.554688, f1 0.395807\n",
      "2017-11-24T23:19:46.607282: step 1755, loss 0.685473, acc 0.566406, f1 0.40962\n",
      "2017-11-24T23:19:47.583740: step 1760, loss 0.689197, acc 0.554688, f1 0.395807\n",
      "2017-11-24T23:19:48.571390: step 1765, loss 0.687414, acc 0.553711, f1 0.394663\n",
      "2017-11-24T23:19:49.526280: step 1770, loss 0.691694, acc 0.541992, f1 0.381008\n",
      "2017-11-24T23:19:50.475743: step 1775, loss 0.682651, acc 0.584961, f1 0.431783\n",
      "2017-11-24T23:19:51.450255: step 1780, loss 0.694733, acc 0.53125, f1 0.368622\n",
      "2017-11-24T23:19:52.418246: step 1785, loss 0.687622, acc 0.552734, f1 0.393519\n",
      "2017-11-24T23:19:53.384548: step 1790, loss 0.688386, acc 0.553711, f1 0.394663\n",
      "2017-11-24T23:19:54.357274: step 1795, loss 0.698, acc 0.517578, f1 0.353046\n",
      "2017-11-24T23:19:55.330190: step 1800, loss 0.686865, acc 0.55957, f1 0.401545\n",
      "2017-11-24T23:19:56.290815: step 1805, loss 0.688391, acc 0.550781, f1 0.391235\n",
      "2017-11-24T23:19:57.259995: step 1810, loss 0.687258, acc 0.556641, f1 0.398099\n",
      "2017-11-24T23:19:58.222175: step 1815, loss 0.690281, acc 0.539062, f1 0.377617\n",
      "2017-11-24T23:19:59.191663: step 1820, loss 0.693581, acc 0.536133, f1 0.374236\n",
      "2017-11-24T23:20:00.153209: step 1825, loss 0.688421, acc 0.548828, f1 0.388955\n",
      "2017-11-24T23:20:01.121677: step 1830, loss 0.686966, acc 0.557617, f1 0.399247\n",
      "2017-11-24T23:20:02.080356: step 1835, loss 0.685218, acc 0.564453, f1 0.407308\n",
      "2017-11-24T23:20:03.025167: step 1840, loss 0.690544, acc 0.550781, f1 0.391235\n",
      "2017-11-24T23:20:03.992194: step 1845, loss 0.687067, acc 0.555664, f1 0.396953\n",
      "2017-11-24T23:20:04.978892: step 1850, loss 0.687295, acc 0.554688, f1 0.395807\n",
      "2017-11-24T23:20:05.936123: step 1855, loss 0.689179, acc 0.545898, f1 0.385543\n",
      "2017-11-24T23:20:06.907364: step 1860, loss 0.689956, acc 0.556641, f1 0.398099\n",
      "2017-11-24T23:20:07.834882: step 1865, loss 0.692224, acc 0.533203, f1 0.370865\n",
      "2017-11-24T23:20:08.774172: step 1870, loss 0.690559, acc 0.538086, f1 0.376489\n",
      "2017-11-24T23:20:09.721108: step 1875, loss 0.68958, acc 0.55957, f1 0.401545\n",
      "2017-11-24T23:20:10.688251: step 1880, loss 0.687863, acc 0.552734, f1 0.393519\n",
      "2017-11-24T23:20:11.642775: step 1885, loss 0.685872, acc 0.561523, f1 0.403847\n",
      "2017-11-24T23:20:12.603045: step 1890, loss 0.686634, acc 0.561523, f1 0.403847\n",
      "2017-11-24T23:20:13.596180: step 1895, loss 0.683353, acc 0.571289, f1 0.415418\n",
      "2017-11-24T23:20:14.550833: step 1900, loss 0.687522, acc 0.554688, f1 0.395807\n",
      "2017-11-24T23:20:15.517117: step 1905, loss 0.692437, acc 0.536133, f1 0.374236\n",
      "2017-11-24T23:20:16.495467: step 1910, loss 0.684149, acc 0.567383, f1 0.410778\n",
      "2017-11-24T23:20:17.462202: step 1915, loss 0.684079, acc 0.567383, f1 0.410778\n",
      "2017-11-24T23:20:18.405840: step 1920, loss 0.689596, acc 0.543945, f1 0.383273\n",
      "2017-11-24T23:20:19.343808: step 1925, loss 0.689433, acc 0.544922, f1 0.384408\n",
      "2017-11-24T23:20:20.335401: step 1930, loss 0.689427, acc 0.543945, f1 0.383273\n",
      "2017-11-24T23:20:21.303659: step 1935, loss 0.688231, acc 0.549805, f1 0.390095\n",
      "2017-11-24T23:20:22.263127: step 1940, loss 0.693724, acc 0.523438, f1 0.359696\n",
      "2017-11-24T23:20:23.237344: step 1945, loss 0.690594, acc 0.538086, f1 0.376489\n",
      "2017-11-24T23:20:24.168550: step 1950, loss 0.685059, acc 0.568359, f1 0.411937\n",
      "2017-11-24T23:20:25.109473: step 1955, loss 0.682666, acc 0.583008, f1 0.429433\n",
      "2017-11-24T23:20:26.104059: step 1960, loss 0.686006, acc 0.560547, f1 0.402696\n",
      "2017-11-24T23:20:27.056828: step 1965, loss 0.686533, acc 0.557617, f1 0.399247\n",
      "2017-11-24T23:20:28.001581: step 1970, loss 0.689605, acc 0.548828, f1 0.388955\n",
      "2017-11-24T23:20:28.963805: step 1975, loss 0.687509, acc 0.558594, f1 0.400396\n",
      "2017-11-24T23:20:29.924730: step 1980, loss 0.681435, acc 0.581055, f1 0.427088\n",
      "2017-11-24T23:20:30.881585: step 1985, loss 0.689453, acc 0.552734, f1 0.393519\n",
      "2017-11-24T23:20:31.843038: step 1990, loss 0.687821, acc 0.551758, f1 0.392377\n",
      "2017-11-24T23:20:32.854905: step 1995, loss 0.694092, acc 0.540039, f1 0.378746\n",
      "2017-11-24T23:20:33.791800: step 2000, loss 0.683762, acc 0.569336, f1 0.413096\n",
      "2017-11-24T23:20:34.740402: step 2005, loss 0.688998, acc 0.551758, f1 0.392377\n",
      "2017-11-24T23:20:35.715095: step 2010, loss 0.683736, acc 0.570312, f1 0.414257\n",
      "2017-11-24T23:20:36.683067: step 2015, loss 0.686978, acc 0.555664, f1 0.396953\n",
      "2017-11-24T23:20:37.635432: step 2020, loss 0.693559, acc 0.53125, f1 0.368622\n",
      "2017-11-24T23:20:38.609688: step 2025, loss 0.689328, acc 0.547852, f1 0.387817\n",
      "2017-11-24T23:20:39.580401: step 2030, loss 0.680757, acc 0.581055, f1 0.427088\n",
      "2017-11-24T23:20:40.558633: step 2035, loss 0.682469, acc 0.582031, f1 0.42826\n",
      "2017-11-24T23:20:41.509020: step 2040, loss 0.690168, acc 0.558594, f1 0.400396\n",
      "2017-11-24T23:20:42.486584: step 2045, loss 0.683047, acc 0.575195, f1 0.420074\n",
      "2017-11-24T23:20:43.449601: step 2050, loss 0.683094, acc 0.571289, f1 0.415418\n",
      "2017-11-24T23:20:44.408526: step 2055, loss 0.686361, acc 0.558594, f1 0.400396\n",
      "2017-11-24T23:20:45.395534: step 2060, loss 0.684739, acc 0.56543, f1 0.408464\n",
      "2017-11-24T23:20:46.366989: step 2065, loss 0.684084, acc 0.567383, f1 0.410778\n",
      "2017-11-24T23:20:47.329106: step 2070, loss 0.680392, acc 0.580078, f1 0.425916\n",
      "2017-11-24T23:20:48.299142: step 2075, loss 0.683689, acc 0.570312, f1 0.414257\n",
      "2017-11-24T23:20:49.265265: step 2080, loss 0.688376, acc 0.551758, f1 0.392377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current epoch:  2\n",
      "2017-11-24T23:20:50.529960: step 2085, loss 0.692986, acc 0.561523, f1 0.403847\n",
      "2017-11-24T23:20:51.477878: step 2090, loss 0.689562, acc 0.547852, f1 0.387817\n",
      "2017-11-24T23:20:52.445824: step 2095, loss 0.6902, acc 0.542969, f1 0.38214\n",
      "2017-11-24T23:20:53.407707: step 2100, loss 0.686133, acc 0.55957, f1 0.401545\n",
      "2017-11-24T23:20:54.377356: step 2105, loss 0.693856, acc 0.537109, f1 0.375362\n",
      "2017-11-24T23:20:55.333540: step 2110, loss 0.696135, acc 0.527344, f1 0.36415\n",
      "2017-11-24T23:20:56.279535: step 2115, loss 0.684404, acc 0.567383, f1 0.410778\n",
      "2017-11-24T23:20:57.238026: step 2120, loss 0.685164, acc 0.564453, f1 0.407308\n",
      "2017-11-24T23:20:58.181488: step 2125, loss 0.689592, acc 0.544922, f1 0.384408\n",
      "2017-11-24T23:20:59.148123: step 2130, loss 0.680814, acc 0.580078, f1 0.425916\n",
      "2017-11-24T23:21:00.103627: step 2135, loss 0.682128, acc 0.574219, f1 0.418909\n",
      "2017-11-24T23:21:01.087073: step 2140, loss 0.686524, acc 0.564453, f1 0.407308\n",
      "2017-11-24T23:21:02.026670: step 2145, loss 0.691215, acc 0.533203, f1 0.370865\n",
      "2017-11-24T23:21:02.968268: step 2150, loss 0.684023, acc 0.574219, f1 0.418909\n",
      "2017-11-24T23:21:03.935185: step 2155, loss 0.6886, acc 0.547852, f1 0.387817\n",
      "2017-11-24T23:21:04.913365: step 2160, loss 0.688795, acc 0.552734, f1 0.393519\n",
      "2017-11-24T23:21:05.870871: step 2165, loss 0.68751, acc 0.556641, f1 0.398099\n",
      "2017-11-24T23:21:06.829807: step 2170, loss 0.695262, acc 0.543945, f1 0.383273\n",
      "2017-11-24T23:21:07.797945: step 2175, loss 0.688808, acc 0.546875, f1 0.386679\n",
      "2017-11-24T23:21:08.752444: step 2180, loss 0.688838, acc 0.546875, f1 0.386679\n",
      "2017-11-24T23:21:09.718169: step 2185, loss 0.684619, acc 0.580078, f1 0.425916\n",
      "2017-11-24T23:21:10.681919: step 2190, loss 0.675531, acc 0.599609, f1 0.449524\n",
      "2017-11-24T23:21:11.631913: step 2195, loss 0.690559, acc 0.542969, f1 0.38214\n",
      "2017-11-24T23:21:12.584620: step 2200, loss 0.685202, acc 0.564453, f1 0.407308\n",
      "2017-11-24T23:21:13.554121: step 2205, loss 0.689797, acc 0.547852, f1 0.387817\n",
      "2017-11-24T23:21:14.511883: step 2210, loss 0.68698, acc 0.555664, f1 0.396953\n",
      "2017-11-24T23:21:15.463257: step 2215, loss 0.69169, acc 0.547852, f1 0.387817\n",
      "2017-11-24T23:21:16.415148: step 2220, loss 0.681787, acc 0.580078, f1 0.425916\n",
      "2017-11-24T23:21:17.386038: step 2225, loss 0.688928, acc 0.55957, f1 0.401545\n",
      "2017-11-24T23:21:18.342735: step 2230, loss 0.693782, acc 0.533203, f1 0.370865\n",
      "2017-11-24T23:21:19.305510: step 2235, loss 0.688292, acc 0.550781, f1 0.391235\n",
      "2017-11-24T23:21:20.289581: step 2240, loss 0.690586, acc 0.538086, f1 0.376489\n",
      "2017-11-24T23:21:21.237938: step 2245, loss 0.680618, acc 0.579102, f1 0.424746\n",
      "2017-11-24T23:21:22.205789: step 2250, loss 0.68747, acc 0.560547, f1 0.402696\n",
      "2017-11-24T23:21:23.174572: step 2255, loss 0.687002, acc 0.555664, f1 0.396953\n",
      "2017-11-24T23:21:24.143571: step 2260, loss 0.687192, acc 0.554688, f1 0.395807\n",
      "2017-11-24T23:21:25.072460: step 2265, loss 0.688628, acc 0.548828, f1 0.388955\n",
      "2017-11-24T23:21:26.025648: step 2270, loss 0.687553, acc 0.564453, f1 0.407308\n",
      "2017-11-24T23:21:26.975543: step 2275, loss 0.685343, acc 0.563477, f1 0.406154\n",
      "2017-11-24T23:21:27.920870: step 2280, loss 0.687405, acc 0.553711, f1 0.394663\n",
      "2017-11-24T23:21:28.884181: step 2285, loss 0.68951, acc 0.556641, f1 0.398099\n",
      "2017-11-24T23:21:29.845588: step 2290, loss 0.696024, acc 0.53418, f1 0.371988\n",
      "2017-11-24T23:21:30.790450: step 2295, loss 0.684531, acc 0.566406, f1 0.40962\n",
      "2017-11-24T23:21:31.746633: step 2300, loss 0.685187, acc 0.567383, f1 0.410778\n",
      "2017-11-24T23:21:32.705041: step 2305, loss 0.697584, acc 0.521484, f1 0.357475\n",
      "2017-11-24T23:21:33.672224: step 2310, loss 0.685866, acc 0.560547, f1 0.402696\n",
      "2017-11-24T23:21:34.618517: step 2315, loss 0.681783, acc 0.578125, f1 0.423577\n",
      "2017-11-24T23:21:35.587030: step 2320, loss 0.683495, acc 0.570312, f1 0.414257\n",
      "2017-11-24T23:21:36.554127: step 2325, loss 0.68399, acc 0.569336, f1 0.413096\n",
      "2017-11-24T23:21:37.492040: step 2330, loss 0.689264, acc 0.544922, f1 0.384408\n",
      "2017-11-24T23:21:38.449701: step 2335, loss 0.688223, acc 0.549805, f1 0.390095\n",
      "2017-11-24T23:21:39.427339: step 2340, loss 0.692205, acc 0.537109, f1 0.375362\n",
      "2017-11-24T23:21:40.366040: step 2345, loss 0.683279, acc 0.570312, f1 0.414257\n",
      "2017-11-24T23:21:41.333356: step 2350, loss 0.688315, acc 0.550781, f1 0.391235\n",
      "2017-11-24T23:21:42.291892: step 2355, loss 0.682876, acc 0.579102, f1 0.424746\n",
      "2017-11-24T23:21:43.261907: step 2360, loss 0.6846, acc 0.56543, f1 0.408464\n",
      "2017-11-24T23:21:44.181740: step 2365, loss 0.685834, acc 0.5625, f1 0.405\n",
      "2017-11-24T23:21:45.145307: step 2370, loss 0.692639, acc 0.535156, f1 0.373111\n",
      "2017-11-24T23:21:46.107714: step 2375, loss 0.687395, acc 0.570312, f1 0.414257\n",
      "2017-11-24T23:21:47.053637: step 2380, loss 0.682308, acc 0.576172, f1 0.421241\n",
      "2017-11-24T23:21:48.004715: step 2385, loss 0.686647, acc 0.558594, f1 0.400396\n",
      "2017-11-24T23:21:48.960619: step 2390, loss 0.687904, acc 0.552734, f1 0.393519\n",
      "2017-11-24T23:21:49.929068: step 2395, loss 0.684592, acc 0.566406, f1 0.40962\n",
      "2017-11-24T23:21:50.894652: step 2400, loss 0.691683, acc 0.541016, f1 0.379877\n",
      "2017-11-24T23:21:51.821645: step 2405, loss 0.681092, acc 0.588867, f1 0.436493\n",
      "2017-11-24T23:21:52.781719: step 2410, loss 0.690377, acc 0.543945, f1 0.383273\n",
      "2017-11-24T23:21:53.730475: step 2415, loss 0.680985, acc 0.578125, f1 0.423577\n",
      "2017-11-24T23:21:54.675184: step 2420, loss 0.686621, acc 0.558594, f1 0.400396\n",
      "2017-11-24T23:21:55.654028: step 2425, loss 0.694912, acc 0.538086, f1 0.376489\n",
      "2017-11-24T23:21:56.610879: step 2430, loss 0.690695, acc 0.545898, f1 0.385543\n",
      "2017-11-24T23:21:57.583497: step 2435, loss 0.690875, acc 0.541992, f1 0.381008\n",
      "2017-11-24T23:21:58.538211: step 2440, loss 0.692221, acc 0.525391, f1 0.361921\n",
      "2017-11-24T23:21:59.499587: step 2445, loss 0.686924, acc 0.557617, f1 0.399247\n",
      "2017-11-24T23:22:00.443755: step 2450, loss 0.694302, acc 0.52832, f1 0.365267\n",
      "2017-11-24T23:22:01.383189: step 2455, loss 0.684127, acc 0.567383, f1 0.410778\n",
      "2017-11-24T23:22:02.355451: step 2460, loss 0.684966, acc 0.564453, f1 0.407308\n",
      "2017-11-24T23:22:03.303087: step 2465, loss 0.684085, acc 0.569336, f1 0.413096\n",
      "2017-11-24T23:22:04.282194: step 2470, loss 0.684747, acc 0.566406, f1 0.40962\n",
      "2017-11-24T23:22:05.249333: step 2475, loss 0.686397, acc 0.56543, f1 0.408464\n",
      "2017-11-24T23:22:06.201755: step 2480, loss 0.688909, acc 0.554688, f1 0.395807\n",
      "2017-11-24T23:22:07.169988: step 2485, loss 0.691171, acc 0.542969, f1 0.38214\n",
      "2017-11-24T23:22:08.122735: step 2490, loss 0.679838, acc 0.582031, f1 0.42826\n",
      "2017-11-24T23:22:09.060253: step 2495, loss 0.687022, acc 0.555664, f1 0.396953\n",
      "2017-11-24T23:22:10.028058: step 2500, loss 0.687501, acc 0.553711, f1 0.394663\n",
      "2017-11-24T23:22:10.991342: step 2505, loss 0.69875, acc 0.516602, f1 0.351941\n",
      "2017-11-24T23:22:11.951165: step 2510, loss 0.683059, acc 0.573242, f1 0.417744\n",
      "2017-11-24T23:22:12.917646: step 2515, loss 0.689842, acc 0.541016, f1 0.379877\n",
      "2017-11-24T23:22:13.898354: step 2520, loss 0.67925, acc 0.584961, f1 0.431783\n",
      "2017-11-24T23:22:14.855348: step 2525, loss 0.690526, acc 0.543945, f1 0.383273\n",
      "2017-11-24T23:22:15.822774: step 2530, loss 0.685542, acc 0.5625, f1 0.405\n",
      "2017-11-24T23:22:16.770935: step 2535, loss 0.692638, acc 0.541016, f1 0.379877\n",
      "2017-11-24T23:22:17.747820: step 2540, loss 0.686417, acc 0.558594, f1 0.400396\n",
      "2017-11-24T23:22:18.722146: step 2545, loss 0.692092, acc 0.539062, f1 0.377617\n",
      "2017-11-24T23:22:19.649607: step 2550, loss 0.687382, acc 0.554688, f1 0.395807\n",
      "2017-11-24T23:22:20.627648: step 2555, loss 0.679754, acc 0.585938, f1 0.432959\n",
      "2017-11-24T23:22:21.574341: step 2560, loss 0.683902, acc 0.568359, f1 0.411937\n",
      "2017-11-24T23:22:22.544583: step 2565, loss 0.683553, acc 0.572266, f1 0.416581\n",
      "2017-11-24T23:22:23.506440: step 2570, loss 0.683874, acc 0.589844, f1 0.437673\n",
      "2017-11-24T23:22:24.509207: step 2575, loss 0.681281, acc 0.578125, f1 0.423577\n",
      "2017-11-24T23:22:25.478351: step 2580, loss 0.688027, acc 0.557617, f1 0.399247\n",
      "2017-11-24T23:22:26.428233: step 2585, loss 0.686696, acc 0.557617, f1 0.399247\n",
      "2017-11-24T23:22:27.380733: step 2590, loss 0.683688, acc 0.569336, f1 0.413096\n",
      "2017-11-24T23:22:28.314491: step 2595, loss 0.683262, acc 0.570312, f1 0.414257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-24T23:22:29.247929: step 2600, loss 0.695964, acc 0.532227, f1 0.369743\n",
      "2017-11-24T23:22:30.216816: step 2605, loss 0.689205, acc 0.544922, f1 0.384408\n",
      "2017-11-24T23:22:31.183541: step 2610, loss 0.690037, acc 0.543945, f1 0.383273\n",
      "2017-11-24T23:22:32.143548: step 2615, loss 0.687028, acc 0.555664, f1 0.396953\n",
      "2017-11-24T23:22:33.101711: step 2620, loss 0.684748, acc 0.572266, f1 0.416581\n",
      "2017-11-24T23:22:34.041001: step 2625, loss 0.684842, acc 0.567383, f1 0.410778\n",
      "2017-11-24T23:22:35.001365: step 2630, loss 0.690721, acc 0.552734, f1 0.393519\n",
      "2017-11-24T23:22:35.966296: step 2635, loss 0.684831, acc 0.566406, f1 0.40962\n",
      "2017-11-24T23:22:36.962195: step 2640, loss 0.688473, acc 0.548828, f1 0.388955\n",
      "2017-11-24T23:22:37.937831: step 2645, loss 0.686009, acc 0.561523, f1 0.403847\n",
      "2017-11-24T23:22:38.903094: step 2650, loss 0.691283, acc 0.532227, f1 0.369743\n",
      "2017-11-24T23:22:39.887673: step 2655, loss 0.68795, acc 0.553711, f1 0.394663\n",
      "2017-11-24T23:22:40.819371: step 2660, loss 0.682948, acc 0.599609, f1 0.449524\n",
      "2017-11-24T23:22:41.764338: step 2665, loss 0.682556, acc 0.573242, f1 0.417744\n",
      "2017-11-24T23:22:42.699366: step 2670, loss 0.684386, acc 0.567383, f1 0.410778\n",
      "2017-11-24T23:22:43.678473: step 2675, loss 0.686185, acc 0.568359, f1 0.411937\n",
      "2017-11-24T23:22:44.622087: step 2680, loss 0.690931, acc 0.541992, f1 0.381008\n",
      "2017-11-24T23:22:45.550501: step 2685, loss 0.689695, acc 0.541992, f1 0.381008\n",
      "2017-11-24T23:22:46.516572: step 2690, loss 0.688871, acc 0.546875, f1 0.386679\n",
      "2017-11-24T23:22:47.465418: step 2695, loss 0.687845, acc 0.551758, f1 0.392377\n",
      "2017-11-24T23:22:48.419009: step 2700, loss 0.686552, acc 0.55957, f1 0.401545\n",
      "2017-11-24T23:22:49.401363: step 2705, loss 0.689572, acc 0.548828, f1 0.388955\n",
      "2017-11-24T23:22:50.350205: step 2710, loss 0.685831, acc 0.560547, f1 0.402696\n",
      "2017-11-24T23:22:51.314599: step 2715, loss 0.685443, acc 0.568359, f1 0.411937\n",
      "2017-11-24T23:22:52.267205: step 2720, loss 0.694057, acc 0.538086, f1 0.376489\n",
      "2017-11-24T23:22:53.234930: step 2725, loss 0.687822, acc 0.551758, f1 0.392377\n",
      "2017-11-24T23:22:54.178270: step 2730, loss 0.687122, acc 0.560547, f1 0.402696\n",
      "2017-11-24T23:22:55.139179: step 2735, loss 0.688022, acc 0.550781, f1 0.391235\n",
      "2017-11-24T23:22:56.103632: step 2740, loss 0.683006, acc 0.574219, f1 0.418909\n",
      "2017-11-24T23:22:57.051873: step 2745, loss 0.685987, acc 0.560547, f1 0.402696\n",
      "2017-11-24T23:22:57.998760: step 2750, loss 0.689162, acc 0.546875, f1 0.386679\n",
      "2017-11-24T23:22:58.965951: step 2755, loss 0.698921, acc 0.515625, f1 0.350838\n",
      "2017-11-24T23:22:59.923227: step 2760, loss 0.687849, acc 0.551758, f1 0.392377\n",
      "2017-11-24T23:23:00.879264: step 2765, loss 0.685103, acc 0.563477, f1 0.406154\n",
      "2017-11-24T23:23:01.833564: step 2770, loss 0.685104, acc 0.568359, f1 0.411937\n",
      "2017-11-24T23:23:02.811513: step 2775, loss 0.685866, acc 0.561523, f1 0.403847\n",
      "2017-11-24T23:23:03.767356: step 2780, loss 0.68381, acc 0.568359, f1 0.411937\n",
      "2017-11-24T23:23:04.737606: step 2785, loss 0.686986, acc 0.555664, f1 0.396953\n",
      "2017-11-24T23:23:05.716668: step 2790, loss 0.68235, acc 0.576172, f1 0.421241\n",
      "2017-11-24T23:23:06.685760: step 2795, loss 0.688538, acc 0.548828, f1 0.388955\n",
      "2017-11-24T23:23:07.670800: step 2800, loss 0.686966, acc 0.561523, f1 0.403847\n",
      "2017-11-24T23:23:08.677847: step 2805, loss 0.687776, acc 0.557617, f1 0.399247\n",
      "2017-11-24T23:23:09.618479: step 2810, loss 0.683346, acc 0.575195, f1 0.420074\n",
      "2017-11-24T23:23:10.585506: step 2815, loss 0.68337, acc 0.571289, f1 0.415418\n",
      "2017-11-24T23:23:11.526117: step 2820, loss 0.682334, acc 0.578125, f1 0.423577\n",
      "2017-11-24T23:23:12.483363: step 2825, loss 0.690383, acc 0.539062, f1 0.377617\n",
      "2017-11-24T23:23:13.434060: step 2830, loss 0.689428, acc 0.543945, f1 0.383273\n",
      "2017-11-24T23:23:14.386215: step 2835, loss 0.687973, acc 0.555664, f1 0.396953\n",
      "2017-11-24T23:23:15.392911: step 2840, loss 0.680295, acc 0.586914, f1 0.434136\n",
      "2017-11-24T23:23:16.361319: step 2845, loss 0.691024, acc 0.533203, f1 0.370865\n",
      "2017-11-24T23:23:17.332389: step 2850, loss 0.688638, acc 0.547852, f1 0.387817\n",
      "2017-11-24T23:23:18.313456: step 2855, loss 0.692985, acc 0.542969, f1 0.38214\n",
      "2017-11-24T23:23:19.272841: step 2860, loss 0.679201, acc 0.589844, f1 0.437673\n",
      "2017-11-24T23:23:20.241435: step 2865, loss 0.684096, acc 0.567383, f1 0.410778\n",
      "2017-11-24T23:23:21.233244: step 2870, loss 0.684324, acc 0.567383, f1 0.410778\n",
      "2017-11-24T23:23:22.209137: step 2875, loss 0.690041, acc 0.546875, f1 0.386679\n",
      "2017-11-24T23:23:23.151001: step 2880, loss 0.677254, acc 0.59668, f1 0.445959\n",
      "2017-11-24T23:23:24.099514: step 2885, loss 0.685041, acc 0.564453, f1 0.407308\n",
      "2017-11-24T23:23:25.079818: step 2890, loss 0.691195, acc 0.535156, f1 0.373111\n",
      "2017-11-24T23:23:26.033525: step 2895, loss 0.690076, acc 0.540039, f1 0.378746\n",
      "2017-11-24T23:23:26.989413: step 2900, loss 0.682034, acc 0.576172, f1 0.421241\n",
      "2017-11-24T23:23:27.939087: step 2905, loss 0.68589, acc 0.560547, f1 0.402696\n",
      "2017-11-24T23:23:28.908850: step 2910, loss 0.689083, acc 0.564453, f1 0.407308\n",
      "2017-11-24T23:23:29.870594: step 2915, loss 0.685115, acc 0.563477, f1 0.406154\n",
      "2017-11-24T23:23:30.786927: step 2920, loss 0.689083, acc 0.553711, f1 0.394663\n",
      "2017-11-24T23:23:31.734275: step 2925, loss 0.676271, acc 0.59668, f1 0.445959\n",
      "2017-11-24T23:23:32.674062: step 2930, loss 0.684359, acc 0.567383, f1 0.410778\n",
      "2017-11-24T23:23:33.624370: step 2935, loss 0.686755, acc 0.557617, f1 0.399247\n",
      "2017-11-24T23:23:34.604949: step 2940, loss 0.682306, acc 0.576172, f1 0.421241\n",
      "2017-11-24T23:23:35.547856: step 2945, loss 0.687253, acc 0.558594, f1 0.400396\n",
      "2017-11-24T23:23:36.522450: step 2950, loss 0.689318, acc 0.550781, f1 0.391235\n",
      "2017-11-24T23:23:37.484993: step 2955, loss 0.68554, acc 0.5625, f1 0.405\n",
      "2017-11-24T23:23:38.448404: step 2960, loss 0.688293, acc 0.558594, f1 0.400396\n",
      "2017-11-24T23:23:39.406262: step 2965, loss 0.682868, acc 0.577148, f1 0.422408\n",
      "2017-11-24T23:23:40.377247: step 2970, loss 0.690814, acc 0.539062, f1 0.377617\n",
      "2017-11-24T23:23:41.341929: step 2975, loss 0.690602, acc 0.543945, f1 0.383273\n",
      "2017-11-24T23:23:42.281817: step 2980, loss 0.681415, acc 0.580078, f1 0.425916\n",
      "2017-11-24T23:23:43.223742: step 2985, loss 0.690977, acc 0.536133, f1 0.374236\n",
      "2017-11-24T23:23:44.169776: step 2990, loss 0.692642, acc 0.545898, f1 0.385543\n",
      "2017-11-24T23:23:45.098846: step 2995, loss 0.685929, acc 0.5625, f1 0.405\n",
      "2017-11-24T23:23:46.067071: step 3000, loss 0.688599, acc 0.547852, f1 0.387817\n",
      "2017-11-24T23:23:47.012568: step 3005, loss 0.68457, acc 0.569336, f1 0.413096\n",
      "2017-11-24T23:23:47.953673: step 3010, loss 0.682659, acc 0.573242, f1 0.417744\n",
      "2017-11-24T23:23:48.900045: step 3015, loss 0.687723, acc 0.552734, f1 0.393519\n",
      "2017-11-24T23:23:49.854003: step 3020, loss 0.687554, acc 0.554688, f1 0.395807\n",
      "2017-11-24T23:23:50.784421: step 3025, loss 0.688411, acc 0.548828, f1 0.388955\n",
      "2017-11-24T23:23:51.743741: step 3030, loss 0.677518, acc 0.589844, f1 0.437673\n",
      "2017-11-24T23:23:52.694235: step 3035, loss 0.688744, acc 0.547852, f1 0.387817\n",
      "2017-11-24T23:23:53.655392: step 3040, loss 0.685141, acc 0.569336, f1 0.413096\n",
      "2017-11-24T23:23:54.624857: step 3045, loss 0.695089, acc 0.525391, f1 0.361921\n",
      "2017-11-24T23:23:55.600697: step 3050, loss 0.686617, acc 0.558594, f1 0.400396\n",
      "2017-11-24T23:23:56.568426: step 3055, loss 0.684508, acc 0.569336, f1 0.413096\n",
      "2017-11-24T23:23:57.515324: step 3060, loss 0.688408, acc 0.548828, f1 0.388955\n",
      "2017-11-24T23:23:58.472080: step 3065, loss 0.688769, acc 0.551758, f1 0.392377\n",
      "2017-11-24T23:23:59.443697: step 3070, loss 0.68862, acc 0.547852, f1 0.387817\n",
      "2017-11-24T23:24:00.409477: step 3075, loss 0.689087, acc 0.545898, f1 0.385543\n",
      "2017-11-24T23:24:01.358024: step 3080, loss 0.684088, acc 0.574219, f1 0.418909\n",
      "2017-11-24T23:24:02.302699: step 3085, loss 0.686972, acc 0.555664, f1 0.396953\n",
      "2017-11-24T23:24:03.284609: step 3090, loss 0.685608, acc 0.561523, f1 0.403847\n",
      "2017-11-24T23:24:04.243726: step 3095, loss 0.681697, acc 0.576172, f1 0.421241\n",
      "2017-11-24T23:24:05.200067: step 3100, loss 0.689001, acc 0.557617, f1 0.399247\n",
      "2017-11-24T23:24:06.160577: step 3105, loss 0.688061, acc 0.552734, f1 0.393519\n",
      "2017-11-24T23:24:07.143504: step 3110, loss 0.684101, acc 0.567383, f1 0.410778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-24T23:24:08.107877: step 3115, loss 0.684882, acc 0.570312, f1 0.414257\n",
      "2017-11-24T23:24:09.070704: step 3120, loss 0.692593, acc 0.533203, f1 0.370865\n",
      "2017-11-24T23:24:10.030103: step 3125, loss 0.691524, acc 0.547852, f1 0.387817\n",
      "\n",
      "\n",
      "Current epoch:  3\n",
      "2017-11-24T23:24:11.307623: step 3130, loss 0.683551, acc 0.572266, f1 0.416581\n",
      "2017-11-24T23:24:12.288576: step 3135, loss 0.686329, acc 0.558594, f1 0.400396\n",
      "2017-11-24T23:24:13.262114: step 3140, loss 0.689463, acc 0.547852, f1 0.387817\n",
      "2017-11-24T23:24:14.213715: step 3145, loss 0.678341, acc 0.585938, f1 0.432959\n",
      "2017-11-24T23:24:15.155423: step 3150, loss 0.688213, acc 0.549805, f1 0.390095\n",
      "2017-11-24T23:24:16.102857: step 3155, loss 0.689017, acc 0.547852, f1 0.387817\n",
      "2017-11-24T23:24:17.055997: step 3160, loss 0.688594, acc 0.547852, f1 0.387817\n",
      "2017-11-24T23:24:18.010410: step 3165, loss 0.68956, acc 0.542969, f1 0.38214\n",
      "2017-11-24T23:24:18.964687: step 3170, loss 0.6889, acc 0.554688, f1 0.395807\n",
      "2017-11-24T23:24:19.905089: step 3175, loss 0.687815, acc 0.551758, f1 0.392377\n",
      "2017-11-24T23:24:20.877129: step 3180, loss 0.687192, acc 0.554688, f1 0.395807\n",
      "2017-11-24T23:24:21.835142: step 3185, loss 0.693009, acc 0.530273, f1 0.367503\n",
      "2017-11-24T23:24:22.830199: step 3190, loss 0.687522, acc 0.553711, f1 0.394663\n",
      "2017-11-24T23:24:23.797960: step 3195, loss 0.687229, acc 0.557617, f1 0.399247\n",
      "2017-11-24T23:24:24.745269: step 3200, loss 0.688747, acc 0.547852, f1 0.387817\n",
      "2017-11-24T23:24:25.708600: step 3205, loss 0.684213, acc 0.568359, f1 0.411937\n",
      "2017-11-24T23:24:26.664262: step 3210, loss 0.6906, acc 0.550781, f1 0.391235\n",
      "2017-11-24T23:24:27.630781: step 3215, loss 0.689044, acc 0.545898, f1 0.385543\n",
      "2017-11-24T23:24:28.600649: step 3220, loss 0.688055, acc 0.550781, f1 0.391235\n",
      "2017-11-24T23:24:29.562527: step 3225, loss 0.690605, acc 0.546875, f1 0.386679\n",
      "2017-11-24T23:24:30.530952: step 3230, loss 0.687234, acc 0.556641, f1 0.398099\n",
      "2017-11-24T23:24:31.504001: step 3235, loss 0.689035, acc 0.552734, f1 0.393519\n",
      "2017-11-24T23:24:32.433910: step 3240, loss 0.683627, acc 0.569336, f1 0.413096\n",
      "2017-11-24T23:24:33.377804: step 3245, loss 0.684772, acc 0.56543, f1 0.408464\n",
      "2017-11-24T23:24:34.344547: step 3250, loss 0.685723, acc 0.561523, f1 0.403847\n",
      "2017-11-24T23:24:35.319832: step 3255, loss 0.687171, acc 0.560547, f1 0.402696\n",
      "2017-11-24T23:24:36.265408: step 3260, loss 0.683588, acc 0.574219, f1 0.418909\n",
      "2017-11-24T23:24:37.219457: step 3265, loss 0.682607, acc 0.573242, f1 0.417744\n",
      "2017-11-24T23:24:38.187731: step 3270, loss 0.687205, acc 0.554688, f1 0.395807\n",
      "2017-11-24T23:24:39.137613: step 3275, loss 0.685721, acc 0.561523, f1 0.403847\n",
      "2017-11-24T23:24:40.106300: step 3280, loss 0.684496, acc 0.566406, f1 0.40962\n",
      "2017-11-24T23:24:41.080255: step 3285, loss 0.692437, acc 0.546875, f1 0.386679\n",
      "2017-11-24T23:24:42.051521: step 3290, loss 0.691461, acc 0.540039, f1 0.378746\n",
      "2017-11-24T23:24:43.015905: step 3295, loss 0.690979, acc 0.533203, f1 0.370865\n",
      "2017-11-24T23:24:43.975495: step 3300, loss 0.686759, acc 0.567383, f1 0.410778\n",
      "2017-11-24T23:24:44.962892: step 3305, loss 0.681298, acc 0.577148, f1 0.422408\n",
      "2017-11-24T23:24:45.921291: step 3310, loss 0.685937, acc 0.560547, f1 0.402696\n",
      "2017-11-24T23:24:46.838896: step 3315, loss 0.689687, acc 0.541992, f1 0.381008\n",
      "2017-11-24T23:24:47.795279: step 3320, loss 0.684613, acc 0.56543, f1 0.408464\n",
      "2017-11-24T23:24:48.723121: step 3325, loss 0.688488, acc 0.549805, f1 0.390095\n",
      "2017-11-24T23:24:49.645309: step 3330, loss 0.692042, acc 0.541016, f1 0.379877\n",
      "2017-11-24T23:24:50.608722: step 3335, loss 0.68701, acc 0.556641, f1 0.398099\n",
      "2017-11-24T23:24:51.540050: step 3340, loss 0.688405, acc 0.548828, f1 0.388955\n",
      "2017-11-24T23:24:52.487967: step 3345, loss 0.689089, acc 0.545898, f1 0.385543\n",
      "2017-11-24T23:24:53.442320: step 3350, loss 0.687929, acc 0.5625, f1 0.405\n",
      "2017-11-24T23:24:54.429849: step 3355, loss 0.687401, acc 0.558594, f1 0.400396\n",
      "2017-11-24T23:24:55.405567: step 3360, loss 0.690308, acc 0.540039, f1 0.378746\n",
      "2017-11-24T23:24:56.376045: step 3365, loss 0.68604, acc 0.563477, f1 0.406154\n",
      "2017-11-24T23:24:57.360025: step 3370, loss 0.681232, acc 0.577148, f1 0.422408\n",
      "2017-11-24T23:24:58.287467: step 3375, loss 0.687345, acc 0.55957, f1 0.401545\n",
      "2017-11-24T23:24:59.227268: step 3380, loss 0.692912, acc 0.526367, f1 0.363035\n",
      "2017-11-24T23:25:00.190045: step 3385, loss 0.68183, acc 0.576172, f1 0.421241\n",
      "2017-11-24T23:25:01.158415: step 3390, loss 0.693776, acc 0.540039, f1 0.378746\n",
      "2017-11-24T23:25:02.123831: step 3395, loss 0.685227, acc 0.564453, f1 0.407308\n",
      "2017-11-24T23:25:03.093465: step 3400, loss 0.673509, acc 0.609375, f1 0.461468\n",
      "2017-11-24T23:25:04.070333: step 3405, loss 0.682345, acc 0.574219, f1 0.418909\n",
      "2017-11-24T23:25:05.052946: step 3410, loss 0.692438, acc 0.53418, f1 0.371988\n",
      "2017-11-24T23:25:05.968550: step 3415, loss 0.683346, acc 0.574219, f1 0.418909\n",
      "2017-11-24T23:25:06.926623: step 3420, loss 0.688101, acc 0.553711, f1 0.394663\n",
      "2017-11-24T23:25:07.881105: step 3425, loss 0.684232, acc 0.574219, f1 0.418909\n",
      "2017-11-24T23:25:08.835828: step 3430, loss 0.688504, acc 0.548828, f1 0.388955\n",
      "2017-11-24T23:25:09.825421: step 3435, loss 0.682009, acc 0.575195, f1 0.420074\n",
      "2017-11-24T23:25:10.793399: step 3440, loss 0.684078, acc 0.569336, f1 0.413096\n",
      "2017-11-24T23:25:11.743299: step 3445, loss 0.685108, acc 0.566406, f1 0.40962\n",
      "2017-11-24T23:25:12.697154: step 3450, loss 0.684637, acc 0.567383, f1 0.410778\n",
      "2017-11-24T23:25:13.674764: step 3455, loss 0.69278, acc 0.52832, f1 0.365267\n",
      "2017-11-24T23:25:14.626087: step 3460, loss 0.684263, acc 0.572266, f1 0.416581\n",
      "2017-11-24T23:25:15.573718: step 3465, loss 0.690822, acc 0.539062, f1 0.377617\n",
      "2017-11-24T23:25:16.531573: step 3470, loss 0.687041, acc 0.555664, f1 0.396953\n",
      "2017-11-24T23:25:17.498029: step 3475, loss 0.695154, acc 0.536133, f1 0.374236\n",
      "2017-11-24T23:25:18.447505: step 3480, loss 0.692385, acc 0.527344, f1 0.36415\n",
      "2017-11-24T23:25:19.403162: step 3485, loss 0.690355, acc 0.543945, f1 0.383273\n",
      "2017-11-24T23:25:20.355549: step 3490, loss 0.687065, acc 0.555664, f1 0.396953\n",
      "2017-11-24T23:25:21.285631: step 3495, loss 0.691914, acc 0.538086, f1 0.376489\n",
      "2017-11-24T23:25:22.249561: step 3500, loss 0.68265, acc 0.585938, f1 0.432959\n",
      "2017-11-24T23:25:23.217242: step 3505, loss 0.687203, acc 0.554688, f1 0.395807\n",
      "2017-11-24T23:25:24.174453: step 3510, loss 0.685367, acc 0.5625, f1 0.405\n",
      "2017-11-24T23:25:25.139037: step 3515, loss 0.681004, acc 0.579102, f1 0.424746\n",
      "2017-11-24T23:25:26.090209: step 3520, loss 0.695184, acc 0.529297, f1 0.366384\n",
      "2017-11-24T23:25:27.044647: step 3525, loss 0.689316, acc 0.543945, f1 0.383273\n",
      "2017-11-24T23:25:28.000062: step 3530, loss 0.687356, acc 0.5625, f1 0.405\n",
      "2017-11-24T23:25:28.957981: step 3535, loss 0.687223, acc 0.555664, f1 0.396953\n",
      "2017-11-24T23:25:29.918954: step 3540, loss 0.693546, acc 0.520508, f1 0.356366\n",
      "2017-11-24T23:25:30.879142: step 3545, loss 0.686526, acc 0.557617, f1 0.399247\n",
      "2017-11-24T23:25:31.838376: step 3550, loss 0.68841, acc 0.548828, f1 0.388955\n",
      "2017-11-24T23:25:32.797638: step 3555, loss 0.684814, acc 0.566406, f1 0.40962\n",
      "2017-11-24T23:25:33.734991: step 3560, loss 0.687196, acc 0.558594, f1 0.400396\n",
      "2017-11-24T23:25:34.683976: step 3565, loss 0.687815, acc 0.555664, f1 0.396953\n",
      "2017-11-24T23:25:35.639070: step 3570, loss 0.685153, acc 0.563477, f1 0.406154\n",
      "2017-11-24T23:25:36.582077: step 3575, loss 0.688477, acc 0.549805, f1 0.390095\n",
      "2017-11-24T23:25:37.539453: step 3580, loss 0.691867, acc 0.53418, f1 0.371988\n",
      "2017-11-24T23:25:38.520962: step 3585, loss 0.691474, acc 0.530273, f1 0.367503\n",
      "2017-11-24T23:25:39.496920: step 3590, loss 0.681054, acc 0.579102, f1 0.424746\n",
      "2017-11-24T23:25:40.471429: step 3595, loss 0.686988, acc 0.55957, f1 0.401545\n",
      "2017-11-24T23:25:41.429581: step 3600, loss 0.680973, acc 0.597656, f1 0.447146\n",
      "2017-11-24T23:25:42.412651: step 3605, loss 0.691573, acc 0.545898, f1 0.385543\n",
      "2017-11-24T23:25:43.355683: step 3610, loss 0.684423, acc 0.567383, f1 0.410778\n",
      "2017-11-24T23:25:44.314137: step 3615, loss 0.69027, acc 0.540039, f1 0.378746\n",
      "2017-11-24T23:25:45.285879: step 3620, loss 0.687812, acc 0.551758, f1 0.392377\n",
      "2017-11-24T23:25:46.268250: step 3625, loss 0.693985, acc 0.516602, f1 0.351941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-24T23:25:47.212558: step 3630, loss 0.689897, acc 0.543945, f1 0.383273\n",
      "2017-11-24T23:25:48.193034: step 3635, loss 0.691157, acc 0.541016, f1 0.379877\n",
      "2017-11-24T23:25:49.157524: step 3640, loss 0.684142, acc 0.567383, f1 0.410778\n",
      "2017-11-24T23:25:50.114462: step 3645, loss 0.687783, acc 0.552734, f1 0.393519\n",
      "2017-11-24T23:25:51.089505: step 3650, loss 0.688875, acc 0.547852, f1 0.387817\n",
      "2017-11-24T23:25:52.045744: step 3655, loss 0.678608, acc 0.589844, f1 0.437673\n",
      "2017-11-24T23:25:52.987373: step 3660, loss 0.681813, acc 0.576172, f1 0.421241\n",
      "2017-11-24T23:25:53.918065: step 3665, loss 0.68961, acc 0.544922, f1 0.384408\n",
      "2017-11-24T23:25:54.910705: step 3670, loss 0.687615, acc 0.552734, f1 0.393519\n",
      "2017-11-24T23:25:55.867711: step 3675, loss 0.692074, acc 0.53125, f1 0.368622\n",
      "2017-11-24T23:25:56.811436: step 3680, loss 0.681214, acc 0.588867, f1 0.436493\n",
      "2017-11-24T23:25:57.755515: step 3685, loss 0.685114, acc 0.564453, f1 0.407308\n",
      "2017-11-24T23:25:58.711939: step 3690, loss 0.687638, acc 0.552734, f1 0.393519\n",
      "2017-11-24T23:25:59.657973: step 3695, loss 0.688408, acc 0.548828, f1 0.388955\n",
      "2017-11-24T23:26:00.601945: step 3700, loss 0.6841, acc 0.567383, f1 0.410778\n",
      "2017-11-24T23:26:01.584401: step 3705, loss 0.691229, acc 0.549805, f1 0.390095\n",
      "2017-11-24T23:26:02.549174: step 3710, loss 0.6807, acc 0.583008, f1 0.429433\n",
      "2017-11-24T23:26:03.512957: step 3715, loss 0.689785, acc 0.542969, f1 0.38214\n",
      "2017-11-24T23:26:04.474380: step 3720, loss 0.687503, acc 0.554688, f1 0.395807\n",
      "2017-11-24T23:26:05.406765: step 3725, loss 0.686972, acc 0.555664, f1 0.396953\n",
      "2017-11-24T23:26:06.342741: step 3730, loss 0.684091, acc 0.568359, f1 0.411937\n",
      "2017-11-24T23:26:07.281607: step 3735, loss 0.685663, acc 0.56543, f1 0.408464\n",
      "2017-11-24T23:26:08.244944: step 3740, loss 0.678071, acc 0.595703, f1 0.444772\n",
      "2017-11-24T23:26:09.180616: step 3745, loss 0.685078, acc 0.568359, f1 0.411937\n",
      "2017-11-24T23:26:10.124071: step 3750, loss 0.694213, acc 0.53125, f1 0.368622\n",
      "2017-11-24T23:26:11.075271: step 3755, loss 0.69044, acc 0.538086, f1 0.376489\n",
      "2017-11-24T23:26:12.037508: step 3760, loss 0.68675, acc 0.556641, f1 0.398099\n",
      "2017-11-24T23:26:12.993131: step 3765, loss 0.689249, acc 0.546875, f1 0.386679\n",
      "2017-11-24T23:26:13.983005: step 3770, loss 0.684846, acc 0.569336, f1 0.413096\n",
      "2017-11-24T23:26:14.933581: step 3775, loss 0.687714, acc 0.555664, f1 0.396953\n",
      "2017-11-24T23:26:15.863029: step 3780, loss 0.68893, acc 0.549805, f1 0.390095\n",
      "2017-11-24T23:26:16.812074: step 3785, loss 0.685684, acc 0.561523, f1 0.403847\n",
      "2017-11-24T23:26:17.752001: step 3790, loss 0.686142, acc 0.561523, f1 0.403847\n",
      "2017-11-24T23:26:18.706925: step 3795, loss 0.689775, acc 0.546875, f1 0.386679\n",
      "2017-11-24T23:26:19.678763: step 3800, loss 0.68759, acc 0.553711, f1 0.394663\n",
      "2017-11-24T23:26:20.668165: step 3805, loss 0.684452, acc 0.566406, f1 0.40962\n",
      "2017-11-24T23:26:21.622506: step 3810, loss 0.688278, acc 0.549805, f1 0.390095\n",
      "2017-11-24T23:26:22.587449: step 3815, loss 0.682446, acc 0.583008, f1 0.429433\n",
      "2017-11-24T23:26:23.549028: step 3820, loss 0.689364, acc 0.543945, f1 0.383273\n",
      "2017-11-24T23:26:24.489808: step 3825, loss 0.684677, acc 0.56543, f1 0.408464\n",
      "2017-11-24T23:26:25.426625: step 3830, loss 0.689654, acc 0.541992, f1 0.381008\n",
      "2017-11-24T23:26:26.370770: step 3835, loss 0.684712, acc 0.56543, f1 0.408464\n",
      "2017-11-24T23:26:27.309626: step 3840, loss 0.681086, acc 0.578125, f1 0.423577\n",
      "2017-11-24T23:26:28.255562: step 3845, loss 0.68728, acc 0.554688, f1 0.395807\n",
      "2017-11-24T23:26:29.192017: step 3850, loss 0.687685, acc 0.555664, f1 0.396953\n",
      "2017-11-24T23:26:30.159952: step 3855, loss 0.686756, acc 0.556641, f1 0.398099\n",
      "2017-11-24T23:26:31.110232: step 3860, loss 0.692726, acc 0.541016, f1 0.379877\n",
      "2017-11-24T23:26:32.066098: step 3865, loss 0.690787, acc 0.540039, f1 0.378746\n",
      "2017-11-24T23:26:33.030004: step 3870, loss 0.692054, acc 0.536133, f1 0.374236\n",
      "2017-11-24T23:26:33.962736: step 3875, loss 0.688603, acc 0.547852, f1 0.387817\n",
      "2017-11-24T23:26:34.906938: step 3880, loss 0.682742, acc 0.583008, f1 0.429433\n",
      "2017-11-24T23:26:35.872237: step 3885, loss 0.684455, acc 0.566406, f1 0.40962\n",
      "2017-11-24T23:26:36.840973: step 3890, loss 0.686144, acc 0.55957, f1 0.401545\n",
      "2017-11-24T23:26:37.829548: step 3895, loss 0.689942, acc 0.543945, f1 0.383273\n",
      "2017-11-24T23:26:38.778447: step 3900, loss 0.683551, acc 0.569336, f1 0.413096\n",
      "2017-11-24T23:26:39.759063: step 3905, loss 0.686775, acc 0.556641, f1 0.398099\n",
      "2017-11-24T23:26:40.731066: step 3910, loss 0.68515, acc 0.56543, f1 0.408464\n",
      "2017-11-24T23:26:41.685483: step 3915, loss 0.686872, acc 0.556641, f1 0.398099\n",
      "2017-11-24T23:26:42.688546: step 3920, loss 0.690872, acc 0.53418, f1 0.371988\n",
      "2017-11-24T23:26:43.645086: step 3925, loss 0.690591, acc 0.537109, f1 0.375362\n",
      "2017-11-24T23:26:44.724564: step 3930, loss 0.689192, acc 0.544922, f1 0.384408\n",
      "2017-11-24T23:26:45.706468: step 3935, loss 0.687001, acc 0.555664, f1 0.396953\n",
      "2017-11-24T23:26:46.651752: step 3940, loss 0.685905, acc 0.560547, f1 0.402696\n",
      "2017-11-24T23:26:47.618470: step 3945, loss 0.688091, acc 0.551758, f1 0.392377\n",
      "2017-11-24T23:26:48.586060: step 3950, loss 0.682492, acc 0.575195, f1 0.420074\n",
      "2017-11-24T23:26:49.522968: step 3955, loss 0.683265, acc 0.573242, f1 0.417744\n",
      "2017-11-24T23:26:50.491863: step 3960, loss 0.696158, acc 0.530273, f1 0.367503\n",
      "2017-11-24T23:26:51.448352: step 3965, loss 0.686079, acc 0.55957, f1 0.401545\n",
      "2017-11-24T23:26:52.423951: step 3970, loss 0.695189, acc 0.522461, f1 0.358584\n",
      "2017-11-24T23:26:53.379037: step 3975, loss 0.688551, acc 0.557617, f1 0.399247\n",
      "2017-11-24T23:26:54.332929: step 3980, loss 0.687481, acc 0.557617, f1 0.399247\n",
      "2017-11-24T23:26:55.284311: step 3985, loss 0.68154, acc 0.577148, f1 0.422408\n",
      "2017-11-24T23:26:56.243261: step 3990, loss 0.687669, acc 0.552734, f1 0.393519\n",
      "2017-11-24T23:26:57.204915: step 3995, loss 0.687704, acc 0.556641, f1 0.398099\n",
      "2017-11-24T23:26:58.155845: step 4000, loss 0.684505, acc 0.567383, f1 0.410778\n",
      "2017-11-24T23:26:59.111563: step 4005, loss 0.687061, acc 0.5625, f1 0.405\n",
      "2017-11-24T23:27:00.038540: step 4010, loss 0.688421, acc 0.548828, f1 0.388955\n",
      "2017-11-24T23:27:00.989118: step 4015, loss 0.689805, acc 0.542969, f1 0.38214\n",
      "2017-11-24T23:27:01.965743: step 4020, loss 0.686775, acc 0.558594, f1 0.400396\n",
      "2017-11-24T23:27:02.934696: step 4025, loss 0.685537, acc 0.582031, f1 0.42826\n",
      "2017-11-24T23:27:03.865203: step 4030, loss 0.679887, acc 0.584961, f1 0.431783\n",
      "2017-11-24T23:27:04.854894: step 4035, loss 0.685863, acc 0.560547, f1 0.402696\n",
      "2017-11-24T23:27:05.822084: step 4040, loss 0.68364, acc 0.570312, f1 0.414257\n",
      "2017-11-24T23:27:06.781113: step 4045, loss 0.679095, acc 0.588867, f1 0.436493\n",
      "2017-11-24T23:27:07.755055: step 4050, loss 0.683944, acc 0.568359, f1 0.411937\n",
      "2017-11-24T23:27:08.706862: step 4055, loss 0.693109, acc 0.542969, f1 0.38214\n",
      "2017-11-24T23:27:09.686367: step 4060, loss 0.682415, acc 0.573242, f1 0.417744\n",
      "2017-11-24T23:27:10.671552: step 4065, loss 0.688425, acc 0.549805, f1 0.390095\n",
      "2017-11-24T23:27:11.633822: step 4070, loss 0.685146, acc 0.563477, f1 0.406154\n",
      "2017-11-24T23:27:12.586447: step 4075, loss 0.68937, acc 0.543945, f1 0.383273\n",
      "2017-11-24T23:27:13.549071: step 4080, loss 0.688361, acc 0.552734, f1 0.393519\n",
      "2017-11-24T23:27:14.504131: step 4085, loss 0.68355, acc 0.569336, f1 0.413096\n",
      "2017-11-24T23:27:15.453048: step 4090, loss 0.684149, acc 0.572266, f1 0.416581\n",
      "2017-11-24T23:27:16.388715: step 4095, loss 0.687813, acc 0.551758, f1 0.392377\n",
      "2017-11-24T23:27:17.354875: step 4100, loss 0.689945, acc 0.542969, f1 0.38214\n",
      "2017-11-24T23:27:18.303777: step 4105, loss 0.684622, acc 0.56543, f1 0.408464\n",
      "2017-11-24T23:27:19.268827: step 4110, loss 0.68226, acc 0.582031, f1 0.42826\n",
      "2017-11-24T23:27:20.253750: step 4115, loss 0.693433, acc 0.533203, f1 0.370865\n",
      "2017-11-24T23:27:21.186575: step 4120, loss 0.682312, acc 0.576172, f1 0.421241\n",
      "2017-11-24T23:27:22.149296: step 4125, loss 0.687113, acc 0.558594, f1 0.400396\n",
      "2017-11-24T23:27:23.100546: step 4130, loss 0.689494, acc 0.546875, f1 0.386679\n",
      "2017-11-24T23:27:24.083920: step 4135, loss 0.690589, acc 0.539062, f1 0.377617\n",
      "2017-11-24T23:27:25.017003: step 4140, loss 0.688127, acc 0.553711, f1 0.394663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-24T23:27:25.961560: step 4145, loss 0.686196, acc 0.55957, f1 0.401545\n",
      "2017-11-24T23:27:26.931505: step 4150, loss 0.686554, acc 0.560547, f1 0.402696\n",
      "2017-11-24T23:27:27.893041: step 4155, loss 0.686981, acc 0.555664, f1 0.396953\n",
      "2017-11-24T23:27:28.851446: step 4160, loss 0.685633, acc 0.564453, f1 0.407308\n",
      "2017-11-24T23:27:29.789203: step 4165, loss 0.689167, acc 0.544922, f1 0.384408\n",
      "\n",
      "\n",
      "Current epoch:  4\n",
      "2017-11-24T23:27:31.020995: step 4170, loss 0.685228, acc 0.564453, f1 0.407308\n",
      "2017-11-24T23:27:31.971063: step 4175, loss 0.685811, acc 0.561523, f1 0.403847\n",
      "2017-11-24T23:27:32.914671: step 4180, loss 0.687643, acc 0.553711, f1 0.394663\n",
      "2017-11-24T23:27:33.877247: step 4185, loss 0.689372, acc 0.543945, f1 0.383273\n",
      "2017-11-24T23:27:34.824831: step 4190, loss 0.683123, acc 0.572266, f1 0.416581\n",
      "2017-11-24T23:27:35.776614: step 4195, loss 0.690649, acc 0.548828, f1 0.388955\n",
      "2017-11-24T23:27:36.747752: step 4200, loss 0.683614, acc 0.571289, f1 0.415418\n",
      "2017-11-24T23:27:37.719434: step 4205, loss 0.685662, acc 0.561523, f1 0.403847\n",
      "2017-11-24T23:27:38.692812: step 4210, loss 0.690023, acc 0.543945, f1 0.383273\n",
      "2017-11-24T23:27:39.675620: step 4215, loss 0.684813, acc 0.567383, f1 0.410778\n",
      "2017-11-24T23:27:40.650229: step 4220, loss 0.679325, acc 0.589844, f1 0.437673\n",
      "2017-11-24T23:27:41.574153: step 4225, loss 0.687903, acc 0.560547, f1 0.402696\n",
      "2017-11-24T23:27:42.566331: step 4230, loss 0.68519, acc 0.563477, f1 0.406154\n",
      "2017-11-24T23:27:43.490992: step 4235, loss 0.684599, acc 0.569336, f1 0.413096\n",
      "2017-11-24T23:27:44.428570: step 4240, loss 0.688015, acc 0.550781, f1 0.391235\n",
      "2017-11-24T23:27:45.370864: step 4245, loss 0.684709, acc 0.566406, f1 0.40962\n",
      "2017-11-24T23:27:46.348863: step 4250, loss 0.685348, acc 0.5625, f1 0.405\n",
      "2017-11-24T23:27:47.335093: step 4255, loss 0.686797, acc 0.557617, f1 0.399247\n",
      "2017-11-24T23:27:48.298304: step 4260, loss 0.683486, acc 0.570312, f1 0.414257\n",
      "2017-11-24T23:27:49.258378: step 4265, loss 0.69021, acc 0.540039, f1 0.378746\n",
      "2017-11-24T23:27:50.211476: step 4270, loss 0.689664, acc 0.541992, f1 0.381008\n",
      "2017-11-24T23:27:51.186718: step 4275, loss 0.677599, acc 0.591797, f1 0.440035\n",
      "2017-11-24T23:27:52.166200: step 4280, loss 0.686311, acc 0.561523, f1 0.403847\n",
      "2017-11-24T23:27:53.102470: step 4285, loss 0.691618, acc 0.539062, f1 0.377617\n",
      "2017-11-24T23:27:54.040142: step 4290, loss 0.688909, acc 0.547852, f1 0.387817\n",
      "2017-11-24T23:27:54.972365: step 4295, loss 0.686529, acc 0.557617, f1 0.399247\n",
      "2017-11-24T23:27:55.950600: step 4300, loss 0.683951, acc 0.569336, f1 0.413096\n",
      "2017-11-24T23:27:56.912225: step 4305, loss 0.683948, acc 0.568359, f1 0.411937\n",
      "2017-11-24T23:27:57.854689: step 4310, loss 0.690948, acc 0.560547, f1 0.402696\n",
      "2017-11-24T23:27:58.828845: step 4315, loss 0.689628, acc 0.544922, f1 0.384408\n",
      "2017-11-24T23:27:59.786390: step 4320, loss 0.690525, acc 0.538086, f1 0.376489\n",
      "2017-11-24T23:28:00.732572: step 4325, loss 0.68786, acc 0.552734, f1 0.393519\n",
      "2017-11-24T23:28:01.707499: step 4330, loss 0.692398, acc 0.527344, f1 0.36415\n",
      "2017-11-24T23:28:02.674263: step 4335, loss 0.685117, acc 0.566406, f1 0.40962\n",
      "2017-11-24T23:28:03.628010: step 4340, loss 0.68656, acc 0.557617, f1 0.399247\n",
      "2017-11-24T23:28:04.609597: step 4345, loss 0.691411, acc 0.537109, f1 0.375362\n",
      "2017-11-24T23:28:05.591924: step 4350, loss 0.687326, acc 0.566406, f1 0.40962\n",
      "2017-11-24T23:28:06.564292: step 4355, loss 0.688452, acc 0.548828, f1 0.388955\n",
      "2017-11-24T23:28:07.535160: step 4360, loss 0.684634, acc 0.567383, f1 0.410778\n",
      "2017-11-24T23:28:08.530549: step 4365, loss 0.687973, acc 0.551758, f1 0.392377\n",
      "2017-11-24T23:28:09.480334: step 4370, loss 0.68884, acc 0.548828, f1 0.388955\n",
      "2017-11-24T23:28:10.452684: step 4375, loss 0.68323, acc 0.575195, f1 0.420074\n",
      "2017-11-24T23:28:11.421198: step 4380, loss 0.687157, acc 0.555664, f1 0.396953\n",
      "2017-11-24T23:28:12.394880: step 4385, loss 0.688409, acc 0.548828, f1 0.388955\n",
      "2017-11-24T23:28:13.363285: step 4390, loss 0.687636, acc 0.553711, f1 0.394663\n",
      "2017-11-24T23:28:14.331185: step 4395, loss 0.681542, acc 0.582031, f1 0.42826\n",
      "2017-11-24T23:28:15.286788: step 4400, loss 0.683538, acc 0.578125, f1 0.423577\n",
      "2017-11-24T23:28:16.273716: step 4405, loss 0.685649, acc 0.561523, f1 0.403847\n",
      "2017-11-24T23:28:17.230919: step 4410, loss 0.683148, acc 0.573242, f1 0.417744\n",
      "2017-11-24T23:28:18.201793: step 4415, loss 0.685455, acc 0.5625, f1 0.405\n",
      "2017-11-24T23:28:19.168707: step 4420, loss 0.688855, acc 0.552734, f1 0.393519\n",
      "2017-11-24T23:28:20.131771: step 4425, loss 0.691454, acc 0.536133, f1 0.374236\n",
      "2017-11-24T23:28:21.099776: step 4430, loss 0.682319, acc 0.574219, f1 0.418909\n",
      "2017-11-24T23:28:22.047264: step 4435, loss 0.687608, acc 0.552734, f1 0.393519\n",
      "2017-11-24T23:28:23.004694: step 4440, loss 0.689933, acc 0.541016, f1 0.379877\n",
      "2017-11-24T23:28:23.979779: step 4445, loss 0.68368, acc 0.572266, f1 0.416581\n",
      "2017-11-24T23:28:24.921223: step 4450, loss 0.684023, acc 0.579102, f1 0.424746\n",
      "2017-11-24T23:28:25.835393: step 4455, loss 0.694319, acc 0.517578, f1 0.353046\n",
      "2017-11-24T23:28:26.776811: step 4460, loss 0.688823, acc 0.549805, f1 0.390095\n",
      "2017-11-24T23:28:27.724204: step 4465, loss 0.693041, acc 0.530273, f1 0.367503\n",
      "2017-11-24T23:28:28.657060: step 4470, loss 0.689729, acc 0.545898, f1 0.385543\n",
      "2017-11-24T23:28:29.599860: step 4475, loss 0.689975, acc 0.543945, f1 0.383273\n",
      "2017-11-24T23:28:30.578212: step 4480, loss 0.685692, acc 0.564453, f1 0.407308\n",
      "2017-11-24T23:28:31.543026: step 4485, loss 0.675127, acc 0.59668, f1 0.445959\n",
      "2017-11-24T23:28:32.509582: step 4490, loss 0.697668, acc 0.513672, f1 0.348634\n",
      "2017-11-24T23:28:33.513583: step 4495, loss 0.686626, acc 0.557617, f1 0.399247\n",
      "2017-11-24T23:28:34.475645: step 4500, loss 0.691402, acc 0.537109, f1 0.375362\n",
      "2017-11-24T23:28:35.436982: step 4505, loss 0.688427, acc 0.550781, f1 0.391235\n",
      "2017-11-24T23:28:36.380350: step 4510, loss 0.683856, acc 0.569336, f1 0.413096\n",
      "2017-11-24T23:28:37.300544: step 4515, loss 0.688907, acc 0.547852, f1 0.387817\n",
      "2017-11-24T23:28:38.248552: step 4520, loss 0.687035, acc 0.555664, f1 0.396953\n",
      "2017-11-24T23:28:39.186395: step 4525, loss 0.689712, acc 0.543945, f1 0.383273\n",
      "2017-11-24T23:28:40.154203: step 4530, loss 0.687399, acc 0.553711, f1 0.394663\n",
      "2017-11-24T23:28:41.125370: step 4535, loss 0.68718, acc 0.569336, f1 0.413096\n",
      "2017-11-24T23:28:42.092595: step 4540, loss 0.689812, acc 0.541016, f1 0.379877\n",
      "2017-11-24T23:28:43.072699: step 4545, loss 0.689791, acc 0.543945, f1 0.383273\n",
      "2017-11-24T23:28:44.050778: step 4550, loss 0.683423, acc 0.574219, f1 0.418909\n",
      "2017-11-24T23:28:45.013046: step 4555, loss 0.688794, acc 0.552734, f1 0.393519\n",
      "2017-11-24T23:28:45.959674: step 4560, loss 0.680418, acc 0.583984, f1 0.430607\n",
      "2017-11-24T23:28:46.897919: step 4565, loss 0.685394, acc 0.5625, f1 0.405\n",
      "2017-11-24T23:28:47.851250: step 4570, loss 0.685785, acc 0.561523, f1 0.403847\n",
      "2017-11-24T23:28:48.829234: step 4575, loss 0.692306, acc 0.533203, f1 0.370865\n",
      "2017-11-24T23:28:49.822732: step 4580, loss 0.684847, acc 0.564453, f1 0.407308\n",
      "2017-11-24T23:28:50.783020: step 4585, loss 0.688176, acc 0.552734, f1 0.393519\n",
      "2017-11-24T23:28:51.743144: step 4590, loss 0.687028, acc 0.556641, f1 0.398099\n",
      "2017-11-24T23:28:52.702263: step 4595, loss 0.688798, acc 0.546875, f1 0.386679\n",
      "2017-11-24T23:28:53.654000: step 4600, loss 0.688793, acc 0.549805, f1 0.390095\n",
      "2017-11-24T23:28:54.627622: step 4605, loss 0.685257, acc 0.56543, f1 0.408464\n",
      "2017-11-24T23:28:55.642365: step 4610, loss 0.688788, acc 0.546875, f1 0.386679\n",
      "2017-11-24T23:28:56.594392: step 4615, loss 0.688302, acc 0.550781, f1 0.391235\n",
      "2017-11-24T23:28:57.530659: step 4620, loss 0.690544, acc 0.538086, f1 0.376489\n",
      "2017-11-24T23:28:58.488453: step 4625, loss 0.689371, acc 0.551758, f1 0.392377\n",
      "2017-11-24T23:28:59.433886: step 4630, loss 0.689925, acc 0.541016, f1 0.379877\n",
      "2017-11-24T23:29:00.395400: step 4635, loss 0.691497, acc 0.53418, f1 0.371988\n",
      "2017-11-24T23:29:01.352866: step 4640, loss 0.688597, acc 0.547852, f1 0.387817\n",
      "2017-11-24T23:29:02.345586: step 4645, loss 0.686975, acc 0.555664, f1 0.396953\n",
      "2017-11-24T23:29:03.317165: step 4650, loss 0.678848, acc 0.584961, f1 0.431783\n",
      "2017-11-24T23:29:04.284161: step 4655, loss 0.688506, acc 0.551758, f1 0.392377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-24T23:29:05.271219: step 4660, loss 0.683967, acc 0.569336, f1 0.413096\n",
      "2017-11-24T23:29:06.228124: step 4665, loss 0.695215, acc 0.527344, f1 0.36415\n",
      "2017-11-24T23:29:07.169857: step 4670, loss 0.685511, acc 0.5625, f1 0.405\n",
      "2017-11-24T23:29:08.119826: step 4675, loss 0.683999, acc 0.572266, f1 0.416581\n",
      "2017-11-24T23:29:09.097256: step 4680, loss 0.682753, acc 0.573242, f1 0.417744\n",
      "2017-11-24T23:29:10.055792: step 4685, loss 0.686347, acc 0.558594, f1 0.400396\n",
      "2017-11-24T23:29:11.009251: step 4690, loss 0.687355, acc 0.554688, f1 0.395807\n",
      "2017-11-24T23:29:11.993682: step 4695, loss 0.693731, acc 0.535156, f1 0.373111\n",
      "2017-11-24T23:29:12.963985: step 4700, loss 0.688062, acc 0.550781, f1 0.391235\n",
      "2017-11-24T23:29:13.933208: step 4705, loss 0.683684, acc 0.580078, f1 0.425916\n",
      "2017-11-24T23:29:14.914700: step 4710, loss 0.678795, acc 0.585938, f1 0.432959\n",
      "2017-11-24T23:29:15.882536: step 4715, loss 0.684904, acc 0.56543, f1 0.408464\n",
      "2017-11-24T23:29:16.842231: step 4720, loss 0.6864, acc 0.558594, f1 0.400396\n",
      "2017-11-24T23:29:17.793265: step 4725, loss 0.689602, acc 0.547852, f1 0.387817\n",
      "2017-11-24T23:29:18.739753: step 4730, loss 0.685984, acc 0.573242, f1 0.417744\n",
      "2017-11-24T23:29:19.686761: step 4735, loss 0.688594, acc 0.547852, f1 0.387817\n",
      "2017-11-24T23:29:20.639907: step 4740, loss 0.688104, acc 0.550781, f1 0.391235\n",
      "2017-11-24T23:29:21.608110: step 4745, loss 0.689312, acc 0.543945, f1 0.383273\n",
      "2017-11-24T23:29:22.557581: step 4750, loss 0.688898, acc 0.548828, f1 0.388955\n",
      "2017-11-24T23:29:23.493653: step 4755, loss 0.687443, acc 0.554688, f1 0.395807\n",
      "2017-11-24T23:29:24.453652: step 4760, loss 0.685358, acc 0.5625, f1 0.405\n",
      "2017-11-24T23:29:25.411296: step 4765, loss 0.685452, acc 0.5625, f1 0.405\n",
      "2017-11-24T23:29:26.362801: step 4770, loss 0.689539, acc 0.546875, f1 0.386679\n",
      "2017-11-24T23:29:27.346849: step 4775, loss 0.687347, acc 0.554688, f1 0.395807\n",
      "2017-11-24T23:29:28.293603: step 4780, loss 0.686437, acc 0.558594, f1 0.400396\n",
      "2017-11-24T23:29:29.261712: step 4785, loss 0.688948, acc 0.554688, f1 0.395807\n",
      "2017-11-24T23:29:30.214222: step 4790, loss 0.683051, acc 0.572266, f1 0.416581\n",
      "2017-11-24T23:29:31.193950: step 4795, loss 0.683092, acc 0.572266, f1 0.416581\n",
      "2017-11-24T23:29:32.158253: step 4800, loss 0.690104, acc 0.540039, f1 0.378746\n",
      "2017-11-24T23:29:33.118069: step 4805, loss 0.688868, acc 0.548828, f1 0.388955\n",
      "2017-11-24T23:29:34.070621: step 4810, loss 0.691969, acc 0.529297, f1 0.366384\n",
      "2017-11-24T23:29:35.024494: step 4815, loss 0.682664, acc 0.573242, f1 0.417744\n",
      "2017-11-24T23:29:35.976528: step 4820, loss 0.685924, acc 0.563477, f1 0.406154\n",
      "2017-11-24T23:29:36.926293: step 4825, loss 0.689896, acc 0.541016, f1 0.379877\n",
      "2017-11-24T23:29:37.999435: step 4830, loss 0.696338, acc 0.518555, f1 0.354151\n",
      "2017-11-24T23:29:38.947949: step 4835, loss 0.689669, acc 0.541992, f1 0.381008\n",
      "2017-11-24T23:29:39.912006: step 4840, loss 0.685706, acc 0.561523, f1 0.403847\n",
      "2017-11-24T23:29:40.858945: step 4845, loss 0.681958, acc 0.578125, f1 0.423577\n",
      "2017-11-24T23:29:41.808025: step 4850, loss 0.691997, acc 0.539062, f1 0.377617\n",
      "2017-11-24T23:29:42.745364: step 4855, loss 0.679681, acc 0.584961, f1 0.431783\n",
      "2017-11-24T23:29:43.712928: step 4860, loss 0.693744, acc 0.53125, f1 0.368622\n",
      "2017-11-24T23:29:44.654609: step 4865, loss 0.685145, acc 0.563477, f1 0.406154\n",
      "2017-11-24T23:29:45.592251: step 4870, loss 0.682214, acc 0.575195, f1 0.420074\n",
      "2017-11-24T23:29:46.547819: step 4875, loss 0.680969, acc 0.583008, f1 0.429433\n",
      "2017-11-24T23:29:47.471883: step 4880, loss 0.687815, acc 0.551758, f1 0.392377\n",
      "2017-11-24T23:29:48.404249: step 4885, loss 0.688829, acc 0.549805, f1 0.390095\n",
      "2017-11-24T23:29:49.341672: step 4890, loss 0.684028, acc 0.570312, f1 0.414257\n",
      "2017-11-24T23:29:50.331809: step 4895, loss 0.682046, acc 0.580078, f1 0.425916\n",
      "2017-11-24T23:29:51.282240: step 4900, loss 0.686055, acc 0.5625, f1 0.405\n",
      "2017-11-24T23:29:52.256453: step 4905, loss 0.692243, acc 0.540039, f1 0.378746\n",
      "2017-11-24T23:29:53.230190: step 4910, loss 0.67694, acc 0.595703, f1 0.444772\n",
      "2017-11-24T23:29:54.183783: step 4915, loss 0.682734, acc 0.572266, f1 0.416581\n",
      "2017-11-24T23:29:55.145706: step 4920, loss 0.685504, acc 0.5625, f1 0.405\n",
      "2017-11-24T23:29:56.114795: step 4925, loss 0.695412, acc 0.526367, f1 0.363035\n",
      "2017-11-24T23:29:57.071363: step 4930, loss 0.687694, acc 0.554688, f1 0.395807\n",
      "2017-11-24T23:29:58.050647: step 4935, loss 0.694553, acc 0.540039, f1 0.378746\n",
      "2017-11-24T23:29:59.036321: step 4940, loss 0.690793, acc 0.544922, f1 0.384408\n",
      "2017-11-24T23:30:00.001463: step 4945, loss 0.689805, acc 0.545898, f1 0.385543\n",
      "2017-11-24T23:30:00.981558: step 4950, loss 0.68809, acc 0.550781, f1 0.391235\n",
      "2017-11-24T23:30:01.923566: step 4955, loss 0.682171, acc 0.581055, f1 0.427088\n",
      "2017-11-24T23:30:02.911437: step 4960, loss 0.688746, acc 0.549805, f1 0.390095\n",
      "2017-11-24T23:30:03.876059: step 4965, loss 0.683545, acc 0.569336, f1 0.413096\n",
      "2017-11-24T23:30:04.811807: step 4970, loss 0.689063, acc 0.546875, f1 0.386679\n",
      "2017-11-24T23:30:05.776988: step 4975, loss 0.682336, acc 0.582031, f1 0.42826\n",
      "2017-11-24T23:30:06.713277: step 4980, loss 0.681149, acc 0.580078, f1 0.425916\n",
      "2017-11-24T23:30:07.672002: step 4985, loss 0.678695, acc 0.588867, f1 0.436493\n",
      "2017-11-24T23:30:08.635484: step 4990, loss 0.697864, acc 0.542969, f1 0.38214\n",
      "2017-11-24T23:30:09.566567: step 4995, loss 0.686352, acc 0.558594, f1 0.400396\n",
      "2017-11-24T23:30:10.541114: step 5000, loss 0.688888, acc 0.553711, f1 0.394663\n",
      "2017-11-24T23:30:11.500881: step 5005, loss 0.684696, acc 0.567383, f1 0.410778\n",
      "2017-11-24T23:30:12.487448: step 5010, loss 0.685775, acc 0.5625, f1 0.405\n",
      "2017-11-24T23:30:13.420314: step 5015, loss 0.680009, acc 0.581055, f1 0.427088\n",
      "2017-11-24T23:30:14.368066: step 5020, loss 0.687401, acc 0.553711, f1 0.394663\n",
      "2017-11-24T23:30:15.326699: step 5025, loss 0.686109, acc 0.55957, f1 0.401545\n",
      "2017-11-24T23:30:16.258592: step 5030, loss 0.688781, acc 0.546875, f1 0.386679\n",
      "2017-11-24T23:30:17.190861: step 5035, loss 0.685346, acc 0.5625, f1 0.405\n",
      "2017-11-24T23:30:18.139279: step 5040, loss 0.68742, acc 0.553711, f1 0.394663\n",
      "2017-11-24T23:30:19.094565: step 5045, loss 0.687028, acc 0.555664, f1 0.396953\n",
      "2017-11-24T23:30:20.060601: step 5050, loss 0.682331, acc 0.576172, f1 0.421241\n",
      "2017-11-24T23:30:21.020250: step 5055, loss 0.685558, acc 0.5625, f1 0.405\n",
      "2017-11-24T23:30:21.997535: step 5060, loss 0.686532, acc 0.557617, f1 0.399247\n",
      "2017-11-24T23:30:22.947543: step 5065, loss 0.688441, acc 0.548828, f1 0.388955\n",
      "2017-11-24T23:30:23.896527: step 5070, loss 0.687484, acc 0.561523, f1 0.403847\n",
      "2017-11-24T23:30:24.843344: step 5075, loss 0.687704, acc 0.554688, f1 0.395807\n",
      "2017-11-24T23:30:25.811530: step 5080, loss 0.688612, acc 0.547852, f1 0.387817\n",
      "2017-11-24T23:30:26.776011: step 5085, loss 0.688334, acc 0.552734, f1 0.393519\n",
      "2017-11-24T23:30:27.769514: step 5090, loss 0.686373, acc 0.560547, f1 0.402696\n",
      "2017-11-24T23:30:28.717601: step 5095, loss 0.692267, acc 0.52832, f1 0.365267\n",
      "2017-11-24T23:30:29.661150: step 5100, loss 0.692782, acc 0.525391, f1 0.361921\n",
      "2017-11-24T23:30:30.610668: step 5105, loss 0.686938, acc 0.556641, f1 0.398099\n",
      "2017-11-24T23:30:31.607785: step 5110, loss 0.690655, acc 0.547852, f1 0.387817\n",
      "2017-11-24T23:30:32.577446: step 5115, loss 0.690223, acc 0.545898, f1 0.385543\n",
      "2017-11-24T23:30:33.538548: step 5120, loss 0.689333, acc 0.544922, f1 0.384408\n",
      "2017-11-24T23:30:34.498471: step 5125, loss 0.683859, acc 0.568359, f1 0.411937\n",
      "2017-11-24T23:30:35.466614: step 5130, loss 0.689213, acc 0.545898, f1 0.385543\n",
      "2017-11-24T23:30:36.415016: step 5135, loss 0.692434, acc 0.52832, f1 0.365267\n",
      "2017-11-24T23:30:37.380407: step 5140, loss 0.68195, acc 0.576172, f1 0.421241\n",
      "2017-11-24T23:30:38.315315: step 5145, loss 0.680318, acc 0.589844, f1 0.437673\n",
      "2017-11-24T23:30:39.238144: step 5150, loss 0.689842, acc 0.541016, f1 0.379877\n",
      "2017-11-24T23:30:40.185545: step 5155, loss 0.688416, acc 0.548828, f1 0.388955\n",
      "2017-11-24T23:30:41.144147: step 5160, loss 0.685472, acc 0.566406, f1 0.40962\n",
      "2017-11-24T23:30:42.107017: step 5165, loss 0.685585, acc 0.5625, f1 0.405\n",
      "2017-11-24T23:30:43.070706: step 5170, loss 0.693551, acc 0.524414, f1 0.360808\n",
      "2017-11-24T23:30:44.042160: step 5175, loss 0.690117, acc 0.541992, f1 0.381008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-24T23:30:44.993731: step 5180, loss 0.686666, acc 0.557617, f1 0.399247\n",
      "2017-11-24T23:30:45.940241: step 5185, loss 0.6847, acc 0.56543, f1 0.408464\n",
      "2017-11-24T23:30:46.896080: step 5190, loss 0.685784, acc 0.5625, f1 0.405\n",
      "2017-11-24T23:30:47.846184: step 5195, loss 0.693432, acc 0.524414, f1 0.360808\n",
      "2017-11-24T23:30:48.797622: step 5200, loss 0.689649, acc 0.541992, f1 0.381008\n",
      "2017-11-24T23:30:49.759542: step 5205, loss 0.688197, acc 0.550781, f1 0.391235\n",
      "2017-11-24T23:30:50.709108: step 5210, loss 0.686732, acc 0.556939, f1 0.39845\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "num_epochs = 5\n",
    "\n",
    "num_checkpoints = 5\n",
    "print_train_every = 5\n",
    "evaluate_every = 10000000\n",
    "checkpoint_every = 10000000\n",
    "allow_soft_placement=True\n",
    "log_device_placement=False\n",
    "\n",
    "with graph.as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=allow_soft_placement,\n",
    "      log_device_placement=log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        \n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdadeltaOptimizer(1.0)\n",
    "        grads_and_vars = optimizer.compute_gradients(loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=num_checkpoints)\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              input_x: x_batch,\n",
    "              input_y: y_batch,\n",
    "            }\n",
    "            _, step, cur_loss, cur_accuracy, y_pred = sess.run(\n",
    "                [train_op, global_step, loss, accuracy, predictions],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "\n",
    "            if step % print_train_every == 0:\n",
    "                f1 = f1_score(y_batch, y_pred, average = 'weighted')\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}, f1 {:g}\".format(time_str, step, cur_loss, cur_accuracy,\n",
    "                                                                    f1))\n",
    "     \n",
    "        def dev_step_batch(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              input_x: x_batch,\n",
    "              input_y: y_batch,\n",
    "            }\n",
    "            \n",
    "            step, cur_loss, cur_accuracy, y_pred = sess.run(\n",
    "                [global_step, loss, accuracy, predictions],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            f1 = f1_score(y_batch, y_pred, average = 'weighted')\n",
    "\n",
    "            return cur_loss, cur_accuracy, f1\n",
    "        \n",
    "        \n",
    "        sess.run(embedding_init, feed_dict={embedding_placeholder: original_embeddings})\n",
    "        # Generate batches\n",
    "        batches = batch_iter(\n",
    "            list(zip(x_train_distance, y_train)), batch_size, num_epochs)\n",
    "        \n",
    "        batches_test = list(batch_iter(\n",
    "            list(zip(x_test_distance, y_test)), batch_size, 1))\n",
    "        \n",
    "        # Training loop. For each batch...\n",
    "        for batch, end_of_epoch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            \n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if end_of_epoch:\n",
    "                print(\"\\n\")\n",
    "            if current_step % checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "        final_embeddings = embeddings_words.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del x_train_distance, x_test_distance, y_test,y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9770612, 52)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save embeddings\n",
    "np.save('final_embeddings_52_distance_modified_paper.npy', final_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings from previous work\n",
    "final_embeddings = np.load('./final_embeddings_52_distance_modified_paper.npy')\n",
    "word_dict = {}\n",
    "with open('./data/embed_tweets_en_590M_52D_data/vocabulary_dict_52.pickle', 'rb') as myfile:\n",
    "    word_dict = pickle.load(myfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(186818, 52)\n",
      "9770611\n"
     ]
    }
   ],
   "source": [
    "print(final_embeddings.shape)\n",
    "print(len(word_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20632\n",
      "53\n",
      "dropped  0\n"
     ]
    }
   ],
   "source": [
    "#Load label data\n",
    "x_train_sentence, y_train, x_test_sentence, y_test = load_data_and_labels('./data/supervised_data/en_full.tsv.txt', './data/supervised_data/en_test.tsv')\n",
    "print(len(x_test_sentence))\n",
    "x_train_token, x_test_token, _, max_length = tokenize_tweet(x_train_sentence, x_test_sentence, word_dict)\n",
    "del x_test_sentence,x_train_sentence\n",
    "x_train, x_test = process_tweet(x_train_token, x_test_token, word_dict, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 53)\n",
      "(?, 53, 52, 1)\n",
      "CNN filter (4, 52, 1, 200)\n",
      "CNN filter (3, 1, 200, 200)\n",
      "(?, 22, 1, 200)\n",
      "h_pool_flat (?, 200)\n",
      "scores (?, 3)\n",
      "predictions (?,)\n",
      "losses (?,)\n"
     ]
    }
   ],
   "source": [
    "# put Word2Vec on 590 million English Tweets using 52 dimensions.\n",
    "vocabulary_size = final_embeddings.shape[0]\n",
    "embedding_size = final_embeddings.shape[1]  # Dimension of the embedding vector.\n",
    "graph = tf.Graph()\n",
    "\n",
    "sequence_length=x_train.shape[1]\n",
    "num_classes=3\n",
    "\n",
    "# filter_sizes: The number of words we want our convolutional filters to cover. \n",
    "# We will have num_filters for each size specified here. \n",
    "# For example, [3, 4, 5] means that we will have filters that slide over 3, 4 and 5 words respectively, for a total of 3 * num_filters filters.\n",
    "first_filter_sizes = [4]\n",
    "first_pool_window_sizes = [4]\n",
    "first_pool_strides = [2]\n",
    "\n",
    "\n",
    "second_filter_window_sizes = [3]\n",
    "num_filters = 200\n",
    "\n",
    "# No L2 norm\n",
    "l2_reg_lambda=0.0\n",
    "\n",
    "with graph.as_default():\n",
    "    input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "    input_y = tf.placeholder(tf.int64, [None], name=\"input_y\")\n",
    "#     with tf.device('/cpu:0'):\n",
    "    embeddings_words = tf.Variable(tf.constant(0.0, shape=[vocabulary_size, embedding_size]),\n",
    "                     name=\"embedding_words\")\n",
    "    embedding_padding = tf.Variable(tf.constant(0.0, shape=[1, embedding_size]),\n",
    "                     trainable = False, name=\"embedding_padding\")\n",
    "\n",
    "    embedding_placeholder = tf.placeholder(tf.float32, [vocabulary_size, embedding_size], name='word_embedding_placeholder')\n",
    "    embedding_init = embeddings_words.assign(embedding_placeholder)  # assign exist word embeddings\n",
    "\n",
    "    embeddings = tf.concat([embeddings_words, embedding_padding], 0, name = 'embedding')\n",
    "\n",
    "    embedded_chars = tf.nn.embedding_lookup(embeddings, input_x)\n",
    "    embedded_chars_expanded = tf.expand_dims(embedded_chars, -1)\n",
    "    print(input_x.shape)\n",
    "    print(embedded_chars_expanded.shape)\n",
    "    # Keeping track of l2 regularization loss (optional)\n",
    "    l2_loss = tf.constant(0.0)\n",
    "    \n",
    "     # Add variable initializer.\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    \n",
    "    # Create first cnn : a convolution + maxpool layer for each filter size    \n",
    "    # 1st Convolution Layer\n",
    "    for i, first_filter_size in enumerate(first_filter_sizes):\n",
    "        with tf.name_scope(\"conv-maxpool-1\"):\n",
    "            # Convolution Layer\n",
    "            filter_shape = [first_filter_size, embedding_size, 1, num_filters]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "            print(\"CNN filter\", W.shape)\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "            conv = tf.nn.conv2d(\n",
    "                embedded_chars_expanded,\n",
    "                W,\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding=\"VALID\",\n",
    "                name=\"conv\")\n",
    "            # Apply nonlinearity\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "            # Maxpooling over the outputs and transform for next layer, so the \"channel\" of convolution \n",
    "            # will become \"input_width\" for next layer\n",
    "            pooled = tf.nn.max_pool(\n",
    "                h,\n",
    "                ksize=[1, first_pool_window_sizes[i], 1, 1],\n",
    "                strides=[1, first_pool_strides[i], 1, 1],\n",
    "                padding='VALID',\n",
    "                name=\"pool\")\n",
    "\n",
    "    for i, second_filter_size in enumerate(second_filter_window_sizes):\n",
    "        with tf.name_scope(\"conv-maxpool-2\"):\n",
    "            # Convolution Layer\n",
    "            filter_shape = [second_filter_size, 1, num_filters, num_filters]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "            print(\"CNN filter\", W.shape)\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "            conv = tf.nn.conv2d(\n",
    "                pooled,\n",
    "                W,\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding=\"VALID\",\n",
    "                name=\"conv\")\n",
    "            # Apply nonlinearity\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "            print(h.shape)\n",
    "            # Maxpooling over the outputs and transform for next layer, so the \"channel\" of convolution \n",
    "            # will become \"input_width\" for next layer\n",
    "            pooled = tf.nn.max_pool(\n",
    "                h,\n",
    "                ksize=[1, h.shape[1], 1, 1],\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding='VALID',\n",
    "                name=\"pool\")\n",
    " \n",
    "\n",
    "    h_pool_flat = tf.reshape(pooled, [-1, num_filters])  # flatten pooling layers\n",
    "    print(\"h_pool_flat\", h_pool_flat.shape)\n",
    "    \n",
    "    # Final (unnormalized) scores and predictions\n",
    "    \n",
    "    # Fully connected hidden layer\n",
    "    with tf.name_scope(\"hidden\"):\n",
    "        with tf.variable_scope(\"hidden\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters, num_filters],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            out = tf.nn.relu(tf.nn.xw_plus_b(h_pool_flat, W, b))\n",
    "        \n",
    "    \n",
    "    with tf.name_scope(\"output\"):\n",
    "        with tf.variable_scope(\"output\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            scores = tf.nn.xw_plus_b(out, W, b, name=\"scores\")\n",
    "            print(\"scores\", scores.shape)\n",
    "            predictions = tf.argmax(scores, 1, name=\"predictions\")\n",
    "            print(\"predictions\", predictions.shape)\n",
    "\n",
    "\n",
    "    # Calculate mean cross-entropy loss\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=scores, labels=input_y)\n",
    "        print(\"losses\", losses.shape)\n",
    "        loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "    # Accuracy\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        correct_predictions = tf.equal(predictions, input_y)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.04302618511916623&quot;).pbtxt = 'node {\\n  name: &quot;input_x&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: -1\\n        }\\n        dim {\\n          size: 53\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;input_y&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: -1\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Const&quot;\\n  op: &quot;Const&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 9770612\\n          }\\n          dim {\\n            size: 52\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;embedding_words&quot;\\n  op: &quot;VariableV2&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 9770612\\n        }\\n        dim {\\n          size: 52\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;embedding_words/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;embedding_words&quot;\\n  input: &quot;Const&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding_words&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;embedding_words/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;embedding_words&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding_words&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Const_1&quot;\\n  op: &quot;Const&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n          dim {\\n            size: 52\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;embedding_padding&quot;\\n  op: &quot;VariableV2&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 1\\n        }\\n        dim {\\n          size: 52\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;embedding_padding/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;embedding_padding&quot;\\n  input: &quot;Const_1&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding_padding&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;embedding_padding/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;embedding_padding&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding_padding&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;word_embedding_placeholder&quot;\\n  op: &quot;Placeholder&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 9770612\\n        }\\n        dim {\\n          size: 52\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;embedding_words&quot;\\n  input: &quot;word_embedding_placeholder&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding_words&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;embedding/axis&quot;\\n  op: &quot;Const&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;embedding&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;embedding_words/read&quot;\\n  input: &quot;embedding_padding/read&quot;\\n  input: &quot;embedding/axis&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;embedding_lookup&quot;\\n  op: &quot;Gather&quot;\\n  input: &quot;embedding&quot;\\n  input: &quot;input_x&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;Tindices&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tparams&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_indices&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ExpandDims/dim&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: -1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ExpandDims&quot;\\n  op: &quot;ExpandDims&quot;\\n  input: &quot;embedding_lookup&quot;\\n  input: &quot;ExpandDims/dim&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tdim&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Const_2&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;init&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^embedding_words/Assign&quot;\\n  input: &quot;^embedding_padding/Assign&quot;\\n  device: &quot;/device:CPU:0&quot;\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/truncated_normal/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\004\\\\000\\\\000\\\\0004\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\310\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/truncated_normal/mean&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/truncated_normal/stddev&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.10000000149011612\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/truncated_normal/TruncatedNormal&quot;\\n  op: &quot;TruncatedNormal&quot;\\n  input: &quot;conv-maxpool-1/truncated_normal/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/truncated_normal/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;conv-maxpool-1/truncated_normal/TruncatedNormal&quot;\\n  input: &quot;conv-maxpool-1/truncated_normal/stddev&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/truncated_normal&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;conv-maxpool-1/truncated_normal/mul&quot;\\n  input: &quot;conv-maxpool-1/truncated_normal/mean&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/W&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 4\\n        }\\n        dim {\\n          size: 52\\n        }\\n        dim {\\n          size: 1\\n        }\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/W/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;conv-maxpool-1/W&quot;\\n  input: &quot;conv-maxpool-1/truncated_normal&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-1/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/W/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;conv-maxpool-1/W&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-1/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 200\\n          }\\n        }\\n        float_val: 0.10000000149011612\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/b&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/b/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;conv-maxpool-1/b&quot;\\n  input: &quot;conv-maxpool-1/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-1/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/b/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;conv-maxpool-1/b&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-1/b&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/conv&quot;\\n  op: &quot;Conv2D&quot;\\n  input: &quot;ExpandDims&quot;\\n  input: &quot;conv-maxpool-1/W/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;VALID&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;conv-maxpool-1/conv&quot;\\n  input: &quot;conv-maxpool-1/b/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;conv-maxpool-1/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/pool&quot;\\n  op: &quot;MaxPool&quot;\\n  input: &quot;conv-maxpool-1/relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;ksize&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 4\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;VALID&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 2\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/truncated_normal/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\003\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\310\\\\000\\\\000\\\\000\\\\310\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/truncated_normal/mean&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/truncated_normal/stddev&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.10000000149011612\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/truncated_normal/TruncatedNormal&quot;\\n  op: &quot;TruncatedNormal&quot;\\n  input: &quot;conv-maxpool-2/truncated_normal/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/truncated_normal/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;conv-maxpool-2/truncated_normal/TruncatedNormal&quot;\\n  input: &quot;conv-maxpool-2/truncated_normal/stddev&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/truncated_normal&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;conv-maxpool-2/truncated_normal/mul&quot;\\n  input: &quot;conv-maxpool-2/truncated_normal/mean&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/W&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 3\\n        }\\n        dim {\\n          size: 1\\n        }\\n        dim {\\n          size: 200\\n        }\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/W/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;conv-maxpool-2/W&quot;\\n  input: &quot;conv-maxpool-2/truncated_normal&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-2/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/W/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;conv-maxpool-2/W&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-2/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 200\\n          }\\n        }\\n        float_val: 0.10000000149011612\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/b&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/b/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;conv-maxpool-2/b&quot;\\n  input: &quot;conv-maxpool-2/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-2/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/b/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;conv-maxpool-2/b&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-2/b&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/conv&quot;\\n  op: &quot;Conv2D&quot;\\n  input: &quot;conv-maxpool-1/pool&quot;\\n  input: &quot;conv-maxpool-2/W/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;VALID&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;conv-maxpool-2/conv&quot;\\n  input: &quot;conv-maxpool-2/b/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;conv-maxpool-2/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/pool&quot;\\n  op: &quot;MaxPool&quot;\\n  input: &quot;conv-maxpool-2/relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;ksize&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 22\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;VALID&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\310\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;conv-maxpool-2/pool&quot;\\n  input: &quot;Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\310\\\\000\\\\000\\\\000\\\\310\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.12247448414564133\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.12247448414564133\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform/max&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform/mul&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 200\\n        }\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden/W&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden/W&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 200\\n          }\\n        }\\n        float_val: 0.10000000149011612\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/b&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/b/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden/hidden/b&quot;\\n  input: &quot;hidden/hidden/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/hidden/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/b/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden/hidden/b&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/hidden/b&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/L2Loss&quot;\\n  op: &quot;L2Loss&quot;\\n  input: &quot;hidden/W/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Const_2&quot;\\n  input: &quot;hidden/hidden/L2Loss&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/L2Loss_1&quot;\\n  op: &quot;L2Loss&quot;\\n  input: &quot;hidden/hidden/b/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/add_1&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden/hidden/add&quot;\\n  input: &quot;hidden/hidden/L2Loss_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/xw_plus_b/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Reshape&quot;\\n  input: &quot;hidden/W/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/xw_plus_b&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;hidden/hidden/xw_plus_b/MatMul&quot;\\n  input: &quot;hidden/hidden/b/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;hidden/hidden/xw_plus_b&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\310\\\\000\\\\000\\\\000\\\\003\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.17192047834396362\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.17192047834396362\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;output/W/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;output/W/Initializer/random_uniform/max&quot;\\n  input: &quot;output/W/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;output/W/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;output/W/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;output/W/Initializer/random_uniform/mul&quot;\\n  input: &quot;output/W/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 200\\n        }\\n        dim {\\n          size: 3\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;output/W&quot;\\n  input: &quot;output/W/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;output/W&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        float_val: 0.10000000149011612\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/b&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 3\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/b/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;output/output/b&quot;\\n  input: &quot;output/output/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/output/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/b/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;output/output/b&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/output/b&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/L2Loss&quot;\\n  op: &quot;L2Loss&quot;\\n  input: &quot;output/W/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden/hidden/add_1&quot;\\n  input: &quot;output/output/L2Loss&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/L2Loss_1&quot;\\n  op: &quot;L2Loss&quot;\\n  input: &quot;output/output/b/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/add_1&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;output/output/add&quot;\\n  input: &quot;output/output/L2Loss_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/scores/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;hidden/hidden/Relu&quot;\\n  input: &quot;output/W/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/scores&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;output/output/scores/MatMul&quot;\\n  input: &quot;output/output/b/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/predictions/dimension&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/predictions&quot;\\n  op: &quot;ArgMax&quot;\\n  input: &quot;output/output/scores&quot;\\n  input: &quot;output/output/predictions/dimension&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;output_type&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;input_y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  op: &quot;SparseSoftmaxCrossEntropyWithLogits&quot;\\n  input: &quot;output/output/scores&quot;\\n  input: &quot;input_y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tlabels&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/Mean&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  input: &quot;loss/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/mul/x&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;loss/mul/x&quot;\\n  input: &quot;output/output/add_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;loss/Mean&quot;\\n  input: &quot;loss/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;accuracy/Equal&quot;\\n  op: &quot;Equal&quot;\\n  input: &quot;output/output/predictions&quot;\\n  input: &quot;input_y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;accuracy/Cast&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;accuracy/Equal&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_BOOL\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;accuracy/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;accuracy/accuracy&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;accuracy/Cast&quot;\\n  input: &quot;accuracy/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nversions {\\n  producer: 24\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.04302618511916623&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_graph(graph.as_graph_def())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /home/phejimlin/Documents/Machine-learning/Milestone2/runs/1511514513\n",
      "\n",
      "Current epoch:  0\n",
      "Current epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phejimlin/anaconda3/envs/tensorflow_3.5/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-24T17:09:37.864492: step 5, loss 1.20933, acc 0.408203, f1 0.236656\n",
      "2017-11-24T17:09:39.351219: step 10, loss 1.00794, acc 0.445312, f1 0.334273\n",
      "2017-11-24T17:09:40.560571: step 15, loss 1.01492, acc 0.429688, f1 0.303723\n",
      "2017-11-24T17:09:41.764891: step 20, loss 0.966795, acc 0.478516, f1 0.447537\n",
      "2017-11-24T17:09:42.963783: step 25, loss 0.992414, acc 0.486328, f1 0.396955\n",
      "2017-11-24T17:09:44.209862: step 30, loss 1.06099, acc 0.400391, f1 0.238996\n",
      "2017-11-24T17:09:45.465527: step 35, loss 0.980912, acc 0.429688, f1 0.352373\n",
      "\n",
      "Evaluation:\n",
      "2017-11-24T17:09:47.077845: step 36, loss 0.993952, acc 0.458123, f1 0.410743\n",
      "\n",
      "Current epoch:  1\n",
      "2017-11-24T17:09:48.098661: step 40, loss 0.956805, acc 0.525391, f1 0.470564\n",
      "2017-11-24T17:09:49.321138: step 45, loss 1.00758, acc 0.455078, f1 0.367322\n",
      "2017-11-24T17:09:50.536221: step 50, loss 1.00451, acc 0.464844, f1 0.352808\n",
      "2017-11-24T17:09:51.746454: step 55, loss 0.943981, acc 0.529297, f1 0.487664\n",
      "2017-11-24T17:09:52.970112: step 60, loss 0.940675, acc 0.486328, f1 0.410508\n",
      "2017-11-24T17:09:54.185386: step 65, loss 0.950074, acc 0.513672, f1 0.456809\n",
      "2017-11-24T17:09:55.400503: step 70, loss 0.963686, acc 0.486328, f1 0.429698\n",
      "\n",
      "Evaluation:\n",
      "2017-11-24T17:09:56.207800: step 72, loss 1.1593, acc 0.351493, f1 0.198743\n",
      "\n",
      "Current epoch:  2\n",
      "2017-11-24T17:09:56.969591: step 75, loss 0.940402, acc 0.472656, f1 0.348737\n",
      "2017-11-24T17:09:58.167764: step 80, loss 0.972378, acc 0.453125, f1 0.360928\n",
      "2017-11-24T17:09:59.383780: step 85, loss 0.932305, acc 0.503906, f1 0.419992\n",
      "2017-11-24T17:10:00.573478: step 90, loss 0.942852, acc 0.501953, f1 0.468971\n",
      "2017-11-24T17:10:01.781480: step 95, loss 0.908814, acc 0.533203, f1 0.465764\n",
      "2017-11-24T17:10:02.980907: step 100, loss 0.945872, acc 0.507812, f1 0.427238\n",
      "2017-11-24T17:10:04.182711: step 105, loss 0.949904, acc 0.486328, f1 0.452182\n",
      "\n",
      "Evaluation:\n",
      "2017-11-24T17:10:05.228583: step 108, loss 0.948222, acc 0.512408, f1 0.377226\n",
      "\n",
      "Current epoch:  3\n",
      "2017-11-24T17:10:05.733020: step 110, loss 1.14138, acc 0.414062, f1 0.263511\n",
      "2017-11-24T17:10:06.932909: step 115, loss 0.905757, acc 0.570312, f1 0.553154\n",
      "2017-11-24T17:10:08.142036: step 120, loss 0.935817, acc 0.53125, f1 0.477798\n",
      "2017-11-24T17:10:09.361557: step 125, loss 0.889308, acc 0.585938, f1 0.579216\n",
      "2017-11-24T17:10:10.574005: step 130, loss 0.938441, acc 0.535156, f1 0.494724\n",
      "2017-11-24T17:10:11.796639: step 135, loss 0.902538, acc 0.589844, f1 0.576511\n",
      "2017-11-24T17:10:13.032798: step 140, loss 0.918664, acc 0.505859, f1 0.461136\n",
      "\n",
      "Evaluation:\n",
      "2017-11-24T17:10:14.346130: step 144, loss 0.925643, acc 0.547402, f1 0.537519\n",
      "\n",
      "Current epoch:  4\n",
      "2017-11-24T17:10:14.614948: step 145, loss 0.893265, acc 0.570312, f1 0.550963\n",
      "2017-11-24T17:10:15.859697: step 150, loss 0.92298, acc 0.537109, f1 0.502772\n",
      "2017-11-24T17:10:17.079587: step 155, loss 0.902033, acc 0.572266, f1 0.528766\n",
      "2017-11-24T17:10:18.311224: step 160, loss 1.03347, acc 0.474609, f1 0.37196\n",
      "2017-11-24T17:10:19.526370: step 165, loss 0.994217, acc 0.458984, f1 0.372899\n",
      "2017-11-24T17:10:20.706442: step 170, loss 0.892721, acc 0.589844, f1 0.579387\n",
      "2017-11-24T17:10:21.923787: step 175, loss 0.863674, acc 0.585938, f1 0.573768\n",
      "2017-11-24T17:10:23.121148: step 180, loss 0.949518, acc 0.572581, f1 0.587039\n",
      "\n",
      "Evaluation:\n",
      "2017-11-24T17:10:23.476333: step 180, loss 1.12337, acc 0.463988, f1 0.40077\n",
      "\n",
      "Current epoch:  5\n",
      "2017-11-24T17:10:24.689074: step 185, loss 0.872336, acc 0.576172, f1 0.525346\n",
      "2017-11-24T17:10:25.903293: step 190, loss 0.857423, acc 0.601562, f1 0.596768\n",
      "2017-11-24T17:10:27.089265: step 195, loss 0.874643, acc 0.585938, f1 0.579819\n",
      "2017-11-24T17:10:28.269654: step 200, loss 0.830217, acc 0.607422, f1 0.572122\n",
      "2017-11-24T17:10:29.483165: step 205, loss 0.885847, acc 0.572266, f1 0.543203\n",
      "2017-11-24T17:10:30.689307: step 210, loss 0.905779, acc 0.585938, f1 0.542176\n",
      "2017-11-24T17:10:31.889739: step 215, loss 1.03226, acc 0.445312, f1 0.355644\n",
      "\n",
      "Evaluation:\n",
      "2017-11-24T17:10:32.446313: step 216, loss 0.911004, acc 0.553267, f1 0.524364\n",
      "\n",
      "Current epoch:  6\n",
      "2017-11-24T17:10:33.386263: step 220, loss 0.85944, acc 0.572266, f1 0.542637\n",
      "2017-11-24T17:10:34.613466: step 225, loss 0.838626, acc 0.636719, f1 0.622738\n",
      "2017-11-24T17:10:35.817776: step 230, loss 0.847838, acc 0.601562, f1 0.566919\n",
      "2017-11-24T17:10:37.056116: step 235, loss 0.928872, acc 0.529297, f1 0.459268\n",
      "2017-11-24T17:10:38.273507: step 240, loss 0.843722, acc 0.609375, f1 0.590524\n",
      "2017-11-24T17:10:39.466843: step 245, loss 0.84407, acc 0.589844, f1 0.553152\n",
      "2017-11-24T17:10:40.661119: step 250, loss 0.863859, acc 0.59375, f1 0.591815\n",
      "\n",
      "Evaluation:\n",
      "2017-11-24T17:10:41.465962: step 252, loss 0.882096, acc 0.585353, f1 0.542974\n",
      "\n",
      "Current epoch:  7\n",
      "2017-11-24T17:10:42.238044: step 255, loss 0.838611, acc 0.576172, f1 0.536641\n",
      "2017-11-24T17:10:43.437958: step 260, loss 0.912368, acc 0.566406, f1 0.49564\n",
      "2017-11-24T17:10:44.652530: step 265, loss 0.862453, acc 0.585938, f1 0.554416\n",
      "2017-11-24T17:10:45.860282: step 270, loss 0.876067, acc 0.605469, f1 0.614301\n",
      "2017-11-24T17:10:47.050337: step 275, loss 0.867969, acc 0.589844, f1 0.535284\n",
      "2017-11-24T17:10:48.270575: step 280, loss 0.789357, acc 0.648438, f1 0.645721\n",
      "2017-11-24T17:10:49.497731: step 285, loss 0.837835, acc 0.605469, f1 0.567652\n",
      "\n",
      "Evaluation:\n",
      "2017-11-24T17:10:50.538745: step 288, loss 1.03763, acc 0.441256, f1 0.35697\n",
      "\n",
      "Current epoch:  8\n",
      "2017-11-24T17:10:51.037717: step 290, loss 0.853219, acc 0.560547, f1 0.519969\n",
      "2017-11-24T17:10:52.261568: step 295, loss 0.830054, acc 0.632812, f1 0.632542\n",
      "2017-11-24T17:10:53.444460: step 300, loss 0.837041, acc 0.605469, f1 0.589597\n",
      "2017-11-24T17:10:54.647250: step 305, loss 0.882163, acc 0.548828, f1 0.504173\n",
      "2017-11-24T17:10:55.847550: step 310, loss 0.884768, acc 0.550781, f1 0.526437\n",
      "2017-11-24T17:10:57.055798: step 315, loss 0.811821, acc 0.621094, f1 0.600172\n",
      "2017-11-24T17:10:58.258328: step 320, loss 0.885755, acc 0.589844, f1 0.543258\n",
      "\n",
      "Evaluation:\n",
      "2017-11-24T17:10:59.528672: step 324, loss 0.91181, acc 0.549001, f1 0.454193\n",
      "\n",
      "Current epoch:  9\n",
      "2017-11-24T17:10:59.819032: step 325, loss 0.953841, acc 0.482422, f1 0.382352\n",
      "2017-11-24T17:11:01.041946: step 330, loss 0.837301, acc 0.617188, f1 0.622198\n",
      "2017-11-24T17:11:02.244977: step 335, loss 0.826719, acc 0.623047, f1 0.612203\n",
      "2017-11-24T17:11:03.474850: step 340, loss 0.788809, acc 0.658203, f1 0.621016\n",
      "2017-11-24T17:11:04.685091: step 345, loss 0.846437, acc 0.572266, f1 0.516789\n",
      "2017-11-24T17:11:05.909208: step 350, loss 0.822581, acc 0.642578, f1 0.639056\n",
      "2017-11-24T17:11:07.137847: step 355, loss 0.816119, acc 0.621094, f1 0.590861\n",
      "2017-11-24T17:11:08.330721: step 360, loss 0.840641, acc 0.620968, f1 0.62301\n",
      "\n",
      "Evaluation:\n",
      "2017-11-24T17:11:08.670324: step 360, loss 0.938413, acc 0.569891, f1 0.524967\n",
      "\n",
      "Current epoch:  10\n",
      "2017-11-24T17:11:09.894419: step 365, loss 0.900676, acc 0.542969, f1 0.484676\n",
      "2017-11-24T17:11:11.102815: step 370, loss 0.78668, acc 0.636719, f1 0.626238\n",
      "2017-11-24T17:11:12.313408: step 375, loss 0.833474, acc 0.591797, f1 0.582883\n",
      "2017-11-24T17:11:13.528706: step 380, loss 0.752959, acc 0.646484, f1 0.630581\n",
      "2017-11-24T17:11:14.725272: step 385, loss 0.80829, acc 0.611328, f1 0.590427\n",
      "2017-11-24T17:11:15.908065: step 390, loss 0.838075, acc 0.583984, f1 0.545706\n",
      "2017-11-24T17:11:17.128560: step 395, loss 0.790668, acc 0.630859, f1 0.621535\n",
      "\n",
      "Evaluation:\n",
      "2017-11-24T17:11:17.693153: step 396, loss 0.84689, acc 0.597518, f1 0.596164\n",
      "\n",
      "Current epoch:  11\n",
      "2017-11-24T17:11:18.686965: step 400, loss 0.79318, acc 0.626953, f1 0.624091\n",
      "2017-11-24T17:11:19.890581: step 405, loss 0.822992, acc 0.603516, f1 0.572258\n",
      "2017-11-24T17:11:21.099043: step 410, loss 0.802617, acc 0.630859, f1 0.625622\n",
      "2017-11-24T17:11:22.321448: step 415, loss 0.860319, acc 0.595703, f1 0.55539\n",
      "2017-11-24T17:11:23.538248: step 420, loss 0.761584, acc 0.660156, f1 0.641857\n",
      "2017-11-24T17:11:24.744705: step 425, loss 0.776968, acc 0.677734, f1 0.678748\n",
      "2017-11-24T17:11:25.961516: step 430, loss 0.784862, acc 0.640625, f1 0.616423\n",
      "\n",
      "Evaluation:\n",
      "2017-11-24T17:11:26.771047: step 432, loss 0.838627, acc 0.603383, f1 0.587935\n",
      "\n",
      "Current epoch:  12\n",
      "2017-11-24T17:11:27.515976: step 435, loss 0.737245, acc 0.654297, f1 0.651003\n",
      "2017-11-24T17:11:28.732685: step 440, loss 0.835746, acc 0.621094, f1 0.575697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-24T17:11:29.969991: step 445, loss 0.845208, acc 0.601562, f1 0.583724\n",
      "2017-11-24T17:11:31.183773: step 450, loss 0.766413, acc 0.615234, f1 0.605797\n",
      "2017-11-24T17:11:32.370565: step 455, loss 0.781606, acc 0.626953, f1 0.606047\n",
      "2017-11-24T17:11:33.548039: step 460, loss 0.76863, acc 0.650391, f1 0.623468\n",
      "2017-11-24T17:11:34.741019: step 465, loss 0.852112, acc 0.583984, f1 0.568946\n",
      "\n",
      "Evaluation:\n",
      "2017-11-24T17:11:35.788198: step 468, loss 0.862806, acc 0.584432, f1 0.51794\n",
      "\n",
      "Current epoch:  13\n",
      "2017-11-24T17:11:36.287249: step 470, loss 0.862316, acc 0.554688, f1 0.503185\n",
      "2017-11-24T17:11:37.499659: step 475, loss 0.775658, acc 0.650391, f1 0.652549\n",
      "2017-11-24T17:11:38.692226: step 480, loss 0.745664, acc 0.669922, f1 0.666043\n",
      "2017-11-24T17:11:39.911124: step 485, loss 0.752581, acc 0.679688, f1 0.674959\n",
      "2017-11-24T17:11:41.105062: step 490, loss 0.859788, acc 0.59375, f1 0.544818\n",
      "2017-11-24T17:11:42.316419: step 495, loss 0.867671, acc 0.587891, f1 0.556627\n",
      "2017-11-24T17:11:43.522476: step 500, loss 0.780072, acc 0.623047, f1 0.597932\n",
      "\n",
      "Evaluation:\n",
      "2017-11-24T17:11:44.815403: step 504, loss 0.820433, acc 0.618263, f1 0.597723\n",
      "\n",
      "Current epoch:  14\n",
      "2017-11-24T17:11:45.082390: step 505, loss 0.763088, acc 0.617188, f1 0.59465\n",
      "2017-11-24T17:11:46.270423: step 510, loss 0.756342, acc 0.679688, f1 0.673783\n",
      "2017-11-24T17:11:47.477786: step 515, loss 0.818976, acc 0.636719, f1 0.628921\n",
      "2017-11-24T17:11:48.660865: step 520, loss 0.780361, acc 0.632812, f1 0.608744\n",
      "2017-11-24T17:11:49.851137: step 525, loss 0.773075, acc 0.617188, f1 0.604753\n",
      "2017-11-24T17:11:51.056021: step 530, loss 0.807042, acc 0.601562, f1 0.572443\n",
      "2017-11-24T17:11:52.271603: step 535, loss 0.82639, acc 0.599609, f1 0.596975\n",
      "2017-11-24T17:11:53.470853: step 540, loss 0.774699, acc 0.620968, f1 0.610231\n",
      "\n",
      "Evaluation:\n",
      "2017-11-24T17:11:53.810255: step 540, loss 0.825494, acc 0.612107, f1 0.61043\n",
      "\n",
      "Current epoch:  15\n",
      "2017-11-24T17:11:55.004423: step 545, loss 0.761998, acc 0.654297, f1 0.643392\n",
      "2017-11-24T17:11:56.190652: step 550, loss 0.725788, acc 0.666016, f1 0.646251\n",
      "2017-11-24T17:11:57.388243: step 555, loss 0.70992, acc 0.697266, f1 0.6964\n",
      "2017-11-24T17:11:58.572099: step 560, loss 0.745237, acc 0.667969, f1 0.6524\n",
      "2017-11-24T17:11:59.772992: step 565, loss 0.798592, acc 0.611328, f1 0.623371\n",
      "2017-11-24T17:12:00.984790: step 570, loss 0.794538, acc 0.623047, f1 0.59649\n",
      "2017-11-24T17:12:02.186668: step 575, loss 0.741595, acc 0.675781, f1 0.654827\n",
      "\n",
      "Evaluation:\n",
      "2017-11-24T17:12:02.754185: step 576, loss 0.864182, acc 0.598875, f1 0.572051\n",
      "\n",
      "Current epoch:  16\n",
      "2017-11-24T17:12:03.739568: step 580, loss 0.899058, acc 0.597656, f1 0.575339\n",
      "2017-11-24T17:12:04.950904: step 585, loss 0.737312, acc 0.667969, f1 0.664188\n",
      "2017-11-24T17:12:06.180159: step 590, loss 0.699453, acc 0.691406, f1 0.66303\n",
      "2017-11-24T17:12:07.391137: step 595, loss 0.751468, acc 0.675781, f1 0.651393\n",
      "2017-11-24T17:12:08.603624: step 600, loss 0.728749, acc 0.632812, f1 0.620648\n",
      "2017-11-24T17:12:09.821767: step 605, loss 0.687627, acc 0.708984, f1 0.700157\n",
      "2017-11-24T17:12:11.024211: step 610, loss 0.789982, acc 0.632812, f1 0.59696\n",
      "\n",
      "Evaluation:\n",
      "2017-11-24T17:12:11.822723: step 612, loss 0.910114, acc 0.549244, f1 0.536502\n",
      "\n",
      "Current epoch:  17\n",
      "2017-11-24T17:12:12.561807: step 615, loss 0.752158, acc 0.664062, f1 0.643096\n",
      "2017-11-24T17:12:13.766206: step 620, loss 0.742205, acc 0.642578, f1 0.621796\n",
      "2017-11-24T17:12:14.972460: step 625, loss 0.846532, acc 0.638672, f1 0.590933\n",
      "2017-11-24T17:12:16.183236: step 630, loss 0.705361, acc 0.662109, f1 0.649945\n",
      "2017-11-24T17:12:17.385424: step 635, loss 0.772355, acc 0.625, f1 0.621466\n",
      "2017-11-24T17:12:18.592067: step 640, loss 0.691425, acc 0.662109, f1 0.65432\n",
      "2017-11-24T17:12:19.791828: step 645, loss 0.771105, acc 0.642578, f1 0.61524\n",
      "\n",
      "Evaluation:\n",
      "2017-11-24T17:12:20.832612: step 648, loss 1.00665, acc 0.480709, f1 0.433355\n",
      "\n",
      "Current epoch:  18\n",
      "2017-11-24T17:12:21.340654: step 650, loss 0.78645, acc 0.609375, f1 0.587117\n",
      "2017-11-24T17:12:22.555875: step 655, loss 0.663465, acc 0.705078, f1 0.701181\n",
      "2017-11-24T17:12:23.737939: step 660, loss 0.662898, acc 0.720703, f1 0.712789\n",
      "2017-11-24T17:12:24.952069: step 665, loss 0.789349, acc 0.601562, f1 0.559021\n",
      "2017-11-24T17:12:26.173393: step 670, loss 0.721144, acc 0.660156, f1 0.660489\n",
      "2017-11-24T17:12:27.352371: step 675, loss 0.787551, acc 0.646484, f1 0.60258\n",
      "2017-11-24T17:12:28.571521: step 680, loss 0.755277, acc 0.654297, f1 0.656705\n",
      "\n",
      "Evaluation:\n",
      "2017-11-24T17:12:29.854722: step 684, loss 0.857615, acc 0.592429, f1 0.583079\n",
      "\n",
      "Current epoch:  19\n",
      "2017-11-24T17:12:30.128274: step 685, loss 0.700632, acc 0.679688, f1 0.665469\n",
      "2017-11-24T17:12:31.324297: step 690, loss 0.731295, acc 0.662109, f1 0.652021\n",
      "2017-11-24T17:12:32.520549: step 695, loss 0.674656, acc 0.699219, f1 0.694512\n",
      "2017-11-24T17:12:33.726242: step 700, loss 0.830001, acc 0.609375, f1 0.562773\n",
      "2017-11-24T17:12:34.935380: step 705, loss 0.689737, acc 0.712891, f1 0.720466\n",
      "2017-11-24T17:12:36.142071: step 710, loss 0.716404, acc 0.652344, f1 0.635468\n",
      "2017-11-24T17:12:37.357545: step 715, loss 0.76422, acc 0.619141, f1 0.587356\n",
      "2017-11-24T17:12:38.544500: step 720, loss 0.66766, acc 0.717742, f1 0.719147\n",
      "\n",
      "Evaluation:\n",
      "2017-11-24T17:12:38.882156: step 720, loss 0.967861, acc 0.53417, f1 0.506267\n",
      "\n",
      "Current epoch:  20\n",
      "2017-11-24T17:12:40.103938: step 725, loss 0.838224, acc 0.623047, f1 0.581599\n",
      "2017-11-24T17:12:41.299577: step 730, loss 0.664928, acc 0.720703, f1 0.719695\n",
      "2017-11-24T17:12:42.509960: step 735, loss 0.673733, acc 0.720703, f1 0.715281\n",
      "2017-11-24T17:12:43.721469: step 740, loss 0.623567, acc 0.751953, f1 0.746797\n",
      "2017-11-24T17:12:44.924998: step 745, loss 0.787749, acc 0.595703, f1 0.543553\n",
      "2017-11-24T17:12:46.151285: step 750, loss 0.727245, acc 0.65625, f1 0.640395\n",
      "2017-11-24T17:12:47.347063: step 755, loss 0.828857, acc 0.664062, f1 0.609912\n",
      "\n",
      "Evaluation:\n",
      "2017-11-24T17:12:47.905245: step 756, loss 0.824516, acc 0.616372, f1 0.602908\n",
      "\n",
      "Current epoch:  21\n",
      "2017-11-24T17:12:48.891136: step 760, loss 0.616945, acc 0.748047, f1 0.746806\n",
      "2017-11-24T17:12:50.112013: step 765, loss 0.687893, acc 0.712891, f1 0.681153\n",
      "2017-11-24T17:12:51.324597: step 770, loss 0.653741, acc 0.708984, f1 0.708502\n",
      "2017-11-24T17:12:52.521785: step 775, loss 0.854198, acc 0.636719, f1 0.589462\n",
      "2017-11-24T17:12:53.697734: step 780, loss 0.673252, acc 0.71875, f1 0.721284\n",
      "2017-11-24T17:12:54.903361: step 785, loss 0.676547, acc 0.693359, f1 0.677212\n",
      "2017-11-24T17:12:56.111727: step 790, loss 0.913319, acc 0.585938, f1 0.543656\n",
      "\n",
      "Evaluation:\n",
      "2017-11-24T17:12:56.909986: step 792, loss 0.821604, acc 0.617778, f1 0.605836\n",
      "\n",
      "Current epoch:  22\n",
      "2017-11-24T17:12:57.644696: step 795, loss 0.627339, acc 0.703125, f1 0.68473\n",
      "2017-11-24T17:12:58.850919: step 800, loss 0.687811, acc 0.660156, f1 0.645342\n",
      "2017-11-24T17:13:00.053723: step 805, loss 0.733757, acc 0.658203, f1 0.653393\n",
      "2017-11-24T17:13:01.261251: step 810, loss 0.689582, acc 0.677734, f1 0.651656\n",
      "2017-11-24T17:13:02.486894: step 815, loss 0.65645, acc 0.740234, f1 0.738766\n",
      "2017-11-24T17:13:03.708343: step 820, loss 0.696531, acc 0.675781, f1 0.658218\n",
      "2017-11-24T17:13:04.891736: step 825, loss 0.650164, acc 0.71875, f1 0.703004\n",
      "\n",
      "Evaluation:\n",
      "2017-11-24T17:13:05.931775: step 828, loss 0.843993, acc 0.624854, f1 0.59169\n",
      "\n",
      "Current epoch:  23\n",
      "2017-11-24T17:13:06.417274: step 830, loss 0.6202, acc 0.744141, f1 0.743419\n",
      "2017-11-24T17:13:07.624224: step 835, loss 0.718614, acc 0.677734, f1 0.635186\n",
      "2017-11-24T17:13:08.805257: step 840, loss 0.615603, acc 0.740234, f1 0.745597\n",
      "2017-11-24T17:13:10.012426: step 845, loss 0.702546, acc 0.675781, f1 0.668294\n",
      "2017-11-24T17:13:11.220792: step 850, loss 0.725689, acc 0.652344, f1 0.628347\n",
      "2017-11-24T17:13:12.406198: step 855, loss 0.700318, acc 0.640625, f1 0.621831\n",
      "2017-11-24T17:13:13.622675: step 860, loss 0.626845, acc 0.75, f1 0.745353\n",
      "\n",
      "Evaluation:\n",
      "2017-11-24T17:13:14.918067: step 864, loss 0.867126, acc 0.614143, f1 0.586479\n",
      "\n",
      "Current epoch:  24\n",
      "2017-11-24T17:13:15.203420: step 865, loss 0.678705, acc 0.707031, f1 0.669146\n",
      "2017-11-24T17:13:16.427221: step 870, loss 0.718001, acc 0.6875, f1 0.67476\n",
      "2017-11-24T17:13:17.630075: step 875, loss 0.609748, acc 0.724609, f1 0.704124\n",
      "2017-11-24T17:13:18.840515: step 880, loss 0.638486, acc 0.730469, f1 0.731844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-24T17:13:20.051105: step 885, loss 0.633775, acc 0.697266, f1 0.669251\n",
      "2017-11-24T17:13:21.254197: step 890, loss 0.770465, acc 0.650391, f1 0.626632\n",
      "2017-11-24T17:13:22.442132: step 895, loss 0.619259, acc 0.753906, f1 0.754585\n",
      "2017-11-24T17:13:23.633464: step 900, loss 0.66105, acc 0.766129, f1 0.737891\n",
      "\n",
      "Evaluation:\n",
      "2017-11-24T17:13:23.989899: step 900, loss 0.896277, acc 0.568486, f1 0.561957\n",
      "\n",
      "Current epoch:  25\n",
      "2017-11-24T17:13:25.228396: step 905, loss 0.699867, acc 0.664062, f1 0.651615\n",
      "2017-11-24T17:13:26.397433: step 910, loss 0.594641, acc 0.755859, f1 0.756566\n",
      "2017-11-24T17:13:27.596779: step 915, loss 0.726976, acc 0.626953, f1 0.593541\n",
      "2017-11-24T17:13:28.808586: step 920, loss 0.640307, acc 0.755859, f1 0.759462\n",
      "2017-11-24T17:13:30.011660: step 925, loss 0.675514, acc 0.695312, f1 0.662511\n",
      "2017-11-24T17:13:31.229505: step 930, loss 0.580996, acc 0.753906, f1 0.751009\n",
      "2017-11-24T17:13:32.452140: step 935, loss 0.788758, acc 0.675781, f1 0.6253\n",
      "\n",
      "Evaluation:\n",
      "2017-11-24T17:13:33.014883: step 936, loss 0.808649, acc 0.627762, f1 0.617908\n",
      "\n",
      "Current epoch:  26\n",
      "2017-11-24T17:13:33.990000: step 940, loss 0.739301, acc 0.638672, f1 0.615712\n",
      "2017-11-24T17:13:35.189074: step 945, loss 0.590338, acc 0.744141, f1 0.74617\n",
      "2017-11-24T17:13:36.407600: step 950, loss 0.589772, acc 0.738281, f1 0.723026\n",
      "2017-11-24T17:13:37.613314: step 955, loss 0.710267, acc 0.681641, f1 0.647869\n",
      "2017-11-24T17:13:38.846891: step 960, loss 0.571969, acc 0.753906, f1 0.748683\n",
      "2017-11-24T17:13:40.054539: step 965, loss 0.609231, acc 0.724609, f1 0.712672\n",
      "2017-11-24T17:13:41.271258: step 970, loss 0.63505, acc 0.71875, f1 0.710475\n",
      "\n",
      "Evaluation:\n",
      "2017-11-24T17:13:42.077147: step 972, loss 0.927612, acc 0.605661, f1 0.544185\n",
      "\n",
      "Current epoch:  27\n",
      "2017-11-24T17:13:42.803169: step 975, loss 0.657462, acc 0.708984, f1 0.701204\n",
      "2017-11-24T17:13:44.025457: step 980, loss 0.56784, acc 0.775391, f1 0.771324\n",
      "2017-11-24T17:13:45.214353: step 985, loss 0.583691, acc 0.732422, f1 0.725446\n",
      "2017-11-24T17:13:46.396011: step 990, loss 0.702197, acc 0.677734, f1 0.65328\n",
      "2017-11-24T17:13:47.582765: step 995, loss 0.589151, acc 0.742188, f1 0.739482\n",
      "2017-11-24T17:13:48.764418: step 1000, loss 0.599778, acc 0.753906, f1 0.751561\n",
      "2017-11-24T17:13:49.956871: step 1005, loss 0.791031, acc 0.6875, f1 0.640005\n",
      "\n",
      "Evaluation:\n",
      "2017-11-24T17:13:51.001588: step 1008, loss 0.827556, acc 0.615112, f1 0.615446\n",
      "\n",
      "Current epoch:  28\n",
      "2017-11-24T17:13:51.503059: step 1010, loss 0.518665, acc 0.800781, f1 0.8014\n",
      "2017-11-24T17:13:52.702097: step 1015, loss 0.6924, acc 0.658203, f1 0.641279\n",
      "2017-11-24T17:13:53.911974: step 1020, loss 0.597092, acc 0.720703, f1 0.700895\n",
      "2017-11-24T17:13:55.116653: step 1025, loss 0.568673, acc 0.763672, f1 0.763737\n",
      "2017-11-24T17:13:56.304825: step 1030, loss 0.549173, acc 0.777344, f1 0.77829\n",
      "2017-11-24T17:13:57.511580: step 1035, loss 0.614677, acc 0.75, f1 0.715032\n",
      "2017-11-24T17:13:58.726726: step 1040, loss 0.71179, acc 0.669922, f1 0.654059\n",
      "\n",
      "Evaluation:\n",
      "2017-11-24T17:14:00.000016: step 1044, loss 0.851905, acc 0.616566, f1 0.583629\n",
      "\n",
      "Current epoch:  29\n",
      "2017-11-24T17:14:00.270501: step 1045, loss 0.644774, acc 0.664062, f1 0.641482\n",
      "2017-11-24T17:14:01.477125: step 1050, loss 0.582487, acc 0.744141, f1 0.73205\n",
      "2017-11-24T17:14:02.695592: step 1055, loss 0.611419, acc 0.732422, f1 0.737436\n",
      "2017-11-24T17:14:03.911775: step 1060, loss 0.664502, acc 0.683594, f1 0.672936\n",
      "2017-11-24T17:14:05.129541: step 1065, loss 0.575066, acc 0.763672, f1 0.751918\n",
      "2017-11-24T17:14:06.338952: step 1070, loss 0.568197, acc 0.775391, f1 0.776578\n",
      "2017-11-24T17:14:07.543069: step 1075, loss 0.542115, acc 0.767578, f1 0.749496\n",
      "2017-11-24T17:14:08.722313: step 1080, loss 0.515315, acc 0.790323, f1 0.79177\n",
      "\n",
      "Evaluation:\n",
      "2017-11-24T17:14:09.065877: step 1080, loss 0.825089, acc 0.62878, f1 0.614504\n",
      "\n",
      "Current epoch:  30\n",
      "2017-11-24T17:14:10.306983: step 1085, loss 0.826721, acc 0.544922, f1 0.504939\n",
      "2017-11-24T17:14:11.527416: step 1090, loss 0.556885, acc 0.771484, f1 0.769262\n",
      "2017-11-24T17:14:12.747286: step 1095, loss 0.607726, acc 0.763672, f1 0.773835\n",
      "2017-11-24T17:14:13.966992: step 1100, loss 0.525184, acc 0.771484, f1 0.766547\n",
      "2017-11-24T17:14:15.167693: step 1105, loss 0.541935, acc 0.779297, f1 0.774347\n",
      "2017-11-24T17:14:16.391376: step 1110, loss 0.661424, acc 0.695312, f1 0.681565\n",
      "2017-11-24T17:14:17.601381: step 1115, loss 0.566104, acc 0.771484, f1 0.766846\n",
      "\n",
      "Evaluation:\n",
      "2017-11-24T17:14:18.162827: step 1116, loss 0.895014, acc 0.581911, f1 0.588838\n",
      "\n",
      "Current epoch:  31\n",
      "2017-11-24T17:14:19.146270: step 1120, loss 0.883772, acc 0.710938, f1 0.653508\n",
      "2017-11-24T17:14:20.362420: step 1125, loss 0.570198, acc 0.757812, f1 0.742926\n",
      "2017-11-24T17:14:21.583379: step 1130, loss 0.597234, acc 0.755859, f1 0.760643\n",
      "2017-11-24T17:14:22.782059: step 1135, loss 0.616231, acc 0.714844, f1 0.696697\n",
      "2017-11-24T17:14:23.995969: step 1140, loss 0.529535, acc 0.767578, f1 0.763824\n",
      "2017-11-24T17:14:25.215608: step 1145, loss 0.544171, acc 0.785156, f1 0.780706\n",
      "2017-11-24T17:14:26.434353: step 1150, loss 0.529385, acc 0.767578, f1 0.764776\n",
      "\n",
      "Evaluation:\n",
      "2017-11-24T17:14:27.237358: step 1152, loss 0.980488, acc 0.553751, f1 0.536412\n",
      "\n",
      "Current epoch:  32\n",
      "2017-11-24T17:14:27.972139: step 1155, loss 0.67797, acc 0.734375, f1 0.691258\n",
      "2017-11-24T17:14:29.182317: step 1160, loss 0.54674, acc 0.732422, f1 0.727718\n",
      "2017-11-24T17:14:30.369505: step 1165, loss 0.561674, acc 0.773438, f1 0.76702\n",
      "2017-11-24T17:14:31.586892: step 1170, loss 0.531866, acc 0.773438, f1 0.759946\n",
      "2017-11-24T17:14:32.821758: step 1175, loss 0.53278, acc 0.791016, f1 0.790364\n",
      "2017-11-24T17:14:34.056455: step 1180, loss 0.54333, acc 0.771484, f1 0.759289\n",
      "2017-11-24T17:14:35.266965: step 1185, loss 0.53418, acc 0.763672, f1 0.754089\n",
      "\n",
      "Evaluation:\n",
      "2017-11-24T17:14:36.322319: step 1188, loss 1.13192, acc 0.486041, f1 0.444158\n",
      "\n",
      "Current epoch:  33\n",
      "2017-11-24T17:14:36.855218: step 1190, loss 0.811218, acc 0.621094, f1 0.592326\n",
      "2017-11-24T17:14:38.052460: step 1195, loss 0.518281, acc 0.808594, f1 0.807322\n",
      "2017-11-24T17:14:39.237578: step 1200, loss 0.542088, acc 0.759766, f1 0.741796\n",
      "2017-11-24T17:14:40.427214: step 1205, loss 0.480709, acc 0.826172, f1 0.828882\n",
      "2017-11-24T17:14:41.620599: step 1210, loss 0.69469, acc 0.703125, f1 0.664665\n",
      "2017-11-24T17:14:42.802574: step 1215, loss 0.529302, acc 0.748047, f1 0.738704\n",
      "2017-11-24T17:14:43.969874: step 1220, loss 0.450798, acc 0.853516, f1 0.853422\n",
      "\n",
      "Evaluation:\n",
      "2017-11-24T17:14:45.244477: step 1224, loss 0.878156, acc 0.5965, f1 0.595046\n",
      "\n",
      "Current epoch:  34\n",
      "2017-11-24T17:14:45.501235: step 1225, loss 0.465273, acc 0.822266, f1 0.820472\n",
      "2017-11-24T17:14:46.713467: step 1230, loss 0.598638, acc 0.693359, f1 0.682695\n",
      "2017-11-24T17:14:47.935400: step 1235, loss 0.547874, acc 0.761719, f1 0.752753\n",
      "2017-11-24T17:14:49.168726: step 1240, loss 0.543645, acc 0.785156, f1 0.796329\n",
      "2017-11-24T17:14:50.385130: step 1245, loss 0.470984, acc 0.820312, f1 0.815305\n",
      "2017-11-24T17:14:51.599928: step 1250, loss 0.682497, acc 0.683594, f1 0.673437\n",
      "2017-11-24T17:14:52.808968: step 1255, loss 0.485448, acc 0.849609, f1 0.847859\n",
      "2017-11-24T17:14:54.012997: step 1260, loss 0.51326, acc 0.766129, f1 0.740199\n",
      "\n",
      "Evaluation:\n",
      "2017-11-24T17:14:54.361828: step 1260, loss 0.950047, acc 0.563251, f1 0.55949\n",
      "\n",
      "Current epoch:  35\n",
      "2017-11-24T17:14:55.618491: step 1265, loss 0.517473, acc 0.773438, f1 0.762316\n",
      "2017-11-24T17:14:56.831377: step 1270, loss 0.506172, acc 0.802734, f1 0.800936\n",
      "2017-11-24T17:14:58.049618: step 1275, loss 0.584209, acc 0.755859, f1 0.71489\n",
      "2017-11-24T17:14:59.263618: step 1280, loss 0.608244, acc 0.730469, f1 0.724706\n",
      "2017-11-24T17:15:00.470325: step 1285, loss 0.564544, acc 0.75, f1 0.740418\n",
      "2017-11-24T17:15:01.684725: step 1290, loss 0.611238, acc 0.722656, f1 0.711934\n",
      "2017-11-24T17:15:02.892903: step 1295, loss 0.536267, acc 0.753906, f1 0.739301\n",
      "\n",
      "Evaluation:\n",
      "2017-11-24T17:15:03.462306: step 1296, loss 1.03046, acc 0.524234, f1 0.499621\n",
      "\n",
      "Current epoch:  36\n",
      "2017-11-24T17:15:04.470814: step 1300, loss 0.494633, acc 0.78125, f1 0.781198\n",
      "2017-11-24T17:15:05.696103: step 1305, loss 0.461883, acc 0.820312, f1 0.819196\n",
      "2017-11-24T17:15:06.909004: step 1310, loss 0.502503, acc 0.792969, f1 0.793346\n",
      "2017-11-24T17:15:08.120658: step 1315, loss 0.591642, acc 0.740234, f1 0.703703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-24T17:15:09.334606: step 1320, loss 0.529832, acc 0.771484, f1 0.763673\n",
      "2017-11-24T17:15:10.556942: step 1325, loss 0.576012, acc 0.742188, f1 0.738936\n",
      "2017-11-24T17:15:11.762321: step 1330, loss 0.528378, acc 0.742188, f1 0.718884\n",
      "\n",
      "Evaluation:\n",
      "2017-11-24T17:15:12.555677: step 1332, loss 0.881789, acc 0.598778, f1 0.596599\n",
      "\n",
      "Current epoch:  37\n",
      "2017-11-24T17:15:13.333444: step 1335, loss 0.488398, acc 0.808594, f1 0.807638\n",
      "2017-11-24T17:15:14.505805: step 1340, loss 0.47589, acc 0.8125, f1 0.812876\n",
      "2017-11-24T17:15:15.694013: step 1345, loss 0.485572, acc 0.806641, f1 0.793619\n",
      "2017-11-24T17:15:16.892714: step 1350, loss 0.485777, acc 0.791016, f1 0.788707\n",
      "2017-11-24T17:15:18.090903: step 1355, loss 0.470454, acc 0.824219, f1 0.823956\n",
      "2017-11-24T17:15:19.289742: step 1360, loss 0.524807, acc 0.753906, f1 0.731829\n",
      "2017-11-24T17:15:20.495663: step 1365, loss 0.427227, acc 0.869141, f1 0.868341\n",
      "\n",
      "Evaluation:\n",
      "2017-11-24T17:15:21.525502: step 1368, loss 1.06809, acc 0.527627, f1 0.511099\n",
      "\n",
      "Current epoch:  38\n",
      "2017-11-24T17:15:22.055858: step 1370, loss 0.565755, acc 0.736328, f1 0.705969\n",
      "2017-11-24T17:15:23.254524: step 1375, loss 0.480067, acc 0.800781, f1 0.790303\n",
      "2017-11-24T17:15:24.457868: step 1380, loss 0.637845, acc 0.705078, f1 0.684228\n",
      "2017-11-24T17:15:25.664409: step 1385, loss 0.437603, acc 0.832031, f1 0.828359\n",
      "2017-11-24T17:15:26.860571: step 1390, loss 0.412234, acc 0.855469, f1 0.855166\n",
      "2017-11-24T17:15:28.058938: step 1395, loss 0.545518, acc 0.714844, f1 0.701933\n",
      "2017-11-24T17:15:29.263912: step 1400, loss 0.479474, acc 0.804688, f1 0.799683\n",
      "\n",
      "Evaluation:\n",
      "2017-11-24T17:15:30.540451: step 1404, loss 1.13859, acc 0.50824, f1 0.524001\n",
      "\n",
      "Current epoch:  39\n",
      "2017-11-24T17:15:30.792254: step 1405, loss 0.617532, acc 0.708984, f1 0.727335\n",
      "2017-11-24T17:15:31.985332: step 1410, loss 0.423663, acc 0.855469, f1 0.853969\n",
      "2017-11-24T17:15:33.169116: step 1415, loss 0.421949, acc 0.824219, f1 0.817899\n",
      "2017-11-24T17:15:34.382820: step 1420, loss 0.510336, acc 0.769531, f1 0.755046\n",
      "2017-11-24T17:15:35.588623: step 1425, loss 0.39885, acc 0.863281, f1 0.863419\n",
      "2017-11-24T17:15:36.799172: step 1430, loss 0.418853, acc 0.833984, f1 0.824864\n",
      "2017-11-24T17:15:38.009417: step 1435, loss 0.948447, acc 0.597656, f1 0.554934\n",
      "2017-11-24T17:15:39.211028: step 1440, loss 0.399455, acc 0.862903, f1 0.862176\n",
      "\n",
      "Evaluation:\n",
      "2017-11-24T17:15:39.552426: step 1440, loss 0.873031, acc 0.616809, f1 0.610973\n",
      "\n",
      "Current epoch:  40\n",
      "2017-11-24T17:15:40.788040: step 1445, loss 0.370871, acc 0.896484, f1 0.896247\n",
      "2017-11-24T17:15:42.005783: step 1450, loss 0.386869, acc 0.837891, f1 0.835533\n",
      "2017-11-24T17:15:43.216918: step 1455, loss 0.5669, acc 0.742188, f1 0.743436\n",
      "2017-11-24T17:15:44.447932: step 1460, loss 0.564007, acc 0.748047, f1 0.736546\n",
      "2017-11-24T17:15:45.666011: step 1465, loss 0.404714, acc 0.861328, f1 0.85947\n",
      "2017-11-24T17:15:46.872575: step 1470, loss 0.319588, acc 0.919922, f1 0.919775\n",
      "2017-11-24T17:15:48.083084: step 1475, loss 0.430916, acc 0.818359, f1 0.796983\n",
      "\n",
      "Evaluation:\n",
      "2017-11-24T17:15:48.647146: step 1476, loss 0.93761, acc 0.617245, f1 0.601807\n",
      "\n",
      "Current epoch:  41\n",
      "2017-11-24T17:15:49.630484: step 1480, loss 0.39571, acc 0.841797, f1 0.840838\n",
      "2017-11-24T17:15:50.813837: step 1485, loss 0.509962, acc 0.75, f1 0.738617\n",
      "2017-11-24T17:15:52.007726: step 1490, loss 0.599611, acc 0.751953, f1 0.702293\n",
      "2017-11-24T17:15:53.201561: step 1495, loss 0.361383, acc 0.904297, f1 0.90473\n",
      "2017-11-24T17:15:54.401772: step 1500, loss 0.369819, acc 0.867188, f1 0.863668\n",
      "2017-11-24T17:15:55.604138: step 1505, loss 0.401548, acc 0.863281, f1 0.861495\n",
      "2017-11-24T17:15:56.784998: step 1510, loss 0.529141, acc 0.775391, f1 0.763099\n",
      "\n",
      "Evaluation:\n",
      "2017-11-24T17:15:57.578321: step 1512, loss 0.896988, acc 0.611041, f1 0.603259\n",
      "\n",
      "Current epoch:  42\n",
      "2017-11-24T17:15:58.311670: step 1515, loss 0.39583, acc 0.859375, f1 0.853954\n",
      "2017-11-24T17:15:59.512130: step 1520, loss 0.460093, acc 0.792969, f1 0.791201\n",
      "2017-11-24T17:16:00.727569: step 1525, loss 0.5702, acc 0.763672, f1 0.72306\n",
      "2017-11-24T17:16:01.932094: step 1530, loss 0.34653, acc 0.904297, f1 0.904329\n",
      "2017-11-24T17:16:03.137866: step 1535, loss 0.347652, acc 0.876953, f1 0.87623\n",
      "2017-11-24T17:16:04.329413: step 1540, loss 0.605815, acc 0.724609, f1 0.700978\n",
      "2017-11-24T17:16:05.549941: step 1545, loss 0.374073, acc 0.853516, f1 0.854014\n",
      "\n",
      "Evaluation:\n",
      "2017-11-24T17:16:06.602150: step 1548, loss 0.961098, acc 0.578809, f1 0.57838\n",
      "\n",
      "Current epoch:  43\n",
      "2017-11-24T17:16:07.106260: step 1550, loss 0.33232, acc 0.884766, f1 0.88458\n",
      "2017-11-24T17:16:08.317661: step 1555, loss 0.494625, acc 0.779297, f1 0.763388\n",
      "2017-11-24T17:16:09.504838: step 1560, loss 0.357233, acc 0.882812, f1 0.882717\n",
      "2017-11-24T17:16:10.706630: step 1565, loss 0.387856, acc 0.841797, f1 0.83708\n",
      "2017-11-24T17:16:11.889303: step 1570, loss 0.467298, acc 0.802734, f1 0.803894\n",
      "2017-11-24T17:16:13.095590: step 1575, loss 0.419887, acc 0.832031, f1 0.830998\n",
      "2017-11-24T17:16:14.304397: step 1580, loss 0.333346, acc 0.878906, f1 0.8726\n",
      "\n",
      "Evaluation:\n",
      "2017-11-24T17:16:15.581099: step 1584, loss 1.33023, acc 0.473051, f1 0.482137\n",
      "\n",
      "Current epoch:  44\n",
      "2017-11-24T17:16:15.850223: step 1585, loss 0.537619, acc 0.755859, f1 0.780545\n",
      "2017-11-24T17:16:17.054755: step 1590, loss 0.388929, acc 0.861328, f1 0.860196\n",
      "2017-11-24T17:16:18.277596: step 1595, loss 0.53214, acc 0.755859, f1 0.741305\n",
      "2017-11-24T17:16:19.489959: step 1600, loss 0.325936, acc 0.912109, f1 0.912059\n",
      "2017-11-24T17:16:20.698205: step 1605, loss 0.291133, acc 0.908203, f1 0.907995\n",
      "2017-11-24T17:16:21.881919: step 1610, loss 0.290379, acc 0.921875, f1 0.921878\n",
      "2017-11-24T17:16:23.079651: step 1615, loss 0.39335, acc 0.833984, f1 0.829947\n",
      "2017-11-24T17:16:24.269832: step 1620, loss 0.44353, acc 0.830645, f1 0.817896\n",
      "\n",
      "Evaluation:\n",
      "2017-11-24T17:16:24.602242: step 1620, loss 0.951592, acc 0.5807, f1 0.580587\n",
      "\n",
      "Current epoch:  45\n",
      "2017-11-24T17:16:25.855200: step 1625, loss 0.309268, acc 0.894531, f1 0.895215\n",
      "2017-11-24T17:16:27.079111: step 1630, loss 0.372027, acc 0.859375, f1 0.858301\n",
      "2017-11-24T17:16:28.286818: step 1635, loss 0.317467, acc 0.896484, f1 0.896958\n",
      "2017-11-24T17:16:29.511028: step 1640, loss 0.385169, acc 0.849609, f1 0.843145\n",
      "2017-11-24T17:16:30.723118: step 1645, loss 0.366662, acc 0.859375, f1 0.85524\n",
      "2017-11-24T17:16:31.923255: step 1650, loss 0.358612, acc 0.847656, f1 0.846497\n",
      "2017-11-24T17:16:33.150603: step 1655, loss 0.354352, acc 0.861328, f1 0.85918\n",
      "\n",
      "Evaluation:\n",
      "2017-11-24T17:16:33.713917: step 1656, loss 1.02341, acc 0.592526, f1 0.578531\n",
      "\n",
      "Current epoch:  46\n",
      "2017-11-24T17:16:34.700231: step 1660, loss 0.329563, acc 0.900391, f1 0.902201\n",
      "2017-11-24T17:16:35.889275: step 1665, loss 0.303992, acc 0.896484, f1 0.887253\n",
      "2017-11-24T17:16:37.091112: step 1670, loss 0.316653, acc 0.898438, f1 0.896363\n",
      "2017-11-24T17:16:38.316922: step 1675, loss 0.548551, acc 0.738281, f1 0.723784\n",
      "2017-11-24T17:16:39.518595: step 1680, loss 0.27226, acc 0.929688, f1 0.929506\n",
      "2017-11-24T17:16:40.729625: step 1685, loss 0.275697, acc 0.931641, f1 0.931392\n",
      "2017-11-24T17:16:41.933505: step 1690, loss 0.262725, acc 0.927734, f1 0.927901\n",
      "\n",
      "Evaluation:\n",
      "2017-11-24T17:16:42.736810: step 1692, loss 0.942171, acc 0.614724, f1 0.610871\n",
      "\n",
      "Current epoch:  47\n",
      "2017-11-24T17:16:43.489518: step 1695, loss 0.213488, acc 0.945312, f1 0.944475\n",
      "2017-11-24T17:16:44.709925: step 1700, loss 0.604525, acc 0.746094, f1 0.753901\n",
      "2017-11-24T17:16:45.934151: step 1705, loss 0.537632, acc 0.75, f1 0.727622\n",
      "2017-11-24T17:16:47.124730: step 1710, loss 0.28755, acc 0.910156, f1 0.9096\n",
      "2017-11-24T17:16:48.333866: step 1715, loss 0.250749, acc 0.945312, f1 0.945346\n",
      "2017-11-24T17:16:49.548302: step 1720, loss 0.236684, acc 0.947266, f1 0.947322\n",
      "2017-11-24T17:16:50.739881: step 1725, loss 0.249519, acc 0.923828, f1 0.924519\n",
      "\n",
      "Evaluation:\n",
      "2017-11-24T17:16:51.758689: step 1728, loss 1.03506, acc 0.577792, f1 0.58021\n",
      "\n",
      "Current epoch:  48\n",
      "2017-11-24T17:16:52.242725: step 1730, loss 0.260106, acc 0.892578, f1 0.891581\n",
      "2017-11-24T17:16:53.449371: step 1735, loss 0.529275, acc 0.746094, f1 0.712448\n",
      "2017-11-24T17:16:54.656683: step 1740, loss 0.613635, acc 0.732422, f1 0.713501\n",
      "2017-11-24T17:16:55.848139: step 1745, loss 0.275061, acc 0.921875, f1 0.91902\n",
      "2017-11-24T17:16:57.043781: step 1750, loss 0.237508, acc 0.933594, f1 0.932974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-24T17:16:58.261478: step 1755, loss 0.223899, acc 0.935547, f1 0.935684\n",
      "2017-11-24T17:16:59.479901: step 1760, loss 0.230829, acc 0.939453, f1 0.939511\n",
      "\n",
      "Evaluation:\n",
      "2017-11-24T17:17:00.756949: step 1764, loss 1.02502, acc 0.588697, f1 0.593134\n",
      "\n",
      "Current epoch:  49\n",
      "2017-11-24T17:17:01.029797: step 1765, loss 0.229727, acc 0.935547, f1 0.935796\n",
      "2017-11-24T17:17:02.232498: step 1770, loss 0.283297, acc 0.902344, f1 0.898347\n",
      "2017-11-24T17:17:03.455720: step 1775, loss 0.566632, acc 0.740234, f1 0.730963\n",
      "2017-11-24T17:17:04.669161: step 1780, loss 0.213134, acc 0.947266, f1 0.947164\n",
      "2017-11-24T17:17:05.873821: step 1785, loss 0.211896, acc 0.947266, f1 0.947219\n",
      "2017-11-24T17:17:07.090354: step 1790, loss 0.243707, acc 0.927734, f1 0.927613\n",
      "2017-11-24T17:17:08.298603: step 1795, loss 0.236436, acc 0.917969, f1 0.91759\n",
      "2017-11-24T17:17:09.490654: step 1800, loss 0.25295, acc 0.927419, f1 0.92578\n",
      "\n",
      "Evaluation:\n",
      "2017-11-24T17:17:09.823991: step 1800, loss 1.07132, acc 0.613949, f1 0.593892\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_epochs = 50\n",
    "\n",
    "num_checkpoints = 5\n",
    "print_train_every = 5\n",
    "evaluate_every = 50\n",
    "checkpoint_every = 10000000\n",
    "allow_soft_placement=True\n",
    "log_device_placement=False\n",
    "\n",
    "# for plot acc/epoch and loss/epoch\n",
    "train_accuracy_temp = []\n",
    "train_accuracy_list = []\n",
    "test_accuracy_list = []\n",
    "\n",
    "train_loss_temp = []\n",
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "\n",
    "epoch_list = []\n",
    "epoch_counter = 0\n",
    "\n",
    "with graph.as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=allow_soft_placement,\n",
    "      log_device_placement=log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        \n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdadeltaOptimizer(1.0)\n",
    "        grads_and_vars = optimizer.compute_gradients(loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=num_checkpoints)\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              input_x: x_batch,\n",
    "              input_y: y_batch,\n",
    "            }\n",
    "            _, step, cur_loss, cur_accuracy, y_pred = sess.run(\n",
    "                [train_op, global_step, loss, accuracy, predictions],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "\n",
    "            if step % print_train_every == 0:\n",
    "                f1 = f1_score(y_batch, y_pred, average = 'weighted')\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}, f1 {:g}\".format(time_str, step, cur_loss, cur_accuracy,\n",
    "                                                                    f1))\n",
    "            # For record accuracy and loss\n",
    "            train_accuracy_temp.append(cur_accuracy)\n",
    "            train_loss_temp.append(cur_loss)\n",
    "\n",
    "\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              input_x: x_batch,\n",
    "              input_y: y_batch,\n",
    "            }\n",
    "            step, cur_loss, cur_accuracy, y_pred = sess.run(\n",
    "                [global_step, loss, accuracy, predictions],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            f1 = f1_score(y_batch, y_pred, average = 'weighted')\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}, f1 {:g}\".format(time_str, step, cur_loss, cur_accuracy,\n",
    "                                                                    f1))\n",
    "            # for record history\n",
    "            test_accuracy_list.append(cur_accuracy)\n",
    "            test_loss_list.append(cur_loss)\n",
    "        \n",
    "        sess.run(embedding_init, feed_dict={embedding_placeholder: final_embeddings})\n",
    "        # Generate batches\n",
    "        batches = batch_iter(\n",
    "            list(zip(x_train, y_train)), batch_size, num_epochs)\n",
    "        \n",
    "        batches_test = list(batch_iter(\n",
    "            list(zip(x_test, y_test)), batch_size, 1))\n",
    "        \n",
    "        # Training loop. For each batch...\n",
    "        for batch, end_of_epoch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            \n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if end_of_epoch:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_test, y_test)\n",
    "                print(\"\")\n",
    "                # For record training history\n",
    "                \n",
    "                train_accuracy_list.append(sum(train_accuracy_temp)/len(train_accuracy_temp))\n",
    "                train_loss_list.append(sum(train_loss_temp)/len(train_loss_temp))\n",
    "                epoch_list.append(epoch_counter)\n",
    "                epoch_counter+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "50\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "print(len(test_accuracy_list))\n",
    "print(len(train_accuracy_list))\n",
    "print(len(epoch_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Plot word2vec distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size = final_embeddings.shape[0]\n",
    "embedding_size = final_embeddings.shape[1]  # Dimension of the embedding vector.\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    embeddings = tf.Variable(tf.constant(0.0, shape=[vocabulary_size, embedding_size]),\n",
    "                    trainable=False, name=\"W\")\n",
    "\n",
    "    embedding_placeholder = tf.placeholder(tf.float32, [vocabulary_size, embedding_size])\n",
    "    embedding_init = embeddings.assign(embedding_placeholder)  # assign exist word embeddings\n",
    "\n",
    "    valid_dataset = tf.constant(word_index, dtype=tf.int32)\n",
    "    valid_embeddings = tf.nn.embedding_lookup(embeddings, valid_dataset)\n",
    "\n",
    "    # calculate cosine similarity\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    similarity = tf.matmul(\n",
    "          valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "    \n",
    "     # Add variable initializer.\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_plot(words, matrix, v_dic, nbest=15):\n",
    "    word_vectors = []\n",
    "    for word in words:\n",
    "        index = v_dic[word]\n",
    "        word_vectors.append(matrix[index])  \n",
    "    pca = PCA(n_components=2)  \n",
    "    pca.fit(word_vectors)\n",
    "    X = pca.transform(word_vectors)\n",
    "    \n",
    "    xs = X[:, 0]\n",
    "    ys = X[:, 1]\n",
    "\n",
    "    # draw\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.scatter(xs, ys, marker = 'o')\n",
    "    for i, w in enumerate(words):\n",
    "        plt.annotate(w, (xs[i], ys[i]))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2vec with distance supervised learning\n",
      "\n",
      "Before\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuwAAAHdCAYAAABG7ixpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxI0pYBcRERERcWP/DzQ3HY40hsrsAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd7651e1fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAw0AAAHdCAYAAABfQJXmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIiIiEcKGkRERERExKP/B23gL2taiGhyAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd672bbf080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "words = ['sadness', 'sobbing', 'cry', 'weep', 'horrible', 'worst', 'awful', 'enjoyable', 'wonderful',\n",
    "         'bad', 'fantastic', 'great', 'good', 'beautiful', 'stunning','gorgeous','glad','well','dumb']\n",
    "\n",
    "plt.rcParams['font.size'] = 18\n",
    "\n",
    "print(\"Word2vec with distance supervised learning\\n\")\n",
    "print(\"Before\")\n",
    "draw_plot(words, original_embeddings, original_word_dict)\n",
    "print(\"After\")\n",
    "draw_plot(words, final_embeddings, word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot loss/epoch and acc/epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzsnWd4HNXZsO9HzSqWLKvYkqvce8MFGzDYYIMLxBAIvSc4\nhBBCPpIASQhp70tJAZLQ8wKhF1NMccOAbcA2uOBe5apuWZasYnWd78eZkdbrXW2RVitrz31de+3u\nzJyZM1vmmaeLUgqDwWAwGDwRFuwJGAwGg+H0wAgMg8FgMHiFERgGg8Fg8AojMAwGg8HgFUZgGAwG\ng8ErjMAwGAwGg1cYgWFod4jISyLyFy+3PSgiMwI4l+tEZFmg9m8wnE4YgWHosPgieNyhlHpNKXWh\nj8d9VkTmi8jNIvJVS47vsM+ACkaDwRuMwDCELCISEaBdzwYWBWjf7ZIAfpaGdoQRGAa/sO54fyUi\nW0SkQkT+T0S6i8hiESkTkeUi0tVh+++JyHYRKRGRFSIyzGHdOBHZaI17C4h2OtbFIrLJGrtaREZ7\nMb/5wHXAr0WkXEQ+cpj3vSKyBagQkQgRuU9E9lnH3yEilzns5yQtQUSUiNwuInut+TwpIuKwfjRQ\nAsQDzwBTrOOXWOs7icjfROSwiBSIyDMiEmOtSxGRj639HhORL0UkTEReAfoAH1n7+rWL8+1qjS0U\nkWLrdS+H9Uki8qKI5FrrP3BYN8/6fEutz2GWw2c1w2G7P4jIq9brDOuz+KGIHAY+t5a/IyL5InJc\nRFaJyAiH8TEi8ncROWSt/8pa9omI/MzpfLY4fg+GdoJSyjzMw+cHcBBYC3QHegJHgI3AOPQF/3Pg\nQWvbwUAFMBOIBH4NZAJR1uMQ8Atr3RVALfAXa+w4a99nAuHATdaxOznMY4abOb5k78dp3puA3kCM\ntewHQA/0DdRV1lzTrXU3A185jFfAx0Ai+iJeCMxyWH8f8JCrsdayx4APgSS0UPnIYfuH0EIm0npM\nBcTTeVrrk4HLgVhrv+8AHzis/wR4C+hq7fs8a/kk4Lj13YRZ3+VQV8cE/gC8ar3OsD6Ll4E4h8/y\nVuv4nYDHgU0O458EVljHCAfOsra7EvjGYbsxQBEQFezfuXk4/c6CPQHzOD0f1sXkOof37wJPO7z/\nmX3BAh4A3nZYFwbkANOAc4Fc+8JorV9Nk8B4Gviz07F3O1zw3F5IcS8wbvVwbpuAedbrky761kXy\nHIf3bwP3Obz/EpjqZqyghdEAh2VTgAPW6z8BC4GBbj5vtwLDxfZjgWLrdTrQAHR1sd2zwGPNfMee\nBEb/ZuaQaG3TxfrOK4ExLraLBoqBQdb7vwFPBfs3bh6nPoxJytASChxeV7p439l63QOtRQCglGoA\nstB3mj2AHGVdKSwOObzuC9xjmWlKLNNOb2ucv2Q5vhGRGx1MXiXASCClmfH5Dq9PYJ2niCQCQ9EC\nzxWpaA1gg8OxlljLAf6K1ryWich+EbnP2xMSkVjL2X5IREqBVUCiiISjP69jSqliF0N7A/u8PY4L\nGj9LEQkXkYcts1YpWuCA/ixT0ILhlGMpparQ2s/1IhIGXAO80oI5GQKEERiGtiAXfeEHwLL590Zr\nGXlAT0c/ANrUY5MF/I9SKtHhEauUesOL47orxdy4XET6As8DdwLJSqlEYBtaG/CVi4DPlVL1bo5/\nFC1IRzicSxelVGcApVSZUuoepVR/4HvA/xORCzyci809wBDgTKVUAlpzwzqPLCDJEmjOZAED3Oyz\nAi3gbNJcbOM4r2uBecAMtFaR4TCHo0BVM8f6L9rndAFwQim1xs12hiBiBIahLXgbmCsiF4hIJPri\nVo2+E18D1AF3iUikiHwfbVe3eR64XUTOFE2ciMwVkXgvjlsA9PewTRz6olcIICK3oDUMf5iD9hU4\nHr+XiERBo2b1PPCYiHSzjtdTRC6yXl8sIgMt4XkcqEebkrw5l3i0MCoRkSTgQXuFUioPWAw8ZTnH\nI0XEFij/B9xifTdh1nyGWus2AVdb209A+5eaIx79vRahBc3/OsyhAXgB+IeI9LC0kSki0slav8Y6\n179jtIt2ixEYhoCjlNoNXA/8C32neQlwiVKqRilVA3wfbe8/hnY6v+cwdj1wG/BvtJ0709rWG/4P\nGG6Zfz5wtYFSagf6IrUGfVEeBXzt2xk2ak0XoU1MNp8D24F8ETlqLbvXOoe1ltlmOVozABhkvS+3\n5vOUUuoLa91DwO+sc/mliyk8DsSgP9+1TvMAuAEdTLALHURwt3X+3wK3oJ3xx4GVNGmDD6A1gmLg\nj8DrHj6Gl9HmxBxghzUPR34JbAXWob/rRzj5GvQy+vN/1cNxDEFCTjYdGwwGfxCRScC/lVKTPG5s\ncImI3AjMV0qdE+y5GFxjNAyDofV40PMmBleISCxwB/BcsOdicI/RMAwGQ1CxfDjvoc1xlyul6oI8\nJYMbjMAwGAwGg1cYk5TBYDAYvKJDFQxLSUlRGRkZwZ6GwWAwnDZs2LDhqFIq1fOWHUxgZGRksH79\n+mBPw2AwGE4bROSQ5600xiRlMBgMBq8wAsNgMBgMXmEEhsFgMBi8okP5MFxRW1tLdnY2VVVVwZ5K\nQImOjqZXr15ERkYGeyoGg6GD0uEFRnZ2NvHx8WRkZHByQdSOg1KKoqIisrOz6devX7CnYzAYOigd\n3iRVVVVFcnJyhxUWACJCcnJyh9eiDAZDcOnwAgPo0MLCJhTO0WAwBJeQEBgGg8HQEamuq2fJtjye\nWdmSponeE1CBISKzRGS3iGS6ajcpIr+yWmNuEpFtIlJvNX/xOPZ0oaSkhKeeesrncXPmzKGkpCQA\nMzIYDKczDQ2KtfuLuO/dLUz8y3Juf3Ujr6w5RE1dg+fBLSRgTm+rl/CTwEwgG1gnIh9aDWsAUEr9\nFd3HGBG5BPiFUuqYN2NPF2yBcccdd5y0vK6ujogI9x//okWLAj01g8FwGrE7v4wPNuWw8Lscco9X\nERsVzqwRaVw6ridnDUgmIjzwBqNARklNAjKVUvsBRORNdL9fdxf9a4A3/BzbbrnvvvvYt28fY8eO\nJTIykujoaLp27cquXbvYs2cPl156KVlZWVRVVfHzn/+c+fPnA01lTsrLy5k9ezbnnHMOq1evpmfP\nnixcuJCYmJggn5nBYAg0h4tO8NGWXBZuymFPQTnhYcK5g1K4d/ZQZg7vTmxU2wa6BvJoPdEN5m2y\ngTNdbWg1T5kF3OnH2PnAfIA+ffo0O6E/frSdHbmlXkzde4b3SODBS0a4Xf/www+zbds2Nm3axIoV\nK5g7dy7btm1rDH994YUXSEpKorKykokTJ3L55ZeTnJx80j727t3LG2+8wfPPP8+VV17Ju+++y/XX\nX9+q52EwGNoHR0qr+HhLHh9uzmVTljZLT+jblT/NG8GcUemkdO4UtLm1lzyMS4CvlVLHfB2olHoO\nq0vXhAkT2n1zj0mTJp2UK/HPf/6T999/H4CsrCz27t17isDo168fY8eOBWD8+PEcPHiwzeZrMBgC\nz4maOpZtL+C973L4am8hDQqGpSdw3+yhXDw6nV5dY4M9RSCwAiMH6O3wvpe1zBVX02SO8nWs1zSn\nCbQVcXFxja9XrFjB8uXLWbNmDbGxsUybNs1lLkWnTk13FOHh4VRWVrbJXA0GQ+Cot5zX723MYcm2\nPCpq6umZGMMd0wZy6bgeDOwWH+wpnkIgBcY6YJCI9ENf7K8GrnXeSES6AOcB1/s69nQgPj6esrIy\nl+uOHz9O165diY2NZdeuXaxdu7aNZ2cwGNqanXmlfPBdDh9uziXveBXxnSK4eHQPLjujJ5MykggL\na785VQETGEqpOhG5E1gKhAMvKKW2i8jt1vpnrE0vA5YppSo8jQ3UXANJcnIyZ599NiNHjiQmJobu\n3bs3rps1axbPPPMMw4YNY8iQIUyePDmIMzUYDIEip6SShZtyWPhdLrsLyogIE84bnMpv5w5jxrDu\nREeGB3uKXtGhenpPmDBBOTdQ2rlzJ8OGDQvSjNqWUDpXg6G9U3KihkVb8/lgUw7fHtDu2fF9u3Lp\n2B7MHd2DpLioIM9QIyIblFITvNm2vTi9DQaD4bSnsqae5TsLWLgpl5V7jlBbr+ifGsc9Mwczb2xP\n+iS3D+e1vxiBYTAYDC2guq6e1ZlFfLQll6Xb8qmoqad7QiduPiuDeWN7MqJHQoep9WYEhsFgMPhI\neXUdX+w6wtLt+azYXUh5dR3x0dp5PW9sD87sn0x4O3Ze+4sRGAaDweAFxRU1fLqjgCXb8/lq71Fq\n6htIjovi4tHpXDQijbMGJtMp4vRwXvuLERgGg8HghmMVNSzbns8nW/NYva+I+gZFz8QYbpjSl4tG\npDG+b9cOqUm4wwgMg8FgcKCovJql2wtYtDWPNfu1kOiTFMttU/szd1Q6I3t2HJ+ErxiBEWBKSkp4\n/fXXT6lW6w2PP/448+fPJzb29I6sMBjaO0fLq1myLZ9FW/NYu7+IBgUZybH8+Nz+zBmV3qEc1y3B\nCIwA4668uTc8/vjjXH/99UZgGAwBIKekks93FrBoaz7fHNBCon9KHHdMG8jsUWkMTzdCwhkjMAKM\nY3nzmTNn0q1bN95++22qq6u57LLL+OMf/0hFRQVXXnkl2dnZ1NfX88ADD1BQUEBubi7Tp08nJSWF\nL774ItinYjCc1lTX1bPuQDErdh9h5Z5C9h4pB6B/ahw/nT6QOaPSGZoWb4REM4SWwFh8H+Rvbd19\npo2C2Q+7Xe1Y3nzZsmUsWLCAb7/9FqUU3/ve91i1ahWFhYX06NGDTz75BNA1prp06cI//vEPvvji\nC1JSUlp3zgZDiHCipo7FW7WpafW+Iipr64kKD2NSvySumtib8wanMrBbZyMkvCS0BEaQWbZsGcuW\nLWPcuHEAlJeXs3fvXqZOnco999zDvffey8UXX8zUqVODPFOD4fRFKcW6g8Us2JDFJ1t0FdheXWP4\nwYRenDc4lSkDktu88VBHIbQ+tWY0gbZAKcX999/Pj3/841PWbdy4kUWLFvG73/2OCy64gN///vdB\nmKHBcPqSdewEH3yXw4KN2RwqOkFcVDhzR6dzxfjeTMzoarSIViC0BEYQcCxvftFFF/HAAw9w3XXX\n0blzZ3JycoiMjKSuro6kpCSuv/56EhMT+c9//nPSWGOSMhhORSnFnoJylm7PZ8m2fHbk6W6ak/sn\ncdf5g5g1Mo24TuYS15qYTzPAOJY3nz17Ntdeey1TpkwBoHPnzrz66qtkZmbyq1/9irCwMCIjI3n6\n6acBmD9/PrNmzaJHjx7G6W0woJsObcoqYdmOfJZtL+DA0QpE4Iw+XfnNnKHMHplO7yQTVRgoTHnz\nDkQonashdCgqr2bV3kJW7C5k1Z5Cik/UEhEmTBmQzEUj0rhweHe6JUQHe5qnLaa8ucFgOK3JPFLG\noq35fL7rCJuzS1AKkuOimD60G9OHdOPcwal0iYkM9jRDDiMwDAZDu+DA0Qo+3pzLx1vy2F1QhgiM\n6ZXI3RcMZvrQVEb26NKu25eGAiEhMJRSHT5CoiOZFg2hw4GjFSzelsfHm/MandYT+nblD5cMZ86o\ndGNqamd0eIERHR1NUVERycnJHVZoKKUoKioiOtr8uQztG6UU23JKWbo9n6Xb8xuzrcf1SeR3c4cx\nd3Q66V1igjxLgzsCKjBEZBbwBBAO/EcpdUoihIhMAx4HIoGjSqnzrOUHgTKgHqjz1injTK9evcjO\nzqawsNCvczhdiI6OplevXsGehsFwCpU19Xx78Bhf7DrCpzsKyCmpJExgUr8krj1zOBeOSKNnohES\npwMBExgiEg48CcwEsoF1IvKhUmqHwzaJwFPALKXUYRHp5rSb6Uqpoy2ZR2RkJP369WvJLgwGgw/U\n1TewNec4X2ce5avMo2w8VEJNfQOdIsKYOiiVu2cM4oJh3UmKiwr2VA0+EkgNYxKQqZTaDyAibwLz\ngB0O21wLvKeUOgyglDoSwPkYDIYAUVZVy+dWy9Iv9x6lrKoOgOHpCdx8dgZnD0xhYkZXU5LjNCeQ\n315PIMvhfTZwptM2g4FIEVkBxANPKKVettYpYLmI1APPKqWec3UQEZkPzAfo06dP683eYDA0i6uW\npSmdOzFnZDrnDErhrAHJJHfuFOxpGlqRYIv7CGA8cAEQA6wRkbVKqT3AOUqpHMtM9amI7FJKrXLe\ngSVIngOduNeGczcYQo7DRSdYvrOA5TsL+ObAscaWpTdO6cuskWmM6xNaLUtDjUAKjBygt8P7XtYy\nR7KBIqVUBVAhIquAMcAepVQOaDOViLyPNnGdIjAMBkPgsEtxLN9ZwGc7C9hToKOaBnXrzE/OG8Cs\nkWmmG10IEUiBsQ4YJCL90ILiarTPwpGFwL9FJAKIQpusHhOROCBMKVVmvb4Q+FMA52owGCwqa+r5\ncm8hy3cW8PmuIxwtryE8TDizXxJXT+zDjGHd6ZNs6jWFIgETGEqpOhG5E1iKDqt9QSm1XURut9Y/\no5TaKSJLgC1AAzr0dpuI9Afet+5aIoDXlVJLAjVXgyHUOVpezec7j7BsRwFfZRZSVdtAfKcIpg3t\nxoxh3Zg2uBtdYk0pjlCnwxcfNBgMrsk7XsmSbfks3prPukPHUAp6JsYwY1g3Zg5PY1K/JKIiwoI9\nTUOAMcUHDQaDS7KOnWDxtjwWb8vnu8MlAAxNi+eu8wdx4YjuDE83/giDe4zAMBg6OFnHTrBoax4f\nb8lja85xAEb17MKvLhrC7JFp9E/tHOQZGk4XjMAwGDoguSWVjUJiU5bWJMb06mKaDBlahBEYBkMH\n4VhFDZ9szePDTTmsO1gMwIgeCdw7aygXjzZCwtByjMAwGE5jTtTU8emOAhZuymXVnkLqGhSDunXm\nnpmDuXhMD/qlxAV7ioYOhBEYBkNbUlsF9TUQneD3Lipr6lm5p5Al2/JYtqOAEzX1pHeJ5ofn9GPe\nmHSG7XwCSR0GKYNaceIGgxEYBkPb8vEvIPtbuGMthHuf13C8spYvdh1hybZ8Vuw5QlVtA4mxkcwb\n24N5Y3syKSNJd6Nb+ltY82/oPhJGXxnAEzGEIkZgGAyeOHYAdnwAE38EneJbtq/Da6D4AGx+A864\nsdlNj5ZX6+J+2/JZve8otfWKbvGduHJCb2aN0HkSEeEOeRJfP6GFRXwPKNgOlSUQk9iy+RoMDhiB\nYTC4o75OX4BXPAx1lbDzY7j+Xf8vwlWlWlgArPorjL4aIk7uCZFTUsnSbfks2Z7P+oPHaFDQJymW\nW87ux0Uj0hjXO9F1X+tNr8Onv4cRl8EZN8Erl0LWtzD4Qv/majC4wAgMg8EVud/Bhz+D/K0wZC4M\nmQWf3AP/vQRu+ADikn3fZ8F2/TzxNlj3PGx+HcbfTNaxE3yyNY9FW/PYkq3zJIamxfOz8wdx0Yg0\nhqXHN59Mt3sJLLwT+p0Hlz0LDfUQFgGHVxuBYWhVjMAwGBypqYAv/hfWPgVx3eDKV2D49/S6+B7w\n1nXw0ly4cSHEd/dt3/lb9fM5v6AmawPVnz7Mzav7siHnBABjeidy3+yhXDQizfvopsPfwDs3Q9oo\nuPo1iLD6T6SPhUNrfJufweABIzAMLacsH96+CYZdDJPmN1202jt1NVCaDcWHoOSwfmx9Wz+PvwVm\n/OFk89OgGXDt2/DGNfDSHLjxQ+jS0+vDlR/6jvDIRK579QDxOTP5b9QjTA9fzoWzb2XOKD/yJI7s\nhNevhIQecN2Ck/0rfafAN8/qqKzIaN/2e7qiFJw4BrUnoLby5OeYROg5PtgzPO0xAsPQcra8DVlr\n9ePb5+CCB2Hk5dBeaxKt/hesfQbKckE1NC2XcEgbqc06fc9yPbb/eXDDe/DqFfDibLjpQ+ia4XJT\npRQ788pYuj2fZTsKeKRoDcdVLyrrFBfM+AHVe5ZzZ+VCOPvBU3wZJ1GaB0WZUHwQSg7p5+JDcGQH\nRMXp+XROPXlMnyn6PHM3uj+XjsZHP4eN/3W/fsAFcOFfoPvwtptTB8MIDEPL2bFQm0BmPAjLfg/v\n/lCbdC78S/u8WG1+S4e0nvtrSOzT9EjoCeFe/CX6TIabFsIr34cX58CtSyFR9wpTSrE9t5QPN+ey\neFseWccqEYEz+8QzPCKHE2N+yOJ5U639/AZevRy+ewUm/vDU4ygFKx+BFQ81LZNwrdUk9oWR34cp\nd7oWWH2m6OdDq9vndxAI8jZDtxEw+ScQGQORsRAVq5+zvoVVj8IzZ8O4G2D6b303KbYmRfv0/+Ty\n/4PkAS3b1/6VWisec7VPodr+YASGoWWUZEHOeq1VDDgffnwebHkLPvuzvgMfejHMftQn001AUUpH\nKo27Hqbf7/9+eo6Hmz+GZ8+FjS9zYPTdfLgpl4Wbc9hfWEFkuHDOwBR+Om0gM4Z3J6ViHzxdQ0K/\nM5r2MeAC6DUJvvy7no+jKa+2Cj68E7a+A6OvgrHXaiHRpZd3F4XYJEgdCofX+n+Opxtl+TBoJpxx\nw6nrek/Sn+HKR3XAwdYFcM7dWuBGBaFkyoqHdWDFlrdb9ju091Warc8vwBiBYWgZOz/Sz8Pn6eew\ncP3DHX4prH0SvnwMXv0+/HAZRHcJ3jxtThRBTTl07dfiXRXEDiKs81DyVy/mkmVjtSbRL4nbpvZn\n9sg0EmMdzEz7LId32qimZSL6YvHKZbDxZZh0m15ecRTevBayvtGC+Jxf+Gfe6zMFtr1rRU2F+3+i\npwP1dVBxBOLT3W8TmwSzH9af8/IH4Yv/gQ3/hds+g/i0tptr4R7YtkC/3rO4ZQIj9zsdDXfR/7bJ\nd2y6oxhaxo6F0H3UqWp1VCyc+yu49k1tf19wq/5TB5tjVh6EG7+DJ4oranjtm0Nc/dwaJj/0GR8U\nZzCkbhe/m9Wfr+89nzfnT+GaSX1OFhYA+VsgvBMkO5Xr6D8dek+GL/8BddVQuBueP1+bV37wX5j6\n//z3BfWZAtWlTeG8HZmKI9of5c2FP3kAXPWqDo8uzdYacVuy6lGIiNbaTd5mKM31f19rnoKoeG1m\nawOMwDD4T2mudnTb2oUr+p0Lc/8Omcth2W/bbm7usBPnkrzXMCqq63hvYzY3v/gtE/9nOb99fxtH\nSqu56/xBXHzJFURRy4/6FdMjMcb9TvK3amers49EBKbdpx3wH/8C/jNTR/bcvAhGXOrHCTrQ1/Jj\nHA6B8NqyPP3cnIbhzIDp2rS4dUFg5uSKwj36eJNu02ZIgD1+dp8uzYPt7+n9tKA2mS8Yk5TBf2xz\nlKcL2/ib9R9l7ZOQMti1g7etKD6onxP7NruZUorvskp4e10WH23OpaKmnp6JMfxwaj++N6ZHU2e6\nEymwGDj0tXvnslJaYAy72PX6/tO0NrDpNe20vfZN7YRvKYl9IKGXFhhn/rjl+2vPlOXr5wQfBAbo\naL6lv4Gje9umWOOqR7UT/qy7IDZZ/w53L4EJt/q+r3XPa3NjG363ARUYIjILeAIIB/6jlHrYxTbT\ngMeBSOCoUuo8b8caAkBDA+z6CBrq9J+pOXYshG7DvfujXfhnbZpa9CtI6q/v7oLBsQM6Ac9NbsKx\nihre25jN2+uz2FNQTkxkOHNHp3PlhN5MzOh6asZ1bJK+yB9a7f6YpblQeQzSRrteLwIXP6brS039\nZeveLfadAge+1EKrvYY5twb+aBigS6ks/S1sew+m3dv683KkcLfWLs7+OcSl6GVDZsOGl6DmhG/O\n95oTsP4FGDrXJ225pQRMYIhIOPAkMBPIBtaJyIdKqR0O2yQCTwGzlFKHRaSbt2MNrYxSkPkZfPZH\nbW+XMK0NODppHSkr0BfJafd5t/+wcLj8P/DCRTrJ70fLIXVw683fW4oPnvIHq6tvYNXeQt5Zn83y\nnQXU1ivG9E7koe+P4uLR6cRHe4hK6nuWruVUX+s6ginfhcPbmW7DYOaffDsXb+gzWUdaFR/Qgrqj\nUpavf7NxqZ63dSShB/Q9Wzuhz/t1YIXqSlu7+FnTssGz4JtnYP8KGDrH+31teRMqi2HyHa0+zeYI\npA9jEpCplNqvlKoB3gScjd3XAu8ppQ4DKKWO+DDW0Foc/kaXu3jtcqgqgUuegOhEWHyvFiSu2PUR\noJr3XzgTnQDXvKkvqq9fqbNy25riA40O7935Zfzvop1Mfuhzbn1pPd8eOMYNkzNYeve5LPzp2Vwz\nqY9nYQFaYNRWQN4W1+ttgdF9ROucgy/0scxkHb1MSGkedO7uX6TQyO/D0T1QsK3152VTuFtHrE26\nrUm7AC2sOiXoaClvaWiAtU9D+pg2z7EJpMDoCWQ5vM+2ljkyGOgqIitEZIOI3OjDWABEZL6IrBeR\n9YWFha009RChcA+8fjW8cKG24c75G9y5QfscLvi9tstvf8/12B0LtQaSOtS3Y3btC1e/rs00iwNs\nAnCmthLK8thYlsj3/v0VFz2+ihe+OsD4vok8f+ME1v7mAn5/yXCGpPlYwrzv2fr50Neu1+dv0Xf3\nLS2N7g+pQ7Xwb6+O75wN8MRYqChq2X7K8nw3R9kMn6cTIre927I5NMdKB9+FIxFROn9pz1ItCLxh\n3+dawE3+aZubGYMdJRUBjAfmAhcBD4iIT3YKpdRzSqkJSqkJqak+qqOhTH0tvDxPm5Uu+D38fJO+\n+7FLVJxxo76DWfaALsjnSHkhHPzK+qP5kx9wpr6r2/+Few2mFamoruP977K5/4WPAXhpp1BXr/j9\nxcP55jcX8OwNE5g5vDuR4X7+HeK7Q/LAZgTG1ubNUYEkLEw71FtbYHz+F62ZtpQDq7TWl/tdy/ZT\nlu+/wIhL0T61be8G5vd4ZJfe95nzXVc5HjIbygu8/wzWPgmd07T/pY0JpMDIAXo7vO9lLXMkG1iq\nlKpQSh0FVgFjvBxraAm7F+tQzu8/C1Pv0TWJHAkL1xnapTk6R8CRXR/rmHdfzFHO9JoIFYVNYa6t\nTE1dA5/vKuCuN75jwl+W84u3NlN/dD8Av7z6Qhb9fCq3ntOP5M6tVCix71na7NNQf/JyuwdGsAQG\naMd3USaUH/G8rTfUVup+Hisfafm+ijL189E9LdtPWV7Lku9GXq7La2Svb9k8XGFHRk35mev1gy7U\n/hdvzFJ+YUnTAAAgAElEQVRHdmoNY9KPmq8/FiACKTDWAYNEpJ+IRAFXAx86bbMQOEdEIkQkFjgT\n2OnlWENL2PCirp00cKb7bfpM1mUpVv8Tju1vWr5jISQN0G1A/aX3mfo5a53/+7DZ+THUVVPfoFid\neZT739vCpP9dzq0vrWfV3kK+f0ZP3rl9Cg9P7wxAnwEtmLc7+p4D1cd1QUBH7KQ5dxFSbUGfVs7H\nKC/QzwdWttwPVbRPP7dEYNRW6Sg0fzUM0NFG4Z1a3yx1ZKeOwHKnXYCOtOs9WYfXemLt0zrpb7wf\nYbitQMAEhlKqDrgTWIoWAm8rpbaLyO0icru1zU5gCbAF+BYdPrvN3dhAzTXkOHZA36WccaPnYnsz\n/ghhkbD0d/r9iWPajOCvOcqm2zCdoZr9rf/7AFThbnjrOj58+R9Mfugzrv3PNyzclMu0wam8cPME\nvv3NDP7nslFMzEgirOSQdjDGJrXomC6xnY8HncxS3kRIBZr0sRAR03p1peych4Y6rW22hEYNY6//\n+yi35tMSDSO6i65Dtf29U7VEfynL16Xwo7u41y5shsyCgq26Nps7Kop0Vvroq/xr4NUKBDQPQym1\nCFjktOwZp/d/Bf7qzVhDK7Hxv1oF9qacQEI6nPcrWP4Hna1dmgeqvmXmKNAmr55n6HpJflBUXs37\n3+Wwd81iHgHKD65n/KALuWRMD84f2o2YKBfRMsUHtNM9EI7CxN7QpY/2Y0y+vWl5/hadoNWSu9+W\nEhEFvSY0nyviC3bOQ0Q0bH/fY29yt1SWaLMk0jINw9+kPWdGXq4F4KGvdYWClnDimK4RVn5EN9vy\ndIEfPFu32N2zpKmmmDPrX4C6qjYPpXUk2E5vQ1tTVwPfvarjv72tIDv5Dh3ls/g+HdOf2Fc7xFtK\n7zO1yaa63KvNGxoUq/YU8tPXNjL5oc/4yyc76RmuW5pe2auYZ24Yz9zR6a6FBWjNqhWKDrol42x9\nUXZ0nNoO72AnzfWZooVXdVnL92VfoEdfpUtr+2uWss1Rvc/UtaAqi/2cj59Je84MngWRcS03S1WX\nwWtX6PO75g3oPdHzmJRB+j/mrkxI5nL48m/a39HNx8jEVsQIjPZK/rbWiUJxZvcn+q5u/C3ej4no\nBLMehqK92m7dUnOUTe9J2nmeu7HZzfYXlvP3ZbuZ+ugX3PjCt3y972hjvsRdk7RfIuLIjuZNCQ0N\nuvmQn0UHvaLvWXDiaNPdcn2ttmEH039h02ey/qyzW8FnVJYP4VEw4RatbdolYnzFNkcNma2fj2b6\nt5/SVhIYUbE6eW7HQn1j5Q+1VbrScO4m+MGLuuGWN4hoLePAqlNvoPYsgzeu1ULl0mdcj28jjMBo\nr7z/Y12MrrVZ/yJ06Q0DL/Bt3OCL9N0N6NLlrUGvCfrZhVnqWEUNL685yLwnv+b8v6/kyS8y6Z8a\nx7+uGcc3jvkSZZYDtq6y6QLkirJcqK8JbBkF53yMo3uhvrp9CIzek7QZsjUS+MrydVhn+litse34\nwL/9FGXqOdm/K3/NUmV5WoDFdPVvvCMjL9eazv4Vvo+tr4UFt+hSLJc9ox3pvjBklv6N7v+iadnu\nxbqPfLdhuiVwkHwXNqb4YHvkyC6ddRrXrXX3W7RPawjTf+dfRuz3/gW7F2nfQ2sQ0xVShjRGStXV\nN/DZriO8sz6bFbuPUNegGJoWz2/mDGXe2J50T3BR/8m+WNTX6Ezr1CGuj9VY1jyAAiOpv76QHlqt\ni8m1B4e3Tad4LbhaI1LKDmEV0bkAXz+hHbK+XsyKMnWBxJTBOrDCb4GR3zSfljLgfO2k3vYuDL7Q\n+3ENDfDBHfr/MedvMPpK34/dZwp06qKjpYZdojW3d27W5t/r3zu5v3yQMAKjPWLbUE8UtW7RuA0v\n6YxWu6yyr8Sn+VdVszl6T6Rh5yc8vnQXb23IpqC0mtT4TtxydgaXjevF8B4eCvGVF+icjuz1kL8Z\nRv/A9XZ2ldpAmqREtFnq4NdWhdot2jGcPDBwx/SF/tNgzb+1I7ZzC25GyvKbBPOIS+Grf2hn8fib\nfNtP0V7dHyQ8Qveo8DdSqixPF5RsDSI6wbDvaWd+baVu9eoNX/0dtr6tG165c1p7IjwSBs2AvUv1\nNeDd23T59esXtI/mYxiTVPtDqaZuXKoeqo63zn7rqnX57CGzWx5N0grUNyiW7yjgxcPdCasq5pOV\nXzIsPYHnbhjPmvvO57dzh3sWFqAvFl16aZXdXS0n0BFSEq7NcYEk42xt/io+qDWMbi56YASLsdfp\nUNhNr7VsP+UOWdVpo7Vmtf193/ahlNZ4bWGaMqjlGkZrMfJy3ZVx76fejzn4tf4spv6/lh178Gzt\nY1xwqzYj3vBeuxEWYARG+yP3O50kZ9vDK1upQN/Oj7TGMsEHZ3cAqKlr4K11h5n+txX86OX1LD6u\nL+Bvz4ngpVsmceGINCK8LdGhlPZhxKdB+mh9R++utEPxQR36GuiLt6MfI5glQVyROlgXI9z4sv8l\nMGpO6JuY+O76vW2WOrDKt3pQZXlQe6KpU2PKYC3U62t9n1NL6ki5ws6pObLThznkt04Pk0EzdKRW\nxlS4bkFw6o81gxEY7Y1t72p77vib9fvWqui64SUdDtv//NbZn4/U1DXw2jeHmP63Fdz77lYSYyN5\n+rozeO2+GyC6C8nFm3zfaWWxdip3TtN3d5XFcDzb9baBDqm1SRkCMUk6/LjyWPsSGKDNRsf2w8Ev\n/RvfmCTncIEefqnWhnf5EC1lByg0ahiDtfZzzMdSMdVlWhtoTQ0jopPOnSnzoXVqS0uT2MR0hbu+\n0+1jO3Vu+f5aGSMw2hMNDVq1HzijqXfBiRZW8QRtGz74pRZCYW37lVfX1fPKmoNM++sX/Pb9baTG\nd+LFWyay8KdnM3tUOpEREdoHkeVHxrddoiI+rSkvxHY0O1N8oG0azYSF6TtUO8qmPURIOTJ8njZx\nbPivf+PLXGRVp43SpWJ8MUudIjCsJly+mqUak/ZayYdhE9+jKVzXE7WVui1Aa2k58d3bjxnTCSMw\n2hOH1+hif6OuaCpf0RoCY8NLEBbhv7PbD46UVfHE8r1MfeQLHli4nfTEGF6+dRLv33EW04d0O7lz\nXa9JWv331V/TmLCVZvWaEG2WcqayRGsfgXR4O2KbpRDdx7s9ERmjE+52fuif9uoqSa7RLPUlVBz1\nbj9F+3S5kgQreTTZX4Hh8BtoTRLSm/bt9RyC7xsMNEZgtCe2LdBVLYfM1ioxtFxg1FRoJ+fQi1sW\nGeMFSik2Hi7m529+x9kPf85jy/cwLD2B1350Jgtun8K5g1NPbXEKVias0r0RfKHMQcOIitN3q64c\n340RUm3UytK2gQerB4YnzrhJhyFvftP3sfZn3rn7yctHXOpbEt/Rvdp/YWu80Qn6gutrpFRrJe05\nE++DwLDn0A6CSQJN+9R7QpH6Wp1hOmS2vvgppbWClvowvnkm4K0cq2rr+XhLHv9dfZCtOcfp3CmC\n687sy41T+tI/1Qs7bM8JgGiz1AAffCz2H7qzdXeZPtq1acsuod5WvY/TRmmzT3o7M0fZpI3U4Zob\n/wuTf+Jb2HZZnq7q6pwk132kFtg7PvAusKIoU8/DEX8ipQKmYfTQ0Up1NZ7LiIeQhmEERnth/0qt\nTYy8XL8X0VpGSzSME8fgqyd0qF6fM1tnng7kllTy6tpDvLkui2MVNQzs1pk/zxvBZWf0onMnH35a\n0Qk6/NRXP0Z5ga4+GxWr36eN1kEDJ46dXJG2LXIwHAkLh+veDbhG1yLOuAk+ukt/5r78Ntwlydlm\nqS//rs1Sjm1Inamv1d/JCKeKASmDYcs7vuUeleVDVOfW1+Tsi3+5F9FPISQwjEmqvbDNSs4ZOKNp\nWUxSywTGV49BdSlc8EDL52ehlGLNviJuf2UD5zzyOc+s3MeEvl159Ydn8ukvzuWGKRm+CQub3lby\nnbdtKuHUyBT7jt7Zj3HsAMSmtK15qPdEXRm3vTLycn2h3eij87u5ENbhl+p6VTs9tK4pPqTNV84J\njSmDdU8RXxo9tXZIrY3tRPfG8V2Wr/0x7ShfIlB4JTBE5D0RmSsiRsAEgtpK3QRo2CU6pM8mNtl/\nk9TxHPj2ORhzteUQbhlVtfW88e1hZj3+Jdc8v5a1B4qYf+4AVv5qOs/dOIFzBqW49k94S+8z9cXi\n6G7vx5QVnGxLT7MipZz9GMUH2k67OF3o1FkLjW3v+RZsUF7g3vzTfYR2Xu/wIDCcI6Rs/ImUau2k\nPRtbCHkTWutYKqWD460AeAq4FtgrIg+LiJuCPQa/2Psp1JQ1maNsYpP8T9xb+Yiu3jrt/hZNraC0\nir8u3cWUhz7j/ve2Eh4mPHrFaNbefwH3zR5K76TYFu2/kV6T9LMvZinnu8u4ZB114xxaW3yw7fwX\npxPjb9JFG7e+4/2Y5i7QIrqA3qGvmy9ZX2Q5tl1pGOCjwMgNvoZRmtf6Yb3tFK8EhlJquVLqOuAM\n4CCwXERWi8gtIhIZyAmGBNsWQFwqZDg1bfHXh3F0r+55MfGHfptFtmSXcLcV7fTUin1MzEjirfmT\n+eSuc7hyQm+iI/0oXtgcyQO0Cc7bDnxKWXe7TtE6aaNPNknV1ehkvraKkDqd6HEGdB/lfU5Gdbk2\ncTZ3Rz9wpo7Aai4xsChTf9fOnQ/je+gsZ28jpZQKnIYR01U7933RMEIAr43NIpIMXA/cAHwHvAac\nA9wETAvE5EKCqlLYs9R1u9TYJG2SamjwLeHu8z/rePupv/R5OpuzSnh06S6+ziyic6cIbpySwc1n\nZdAnuZU0CXeI6No53moYVSW6+5jz3WXaKF28reaEdoYfz9J2dWOSOhURrWUs+qUuSdNjXPPbNyZK\nNnNH32eKvujv/bSpz4UzjjWkHAkLg5SB3msYlcVaOAVCwxDRQsCThqFU4Pwo7RCvBIaIvA8MAV4B\nLlFK2Z/iWyKyPlCTCwl2L9IXPmdzFGgNQ9Vr2763tf5zNujw3PPug86pXk9jX2E5f1u6m8Xb8kmO\ni+J3c4dx1cTexEe3oQLZa6LuOOYc5eQKd/kA6aO1gDiyQ/fbaOuQ2tONUT+AZQ9oLcOTwHCV5e1M\nRJRuGpT5qftop6JM6D/d9fiUwd43DisLcP5DQg/PuRjublw6KN7etv5TKTVcKfWQg7AAQCk1wd0g\nEZklIrtFJFNE7nOxfpqIHBeRTdbj9w7rDorIVmt5xxVK297VFVRtG74jjcl7PvgxPvuTHjflp15t\nnn+8ivvf28KFj61i1Z5C7p4xiJW/ns6PpvZvW2EB2vEN3iXwuQtltEtx5G3Wz419MDJaPL0OSUyi\nDm/dukAHXzSHc96LOwbOgJLDrk1L1eV6P3bRQWdSBsPxw1pD9ESgkvZs4tOh1INJKoSS9sB7gTFc\nRBq7d4hIVxFpNhNMRMKBJ4HZwHDgGhFxVSfhS6XUWOvxJ6d1063lboXSaY1SugPa4Itcm5x8FRj7\nvtA1jKb+Uuc2NENxRQ0PLd7JeX/9ggUbsrlxSl9W/no6d88Y7F9YbGvQ8wxdgtxFB75TcKwj5Uhi\nH4hObPJjFB/UPSk8XeRCmSFzdNCFp+qs3mgYAINm6udMF+XBj1l9vO2IKGfs5c11T2ycT4CS9mxs\nDaO5yr4hlIMB3guM25RSJfYbpVQx4KlLyCQgUym1XylVA7wJzPNvmh2UymL9R7ULDToT40M9KaXg\nsz9qbWXiD91uVlFdx78+28u5j37Bc6v2M3dUOp/fM40HLxlBSudObse1CVFxOjTTGz9G492uk0lK\nRPsx8hwERteMNi+6eFqROlQ/F3oIaS7L8y7fILGPrtrrqp/EUTcRUja+RErZAixQNwPx6drcVFns\neQ5GYJxEuDgE2Vvag4d8eXoCWQ7vs61lzpwlIltEZLGIOCYMKHQ01gYRme/uICIyX0TWi8j6wsJC\nz2cSSJTyrYZ+yWH97C6T1JcChIfXasflefeenMthUVVbzwtfHeDcR7/g75/uYcqAZJbefS7/uGps\n64XGtga9Jurz8NSvoawAouJdl4BOH6N9GPVWuWxjjmqepH66pH7hrua386UV6qCZOry2puLk5UWW\nhuHuJilpACDeRUqV5embqkgXrXtbA9vMZAsFl3OwTFYhEiXlrcBYgnZwXyAiFwBvWMtaykagj1Jq\nNPAvwLGb/DlKqbFok9ZPReRcVztQSj2nlJqglJqQmuq9kzcgbF0AT032fKdm41FgWCYpb3Ix7PIX\nduE7i4YGxdvrszj/byv408c7GJIWz/t3nMVzN05gcPd2WBiv+3AduunJdlzeTDhl2mh9Z1i019Iw\njMO7WcIjtSnI0++2uaQ9ZwbO0BFMB5zCa4sytRbsrvVpZLQOBfdKwwhwdJLd9rW50NqyfB2Q4m0r\n19McbwXGvcAXwE+sx2fArz2MyQEc+2H2spY1opQqVUqVW68XAZEikmK9z7GejwDvo01c7Ru7H4Cv\nAsNd29BO8frOzxsNo/FOp+kPtDmrhMue+ppfL9hCakI0r/3oTF6/bTLj+ngZcRUMUqycUE8Z383F\n39tNizI/g9oKEyHlDalDvNAwfMg36HuWDq919mMUZbp3eNukDPZewwjknb2tYTQXWlsaOiG14H3i\nXoNS6mml1BXW41mlVL2HYeuAQSLST0SigKuBk2oGiEiabeoSkUnWfIpEJE5E4q3lccCFwDbfTq2N\nqamAfZ/p1/bdvidKDmuziruQWRErF8MbgZGvbctRsRRX1HD/e1u59KmvyT1exeNXjeWDO87i7IHN\nFIRrL3htT88/1X9hkzJYO7p3LNTvjUnKM6lD9e+2uUipsnzvL44RnaDfudqPYZsXG/t4u3F426QM\n1tqhp7pivszHHxrLgzQjMEIoaQ+8z8MYBDyEjnZqNBgqpdwYIkEpVScidwJLgXDgBaXUdhG53Vr/\nDHAF8BMRqQMqgauVUkpEugPvW7IkAnhdKdUaJrDAkblcm0HAN4GR2Kd5m7C39aTK8lDx6bzxzWEe\nXbqLsqo6bj27H3fPGNT24bEtIS5FC9DmBIanDN/wCF391s4aNyYpz6QOAZS+s3dVlt2fVqiDZsCe\nxVpIpAzUVWyrj7t3eNukDNL/peNZ7isVNNT7ZiLzB7tVa3Pm0bJ8/VsLEbyNn3wReBB4DJgO3IIX\n2ollZlrktOwZh9f/Bv7tYtx+YIyXc2sf7PxIO+C69GxKFvOELTCaw0uBUXE0mz0lMfzm/a1M6pfE\nn+eNZEhaO/RReEJE3+02JzCqS3UNpOYuFumjIXcjIJ4/Y8PJmp0rgdGYKOnDBdquvJz5qRYY7mpI\nOdMYKbXXvcCoKNQJmoHOf4hvJnmvLYRWO8NbH0aMUuozQJRSh5RSfwDmBm5apxl1NbBnmY5nTx7o\nnYahlL6D8igwmjdJHS2v5tcLNlNy5DA5dV144uqxvDV/8ukpLGxSBjfvw/AmlNFO4EvoEbgomo5E\n0gCdA+POj+FPzkPXDG1+ssNrG6vUeuHDgOYd36Wn+uwCQnyaew2jolBXYgiRpD3wXmBUW6XN94rI\nnSJyGeBFK7UQ4eAqrWoPu0T/SUoO67uP5qgq0XfKXmkYpwqMuvoGXvjqANP/toL3N2aRFlbChZPH\nMm9sz5aVGW8PpA7V5+yuP3Rj/L0bHwbo0Fow5ihviYjSF3K3AsPPfINBM+HgVzpzuygTwqO8+83H\ndG1eYHibRNhSmuvt3VZCqx3hrcD4ORAL3AWMRxchvClQkzrt2PmRbkbTf5q+QDXUQWlO82M8hdTa\nxFglzh0cgGv2FTH3n1/xp493MLZ3IsvmjyBc1RPVtVeLTqPdkGrdYbozS3lz8eo2HCTMOLx9IXVI\nM5+5n1nVA2dAfbUWGkX7dP5FmIdKxyKeI6XaKsM63qFV6ylzCK2kPfBCYFhJelcppcqVUtlKqVuU\nUpcrpda2wfzaPw31sGuR/mNERjddoDyZpbwVGLHJ2lZbfZzSqlrueXsz1zy/loqaOp69YTwv3zqJ\nfp1K9bYdxZbaaE93c7dbbv9Rm9EwomLhkn/C5Ntbd24dmdShcGw/1FWfuq68ACJjfe9a2PdsPS7z\nUyuk1oP/wsZTf++yfH1DEBfgNri2ucn+zZ00h9DTMDw6vZVS9SJyTltM5rQk61uoOKLNUXCywOjn\nMtdQ44vAADbuzOTOpaUUlFVz5/SB3Hn+wKaeFI1F2DpIE5eEnlpja07D8KaP8xk3tP7cOjKpQ7VN\nvijz1C6N/naVi4yGjKm6hH9Znq6b5g0pg3VPl8pi12HnZblaWDi3BGht4h0aKTn/V22h1Z57t7cy\n3n7a34nIh8A7QGOuv1LqvYDM6nRi18faLjvoQv0+oSeERXinYUR19li2vDoqkU7AX975iujkcSy4\nfcqpiXeBLsLW1jSaJJoRGM35Lwz+kWolTRbuciEwWpDzMGim7lECPmgYtuM7U/dHdyZQjZOcaSwP\n4sLxXZanf4eeTGwdCG99GNFAEXA+cIn1uDhQkzptUEr7L/qd11QdNjxCZ257IzA85GBsyirh7o+0\nJnLFsFg+uWuq6yztsjxAOtadTrP29AAnbIUqyQP1HbOrz70lCWp2eK19DG/oNkw/7/vc9fq2+g3E\nN9OqtTS0kvbASw1DKXVLoCdyWpK/FUoOwdR7Tl7eNcN7geECpRRPr9zH35ftYUznOACuHRUHUW7u\nZMrytLAIP40S9DyROgQ2vwFVx0+tjlqe77nZj8F3ImP0b9fZd2QnSvpbFTapnxYUvvgwumbA0Ivh\n6yd0N0rn0NWyPF2oMtDEJlmtWl0IjLL8kAuq8ErDEJEXReQF50egJ9fu2fWxviMbMufk5V0zmhr3\nuKPEdQ5GfYPiwQ+38+iS3cwamcaLd8zSK5pL3msr9bwtsWtKFTo5PhuzvI2GERBcJU1Wl0HtiZb9\nxobP05p3nA8FQi/8MzTU6pbDjtRV67DrhDbw2dmtWl0KjNyO97/zgLcmqY+BT6zHZ0ACUB6oSZ02\n7PxY9zB2boXaNUOHwlYddz2uskTnbTgJjOq6en72xkZeXnOIH5/bn39dPY4uXbpqH0lz9aRK8zqO\nw9vGtqc7+zHsi5fxYQSG1CFaE6ivbVrWGuGj034Dd6z1zWme1B8m/wQ2vQY5G13Mp40u1gk9TjVJ\n1Vp9MkIoaQ+8Lz74rsPjNeBKoGN2wfOWon1wZLtWm51pjJQ65Hqsiwip0qpabn5hHYu25vPbOcO4\nf84wwsJE/8FiPBQg7IgF0LpmaFOAs3mkrS8WoUbqUJ1HdGx/07LWCKoIj3Ddu8QTU3+ptZIl9zcV\nMWzr/If49FOd3iHWac/G3zZkg4AO5GH1g10f6+ehLiqkeMrFcBIYR8qquPrZtaw7eIzHrhrDbec6\n1XRsrp5UXTWcONrxfrhh4VaPBieTVLkRGAHFMVLKJphCOjoBzn8AstbCtnet+bRxVGB8utYwHJt6\nhWDSHnjvwygTkVL7AXyE7pERuuz8SJefcFUcze6/4FFg9OXg0Qouf3o1B4sq+M9NE7hsnIts7dgk\n902U7N7WHVE1ThnsXsMwPboDQ4qLLPtgC+lx1+seJ58+qMuvt/XFOiFdF7usKmlaZjQM9yil4pVS\nCQ6PwUqpdwM9uXZLaR5kr4Ohl7heH91F51c0JzCiOrO+QPH9p1dTUV3P67dNZtoQN0qbm3pSjXOB\njvnDTR2qP6uaE03LjEkqsETFac3XWcPwJlEyUISFw6xHoDQbVv9Lm4fCIps6UgYa+7/l6MewBUZH\nvFFrBm81jMtEpIvD+0QRuTRw02rn7LYqtg9rJhWludDaksOUdkrj2v98S0J0BAtun8LY3onu99Vc\nxdqOfKeTOhhQTWWxQV+8/ClRYfAe50ip9uAjyzhbR1p99Zh2gMen+5517i8JLlq1luXpJl3Rzfxv\nOyDe+jAeVEo1hvwopUrQ/TFCk8Ld0KlLU80jV7gRGEopjmTvYV1JPOP6JPL+HWfTP9WDMzA2WUdk\nuOpA1pFtqY01pRz8GHYv79O9Im97JnWILvxXX6fftyQHozWZ+Sddu+3gl20rwFxpGHbSXoj9Dr0V\nGK62C3ARl3ZMZTHEdm3+x+KizHlVbT13vbmJTuU5RKdm8MoPz6RrXJTn49kFCB1tqDZluTrsNjbJ\n9/No77jq0dBeLl4dmdShusJsiRXl1x40DND/qSk/1a+DITAcczHK8jteKLsXeCsw1ovIP0RkgPX4\nB7AhkBNr17griOZI1wyddGSVOS8sq+aa59eycvNeusgJzhp/BlERXn78tq3WVaRUWQe+446I0rH4\njrkYHTFJsb2RapXlKNxlJUq2o65yU/+f9rGkjWq7Y0ZG69B2x0ZKIZi0B94LjJ8BNcBbwJtAFfDT\nQE2q3VNV4tl26RBam3XsBJc99TU780p5+mKd5Ce+tA2NsbQHV36M0tyOaY6yca4pZQRG4GnsR7JL\nJ5/WVbaf31inePjZRjj3V2173ASHVq12tYG2yDRvZ3gbJVWhlLpPKTVBKTVRKfUbpVSFp3EiMktE\ndotIpojc52L9NBE5LiKbrMfvvR0bVLzVMICS3L1c+5+1lFXV8faPp3B2ihXx44vAiG1GYHT0Mhmp\nQ3SSZF2NleVdYQRGoOkUDwm9tKBuj1Fp4ZFtr1HHO3Teqy5teamU0xRvo6Q+FZFEh/ddRWSphzHh\nwJPAbGA4cI2IDHex6ZdKqbHW408+jg0O3giMhF6osAg+Xrma4opaXr51EqN7JZ6Ug+E1tknKVS5G\nhxcYVo+GY/tNDkZbkjpEaxgdrXS+vySkNzm9O3Iouwe8NUmlWJFRACilivGc6T0JyFRK7VdK1aBN\nWfO8PF5LxgYWpXQtqJjmTVIl1Q3kkUrXqlxevGUiY+yw2ZLDEBnnm5O60YfhpGFUl0FNWceOBU9x\nMI+0x7vdjkrqUB2d1pHDtn3BbtVaXxvSn4m3AqNBRBptKCKSASi3W2t6AlkO77OtZc6cJSJbRGSx\niCANstYAABQeSURBVNhdW7wdi4jMF5H1IrK+sLDQw5RageoyfcfbjIZRVlXLTS98y/66FM5NrWBi\nhoNw8KIPxilExem6Ss4CoyOH1NqkDAZEt+u0s9qNwAg8qUO07yLrW/0+1Is9JqQDlu8iRJP2wPvQ\n2N8CX4nISkCAqcD8Vjj+RqCPUqpcROYAH6DrVHmNUuo54DmACRMmeBJiLaeyWD+7ERgnauq49aV1\nbM8tJWPYCOLzPj15g5JDkNjbt2OKuE7es6M2OvIFNCpWf16Fu3S/BjAXr7bAzoHZvwKi4v0rHNiR\ncAytNRpG8yillqCr0+4G3gDuASo9DMsBHK+MvaxljvstVUqVW68XAZEikuLN2KBh50K4iJKqqq1n\n/ssb2HComCeuHkevfsP0Rb6qtGkjN30wPBKbDCeKT17WqGF08GiNRvNIPkTEnNpQydD62JFSxQc6\n9g2JtzQm7+Xq32F0YtMNTAjhrdP7R+g+GPcAvwReAf7gYdg6YJCI9BORKOBq4EOn/aaJaNuMiEyy\n5lPkzdig4UbDUEpx/3tb+SrzKI9eMYa5o9ObQmvtBKiq41rg+CUwXGgYoeKQTBmsTVKluRDfvWPm\nnLQ3Yro2BRd09N+XNzSWB8nr+KHszeCtD+PnwETgkFJqOjAOcJF23IRSqg64E1gK7ATeVkptF5Hb\nReR2a7MrgG0ishn4J3C10rgc6+O5BQY3AuPZVft5/7sc7pk5mCvGWxVnbYFhd98rsdwyfmsYLgRG\np4SOby6wM4+zvg3ZP2pQsEudm89c///Co5o0jBAVot76MKqUUlUigoh0UkrtEpEhngZZZqZFTsue\ncXj9b+Df3o5tF1RactJBYHy2s4BHluzi4tHp3Hm+Q89i574YLhoneY2rJkrtpWRDoLEvXKXZ0Cu0\n+3a1KalD4cDK0PiNecKxVWtZXtNvMsTwVmBkW3kYHwCfikgx4KadXAenUcPQPow9BWX8/M1NjOzR\nhb9eMQZxNJfEJGpb5ykCw4ccDJvGAoT1utwzNBVA6+jYobVg7nbbkkYNIwR+Y94Q3wOO5xgNwxNK\nqcusl38QkS+ALsCSgM2qPVNZrMsaR8ZQXFHDj/67npiocJ67cTwxUeGnbu9YtbbksC7N7U8d/9hk\nQGk/iJ3DUZYPfc/y80ROI2IStT29PF/7MAxtQzerppQR0pqEdMj8TIfVh+hn4nPFWaXUykBM5LTB\nyvKurW/gJ69tIL+0ijfnTya9i5uIiaR+kLdFvy455HsOho1j8l5sklXPJkQ0DNB3u+UdPKu9vdF7\nMsx7CobMCfZM2gfxPXRZEAjZ36G/Pb1DF6vw4B8/2s7a/cd4+PujOKNPM2VCHMuc20l7/hBrHcP2\nY5wo0tVwQ6UAmm0eMTkYbUdYGIy7TldrNZycqBeCSXtgBIbvVJaQVxPNq2sP8+Pz+vP9M1z04Hak\nscx5rhYYXXxM2rNxLg8SKiG1NnYiWYLLhH+DIfA4ahUhqmGEbhMkP1BKcfRIPlvLEpg7Kp1fX9RM\nxz0bO1Iqf6v/ORhwak+MxgJoIaJhjLlG+zJSfCoEYDC0HrY2L2EQ56mUXsfEaBhe0tCg+ONHO6ip\nOEbX5G48cfVYwsO88EXYAuPgl/q5xQIjRDWMqFgYeblJ2jMED1uriOsG4aF5r20EhhfU1jdwzzub\neWn1QVLDTzBhaH8iwr386BJ66Taj+61YAX9CakFHVzkWIGws9W1s+gZDmxBvMt+NwPBAVW09P3l1\nI+9/l8OvZ/QjqqESifXQC8OR8AhdPO+Ilajur4YhYmV7WyapslyIS9VtTA0GQ+CJjNEJu6ESaOKC\n0NSrvKS8uo4f/Xcda/cf40/zRnDjqFj4Cs/tWZ2xczEiYiAuxf8JxSY3NVEK4eQhgyFonH13SPvR\njMBohl++vZl1B4t5/KqxXDquZ1NvaU/d9pyx/Rj+5mDYOBYgLM0NHYe3wdBeOOfuYM8gqBiTlBuK\nK2pYvrOAH57TTwsL8NgLwy2OAqMlOBYgNBqGwWBoY4zAcMOyHfnUNSi+N8bhLr6x8KCvJql++rnF\nAsPSMOprdbvIEI0FNxgMwcEIDDd8vCWPvsmxjOiR0LSwxRqGn0l7NrHJWmiV5QEqZLNNDQZDcDAC\nwwXHKmpYva+IOaPST64+66/ASB0Kw74Hgy5q2cTsAoRHdur3RsMwGAxtiHF6u2Dp9nzqGxRzRzld\nkCuLAYFOPrYIjYyGq15p+cTs5L38rfrZCAyDwdCGGA2joQE2vqK7uVl8siWPDGdzFFiFB7voomzB\nwNZsCqycDiMwDAZDG2IERlgYLLkPtr0HQFF5Nav3HWXuaCdzFDSWNg8atoZxZAeERfrXV8NgMBj8\nxAgM0HfqZbkALN1eQIOCuaNc5DhUFvseIdWa2ALi6F4dUhssTcdgMIQkAb3iiMgsEdktIpkicl8z\n200UkToRucJh2UER2Soim0RkfSDnSUJ6Y22mT7bm0j8ljmHp8aduV1nSPjQMVW9yMAwGQ5sTMIEh\nIuHAk8BsYDhwjYgMd7PdI8AyF7uZrpQaq5SaEKh5AlrDKM3jaHk1a/YVuTZHQfBNUlGxuj0sGP+F\nwWBocwKpYUwCMpVS+5VSNcCbwDwX2/0MeBc4EsC5NE98OpTlsWRrrjZHjXZzMQ62wIAmLcMIDIPB\n0MYEUmD0BLIc3mdbyxoRkZ7AZcDTLsYrYLmIbBCR+e4OIiLzRWS9iKwvLCz0b6YJPaChllWbdjMg\nNY4h3V2YoxoaGtuzBpXYJP1skvYMBkMbE2yv6ePAvUqpBhfrzlFKjUWbtH4qIue62oFS6jml1ASl\n1ITU1FT/ZmH5A3Ky9jF3dA/X5qiaMlANRsMwGAwhSyAT93IAx1oYvaxljkwA3rQu0CnAHBGpU0p9\noJTKAVBKHRGR99EmrlUBmalV9bUbxVzcnDkKgi8wYiwNwzi9DQZDGxNIDWMdMEhE+olIFHA18KHj\nBkqpfkqpDKVUBrAAuEMp9YGIxIlIPICIxAEXAtsCNlPLvDM6oYLBrsxR4CAwgm2SsjUMU9rcYDC0\nLQHTMJRSdSJyJ7AUCAdeUEptF5HbrfXPNDO8O/C+pXlEAK8rpZYEaq5HGrqQooQpqbXuN2qsVBtk\nDSPOMrsZDcNgMLQxAa0lpZRaBCxyWuZSUCilbnZ4vR8YE8i5ObJ4ZxFzSGBY53L3G7UXk9T4myB1\nMEQneN7WYDAYWpFgO73bBZ9syaMkPIUutUfdb2QLjGBHScWnwYjLgjsHg8EQkoS8wDhRU8eBogrC\nu/RozPZ2SZWfzZMMBoOhgxDyAiM2KoK1919A74wBjfWkXFJZDBExEBnTdpMzGAyGdkTICwyA8DAh\nMrGnbn9aV+16o2AXHjQYDIYgYwSGjZ0I584sFezCgwaDwRBkjMCwaRQYea7XG4FhMBhCHCMwbOza\nTKVu/BiVxcGPkDIYDIYgYgSGjSeTVJXRMAwGQ2hjBIZNTFcI7+Q+Uso4vQ0GQ4hjBIaNiDZLlbrw\nYdRWQe0JIzAMBkNIYwSGI/Fukveq2kkdKYPBYAgiRmA4Ep/m2iTVXgoPGgwGQxAxAsORhB7aJKXU\nycvbSx0pg8FgCCJGYDgSnw51lVB1/OTl7aVSrcFgMAQRIzAcsXtMOCfvGR+GwWAwGIFxEglWFzvn\n5D2jYRgMBoMRGCfhrjxIZTEg0Mk0LTIYDKGLERiOuDNJVf7/9u43xo6qjOP497e7pZR2V9QuZWkp\ntEIC1WCRFYlUAhhIQbQQUf4Iom8aEjCQSKQYjRFDjG+UN0QgSqihUKtQbAiRP5WgvFC6xQotlNhU\nlJbKVinUklrY9vHFnNsO6112tr3Tu535fZJmZ8786Xk22X12ztxznjezORgd/naZWX35N2DehEnZ\nsNPwyXs7t3k4ysxqr9SEIWm+pJclbZC06H3O+6SkIUmXjvXalus+pvmQlD9Sa2Y1V1rCkNQJ3AFc\nAMwBrpA0Z4TzfgQ8PtZrS9HT1zxh+AnDzGquzCeM04ENEbExIt4BlgILmpz3DeBBYHA/rm297qP/\nf0jKK9WamZWaMKYDr+b2N6W2vSRNBy4BfjrWa3P3WChpQNLA1q1bD7jTdB8Dbw/C7qF9bV6p1sys\n7S+9bwdujog9+3uDiLg7Ivojor+3t/fAe9TTB7EnSxoAe/a42p6ZGdBV4r03A8fm9mektrx+YKkk\ngKnAhZKGCl5bjsZcjO1bsol8u7YD4YRhZrVXZsJYBZwoaRbZL/vLgSvzJ0TErMa2pHuBRyLiYUld\no11bmr2T914DTvPCg2ZmSWkJIyKGJF0PPAZ0AvdExDpJ16bjd4712rL6+h6N5UEadTG8LIiZGVDu\nEwYR8Sjw6LC2pokiIr422rUHxRFToaNr33pSXnjQzAxo/0vv8aejA6YcvW8uxt4nDA9JmVm9OWE0\nk5+85yEpMzPACaO5/OS9RnlWv/Q2s5pzwmgmv57Uzm3QNQkmHN7ePpmZtZkTRjM9fdn8i107PGnP\nzCxxwmimO/fRWi88aGYGOGE0t7eQ0mteeNDMLHHCaGZvbe8tXnjQzCxxwmgmX6rVCcPMDHDCaG5i\nNxzWnRKGh6TMzMAJY2Q9ffDG32Bop+dgmJnhhDGy7j4YfCnb9hOGmZkTxoi6++Ctf2TbThhmZk4Y\nI+rp27ftl95mZk4YI2pM3gM/YZiZ4YQxssZHa8EJw8wMJ4yR9eSeMPwpKTMzJ4wRNWp7qwMm9rS3\nL2Zm44ATxkimTAOUPV10+NtkZlbqb0JJ8yW9LGmDpEVNji+Q9LykNZIGJM3LHXtF0guNY2X2s6nO\nLphylD8hZWaWdJV1Y0mdwB3AecAmYJWkFRHxYu60lcCKiAhJpwDLgJNyx8+JiH+V1cdRdfdlQ1Jm\nZlZewgBOBzZExEYASUuBBcDehBERO3LnTwaixP6M3Vk3tbsHZmbjRpkJYzrwam5/E/Cp4SdJugT4\nIXAU8LncoQCelLQbuCsi7m72n0haCCwEmDlzZmt63nDy51t7PzOzQ1jbx1siYnlEnARcDPwgd2he\nRMwFLgCuk3TWCNffHRH9EdHf29t7EHpsZlZPZSaMzcCxuf0Zqa2piPg9MFvS1LS/OX0dBJaTDXGZ\nmVmblJkwVgEnSpol6TDgcmBF/gRJJ0hS2v4EMBH4t6TJkrpT+2TgfGBtiX01M7NRlPYOIyKGJF0P\nPAZ0AvdExDpJ16bjdwJfBL4q6V1gJ3BZ+sTUNGB5yiVdwP0R8duy+mpmZqNTxPj6YNKB6O/vj4GB\ngz9lw8zsUCVpdUT0Fzm37S+9zczs0OCEYWZmhThhmJlZIZV6hyFpK/D3/bx8KtC+ZUjax3HXi+Ou\nlyJxHxcRhSaxVSphHAhJA0Vf/FSJ464Xx10vrY7bQ1JmZlaIE4aZmRXihLFP08UNa8Bx14vjrpeW\nxu13GGZmVoifMMzMrBAnDDMzK6T2CWO0uuNVIukeSYOS1ubaPiTpCUl/TV8/2M4+tpqkYyU9JelF\nSesk3ZDaqx734ZKelfSXFPf3U3ul426Q1Cnpz5IeSft1ifsVSS9IWiNpILW1LPZaJ4xc3fELgDnA\nFZLmtLdXpboXmD+sbRGwMiJOJKuxXrWkOQR8MyLmAGeQFeOaQ/Xj3gWcGxEfB+YC8yWdQfXjbrgB\neCm3X5e4Ac6JiLm5+Rcti73WCYNc3fGIeAdo1B2vpFSk6o1hzQuAxWl7MVnlw8qIiC0R8Vza/g/Z\nL5HpVD/uiIgdaXdC+hdUPG4ASTPIyj3/LNdc+bjfR8tir3vCaFZ3fHqb+tIu0yJiS9r+JzCtnZ0p\nk6TjgVOBP1GDuNOwzBpgEHgiImoRN3A78C1gT66tDnFD9kfBk5JWS1qY2loWe2kFlOzQk4pXVfJz\n1pKmAA8CN0bE9lScC6hu3BGxG5gr6UiygmQfG3a8cnFLuggYjIjVks5udk4V486ZFxGbJR0FPCFp\nff7ggcZe9yeMMdUdr6jXJfUBpK+Dbe5Py0maQJYslkTEQ6m58nE3RMSbwFNk76+qHveZwBckvUI2\nxHyupPuoftwARMTm9HUQWE427N6y2OueMEatO14DK4Br0vY1wG/a2JeWSzXjfw68FBE/zh2qety9\n6ckCSZOA84D1VDzuiLglImZExPFkP8+/i4irqHjcAJImS+pubAPnA2tpYey1n+kt6UKyMc9G3fHb\n2tyl0kh6ADibbMnj14HvAQ8Dy4CZZEvDfzkihr8YP2RJmgf8AXiBfWPa3yZ7j1HluE8he8HZSfaH\n4bKIuFXSh6lw3HlpSOqmiLioDnFLmk32VAHZ64b7I+K2VsZe+4RhZmbF1H1IyszMCnLCMDOzQpww\nzMysECcMMzMrxAnDzMwKccIwGwcknd1YWdVsvHLCMDOzQpwwzMZA0lWpzsQaSXelBf52SPpJqjux\nUlJvOneupD9Kel7S8kYdAkknSHoy1ap4TtJH0u2nSPq1pPWSlii/4JXZOOCEYVaQpJOBy4AzI2Iu\nsBv4CjAZGIiIjwJPk82gB/gFcHNEnEI207zRvgS4I9Wq+DTQWEn0VOBGstoss8nWRTIbN7xarVlx\nnwVOA1alP/4nkS3ktgf4ZTrnPuAhSR8AjoyIp1P7YuBXaa2f6RGxHCAi/guQ7vdsRGxK+2uA44Fn\nyg/LrBgnDLPiBCyOiFve0yh9d9h5+7vezq7c9m7882njjIekzIpbCVyaag00aiUfR/ZzdGk650rg\nmYh4C9gm6TOp/Wrg6VT1b5Oki9M9Jko64qBGYbaf/BeMWUER8aKk7wCPS+oA3gWuA94GTk/HBsne\nc0C2lPSdKSFsBL6e2q8G7pJ0a7rHlw5iGGb7zavVmh0gSTsiYkq7+2FWNg9JmZlZIX7CMDOzQvyE\nYWZmhThhmJlZIU4YZmZWiBOGmZkV4oRhZmaF/A/IBtrvcRkmQwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efb33c27be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# summarize history for accuracy  \n",
    "plt.plot(train_accuracy_list)  \n",
    "plt.plot(test_accuracy_list)  \n",
    "plt.title('model train/test accuracy')  \n",
    "plt.ylabel('accuracy')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'test'], loc='upper left')  \n",
    "plt.savefig('train_test_accuracy.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efb3073a240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for loss  \n",
    "plt.plot(train_loss_list)  \n",
    "plt.plot(test_loss_list)  \n",
    "plt.title('model train/test loss')  \n",
    "plt.ylabel('loss')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'test'], loc='upper left')  \n",
    "plt.savefig('train_test_loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_3.5]",
   "language": "python",
   "name": "conda-env-tensorflow_3.5-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
