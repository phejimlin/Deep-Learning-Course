{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from tensorflow.contrib import learn\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data_and_labels(train_data_file, test_data_file):\n",
    "    \"\"\"\n",
    "    Loads MR polarity data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "    # Load data from files\n",
    "    train_data =  pd.read_csv(train_data_file, sep='\\t', quoting=csv.QUOTE_NONE, header=None, names=['id', 'language', 'label', 'text'])\n",
    "    test_data = pd.read_csv(test_data_file, sep='\\t', quoting=csv.QUOTE_NONE, header=None, names=['id', 'language', 'label', 'text'])\n",
    "    \n",
    "    x_train = train_data['text'].tolist()\n",
    "    y_train = train_data['label'].tolist()\n",
    "\n",
    "    x_test = train_data['text'].tolist()\n",
    "    y_test = train_data['label'].tolist()\n",
    "    \n",
    "    x_train = [s.strip() for s in x_train]\n",
    "    x_test = [s.strip() for s in x_test]\n",
    "    \n",
    "    y_train_encoding = []\n",
    "    for label in y_train:\n",
    "        if label=='neutral':\n",
    "            y_train_encoding.append([0, 1, 0])\n",
    "        elif label=='positive':\n",
    "            y_train_encoding.append([0, 0, 1])\n",
    "        elif label=='negative':\n",
    "            y_train_encoding.append([1, 0, 0])\n",
    "    \n",
    "    y_test_encoding = []\n",
    "    for label in y_test:\n",
    "        if label=='neutral':\n",
    "            y_train_encoding.append([0, 1, 0])\n",
    "        elif label=='positive':\n",
    "            y_train_encoding.append([0, 0, 1])\n",
    "        elif label=='negative':\n",
    "            y_train_encoding.append([1, 0, 0])\n",
    "    \n",
    "    return [x_train, y_train_encoding, x_test, y_test_encoding]\n",
    "\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_embeddings = np.load('./data/embed_tweets_en_200M_200D/embedding_matrix.npy')\n",
    "word_dict = {}\n",
    "with open('./data/embed_tweets_en_200M_200D/vocabulary.pickle', 'rb') as myfile:\n",
    "    word_dict = pickle.load(myfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load label data\n",
    "x_train_sentence, y_train, x_test_sentence, y_test = load_data_and_labels('./data/supervised_data/en_train.tsv', './data/supervised_data/en_test.tsv')\n",
    "\n",
    "max_document_length = max([len(x.split(\" \")) for x in x_train_sentence])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer()\n",
    "x_train = []\n",
    "for tweet in x_train_sentence:\n",
    "    temp = np.zeros(max_document_length, dtype=np.int).tolist()\n",
    "    for index, word in enumerate(tknzr.tokenize(tweet)):\n",
    "        word = word.lower()\n",
    "        if word in word_dict:\n",
    "            temp[index] = word_dict[word][0]\n",
    "    x_train.append(temp)\n",
    "x_train = np.array(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put Word2Vec on 590 million English Tweets using 52 dimensions.\n",
    "vocabulary_size = 1859185\n",
    "embedding_size = 200  # Dimension of the embedding vector.\n",
    "graph = tf.Graph()\n",
    "\n",
    "sequence_length=x_train.shape[1]\n",
    "num_classes=3\n",
    "\n",
    "first_filter_window_size = 4\n",
    "second_filter_window_size = 3\n",
    "num_filters = 200\n",
    "\n",
    "l2_reg_lambda=0.0\n",
    "\n",
    "with graph.as_default():\n",
    "    input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "    input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "    \n",
    "    embeddings = tf.Variable(tf.constant(0.0, shape=[vocabulary_size, embedding_size]),\n",
    "                    trainable=False, name=\"W\")\n",
    "\n",
    "    embedding_placeholder = tf.placeholder(tf.float32, [vocabulary_size, embedding_size])\n",
    "    embedding_init = embeddings.assign(embedding_placeholder)  # assign exist word embeddings\n",
    "\n",
    "    embedded_chars = tf.nn.embedding_lookup(embeddings, input_x)\n",
    "    embedded_chars_expanded = tf.expand_dims(embedded_chars, -1)\n",
    "\n",
    "    # Keeping track of l2 regularization loss (optional)\n",
    "    l2_loss = tf.constant(0.0)\n",
    "    \n",
    "     # Add variable initializer.\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    \n",
    "    # Create first cnn : a convolution + maxpool layer for each filter size    \n",
    "    # Convolution Layer\n",
    "    filter_shape = [first_filter_window_size, embedding_size, 1, num_filters]\n",
    "    filter_shape = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "    \n",
    "    b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "    \n",
    "    conv = tf.nn.conv2d(\n",
    "        embedded_chars_expanded,\n",
    "        filter_shape,\n",
    "        strides=[1, 1, 1, 1],  # strides size change will cause cnn result different\n",
    "        padding=\"VALID\",\n",
    "        name=\"conv\")\n",
    "    # Apply nonlinearity\n",
    "    h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "    # Maxpooling over the outputs\n",
    "    pooled = tf.nn.max_pool(\n",
    "        h,\n",
    "        ksize=[1, sequence_length - first_filter_window_size + 1, 1, 1],  \n",
    "        strides=[1, 2, 2, 1],\n",
    "        padding='VALID',\n",
    "        name=\"pool\")\n",
    "    \n",
    "#     # Create second cnn : a convolution + maxpool layer for each filter size\n",
    "#     pooled_outputs = []\n",
    "#     # Convolution Layer\n",
    "#     filter_shape = [second_filter_window_size, embedding_size, 1, num_filters]\n",
    "#     filter_shape = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "#     b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "#     conv = tf.nn.conv2d(\n",
    "#         self.embedded_chars_expanded,\n",
    "#         filter_shape,\n",
    "#         strides=[1, 1, 1, 1],  # strides size change will cause cnn result different\n",
    "#         padding=\"VALID\",\n",
    "#         name=\"conv\")\n",
    "#     # Apply nonlinearity\n",
    "#     h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "#     # Maxpooling over the outputs\n",
    "#     pooled = tf.nn.max_pool(\n",
    "#         h,\n",
    "#         ksize=[1, sequence_length - second_filter_window_size + 1, 1, 1],  \n",
    "#         strides=[1, 1, 1, 1],\n",
    "#         padding='VALID',\n",
    "#         name=\"pool\")\n",
    "#     pooled_outputs.append(pooled)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "     # Combine all the pooled features\n",
    "#     num_filters_total = num_filters * len(filter_sizes)\n",
    "    h_pool_flat = tf.reshape(pooled, [-1, num_filters])  # flatten pooling layers\n",
    "\n",
    "    # Add dropout\n",
    "#     with tf.name_scope(\"dropout\"):\n",
    "#         self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "    # Final (unnormalized) scores and predictions\n",
    "    with tf.name_scope(\"output\"):\n",
    "        W = tf.get_variable(\n",
    "            \"W\",\n",
    "            shape=[num_filters, num_classes],\n",
    "            initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "        l2_loss += tf.nn.l2_loss(W)\n",
    "        l2_loss += tf.nn.l2_loss(b)\n",
    "#         scores = tf.nn.xw_plus_b(h_drop, W, b, name=\"scores\")\n",
    "        scores = tf.add(W, b, name=\"scores\")\n",
    "        predictions = tf.argmax(scores, 1, name=\"predictions\")\n",
    "\n",
    "    # Calculate mean cross-entropy loss\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        losses = tf.nn.softmax_cross_entropy_with_logits(logits=scores, labels=input_y)\n",
    "        loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "    # Accuracy\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        correct_predictions = tf.equal(predictions, tf.argmax(input_y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Incompatible shapes: [200] vs. [32]\n\t [[Node: accuracy/Equal = Equal[T=DT_INT64, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](output/predictions, accuracy/ArgMax)]]\n\t [[Node: accuracy/accuracy/_7 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_87_accuracy/accuracy\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'accuracy/Equal', defined at:\n  File \"/home/phejimlin/anaconda3/envs/tensorflow_3.5/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/phejimlin/anaconda3/envs/tensorflow_3.5/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/phejimlin/anaconda3/envs/tensorflow_3.5/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/phejimlin/anaconda3/envs/tensorflow_3.5/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/phejimlin/anaconda3/envs/tensorflow_3.5/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/phejimlin/anaconda3/envs/tensorflow_3.5/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/phejimlin/anaconda3/envs/tensorflow_3.5/lib/python3.5/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/phejimlin/anaconda3/envs/tensorflow_3.5/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/phejimlin/anaconda3/envs/tensorflow_3.5/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/phejimlin/anaconda3/envs/tensorflow_3.5/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/phejimlin/anaconda3/envs/tensorflow_3.5/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/phejimlin/anaconda3/envs/tensorflow_3.5/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/phejimlin/anaconda3/envs/tensorflow_3.5/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/phejimlin/anaconda3/envs/tensorflow_3.5/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/phejimlin/anaconda3/envs/tensorflow_3.5/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/phejimlin/anaconda3/envs/tensorflow_3.5/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/phejimlin/anaconda3/envs/tensorflow_3.5/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/phejimlin/anaconda3/envs/tensorflow_3.5/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/phejimlin/anaconda3/envs/tensorflow_3.5/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/phejimlin/anaconda3/envs/tensorflow_3.5/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-6-0c6ae773a1d8>\", line 112, in <module>\n    correct_predictions = tf.equal(predictions, tf.argmax(input_y, 1))\n  File \"/home/phejimlin/.local/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 753, in equal\n    result = _op_def_lib.apply_op(\"Equal\", x=x, y=y, name=name)\n  File \"/home/phejimlin/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/home/phejimlin/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/phejimlin/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Incompatible shapes: [200] vs. [32]\n\t [[Node: accuracy/Equal = Equal[T=DT_INT64, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](output/predictions, accuracy/ArgMax)]]\n\t [[Node: accuracy/accuracy/_7 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_87_accuracy/accuracy\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_3.5/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [200] vs. [32]\n\t [[Node: accuracy/Equal = Equal[T=DT_INT64, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](output/predictions, accuracy/ArgMax)]]\n\t [[Node: accuracy/accuracy/_7 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_87_accuracy/accuracy\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d3ecf7692aed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;31m#             current_step = tf.train.global_step(sess, global_step)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-d3ecf7692aed>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(x_batch, y_batch)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;31m#             train_summary_writer.add_summary(summaries, step)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mloss_log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_log\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [200] vs. [32]\n\t [[Node: accuracy/Equal = Equal[T=DT_INT64, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](output/predictions, accuracy/ArgMax)]]\n\t [[Node: accuracy/accuracy/_7 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_87_accuracy/accuracy\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'accuracy/Equal', defined at:\n  File \"/home/phejimlin/anaconda3/envs/tensorflow_3.5/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/phejimlin/anaconda3/envs/tensorflow_3.5/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/phejimlin/anaconda3/envs/tensorflow_3.5/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/phejimlin/anaconda3/envs/tensorflow_3.5/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/phejimlin/anaconda3/envs/tensorflow_3.5/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/phejimlin/anaconda3/envs/tensorflow_3.5/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/phejimlin/anaconda3/envs/tensorflow_3.5/lib/python3.5/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/phejimlin/anaconda3/envs/tensorflow_3.5/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/phejimlin/anaconda3/envs/tensorflow_3.5/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/phejimlin/anaconda3/envs/tensorflow_3.5/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/phejimlin/anaconda3/envs/tensorflow_3.5/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/phejimlin/anaconda3/envs/tensorflow_3.5/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/phejimlin/anaconda3/envs/tensorflow_3.5/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/phejimlin/anaconda3/envs/tensorflow_3.5/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/phejimlin/anaconda3/envs/tensorflow_3.5/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/phejimlin/anaconda3/envs/tensorflow_3.5/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/phejimlin/anaconda3/envs/tensorflow_3.5/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/phejimlin/anaconda3/envs/tensorflow_3.5/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/phejimlin/anaconda3/envs/tensorflow_3.5/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/phejimlin/anaconda3/envs/tensorflow_3.5/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-6-0c6ae773a1d8>\", line 112, in <module>\n    correct_predictions = tf.equal(predictions, tf.argmax(input_y, 1))\n  File \"/home/phejimlin/.local/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 753, in equal\n    result = _op_def_lib.apply_op(\"Equal\", x=x, y=y, name=name)\n  File \"/home/phejimlin/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/home/phejimlin/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/phejimlin/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Incompatible shapes: [200] vs. [32]\n\t [[Node: accuracy/Equal = Equal[T=DT_INT64, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](output/predictions, accuracy/ArgMax)]]\n\t [[Node: accuracy/accuracy/_7 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_87_accuracy/accuracy\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "num_checkpoints = 5\n",
    "num_epochs = 200\n",
    "evaluate_every = 100\n",
    "allow_soft_placement=True\n",
    "log_device_placement=False\n",
    "\n",
    "with graph.as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=allow_soft_placement,\n",
    "      log_device_placement=log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        \n",
    "#         # Define Training procedure\n",
    "#         global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "#         optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "#         grads_and_vars = optimizer.compute_gradients(loss)\n",
    "#         train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "#         # Keep track of gradient values and sparsity (optional)\n",
    "#         grad_summaries = []\n",
    "#         for g, v in grads_and_vars:\n",
    "#             if g is not None:\n",
    "#                 grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "#                 sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "#                 grad_summaries.append(grad_hist_summary)\n",
    "#                 grad_summaries.append(sparsity_summary)\n",
    "#         grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "#         # Output directory for models and summaries\n",
    "#         timestamp = str(int(time.time()))\n",
    "#         out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "#         print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "#         # Summaries for loss and accuracy\n",
    "#         loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "#         acc_summary = tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "#         # Train Summaries\n",
    "#         train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "#         train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "#         train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "#         # Dev summaries\n",
    "#         dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "#         dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "#         dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "#         # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "#         checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "#         checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "#         if not os.path.exists(checkpoint_dir):\n",
    "#             os.makedirs(checkpoint_dir)\n",
    "#         saver = tf.train.Saver(tf.global_variables(), max_to_keep=num_checkpoints)\n",
    "\n",
    "#         # Write vocabulary\n",
    "#         vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              input_x: x_batch,\n",
    "              input_y: y_batch,\n",
    "            }\n",
    "#             _, step, summaries, loss, accuracy = sess.run(\n",
    "#                 [train_op, global_step, train_summary_op, loss, accuracy],\n",
    "#                 feed_dict)\n",
    "#             time_str = datetime.datetime.now().isoformat()\n",
    "#             print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "#             train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "            loss_log, accuracy_log = sess.run([loss, accuracy], feed_dict)\n",
    "            print(loss_log, accuracy_log)\n",
    "\n",
    "#         def dev_step(x_batch, y_batch, writer=None):\n",
    "#             \"\"\"\n",
    "#             Evaluates model on a dev set\n",
    "#             \"\"\"\n",
    "#             feed_dict = {\n",
    "#               input_x: x_batch,\n",
    "#               input_y: y_batch,\n",
    "#             }\n",
    "#             step, summaries, loss, accuracy = sess.run(\n",
    "#                 [global_step, dev_summary_op, loss, accuracy],\n",
    "#                 feed_dict)\n",
    "#             time_str = datetime.datetime.now().isoformat()\n",
    "#             print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "#             if writer:\n",
    "#                 writer.add_summary(summaries, step)\n",
    "\n",
    "        \n",
    "        sess.run(embedding_init, feed_dict={embedding_placeholder: final_embeddings})\n",
    "        # Generate batches\n",
    "        batches = batch_iter(\n",
    "            list(zip(x_train, y_train)), batch_size, num_epochs)\n",
    "        \n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            \n",
    "#             current_step = tf.train.global_step(sess, global_step)\n",
    "#             if current_step % evaluate_every == 0:\n",
    "#                 print(\"\\nEvaluation:\")\n",
    "#                 dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "#                 print(\"\")\n",
    "#             if current_step % checkpoint_every == 0:\n",
    "#                 path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "#                 print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_3.5]",
   "language": "python",
   "name": "conda-env-tensorflow_3.5-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
