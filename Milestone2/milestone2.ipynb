{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "\n",
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from tensorflow.contrib import learn\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.metrics import f1_score\n",
    "from IPython.display import clear_output, Image, display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### Do not modify here ###### \n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = \"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = graph_def\n",
    "    #strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))\n",
    "###### Do not modify  here ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data_and_labels(train_data_file, test_data_file):\n",
    "    \"\"\"\n",
    "    Loads MR polarity data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "    # Load data from files\n",
    "    train_data = pd.read_csv(train_data_file, sep='\\t', quoting=csv.QUOTE_NONE, header=None, names=['id', 'language', 'label', 'text'])\n",
    "    test_data = pd.read_csv(test_data_file, sep='\\t', quoting=csv.QUOTE_NONE, header=None, names=['id', 'language', 'label', 'text'])\n",
    "    \n",
    "    x_train = train_data['text'].tolist()\n",
    "    y_train = train_data['label'].tolist()\n",
    "\n",
    "    x_test = test_data['text'].tolist()\n",
    "    y_test = test_data['label'].tolist()\n",
    "    \n",
    "    x_train = [s.strip() for s in x_train]\n",
    "    x_test = [s.strip() for s in x_test]\n",
    "    \n",
    "    label_encoding = {'neutral':0, 'positive':1, 'negative':2}\n",
    "    \n",
    "    y_train_encoding = [label_encoding[label] for label in y_train]    \n",
    "    y_test_encoding = [label_encoding[label] for label in y_test]\n",
    "\n",
    "    \n",
    "    return [x_train, y_train_encoding, x_test, y_test_encoding]\n",
    "\n",
    "def transform_data_and_labels(data):\n",
    "    \"\"\"\n",
    "    Loads MR polarity data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    x = np.array(data['text'].tolist())\n",
    "    y = data['label'].tolist()\n",
    "    \n",
    "    # encoding label\n",
    "    label_encoding = {'neutral':0, 'positive':1, 'negative':2}\n",
    "    y = [label_encoding[label] for label in y]    \n",
    "    \n",
    "    \n",
    "    y = np.array(y)\n",
    "    \n",
    "    # Randomly shuffle data\n",
    "    np.random.seed(10)\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "    x_shuffled = x[shuffle_indices]\n",
    "    y_shuffled = y[shuffle_indices]\n",
    "\n",
    "    # Split train/test set\n",
    "    # maybe we can use cross-validation to improve\n",
    "    dev_sample_index = -1 * int(0.1 * float(len(y)))\n",
    "    x_train, x_test = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "    y_train, y_test = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "    print(\"Train/Test split: {:d}/{:d}\".format(len(y_train), len(y_test)))\n",
    "    \n",
    "    return [x_train, y_train, x_test, y_test]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    This function assumes that the last word in the word embedding is a zero vector, and will use it as padding.\n",
    "    The input 'num_voc' equals to the shape[0] of the word embedding.\n",
    "\"\"\"\n",
    "def process_tweet(train_tweets, test_tweets, num_voc):\n",
    "    # max_document_length = max([len(x.split(\" \")) for x in x_train_sentence])\n",
    "    ppl_re = re.compile(r'@\\S*')\n",
    "    url_re = re.compile(r'http\\S+')\n",
    "    tknzr = TweetTokenizer()\n",
    "    # tknzr = TweetTokenizer(reduce_len=True)\n",
    "    \n",
    "    tokenized_tweets_all = []\n",
    "    max_document_length = 0\n",
    "    \n",
    "    for tweets in [train_tweets, test_tweets]:\n",
    "        tweets = [url_re.sub('URLTOK', ppl_re.sub('USRTOK', tweet.lower())) for tweet in tweets]\n",
    "        tokenized_tweets = [tknzr.tokenize(tweet) for tweet in tweets]\n",
    "        tokenized_tweets_all.append(tokenized_tweets)\n",
    "        max_document_length = max(max_document_length, max([len(tweet) for tweet in tokenized_tweets]))\n",
    "    print(max_document_length)\n",
    "    \n",
    "    x = []\n",
    "    \n",
    "    for tokenized_tweets in tokenized_tweets_all:\n",
    "        x_curr = []\n",
    "        for tokenized_tweet in tokenized_tweets:\n",
    "            if len(tokenized_tweet) == max_document_length:\n",
    "                print(tokenized_tweet)\n",
    "            \"\"\"Not sure if original paper does this, but since index 0 means USRTOK, padding should be a number\n",
    "            higher than total word count, so tf.nn.embedding_lookup will return a tensor of 0 insted of USRTOK.\"\"\"\n",
    "        #     temp = np.zeros(max_document_length, dtype=np.int).tolist()\n",
    "            temp = (np.ones(max_document_length, dtype=np.int)*(num_voc-1)).tolist()\n",
    "\n",
    "            for index, word in enumerate(tokenized_tweet):\n",
    "                if word in word_dict:\n",
    "#                     temp[index] = word_dict[word][0]\n",
    "                    temp[index] = word_dict[word]\n",
    "            x_curr.append(temp)\n",
    "        x_curr = np.array(x_curr)\n",
    "        x.append(x_curr)\n",
    "    \n",
    "    return x[0], x[1]\n",
    "\n",
    "\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Current epoch: \", epoch)\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pre-train word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# final_embeddings = np.load('./data/embed_tweets_en_200M_200D/embedding_matrix.npy')\n",
    "# word_dict = {}\n",
    "# with open('./data/embed_tweets_en_200M_200D/vocabulary.pickle', 'rb') as myfile:\n",
    "#     word_dict = pickle.load(myfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_embeddings = np.load('./data/embed_tweets_en_590M_52D_data/en_word2vec_52.npy')\n",
    "word_dict = {}\n",
    "with open('./data/embed_tweets_en_590M_52D_data/vocabulary_dict_52.pickle', 'rb') as myfile:\n",
    "    word_dict = pickle.load(myfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76.44 94\n"
     ]
    }
   ],
   "source": [
    "# shit\n",
    "for key, val in word_dict.items():\n",
    "    if val == 94:\n",
    "        print(key, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'@lauren_hp': 3568330,\n",
       " 'hadcore': 1145459,\n",
       " '@oobeyme__': 2899054,\n",
       " '@literallylayna': 5024063,\n",
       " '@daisbond_': 372284,\n",
       " '@that_mann_mike': 3085518,\n",
       " '@liveasy10': 1524849,\n",
       " '@bikinimowing': 3013104,\n",
       " '@kianaparrr': 4325737,\n",
       " '@duhhasian18': 3054969,\n",
       " '#nayariveraontheellenshow': 203198,\n",
       " '@niditriyant': 3489693,\n",
       " '@solene_ldn': 4085061,\n",
       " '@moonmunch': 3419030,\n",
       " '@logical_saario': 4210626,\n",
       " '@tylermcflurry': 382946,\n",
       " '@ashleigh_kwele_': 3124617,\n",
       " '@joecrizzydchill': 1304721,\n",
       " '@josidschultz': 1920915,\n",
       " '#koanteykoti': 2494613,\n",
       " '@ms_baum': 3784082,\n",
       " '@mikejacques1': 1809177,\n",
       " '@kidlyznick1020': 1434302,\n",
       " '@cheyannemarieee': 5967408,\n",
       " '@yesigalano': 4104663,\n",
       " '@nabillasaurs': 3283274,\n",
       " '@shaylamarieee_': 3285170,\n",
       " '@dookie92': 2101334,\n",
       " \"buzz'n\": 667825,\n",
       " '@rockingstep': 830543,\n",
       " '@weerapreya': 3941575,\n",
       " '@le0nthedon': 5789832,\n",
       " 'it.lots': 6085389,\n",
       " '@marcussskwan': 3511959,\n",
       " '@seto_balyan': 1060388,\n",
       " '@okikawasa': 3140443,\n",
       " '@xalisonskiba': 212137,\n",
       " '#kidrauhlliveson': 3104926,\n",
       " '@ohmymuslimmalik': 4988244,\n",
       " '@drewdouglasx': 1291153,\n",
       " '@minutemaid_us': 52841,\n",
       " '@alii_huberr': 5685799,\n",
       " '@jinxman': 1809885,\n",
       " '#kyaaa': 4518908,\n",
       " '@jla_ox': 1300788,\n",
       " '@pacs_life': 524814,\n",
       " '@knowsleymammag': 984828,\n",
       " '@xxkusjedaniek': 574500,\n",
       " '@alyssakirch': 4459054,\n",
       " '@sherryyyl': 1696157,\n",
       " '@nikitaxgriffin': 1519742,\n",
       " '@hadabad_dayln': 462800,\n",
       " '@sambo_turner': 2999079,\n",
       " '@tesshahah': 611978,\n",
       " '6462004945': 1948877,\n",
       " '@bc_mjmonkey': 1396788,\n",
       " '#foodp': 383453,\n",
       " '@kushn_highheels': 5559680,\n",
       " '@emilylitton4': 1437437,\n",
       " '@littlefaux': 310519,\n",
       " 'maddened': 722446,\n",
       " 'auto-triggered': 4809158,\n",
       " '@ltalaricowltx': 447265,\n",
       " '@alfons0_o': 471794,\n",
       " '@ifashionistaa': 5502958,\n",
       " '@supportbeau': 5994300,\n",
       " 'hytner': 4952233,\n",
       " '@m_jharvey': 2687438,\n",
       " 'caocao': 5474012,\n",
       " '@iam_too_live': 997098,\n",
       " '@izyannn': 4037833,\n",
       " '@trey_111412': 5045559,\n",
       " '@ohkatieanderson': 1081301,\n",
       " '@scottybahm': 3141170,\n",
       " '@minabrckovic': 4472597,\n",
       " '@_itsallbella': 4293870,\n",
       " '@jessmaex': 4380225,\n",
       " '@anfalalfahad': 4654137,\n",
       " '@kayystackkz': 1597982,\n",
       " '@racheldubs': 4584692,\n",
       " '@pnkdiamonz': 1582144,\n",
       " '@sofia_vaz123': 1706945,\n",
       " '@_roseyyyy_': 3415244,\n",
       " '@realjaejae': 6029818,\n",
       " '@nathanarnold35': 3118623,\n",
       " '@feeberz': 5613635,\n",
       " '@paulsreece': 5822238,\n",
       " '@idespise_love': 5193675,\n",
       " '@jossaye': 2687061,\n",
       " 'blunn': 938641,\n",
       " 'faizaan': 3676059,\n",
       " '@sierrasafox': 1709517,\n",
       " '@_willywacker': 692817,\n",
       " '#followmydreams': 1198565,\n",
       " '@thekidbiebs': 102164,\n",
       " '@z_vantall': 6042939,\n",
       " 'desconectes': 1068558,\n",
       " '@ain_adiya': 4821062,\n",
       " '@twin_or_lose': 2986180,\n",
       " '@_thisjustinn': 3345796,\n",
       " '@queenalicia_ok': 2514512,\n",
       " '@the_queen_teen': 119038,\n",
       " '@asc18': 3603032,\n",
       " '@markstanley62': 789293,\n",
       " '@mariaflorea1997': 1065337,\n",
       " '#sleepingbags': 5570795,\n",
       " '@monet_ware': 2161537,\n",
       " '@kaybeesc': 5739320,\n",
       " '@_yogirljazzie': 2039158,\n",
       " '@013bbuckalew': 2260424,\n",
       " '@katiekarnif': 5635137,\n",
       " '@babyha93': 2054134,\n",
       " '#familyparties': 1520605,\n",
       " '@yamilly1': 4834953,\n",
       " '@_jade_leigh_x': 1427913,\n",
       " 'b8350': 4066189,\n",
       " 'perviously': 3704825,\n",
       " 'marcdproduction@aol.com': 4253906,\n",
       " '@bakasekaikun': 5182300,\n",
       " 'pathiye': 4832876,\n",
       " '@grantsome2': 2259217,\n",
       " '@cydneycurran': 1508792,\n",
       " '@chelcfarrell': 4194468,\n",
       " '@ab_fr3sco': 5529517,\n",
       " '@heartzxc_m': 254438,\n",
       " 'u120h': 5084683,\n",
       " '290.80': 3725167,\n",
       " '@emmagkeller': 2076375,\n",
       " 'makaganti': 4194600,\n",
       " '@corninyahole': 4659351,\n",
       " '<`)>': 209236,\n",
       " '@haley_burg17': 641025,\n",
       " '@endepee': 153292,\n",
       " 'fabrt': 3437528,\n",
       " '@cutesy__': 2859351,\n",
       " '@_taylor_1d': 4900444,\n",
       " 'culinair': 901593,\n",
       " '@blancaeb3': 897449,\n",
       " '@malyssa_dunn': 2101395,\n",
       " '@nicolasminaj': 2549813,\n",
       " 'kiloo': 53144,\n",
       " '@parrish_12': 5872790,\n",
       " '@nelxson': 1817760,\n",
       " 'happennnnnnnn': 3956966,\n",
       " '@nikkkkicondon': 2565516,\n",
       " '@ayebaybayy13': 2947835,\n",
       " '@taslimreza1': 767361,\n",
       " '@saaheba_bedi': 891255,\n",
       " '@kppollinger': 1049100,\n",
       " '@dannyelaaah': 3041194,\n",
       " '@itstbennett': 345728,\n",
       " '@magdalen_alyssa': 184997,\n",
       " '@beckieeehill': 1821067,\n",
       " '@onedremone': 3817351,\n",
       " '@juicyylv': 4442091,\n",
       " '@king_jiff': 1477183,\n",
       " 'meramianwali': 802361,\n",
       " '@nksiren': 4965051,\n",
       " '@heartheart22': 3377197,\n",
       " '@__tashee': 1721440,\n",
       " '@janel_a09': 3491586,\n",
       " '@lynunique': 4384540,\n",
       " '@xoxodedexoxo': 2843205,\n",
       " '@jaeehoney': 324026,\n",
       " '@lyddialst': 5687725,\n",
       " 'emyla': 3888026,\n",
       " '@1stephaniehelen': 650337,\n",
       " '@josenatics': 5615856,\n",
       " '@aricody1dzenny': 4887260,\n",
       " '@nnez_': 895987,\n",
       " '@sjcivi_': 3300623,\n",
       " '@worleygirl': 5523099,\n",
       " '@eli_smalls23': 3855894,\n",
       " '@mondli_ngema': 561726,\n",
       " '@anethbow': 190966,\n",
       " 'mandopony': 3443252,\n",
       " '@mintyziall': 6047494,\n",
       " '@damarishastiti': 4364529,\n",
       " '@alan1_ak': 3039545,\n",
       " '@llamariel': 2985196,\n",
       " '@sunitidamani': 4577780,\n",
       " '@bigdaddyclaudia': 915285,\n",
       " '@justtish13': 5026639,\n",
       " '@sophiehall3': 5284992,\n",
       " '@emilyheminuuuk': 2596506,\n",
       " '@lucastanahkao': 3161735,\n",
       " '@cuhhlexxis': 2843073,\n",
       " '@oh_chloeee': 3118409,\n",
       " '@shannynlouise21': 1749213,\n",
       " '@_nurainnajwa': 34968,\n",
       " '@carmenngarciaa': 4035464,\n",
       " 'megatrain': 4552218,\n",
       " '@swiftselenator1': 3902868,\n",
       " '@lucyjoanne94': 4777464,\n",
       " '@craicwhore_': 4545495,\n",
       " '@klubbuku': 1226538,\n",
       " '@iamazombiebro': 2544528,\n",
       " '@stacybadside16': 1916600,\n",
       " '@realjerrysotelo': 3571688,\n",
       " 'mandew': 4718142,\n",
       " '@champagnedelo': 2963637,\n",
       " '@nonacweety': 2699922,\n",
       " '@h0x0d': 4441409,\n",
       " '@bellind07': 1543867,\n",
       " '@jesshook395': 2120628,\n",
       " '@mrlivestrong': 5892756,\n",
       " '@blklion34': 5585176,\n",
       " '@liltunechiiii': 5921377,\n",
       " '@jng_ailee': 4780686,\n",
       " '@__itsnialler': 5503320,\n",
       " '@anesthesha': 2768034,\n",
       " '@agent_muffin': 2218696,\n",
       " '@mofomunkay': 2142746,\n",
       " '@lesbeehonesty': 3759859,\n",
       " '@exoticc_beautii': 2440368,\n",
       " '@china_edwards': 3918176,\n",
       " '@allysonfalls': 6021742,\n",
       " 'tummarow': 3407712,\n",
       " 'cutla': 4193819,\n",
       " '294.43': 4741481,\n",
       " 'brejon': 2735072,\n",
       " '@hannahdemel13': 3927470,\n",
       " '@_flxwer': 5343080,\n",
       " '@carolmalouf': 1859207,\n",
       " '@ephemera_me': 2960865,\n",
       " 'magbabait': 3507258,\n",
       " '@imfrostblooded': 4202498,\n",
       " '@hanadenouden': 864150,\n",
       " 'intellistage': 4077309,\n",
       " '@ffhumam': 4202366,\n",
       " '@lisamarkwell': 278976,\n",
       " '@ashley_baker05': 2469798,\n",
       " '3.move': 4408772,\n",
       " '@bysimply_daisia': 3571946,\n",
       " '@soulja': 900082,\n",
       " '@steph21rose': 5690615,\n",
       " '@lauramarie1227': 6020998,\n",
       " '@ra_craik': 5307883,\n",
       " '@_lovepaigeyyy': 4742494,\n",
       " '@jaureguisrascal': 2793702,\n",
       " '@ziallscream': 4124438,\n",
       " '@astriipn': 2812761,\n",
       " '@yeong_sheng': 4494895,\n",
       " '@marianaaa_c': 488577,\n",
       " '@supercutediidii': 2674214,\n",
       " 'alkout': 3412417,\n",
       " '@ikiabdurrh': 5976237,\n",
       " '@heartbeat2897': 1068473,\n",
       " '@dorkyyhass': 677698,\n",
       " '@greatambition_': 709650,\n",
       " '@cole_keeley15': 5799566,\n",
       " '@atomy__': 5559947,\n",
       " 'wwpj': 5147221,\n",
       " '@alyvico': 4012558,\n",
       " '@0_vidalll': 2326128,\n",
       " '@fauzanhakimii': 5053232,\n",
       " '@josieee_romano': 1579566,\n",
       " '@paige_wilson': 4255382,\n",
       " '@giolimjoco': 1766673,\n",
       " '@tommo7david': 5239393,\n",
       " '@randy_lewi': 1641921,\n",
       " '@tinadelara': 5126034,\n",
       " '@beicawnic_': 831649,\n",
       " '@faradiba31': 5608191,\n",
       " 'muxus': 3561856,\n",
       " '@ashleycronley': 4434108,\n",
       " 'maufuckas': 199748,\n",
       " 'islaa': 5195639,\n",
       " '1.208': 456418,\n",
       " '@barceyankee': 3922950,\n",
       " '@alimarie1428': 5255748,\n",
       " '@daisyjayne2': 2965105,\n",
       " '@gabby_gotem': 1746757,\n",
       " 'slippered': 3891765,\n",
       " '@gwencoco': 348207,\n",
       " '@mrwalz': 1404272,\n",
       " '@ohb_': 2666424,\n",
       " '#mrf': 275401,\n",
       " '@kaylah_jones17': 3259239,\n",
       " '@k3oni': 1823386,\n",
       " '@blanca_leidiana': 2813993,\n",
       " '@sambrammer': 5824508,\n",
       " '@jonigabrielle': 4283063,\n",
       " '@legendnika': 3403123,\n",
       " '2cali': 2459195,\n",
       " '@oldschoolbieber': 2037036,\n",
       " '@createdgorgeous': 4924176,\n",
       " '@anneaysh': 5032484,\n",
       " '@justsayingjose': 376969,\n",
       " '@_tracyvee': 1941631,\n",
       " '@chelseafontana': 1616918,\n",
       " '@stylesftmendes_': 4148054,\n",
       " '@housey95': 2005557,\n",
       " '@spicysorceress': 547507,\n",
       " '@gabmccarthy': 2390030,\n",
       " 'teamgalaxy': 3539716,\n",
       " '@dazzles_': 1997576,\n",
       " '@bellaballantyne': 960287,\n",
       " '@aye_yoofool': 2440100,\n",
       " '#alittlebetter': 4798748,\n",
       " '@travismorris69': 4275031,\n",
       " '@brook1hart': 807142,\n",
       " '#instasex': 5947260,\n",
       " '@jaybaby098': 4698759,\n",
       " 'winninqq': 1912187,\n",
       " '@gnarly_carlyy': 815160,\n",
       " '@adzrii': 5076560,\n",
       " '@laurameunierr': 1685012,\n",
       " 'larit': 3765897,\n",
       " '@tinman_jr': 2873475,\n",
       " '5792': 1624580,\n",
       " '@imeon050597': 607429,\n",
       " '@wbcb_punk': 991058,\n",
       " '@hanan991': 2762650,\n",
       " 'raagas': 5061764,\n",
       " '4,242': 4060866,\n",
       " '@xoxox_malika': 2458762,\n",
       " '@jae_roberts3': 3639920,\n",
       " '@dancrupt': 1133124,\n",
       " \"jaii'll\": 3564292,\n",
       " '@shehariah': 5188794,\n",
       " '@shawnamuah': 4374995,\n",
       " '@joee_whitworth': 6046854,\n",
       " '3040-2': 4029430,\n",
       " '@joannajay': 202958,\n",
       " '@_abreugetsup': 4341833,\n",
       " '@vickiprybell': 5742496,\n",
       " '#gnightt': 4858044,\n",
       " 'breedable': 5909824,\n",
       " '@vsvpnick': 3124506,\n",
       " '@rhona___': 3676575,\n",
       " '@parga_based_god': 42890,\n",
       " '@imerikasanmateo': 5579282,\n",
       " '@imjordandaniels': 5545506,\n",
       " '@alicemitchellox': 4820653,\n",
       " 'anytimmmme': 783636,\n",
       " '@_thirdeyeking': 422334,\n",
       " '@kristyr_': 519983,\n",
       " '@_itsraeeduhh': 4673917,\n",
       " '@mettaworld_quis': 2084474,\n",
       " '@wheresdecraic': 3460235,\n",
       " '@mizzieloore': 3590332,\n",
       " '@tiiqhatamie': 5447095,\n",
       " '@kchris2016': 2745809,\n",
       " '@farmermikayla': 1536325,\n",
       " '@tsgflowers': 4946078,\n",
       " '#immadatyou': 350072,\n",
       " '@haydenyvonnefan': 5007952,\n",
       " '@ezrahrah': 5224188,\n",
       " '#mcn': 1136454,\n",
       " '@jessie_janeee': 207667,\n",
       " '@camcrev': 1359657,\n",
       " '@nbrown10': 294358,\n",
       " '@vivianlugo09': 496630,\n",
       " '#trigeminalneuralgia': 1064920,\n",
       " 'photoeditor': 895229,\n",
       " '@_mace_face_': 5434242,\n",
       " '@juliedinh1': 4561485,\n",
       " '@taylor2733': 1598760,\n",
       " '@gensanquenatics': 3353579,\n",
       " '@iamravenruth': 624741,\n",
       " '@inmaculadahenar': 1915833,\n",
       " '@mindmymanners': 1474983,\n",
       " '@bethanyhamer1': 2560796,\n",
       " 'garuantee': 2188742,\n",
       " '@twitinno': 4540534,\n",
       " '#notoxenophobia': 2766280,\n",
       " '@qingkei': 283498,\n",
       " '@naddibrahim': 872885,\n",
       " '@harrical': 5791353,\n",
       " '@dunnieboi': 1458107,\n",
       " '@shendddy': 3386213,\n",
       " '@marionholly': 605066,\n",
       " '@__wavyjosh': 3136693,\n",
       " '@_redassbitch': 6044700,\n",
       " '@bapechaser2': 3685948,\n",
       " '@bigboy_vinny': 2675289,\n",
       " '@loadsofcrapp': 4964880,\n",
       " '@call_her_jay': 2696330,\n",
       " '@timmy__chan24': 3460262,\n",
       " '#instajokin': 1609043,\n",
       " 'tweeeeeeeeeeeeeeeeet': 598991,\n",
       " '#happybirthdaymmj': 6038372,\n",
       " 'nokuthula': 2895198,\n",
       " '@tehreemuk': 5697658,\n",
       " '@hannnnyforgg': 2256361,\n",
       " '@yvotran5': 3251096,\n",
       " '@kissdonghae': 3351392,\n",
       " '@am_buuurrr': 517832,\n",
       " '@nycnextlevel': 4443311,\n",
       " '@sema2103': 5080920,\n",
       " '@mindlesswaffles': 1470203,\n",
       " '@_smartmouthlynn': 5209251,\n",
       " '@tikimonroe': 5965594,\n",
       " 'ampah': 1994154,\n",
       " '@madi_sholtz': 2305658,\n",
       " '@mawtinn': 4866098,\n",
       " 'rayshonna': 2052290,\n",
       " 'calientan': 3901755,\n",
       " '@skylablaize': 6017873,\n",
       " '@followmecancun': 2396468,\n",
       " '@larrinv8a': 4651154,\n",
       " '@sheisangelea': 1123291,\n",
       " '@biznessiam': 3799452,\n",
       " '@jademysupport': 4397873,\n",
       " '@delfinabazan1': 5054645,\n",
       " '@denise_105': 2864558,\n",
       " '@nakiasideas': 4882924,\n",
       " '@anthonyenoch': 4867477,\n",
       " '@mink_deville': 815947,\n",
       " '@vannallope': 5464110,\n",
       " '@bbagnall8': 671987,\n",
       " '@azariaheng': 3048699,\n",
       " '@papivchris': 4025774,\n",
       " '@_illustri0us': 4099435,\n",
       " '23/32': 612276,\n",
       " '@josiemlow': 2453477,\n",
       " '@cho33donghyun': 1046007,\n",
       " '@naaziya289': 2740968,\n",
       " '@amona_3': 132448,\n",
       " '@ebonyhovard': 5576832,\n",
       " '@nomes3': 1280675,\n",
       " '@trippywicked': 1762769,\n",
       " '@24hourhiphop': 658360,\n",
       " 'jeydi': 37893,\n",
       " '#divergentfandom': 659844,\n",
       " '@scottjones7': 3894026,\n",
       " '@cricketupdates7': 3089989,\n",
       " 'kadupu': 748400,\n",
       " '@proudestgrandpa': 2387908,\n",
       " '@haneliswe': 3989409,\n",
       " '@tezzydoll': 4587660,\n",
       " '@hentaiqueen_': 1149394,\n",
       " '@sportybebe': 2239663,\n",
       " '@reyesrj_': 3105117,\n",
       " '@jenna_wolfson': 981551,\n",
       " 'halkett': 2518053,\n",
       " '@malissamcdaniel': 3766791,\n",
       " '@iorkara': 4994956,\n",
       " 'delayed': 6111628,\n",
       " '@beckysmall2': 3485552,\n",
       " '@meggie071': 2211115,\n",
       " '@annawrecksic': 5769564,\n",
       " '@session_diva': 4725862,\n",
       " '@manuela_arango': 4592199,\n",
       " '@chrisdjdl': 2757889,\n",
       " '@laurabrennan1d': 4345974,\n",
       " '@matthewnash23': 5428790,\n",
       " '@mrsonline_kecil': 2638640,\n",
       " '@metalmilitia7': 1478664,\n",
       " '@_notdhaaverage': 5022067,\n",
       " '@dj_fad3': 2545276,\n",
       " '@bnnnewslive': 2364413,\n",
       " '@jayycasian': 987714,\n",
       " 'colestream': 6099092,\n",
       " 'auratic': 3890743,\n",
       " '@dannykdr': 3790768,\n",
       " '@neveerstopdream': 3641987,\n",
       " '@littlemixnv': 1104555,\n",
       " '@cecewillliams': 2067539,\n",
       " '@_ttorii_': 1797378,\n",
       " '@aileensosavage': 3233317,\n",
       " '@maggiebend': 4481755,\n",
       " '@louisenikolai': 5956344,\n",
       " '@juli4_cl4re': 5033891,\n",
       " '@shaunster116': 5642958,\n",
       " '@licadoodles': 4492105,\n",
       " '@never_plainjane': 3531437,\n",
       " '@pelozocarla': 5306060,\n",
       " '@iam_jaydeer': 4379202,\n",
       " '@illmatic_rekah': 1922756,\n",
       " '@asapxstyles': 633454,\n",
       " '@agenk_bmx': 5741851,\n",
       " '@emilyriddle': 147457,\n",
       " '@ipacmandapussy': 705424,\n",
       " '@madisonness': 765066,\n",
       " 'sarza': 5848288,\n",
       " '@george_lee92': 4392945,\n",
       " '@yesparamore': 4038894,\n",
       " 'qkwkwk': 3733987,\n",
       " '@bullshetschool': 4715702,\n",
       " '#getmarriedalready': 2077571,\n",
       " '@katz_k': 5711932,\n",
       " '@finfinsharkfin': 4203010,\n",
       " '@liams123flickk': 1791496,\n",
       " '@parkjangstar': 2298224,\n",
       " 'denlee': 632423,\n",
       " '@isabella_aj': 1628327,\n",
       " '@picturestweets': 4789770,\n",
       " \"diss's\": 2607187,\n",
       " '@antlopdoe': 4014306,\n",
       " '@fuckdaniel666': 3246766,\n",
       " '@pur3dope': 3005204,\n",
       " '@alexis_kellam': 3988314,\n",
       " '@2hi2b': 230484,\n",
       " '@jlee_78': 2418824,\n",
       " '@lol_nope_': 77558,\n",
       " '#southtexasprobs': 1373818,\n",
       " '@teamdamonbrazil': 3109718,\n",
       " '@gisela_x3': 605511,\n",
       " '@abbieeeupton': 5477469,\n",
       " '@justdarylll': 3662100,\n",
       " '@ellie_jazmine': 2955920,\n",
       " '@chauntae1': 6020946,\n",
       " '@joelwarren': 5783065,\n",
       " '@daaaaaanixo': 2984204,\n",
       " '#lakynorlove': 4834994,\n",
       " '@can_be_both': 2170461,\n",
       " '@courtney_deeley': 1768269,\n",
       " '@melissa_price78': 2518521,\n",
       " '@heyyoofiona': 6040326,\n",
       " 'weifeng': 1564422,\n",
       " '@barmyluke23': 3885630,\n",
       " 'first-rounders': 1069144,\n",
       " 'ashler': 1410875,\n",
       " '@loutomlinsn': 153750,\n",
       " '@iamayoo': 259103,\n",
       " '@ilovejanay': 5578174,\n",
       " '@assh_lee': 4432584,\n",
       " '@short_cayke': 5459769,\n",
       " '@josiegohh': 2759559,\n",
       " '@clariceylovf': 797929,\n",
       " '@ames_10twins': 5568178,\n",
       " '#sc2012': 4800992,\n",
       " 'rightchoice': 456499,\n",
       " '@north_dairy': 5997589,\n",
       " '@alexiscrutch': 782848,\n",
       " '@carlenerd': 4200237,\n",
       " '@mell_lover': 905040,\n",
       " '@brookie171': 5446441,\n",
       " '@the__for': 4082586,\n",
       " '@5_fields_5': 3039519,\n",
       " '@dwinayp': 4346218,\n",
       " '@presgotups15': 3659457,\n",
       " '@savitrahman': 3140576,\n",
       " '@not_syd_helmick': 1411579,\n",
       " '@kanisha_reb': 3722962,\n",
       " 'tabeem': 3432896,\n",
       " '@krissa_barr': 2532017,\n",
       " '@adamr_94': 265008,\n",
       " '@kennedy_wojno': 871682,\n",
       " '@bdadyslexia': 2494569,\n",
       " '@lashaunarmani_': 4543904,\n",
       " '@gwooh_wooh': 4044047,\n",
       " '@sabriinaato': 2717985,\n",
       " 'bhsister': 5687830,\n",
       " '@shawtybad97': 952947,\n",
       " '@findingoscar': 511777,\n",
       " '@wowhoes': 6041682,\n",
       " '@rawrbaby92fl': 4642248,\n",
       " '@tweetwhileheeat': 5499769,\n",
       " '@scotthowland': 3182931,\n",
       " '@adindagn': 4546859,\n",
       " '@peacefulexistnc': 5493813,\n",
       " '@kerrigansk': 5328950,\n",
       " '@invinceabl': 189085,\n",
       " '@_aldo21': 263679,\n",
       " '@adaoraa': 5072683,\n",
       " '@galos_kate': 3369011,\n",
       " '@danni_smith23': 1587365,\n",
       " '@baboyannie': 2202685,\n",
       " '@m_dawg__': 2287160,\n",
       " \"ensign's\": 1776434,\n",
       " '@zarrywoah': 4156079,\n",
       " '@unreportedworld': 5042970,\n",
       " '@maura_jae': 2887243,\n",
       " '@jazmine_brewer2': 1816766,\n",
       " 'asriiii': 3427782,\n",
       " '@kenidykubitz': 2808681,\n",
       " '@lyssa_0n1': 3422193,\n",
       " '@hansa_1': 2366509,\n",
       " 'barofsky': 1236191,\n",
       " '@shane__whelan': 993089,\n",
       " '@lilbrav': 4474210,\n",
       " '@catalinasanch3z': 4975092,\n",
       " 'japppaaannnnn': 6139845,\n",
       " '@caseyoschmann': 4583011,\n",
       " '@biancadasilvax': 2563820,\n",
       " '@doncherryparody': 5330514,\n",
       " '@tweetgod__': 5822790,\n",
       " '@liddleb7': 5771240,\n",
       " '@kissable_kay': 3183397,\n",
       " '@naeanais': 468424,\n",
       " '@jordanholston': 2205431,\n",
       " 'godson': 6127070,\n",
       " '@shu___shu': 5520068,\n",
       " '@katmurphyy': 1976357,\n",
       " '@luizabfan': 1679714,\n",
       " '@ncad_dublin': 4498650,\n",
       " '@mjvarnsverry': 3038070,\n",
       " '@__sadai': 90478,\n",
       " '@racheyy': 2950354,\n",
       " '@_zajmxo': 2750584,\n",
       " '@juantheg': 5034196,\n",
       " '@lady_multham': 249257,\n",
       " '@kinghollay': 3367478,\n",
       " '@v_dylancook_v': 3051014,\n",
       " '@ytf_chicago': 137804,\n",
       " '@megchristinehun': 3861198,\n",
       " '@brandyjurevitz': 3478084,\n",
       " '@katiefeesey': 1376381,\n",
       " '@justjazlyynnn': 598282,\n",
       " '@christopher_lv': 4591086,\n",
       " '@sammigauthier': 6061937,\n",
       " '@esamulaan_': 1573378,\n",
       " '@xtinaalisaa_06': 5740500,\n",
       " '@audreymrc': 4856148,\n",
       " '@_rebekahohare': 2436052,\n",
       " '@aoife__kav': 1019245,\n",
       " '@bkvssidy_': 1949169,\n",
       " '@elizabetham__': 2301225,\n",
       " '@cacafacecindy': 3712154,\n",
       " '@1oliviarose': 5424081,\n",
       " '@jolizeines_xx': 5104562,\n",
       " '@comfortandadam': 2893293,\n",
       " '@sarahgraice': 3167677,\n",
       " '@emiillly_': 691911,\n",
       " '@omgitslibby': 2596784,\n",
       " '@xmarliessss': 4949478,\n",
       " '@stbridelibrary': 3418109,\n",
       " '@thisfaggot__': 5400619,\n",
       " '@shynebeats': 1597884,\n",
       " 'irressistible': 5634661,\n",
       " '@seminolereverie': 627507,\n",
       " '2poster': 2526628,\n",
       " 'maileen': 520184,\n",
       " '@pronnie2324': 3182638,\n",
       " '@marisseck': 3513730,\n",
       " '@taralobianco': 1353016,\n",
       " '@mccurrie_': 1477328,\n",
       " '#5sosarenumberoneparty': 50110,\n",
       " 'echauz': 1863688,\n",
       " '@unclemad': 2512674,\n",
       " '#truearianator': 2181178,\n",
       " '@jamrokjake': 232589,\n",
       " '#yourgirlsahoeif': 643417,\n",
       " '@bigmarc251': 66360,\n",
       " '@fabgoingham': 3773682,\n",
       " '@fun_sized_yo09': 3207688,\n",
       " '@twdarin': 2196471,\n",
       " '@xbettyfx': 3815330,\n",
       " '@saracakes_': 1176486,\n",
       " '@jodie_lunttw': 4650574,\n",
       " '@barbara_ann4497': 5423498,\n",
       " '@aksesorina': 5775397,\n",
       " '@lilzac34': 1660519,\n",
       " '@dgdizzy': 3469790,\n",
       " '@caiumhoocl': 346354,\n",
       " '@viaaneyyyy': 3070683,\n",
       " '@kayy_faith': 4726274,\n",
       " '@emilyshannon94': 1190203,\n",
       " '@andreaa_p96': 3644872,\n",
       " 'ngerengek': 2367701,\n",
       " '@natashapatel_tw': 2445894,\n",
       " '@aliyahmader': 3576116,\n",
       " '@lou_tomlemonson': 5108889,\n",
       " '@rylezra': 5716974,\n",
       " '#worddddd': 3274023,\n",
       " 'uhhgh': 2427659,\n",
       " '#torontostar': 4261095,\n",
       " '@kuranicole': 3730164,\n",
       " 'myugsoo': 3732966,\n",
       " '@leighemment': 2843357,\n",
       " '@emmyabe': 1391758,\n",
       " '@aimansyamim_': 3029619,\n",
       " '@naya_symone': 1722018,\n",
       " '#rugbyleaguelive2': 5425690,\n",
       " '@hollyskillings': 3699627,\n",
       " '@kaylee_wolff': 4909264,\n",
       " '@olivia_1414': 3254188,\n",
       " '#everythingis4preorder': 283377,\n",
       " '@sarah_seagull13': 2836359,\n",
       " '@arineeda_pencil': 2370047,\n",
       " '@macbarbielubber': 1969967,\n",
       " '@guyedgvvr': 4939939,\n",
       " '#faile': 5847443,\n",
       " '@afinefeline': 1956442,\n",
       " '@kaylaa_wardd': 440164,\n",
       " '@niarimadona': 3351899,\n",
       " '@ishipnarrry': 3699912,\n",
       " '@cheyenne_toon': 5828993,\n",
       " '@d4rkbr0therhood': 2384017,\n",
       " '@t_chillin4': 3901247,\n",
       " '@serdonmaria': 761821,\n",
       " '@wefallforgomez': 5014637,\n",
       " '@jakartadc': 2463453,\n",
       " '@caseythemodel': 335577,\n",
       " '@mga_wolf': 3450711,\n",
       " '@camgohamfuller': 2872844,\n",
       " '@mancboi1987': 5377899,\n",
       " '@asianniggguh': 779367,\n",
       " '@darinharvey': 4879202,\n",
       " '@syrenaica': 5565027,\n",
       " '@chickenjoe342': 3519765,\n",
       " '@xmanfa': 1633134,\n",
       " '@missapanavicius': 2224296,\n",
       " '@looc_si_nae': 3043283,\n",
       " '@irolar': 3067302,\n",
       " 'boards.ie': 1209158,\n",
       " '@danomccarthy121': 745328,\n",
       " '@mrsmartin4144': 4771164,\n",
       " '@amydickenx': 58353,\n",
       " '@courtstruly': 2824853,\n",
       " '@j_lapes': 2333424,\n",
       " '@ericapribila': 1770725,\n",
       " '@iamteddyrozay': 4021176,\n",
       " '@leilersss': 2004906,\n",
       " '@xyouaintdope': 1535406,\n",
       " '@sophiebellamy': 5815498,\n",
       " '#thecomeups': 455010,\n",
       " '@_jo_king_': 1153798,\n",
       " 'jodemeeeee': 1131243,\n",
       " '@molinasemple': 3157308,\n",
       " '@nialls_mofowife': 1114337,\n",
       " '@thaob_le': 5098155,\n",
       " '@samantha_diperi': 589604,\n",
       " '@valleouk': 5947517,\n",
       " '@dirtnddglitter': 4269100,\n",
       " 'gritch': 5924433,\n",
       " '@bockle': 2527690,\n",
       " '@jlrockefeller': 2471439,\n",
       " '@natashaoct': 1589594,\n",
       " '@maddie_shipman': 5831495,\n",
       " '#thugsdontdance': 1934617,\n",
       " '@to_financejob': 1583255,\n",
       " '#zombietime': 184268,\n",
       " '@taylorrminnick': 3448834,\n",
       " '@prettiblaque_': 4985699,\n",
       " '@xmileycyrusr': 5392186,\n",
       " '@the_algerbraic': 3394860,\n",
       " '@rompehuevos3': 5052586,\n",
       " '@mansakaur': 4354571,\n",
       " '@donetbh_': 2724020,\n",
       " '@kpzy': 3365917,\n",
       " '@charlie_angels_': 5346834,\n",
       " '@annamarie_1004': 1557047,\n",
       " '@specialistpsy': 1018055,\n",
       " '@t_summers76': 5806009,\n",
       " 'j-si': 3730673,\n",
       " '@lompss': 994875,\n",
       " 'colegioooooooo': 5327957,\n",
       " '@radyacntn': 5101245,\n",
       " '@oliviaruss1': 3133419,\n",
       " '@lenonooo': 4978391,\n",
       " '@mugasha_': 3579898,\n",
       " '@johnongoco': 5533766,\n",
       " 'tweenteen': 242934,\n",
       " '#youremyperson': 1911554,\n",
       " '@bradleyu': 1539581,\n",
       " '@austinmahboii': 2167171,\n",
       " '@_hmichelle': 2579852,\n",
       " '@shasha_loveso': 2279239,\n",
       " '@aquila_j': 6046252,\n",
       " '@tiger_cocainee': 5592516,\n",
       " '@airsoull': 4534557,\n",
       " '@orinndb': 5345009,\n",
       " '@justbrittney23': 2064724,\n",
       " '@1dlondonon': 1681969,\n",
       " '@lexii_louuuu': 99812,\n",
       " '@laiteux': 3236229,\n",
       " '@mjazmartinez608': 2441556,\n",
       " 'saimung': 2744362,\n",
       " '@rooseveltk': 3293348,\n",
       " '@camlikesbooty': 3425002,\n",
       " '@abbiestonehouse': 4352721,\n",
       " '@shawtyyletssgoo': 3984086,\n",
       " '#dontlistentothem': 4009009,\n",
       " '@midnightsmiles': 6002223,\n",
       " '@austinpollard4': 2537745,\n",
       " 'cilgintvtags': 4927826,\n",
       " '#happybirthdaytristanevans': 3094279,\n",
       " '@anotherlie': 4962362,\n",
       " '@i_beliebin1': 4648187,\n",
       " '@drew_north1': 3675084,\n",
       " '@shylaa_xo': 3975945,\n",
       " '@las2themee': 4258459,\n",
       " '@danceforever148': 5270409,\n",
       " '@blackgirlsrock5': 3576769,\n",
       " 'rediculouss': 4140218,\n",
       " '@gizzalon': 606029,\n",
       " '@kyleadlawan': 4008892,\n",
       " '@linnellcourtney': 5797730,\n",
       " '@jollyriffic': 1644814,\n",
       " '@brenda_booboo': 5383951,\n",
       " '@ethanlyttle': 4743835,\n",
       " '280.62': 2175646,\n",
       " '@fortedpark': 5134333,\n",
       " '@iamredgemoi17': 3493533,\n",
       " '@hxorsnhon': 862913,\n",
       " '@anony_mmis': 1263584,\n",
       " '@michaelwatson85': 870880,\n",
       " 'meeesssss': 1486986,\n",
       " '@mronyxreyes': 239371,\n",
       " '@gutierrezdeee': 835243,\n",
       " '@xosimplyamazing': 1663384,\n",
       " '@dgrog': 3257360,\n",
       " '@tsmooky': 1637128,\n",
       " '@jessrockstarrr': 1613961,\n",
       " '@shineeonew_fba': 4340148,\n",
       " '@i_neeed_moneey': 1599340,\n",
       " '@mc_girrr': 5776463,\n",
       " '@edentodope69': 3681122,\n",
       " '@mayaalyaaa': 5848333,\n",
       " '@aaronellis1': 737420,\n",
       " '#tvfail': 3357682,\n",
       " '@tatiana__nieves': 58856,\n",
       " '@idkemotional': 4387187,\n",
       " '@indahahehahe': 3520291,\n",
       " '@glogirl__': 1440152,\n",
       " '@milah___': 5164594,\n",
       " '@hayleybrock5': 3738798,\n",
       " '@jess_beltonx': 5058471,\n",
       " 'hoyah': 2913235,\n",
       " '@thongstrap': 1623169,\n",
       " '@laurennnf': 3457955,\n",
       " '@naomichristlike': 4519037,\n",
       " '@__leahbeah': 2845247,\n",
       " '@jade_flear': 897602,\n",
       " 'cachetito': 2614416,\n",
       " '@sarashayksbae': 3838601,\n",
       " '@caitlin_marie29': 5165394,\n",
       " '#gameoflove': 2323639,\n",
       " '@_asapslim': 2945516,\n",
       " '@taylorblubaugh': 4759932,\n",
       " '@kaaitliinn_': 561385,\n",
       " '@na5im': 1344450,\n",
       " '@nmrh_': 1253706,\n",
       " '@halakoura_': 2837229,\n",
       " '@beliebers324': 4620961,\n",
       " '@kim_cassidy': 2185733,\n",
       " '@tippmeover': 3648447,\n",
       " '@imperfectyna': 2408132,\n",
       " '#banvafg': 1350624,\n",
       " '@bbb_butta': 5066071,\n",
       " '#ghettobitches': 271362,\n",
       " '#ineedprayer': 3577388,\n",
       " '@tierasarena': 5119683,\n",
       " '@nicolebonhommie': 1454743,\n",
       " '@bingham_jordan': 5993517,\n",
       " '195x': 516043,\n",
       " '@onlybeauty_': 439153,\n",
       " '@olanrehwahju': 169043,\n",
       " '@ifyoudont_stfu': 2315633,\n",
       " '@eloquenteq': 5649136,\n",
       " '@heens_5': 1431168,\n",
       " '@liannaaa_': 1715610,\n",
       " '@_karen_mendez': 2624850,\n",
       " 'gullllll': 567836,\n",
       " '#august14th': 3666780,\n",
       " 'kaskusers': 3991065,\n",
       " '@mgcjdmx': 4672791,\n",
       " '@caoimh_oleary': 616120,\n",
       " '@saruuhdonald': 2638359,\n",
       " 'omeey': 19952,\n",
       " '@methocean': 2857556,\n",
       " '@thatkid_natalyy': 1230240,\n",
       " 'auriee': 1042,\n",
       " 'bedart': 3087936,\n",
       " 'angsle': 1844577,\n",
       " '@misslameface': 2309507,\n",
       " '@emchann': 5220281,\n",
       " '@charleyashmore': 2398238,\n",
       " '@cameroncisik': 4363126,\n",
       " 'hoays': 4867502,\n",
       " '@permescudi': 1453157,\n",
       " '@loveejasss': 2257972,\n",
       " '@nathanielholm': 1432046,\n",
       " '@deedashini': 1471200,\n",
       " '@jennnadestefano': 569357,\n",
       " '#ough': 5221659,\n",
       " 'buccament': 3622878,\n",
       " '#deepsea': 1189308,\n",
       " 'presidnet': 3711952,\n",
       " '@hopeella': 1702968,\n",
       " '@5secsofjaybird': 528646,\n",
       " '@indescribableee': 4566346,\n",
       " '@maddie_stack': 59496,\n",
       " '@pranumars': 4398434,\n",
       " '@katie_n_kaden': 1889306,\n",
       " '@artjourney': 2557339,\n",
       " '@kelliott4real': 4004381,\n",
       " '#kelena': 4849987,\n",
       " '@brittneycacharr': 1639318,\n",
       " '@fabianeloy_': 1623463,\n",
       " '@niallisabeaut20': 3810719,\n",
       " '@luv2dancexoxo': 225323,\n",
       " '@angelinacarochi': 926762,\n",
       " '@_samueldylan': 2714806,\n",
       " '#kissoff': 1303560,\n",
       " '@lukesbiebah': 2027148,\n",
       " '@pigeonboi': 118745,\n",
       " '@hunty_23': 3575260,\n",
       " '@abykhasby': 5658288,\n",
       " '@nalnoubee': 1172360,\n",
       " '@angelachewhs': 4795355,\n",
       " '@npa34': 2887216,\n",
       " '@t4keziall': 3963127,\n",
       " 'panggi': 2541614,\n",
       " '@jonathonio': 432226,\n",
       " '@jakedoganality': 1075921,\n",
       " '@eduardoalp_': 606199,\n",
       " '@hardyswagg4': 892234,\n",
       " '@arinsofia': 2955522,\n",
       " '@imsydirgibson': 5922451,\n",
       " 'benyedder': 5242231,\n",
       " '@kativarian': 475020,\n",
       " '@grace_brettle': 1139951,\n",
       " '@rizal_riyadi': 5533683,\n",
       " 'company': 6107126,\n",
       " 'models': 6110025,\n",
       " '@ali_topnotch': 1839120,\n",
       " '@leethuggin_': 704360,\n",
       " '@covochu': 153993,\n",
       " 'projecy': 956056,\n",
       " 'h0nest': 975285,\n",
       " '@jasmineiida': 5060653,\n",
       " '@zach_keith_': 3287465,\n",
       " '@royce_payne': 260086,\n",
       " '#michealjackaon': 1121587,\n",
       " '@socrazyxo_': 2368066,\n",
       " '@tw0cansam': 4555293,\n",
       " '@bougie__monee': 1277934,\n",
       " '@likethe_movie': 3003434,\n",
       " '@miyeah_': 5792307,\n",
       " '@adam__stratton': 1714180,\n",
       " '@marcus24sk': 6057695,\n",
       " '@umthat_bitch': 2438505,\n",
       " '@fauxoo': 987327,\n",
       " '@iamricosuaavve': 5153336,\n",
       " '@zmrej_': 1657748,\n",
       " '@jenniferwelker': 4925706,\n",
       " 'ersyad': 2788928,\n",
       " '#imsuchafailure': 5826429,\n",
       " '@iam_angimac': 4042757,\n",
       " '@paushinski': 1874377,\n",
       " '@xellabellax': 5083173,\n",
       " 'aikibatto': 5597635,\n",
       " '@ubfootball': 1997324,\n",
       " '@shingkwayla': 4957237,\n",
       " 'baaaaarely': 5468400,\n",
       " '@kmillr21': 1851451,\n",
       " '@murphybieberjb': 5275688,\n",
       " '@ricdodger1': 3910591,\n",
       " '@thatovoxo_x': 2261180,\n",
       " '@yoanieber': 1897544,\n",
       " '@saintnegro27': 2401924,\n",
       " 'kalkma': 5685053,\n",
       " 'mediatemple': 414012,\n",
       " '@owotaiye': 2268743,\n",
       " '@musefanspain': 1560184,\n",
       " '@shortiiejordiii': 3581092,\n",
       " '@lickziam': 5366635,\n",
       " '@cynncynna': 5440871,\n",
       " '@gabgab2016': 4027998,\n",
       " '@erriee_xx': 1492170,\n",
       " '@howtobesick': 1290060,\n",
       " '@ashleigh_hs': 5631265,\n",
       " 'engineers': 6122795,\n",
       " 'rakinah': 3845596,\n",
       " '@deeka_shanov': 4290155,\n",
       " '@jashonxiao': 4467733,\n",
       " '@marsslynn_': 4473775,\n",
       " 'nyseyh': 2283386,\n",
       " '@aye_itsaamandaa': 3916060,\n",
       " '@cpower14': 3550833,\n",
       " '@caseybarnes7': 1874434,\n",
       " '@tintooooy': 3513180,\n",
       " 'pobersito': 5417980,\n",
       " 'non-acoustic': 4613827,\n",
       " '@addit_up': 638896,\n",
       " '@masley_forever': 1371559,\n",
       " 'yourselffffffff': 1445492,\n",
       " '@bradfordjack': 4977900,\n",
       " '@jack_johnson91': 223339,\n",
       " '#happyfamilyday': 127388,\n",
       " '@acrazyrainbow': 3690754,\n",
       " '@sakynayusof': 1179660,\n",
       " '@broaddaywalker': 5144375,\n",
       " '@zavibabess': 4830009,\n",
       " '#especialbelievezonalivrebmbr': 3889941,\n",
       " '@dellajean': 895824,\n",
       " '@booms4c': 481946,\n",
       " '@xosaraxox': 5862715,\n",
       " '@hemmingzsempai': 22691,\n",
       " '@ellienero': 4608456,\n",
       " '@vale_va87': 5287686,\n",
       " '@nochand': 3091541,\n",
       " '@folarinurban_': 854380,\n",
       " \"jum'uah\": 5065930,\n",
       " '@1dlaav': 4886355,\n",
       " '@valadezjazmyn': 2866958,\n",
       " '@hiddenvalley69': 4626188,\n",
       " '@meechydoe': 6024977,\n",
       " '@tino_pham13': 2220857,\n",
       " '@kim_nunziato': 2772796,\n",
       " '@teairraxo': 2873906,\n",
       " '#lovebacklines': 3972865,\n",
       " '@_kaylababiee_': 987205,\n",
       " '@annakaymusic': 2467494,\n",
       " '@melvin5022': 4440061,\n",
       " ...}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6140853, 52)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distant Supervision phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distance_supervised_tweets = pd.read_csv('./data/distant_data/distance_supervised_tweets_corrected', names=[\"id\",\"lan\", \"label\", \"text\"], sep=\"\\t\", header=None, usecols=[\"lan\", \"label\", \"text\"])\n",
    "distance_supervised_tweets_2 = pd.read_csv('./data/distant_data/distance_supervised_tweets_2_corrected', names=[\"id\",\"lan\", \"label\", \"text\"], sep=\"\\t\", header=None, usecols=[\"lan\", \"label\", \"text\"])\n",
    "distance_supervised_tweets_3 = pd.read_csv('./data/distant_data/distance_supervised_tweets_3_corrected', names=[\"id\",\"lan\", \"label\", \"text\"], sep=\"\\t\", header=None, usecols=[\"lan\", \"label\", \"text\"])\n",
    "distance_supervised_tweets = distance_supervised_tweets.append(distance_supervised_tweets_2).append(distance_supervised_tweets_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lan</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>Boston Bruins Morning Thoughtefense Exceeding ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>Bol Bachchan!. #AeZindagiGaleLagale. Aata Majh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>(17) karena lagi sakit, aku lagi gelisah terus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>Telkomsel: Thary422 Terima kasih telah berpart...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>@parisa_khania آخر سر هم از جزوه ی محترم عکس گ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>kyristcl: XL123: frungnarikvnn Bisa ajak selai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>Telkomsel: daff_01 Terima kasih telah berparti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>@chris_randall Just the usual disclaimer that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>@Lin_Manuel Congrats from me and all my friend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>@nicaaaji hahahaha magtext ako beb. \"Hi lola! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>@emergiTEL launches their new website.  #Emegi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>Okulda iki kere üst üste kitli kalıp camdan at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>New Lyft Users get 10 free Lyft rides &amp;lt;&amp;lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>I want Thai food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>@LionArts we are checking with our music searc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>I actually lover Anna Faris https://t.co/sUAG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>latest podcast from Chop Suey Press The Battle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>MAJufri3 Terima kasih telah berpartisipasi. Ku...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>Telkomsel: fadilanurul955 Terima kasih telah b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>@AwaisAhmad718 thanks  ab theek hai medicine l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>Telkomsel: khai_dier Jika keluhan baru hari in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>@taykecare Hi! Saw you follow music and think ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>Telkomsel: InnekeVermarien Pagi, Mbak Inneke. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>@nreatherford Sounds good to us! We are always...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>#ملتقي_لحن_للابداع #قروب_طويق_للدعم تبون ريتويت</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>Yung feeling na di nyo hinahayaan matapos ang ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>Hello baby jane (Babeeh_Jane)  https://t.co/LC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>@kevinthewhippet @iggiesrule89 @bunniemommie i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>هام تم تعديل مسار خط باص ميسلون - جامعة بحيث ي...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>Helloo.  Alyssa (TheJoltaire) https://t.co/25L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160752</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>ANG CUTE CUTE CUTE MO  https:// twitter.com/Gi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160753</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>I’m willing to bet it’s a partial tear. Idk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160754</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>この笑顔はキュン死もの_´ཀ`」 ∠):_ pic.twitter.com/5iBTOampMh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160755</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>really? for real?  https:// twitter.com/readef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160756</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>え？_´ཀ`」 ∠): 良いもん食ってんのな https:// twitter.com/mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160757</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>And still beating us....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160758</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>Kolaborasi kuy  https:// twitter.com/amandasya...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160759</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>captain crunch..  https:// twitter.com/CAMILAH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160760</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>people should start selling fish rice or prawn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160761</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>http://htSomething needs to be done to prevent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160762</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>http://htSomething needs to be done to prevent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160763</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>http://htSomething needs to be done to prevent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160764</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>http://htSomething needs to be done to prevent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160765</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>http://htSomething needs to be done to prevent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160766</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>http://htSomething needs to be done to prevent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160767</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>http://htSomething needs to be done to prevent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160768</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>@Mbrower42 HAPPY BIRTHDAY, i love you mace!!!!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160769</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>http://htSomething needs to be done to prevent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160770</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>http://htSomething needs to be done to prevent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160771</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>https:// twitter.com/NFL/status/928 834503887...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160772</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>I'm so hungry but I'm so done eating goldfish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160773</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>i'm always stuck between wanting to do somethi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160774</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>Please sign &amp; share! {|} http:// fb.me/9qsBceTD1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160775</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>no b you deserve to be happy  don’t say that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160776</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>Kaylee is deadass sobbing her eyes out over ft...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160777</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>http://htSomething needs to be done to prevent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160778</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>Ugh. @DangeRussWilson 's getting no coverage! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160779</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>http://htSomething needs to be done to prevent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160780</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>http://htSomething needs to be done to prevent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160781</th>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>i want to eat hamburger and chicken</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>203512 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       lan     label                                               text\n",
       "0       en  positive  Boston Bruins Morning Thoughtefense Exceeding ...\n",
       "1       en  positive  Bol Bachchan!. #AeZindagiGaleLagale. Aata Majh...\n",
       "2       en  negative  (17) karena lagi sakit, aku lagi gelisah terus...\n",
       "3       en  positive  Telkomsel: Thary422 Terima kasih telah berpart...\n",
       "4       en  positive  @parisa_khania آخر سر هم از جزوه ی محترم عکس گ...\n",
       "5       en  positive  kyristcl: XL123: frungnarikvnn Bisa ajak selai...\n",
       "6       en  positive  Telkomsel: daff_01 Terima kasih telah berparti...\n",
       "7       en  positive  @chris_randall Just the usual disclaimer that ...\n",
       "8       en  positive  @Lin_Manuel Congrats from me and all my friend...\n",
       "9       en  negative  @nicaaaji hahahaha magtext ako beb. \"Hi lola! ...\n",
       "10      en  positive  @emergiTEL launches their new website.  #Emegi...\n",
       "11      en  negative  Okulda iki kere üst üste kitli kalıp camdan at...\n",
       "12      en  positive  New Lyft Users get 10 free Lyft rides &lt;&lt;...\n",
       "13      en  negative                                  I want Thai food \n",
       "14      en  positive  @LionArts we are checking with our music searc...\n",
       "15      en  negative   I actually lover Anna Faris https://t.co/sUAG...\n",
       "16      en  positive  latest podcast from Chop Suey Press The Battle...\n",
       "17      en  positive  MAJufri3 Terima kasih telah berpartisipasi. Ku...\n",
       "18      en  positive  Telkomsel: fadilanurul955 Terima kasih telah b...\n",
       "19      en  positive  @AwaisAhmad718 thanks  ab theek hai medicine l...\n",
       "20      en  positive  Telkomsel: khai_dier Jika keluhan baru hari in...\n",
       "21      en  positive  @taykecare Hi! Saw you follow music and think ...\n",
       "22      en  positive  Telkomsel: InnekeVermarien Pagi, Mbak Inneke. ...\n",
       "23      en  positive  @nreatherford Sounds good to us! We are always...\n",
       "24      en  positive   #ملتقي_لحن_للابداع #قروب_طويق_للدعم تبون ريتويت \n",
       "25      en  positive  Yung feeling na di nyo hinahayaan matapos ang ...\n",
       "26      en  positive  Hello baby jane (Babeeh_Jane)  https://t.co/LC...\n",
       "27      en  positive  @kevinthewhippet @iggiesrule89 @bunniemommie i...\n",
       "28      en  negative  هام تم تعديل مسار خط باص ميسلون - جامعة بحيث ي...\n",
       "29      en  positive  Helloo.  Alyssa (TheJoltaire) https://t.co/25L...\n",
       "...     ..       ...                                                ...\n",
       "160752  en  negative  ANG CUTE CUTE CUTE MO  https:// twitter.com/Gi...\n",
       "160753  en  negative       I’m willing to bet it’s a partial tear. Idk \n",
       "160754  en  negative   この笑顔はキュン死もの_´ཀ`」 ∠):_ pic.twitter.com/5iBTOampMh\n",
       "160755  en  negative  really? for real?  https:// twitter.com/readef...\n",
       "160756  en  negative  え？_´ཀ`」 ∠): 良いもん食ってんのな https:// twitter.com/mu...\n",
       "160757  en  negative                           And still beating us....\n",
       "160758  en  negative  Kolaborasi kuy  https:// twitter.com/amandasya...\n",
       "160759  en  negative  captain crunch..  https:// twitter.com/CAMILAH...\n",
       "160760  en  negative  people should start selling fish rice or prawn...\n",
       "160761  en  negative  http://htSomething needs to be done to prevent...\n",
       "160762  en  negative  http://htSomething needs to be done to prevent...\n",
       "160763  en  negative  http://htSomething needs to be done to prevent...\n",
       "160764  en  negative  http://htSomething needs to be done to prevent...\n",
       "160765  en  negative  http://htSomething needs to be done to prevent...\n",
       "160766  en  negative  http://htSomething needs to be done to prevent...\n",
       "160767  en  negative  http://htSomething needs to be done to prevent...\n",
       "160768  en  negative  @Mbrower42 HAPPY BIRTHDAY, i love you mace!!!!...\n",
       "160769  en  negative  http://htSomething needs to be done to prevent...\n",
       "160770  en  negative  http://htSomething needs to be done to prevent...\n",
       "160771  en  negative   https:// twitter.com/NFL/status/928 834503887...\n",
       "160772  en  negative     I'm so hungry but I'm so done eating goldfish \n",
       "160773  en  negative  i'm always stuck between wanting to do somethi...\n",
       "160774  en  negative   Please sign & share! {|} http:// fb.me/9qsBceTD1\n",
       "160775  en  negative       no b you deserve to be happy  don’t say that\n",
       "160776  en  negative  Kaylee is deadass sobbing her eyes out over ft...\n",
       "160777  en  negative  http://htSomething needs to be done to prevent...\n",
       "160778  en  negative  Ugh. @DangeRussWilson 's getting no coverage! ...\n",
       "160779  en  negative  http://htSomething needs to be done to prevent...\n",
       "160780  en  negative  http://htSomething needs to be done to prevent...\n",
       "160781  en  negative               i want to eat hamburger and chicken \n",
       "\n",
       "[203512 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_supervised_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Test split: 183161/20351\n"
     ]
    }
   ],
   "source": [
    "x_train_distance, y_train, x_test_distance, y_test = transform_data_and_labels(distance_supervised_tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20351\n",
      "2063\n",
      "['people', 'come', 'and', 'people', 'go', '...', \"that's\", 'life', '...', '#aldubmistakenidentity', '33877', 'en', 'positive', \"michiganon't\", 'sell', 'USRTOK', '100m', 'gallons', 'of', 'groundwater', 'for', '$', '200', 'and', '20', 'jobs', '.', \"that's\", 'bananas', '.', 'URLTOK', '33878', 'en', 'positive', 'jio', 'has', 'touched', 'the', 'hearts', 'of', '50', 'millions', 'users', 'with', 'their', 'network', 'really', 'happy', 'for', 'jio', '!', '#jio50million', '33879', 'en', 'positive', 'every', 'second', ',', 'minute', '&', 'hour', 'of', 'our', 'life', 'must', 'be', 'filled', 'with', 'passion', ',', 'dedication', 'and', 'restlessness', 'to', 'make', 'it', 'the', 'best', 'possible', 'life', 'ever', '.', '33880', 'en', 'positive', 'USRTOK', 'USRTOK', 'USRTOK', 'whys', 'this', 'yous', '33881', 'en', 'positive', 'thanks', 'for', 'the', 'recent', 'follow', 'USRTOK', 'USRTOK', 'USRTOK', 'happy', 'to', 'connect', 'have', 'a', 'great', 'tuesday', '.', '>', '>', 'URLTOK', '33882', 'en', 'positive', 'done', 'putting', 'in', 'effort', 'into', 'relationships', 'w', 'people', 'when', 'i', \"don't\", 'get', 'any', 'effort', 'back', ':', '33883', 'en', 'positive', 'USRTOK', 'because', 'of', 'i', 'will.put', 'none', 'option', 'then', 'most', 'of', 'people', 'will', 'vote', 'for', 'none', 'so', 'i', 'wanna', 'see', 'which', '1', 'of', 'them', 'both', 'r', 'popular', '..', '33884', 'en', 'negative', 'i', 'miss', 'my', 'phone', '.', '33885', 'en', 'negative', 'USRTOK', 'ahh', 'thank', 'you', 'ky', 'xoxo', '❣', '️', 'miss', 'you', 'too', '33886', 'en', 'positive', 'conflictenied', 'ops', '(', 'sony', 'playstation', '3', ',', '2008', ')', 'URLTOK', 'URLTOK', '33887', 'en', 'negative', 'lama', 'ndk', 'chat', 'bf', '33888', 'en', 'positive', 'scorching', 'hot', 'services', 'stocks', 'tapeish', 'network', 'corp', '.', '(', 'dish', ')', ',', '...', 'URLTOK', '#dishnetwork', '33889', 'en', 'positive', 'USRTOK', 'of', 'course', 'yeah', '.', 'thanks', 'man', '33890', 'en', 'negative', 'it', 'feels', 'like', 'the', 'world', \"doesn't\", 'want', 'u', 'just', 'because', 'u', 'have', 'this', 'pimples', 'in', 'your', 'face', '33891', 'en', 'positive', 'my', 'fave', 'lm', 'alcott', 'is', 'not', 'little', 'women', 'nor', \"jo's\", 'boys', 'nor', 'little', 'men', 'something', 'simpler', 'and', 'more', 'relatable', 'for', 'me', '33892', 'en', 'negative', 'sleepy', 'na', 'me', '33893', 'en', 'positive', 'jacket', 'reseller', 'very', 'wellcome', '-', 'size', 'm', 'fit', 'l', 'bbm', ':', 'pin', '37314c7', 'lineUSRTOK', ':', 'USRTOK', '(', 'pake', 'USRTOK', 'URLTOK', '33894', 'en', 'positive', 'thank', 'you', 'USRTOK', ':', '*', 'happy', 'birthday', 'kay', 'bby', 'brother', 'mo', '33895', 'en', 'negative', 'meis', 'home', ')', 'megoes', 'to', 'my', 'mom', ')', 'hi', 'mom', 'my', 'brothercomes', 'running', 'after', 'me', ')', 'she', 'met', 'her', 'crush', 'wonho', '33896', 'en', 'positive', '4days', 'drunk', 'saraaaapp', '\\\\', 'ud83d', '\\\\', 'ude', '02', '33897', 'en', 'positive', 'kinda', 'weird', 'but', 'still', 'cute', '33898', 'en', 'negative', \"didn't\", 'even', 'get', 'an', 'interview', 'for', 'ta', 'job', '.', 'apparently', 'i', \"don't\", 'have', 'the', 'skills', 'or', 'the', 'experience', 'i', 'guess', 'my', 'science', 'degree', 'means', 'nothing', '33899', 'en', 'positive', 'thanks', 'for', 'the', 'recent', 'follow', 'USRTOK', 'USRTOK', 'USRTOK', 'happy', 'to', 'connect', 'have', 'a', 'great', 'tuesday', '.', '>', '>', 'get', 'free', '?', 'URLTOK', '33900', 'en', 'positive', 'opiniononald', 'trump', 'and', 'the', 'nuclear', 'danger', 'URLTOK', 'URLTOK', '33901', 'en', 'negative', 'USRTOK', 'i', 'miss', 'you', 'too', 'po', 'ate', '33902', 'en', 'positive', 'USRTOK', 'so', 'true', 'but', 'the', 'beautiful', 'part', ';', 'for', 'ones', 'own', 'admonition', 'it', 'is', 'that', 'reality', 'only', 'confirms', 'the', 'scriptures', '.', '33903', 'en', 'positive', 'USRTOK', 'USRTOK', '4', 'bio', '5', 'icon', '5', 'layout', '5', '–', 'i', 'really', 'like', 'it', 'recent', 'tweets', '4', 'would', 'i', 'follow', ':', 'no', ',', 'but', 'good', 'acc', '33904', 'en', 'negative', 'USRTOK', 'tfw', 'you', \"can't\", 'see', 'your', 'feet', '33905', 'en', 'positive', 'USRTOK', 'ikr', '.', 'that', 'eyes', 'smile', '33906', 'en', 'positive', 'USRTOK', '3t', 'is', 'the', 'better', 'buy', '33907', 'en', 'positive', '#givingtuesdayca', 'is', 'under', 'way', 'at', 'go', 'stations', 'around', 'the', 'city', '!', 'be', 'sure', 'to', 'grab', 'a', '$', '2', 'scone', 'on', 'your', 'way', 'to', 'work', 'this', 'morning', '33908', 'en', 'positive', 'USRTOK', 'wonderful', '!', 'thanks', 'for', 'joining', 'the', '#skypeathon', '.', 'was', 'your', 'call', 'booked', 'through', 'the', 'site', '?', 'we', 'want', 'to', 'make', 'sure', 'your', 'miles', 'count', '!', '33909', 'en', 'positive', 'USRTOK', 'just', 'got', 'off', 'my', 'flight', 'to', 'munich', '33910', 'en', 'positive', 'USRTOK', 'advance', 'happy', 'birthday', 'maria', '!', 'love', 'you', 'queen', 'alwaysalexa', 'bukasna', '33911', 'en', 'negative', 'why', 'are', 'these', 'garbage', 'trucks', 'so', 'loud', '33912', 'en', 'positive', 'a', 'case', 'study', 'in', 'poor', 'portfolio', 'risk', 'decisionsallas', 'police', '&', 'fire', 'pension', '.', 'will', 'pension', 'boards', '/', 'advisors', 'learn', '?', 'URLTOK', '33913', 'en', 'negative', 'USRTOK', 'still', 'the', 'bully', 'huhu', 'thank', 'you', 'mom', '!', '!', 'i', 'love', 'youuu', '♡', '33914', 'en', 'positive', 'USRTOK', 'thanks', 'for', 'the', 'follow', '33915', 'en', 'positive', 'i', 'have', 'to', 'drop', 'a', 'class', 'asan', 'ang', 'sistema', 'isd', '.', 'asan', 'na', '.', 'paki', 'hanap', 'please', '.', '33916', 'en', 'positive', 'USRTOK', 'awesome', '-', 'i', 'wish', 'my', 'dogs', 'would', 'sleep', 'close', 'together', 'like', 'that', '33917', 'en', 'negative', 'USRTOK', 'o', 'omygod', 'i', 'love', 'you', 'i', 'love', 'va', 'i', 'love', 'art', 'yay', '33918', 'en', 'positive', 'USRTOK', 'black', '33919', 'en', 'positive', 'help', 'a', 'sister', 'out', '!', 'i', 'could', 'win', '$', '500', 'from', 'unbound', 'for', 'new', 'lingerie', 'URLTOK', '33920', 'en', 'positive', '#thanks', 'USRTOK', 'USRTOK', 'thanks', 'for', 'the', 'recent', 'follow', '.', 'much', 'appreciated', '>', '>', 'want', 'this', '?', \"it's\", 'free', '!', 'URLTOK', '33921', 'en', 'positive', 'USRTOK', 'and', 'USRTOK', 'support', 'for', 'echo', 'soon', '33922', 'en', 'positive', 'love', '#thegiftyougive', 'to', 'the', 'world', '33923', 'en', 'positive', 'new', 'videof', 'network', '-', 'extra', 'beautiful', 'fitness', 'milf', 'bangs', 'huge', 'cock', '#nsfw', '#xxx', '#sex', '#tube', '#porn', '33924', 'en', 'positive', 'not', 'a', 'bad', 'start', 'a', 'nice', '50/50', 'on', \"today's\", 'clients', 'wing', '#autocleandetailing', '…', 'URLTOK', '33925', 'en', 'positive', 'USRTOK', 'let', 'us', 'know', 'if', 'we', 'can', 'help', 'answer', 'any', 'of', 'your', 'plumbing', 'questions', '!', 'URLTOK', '33926', 'en', 'positive', 'bostonecember', 'is', 'almost', 'here', '.', 'make', 'the', 'most', 'of', 'it', 'and', 'get', 'to', 'one', '(', 'or', 'more', ')', 'of', 'these', 'events', 'around', 'boston', '.', '…', 'URLTOK', '33927', 'en', 'positive', 'good', 'morning', 'people', '!', '!', 'have', 'a', 'lovely', 'day', '#twitter', '#wedding', '#bride', '33928', 'en', 'positive', 'thanks', 'for', 'the', 'recent', 'follow', 'USRTOK', 'USRTOK', 'happy', 'to', 'connect', 'have', 'a', 'great', 'tuesday', '.', '(', 'want', 'this', '\\\\', 'ud83c', '\\\\', 'udd', '93', '?', '>', '>', 'URLTOK', '33929', 'en', 'positive', 'USRTOK', 'oh', 'hey', ',', 'after', 'seeing', 'the', 'picture', 'again', 'i', 'see', 'why', 'you', 'like', 'post-its', 'so', 'much', '.', 'make', 'a', 'breakfast', 'pizza', '!', 'then', 'u', \"won't\", 'b', 'hungry', '.', '33930', 'en', 'negative', 'USRTOK', 'then', 'on', 'his', 'drive', 'into', 'work', 'he', 'hit', 'a', 'big', 'deer', 'with', 'his', 'car', '.', 'let', 'the', 'christmas', 'season', 'celebrations', 'begin', '.', '33931', 'en', 'negative', 'USRTOK', 'its', 'worse', 'when', 'its', '30', 'and', 'over', '33932', 'en', 'positive', 'USRTOK', 'i', \"ain't\", 'actually', 'selling', 'dvds', 'man', 'x', ')', 'just', 'coding', 'a', 'website', 'capable', 'of', 'such', 'things', '.', 'finished', 'the', 'whole', 'registration', 'process', '33933', 'en', 'positive', 'child', 'i', 'can', 'do', 'everything', 'URLTOK', '33934', 'en', 'positive', 'son', 'has', 'no', 'legal', 'right', 'in', 'parents', '’', 'house', ',', 'can', 'stay', 'at', 'their', 'mercyelhi', '...', 'URLTOK', 'by', '#jantakareporter', 'via', 'USRTOK', '33935', 'en', 'positive', 'my', 'face', 'is', 'so', 'sunburn', '33936', 'en', 'positive', 'happy', 'sweet', 'sixteen', 'USRTOK', '\\\\', 'ud83d', '\\\\', 'ude', '1b', '\\\\', 'ud83c', '\\\\', 'udf', '89', '\\\\', 'ud83c', '\\\\', 'udf', '81i', 'hope', 'you', 'have', 'a', 'great', 'day', '33937', 'en', 'positive', 'hii', '.', 'a', '$', 'm', '(', 'youngam', '904', ')', 'URLTOK', '33938', 'en', 'positive', 'thanks', 'for', 'the', 'recent', 'follow', 'USRTOK', 'USRTOK', 'USRTOK', 'happy', 'to', 'connect', 'have', 'a', 'great', 'this', 'week', '33939', 'en', 'positive', 'watch', 'lesbian', 'movieisrobe', ',', 'lesson', 'about', 'to', 'commence', '▶', 'URLTOK', 'URLTOK', '33940', 'en', 'positive', 'URLTOK', 'pls', 'retweet', 'and', 'like', 'if', 'you', 'like', 'video', '33941', 'en', 'positive', 'USRTOK', 'wow', ',', 'how', '?', '33942', 'en', 'positive', 'pathfinder', 'battleseadly', 'foes', 'minis', 'now', 'available', 'URLTOK', '#tabletop', 'URLTOK', '33943', 'en', 'positive', 'this', 'is', 'the', 'day', 'i', 'have', 'been', 'waiting', 'for', '...', 'my', 'last', 'day', 'of', 'unemployment', '!', '!', '!', '33944', 'en', 'negative', 'USRTOK', '12:36', '12', 'from', 'parkway', 'centre', 'to', 'middlesbrough', 'never', 'turned', 'up', ',', 'had', 'to', 'wait', '10', 'minutes', 'when', 'i', 'had', 'a', 'train', 'to', 'catch', '33945', 'en', 'positive', 'USRTOK', 'thank', 'youuuu', '.', '33946', 'en', 'positive', 'we', 'are', 'hiring', 'a', '#marketingmanager', '!', 'ping', 'us', 'if', 'you', 'think', 'you', 'have', 'what', 'it', 'takes', 'URLTOK', '#ai', '#bots', '#marketing', '#jobs', '\\\\', 'ud83d', '\\\\', 'ude', '4c', '33947', 'en', 'positive', 'i', 'love', 'this', '#watch', '⌚', '️', 'dropped', 'hours', ',', 'grey', '#twt247', '\\\\', 'ud83d', '\\\\', 'udecd', '➡', '️', 'URLTOK', 'hours', ',', 'grey', 'URLTOK', '#gift', '#watch', '33948', 'en', 'positive', 'injuriesanny', 'trevathan', 'done', 'for', 'remainder', 'of', 'year', 'URLTOK', '33949', 'en', 'positive', 'german', 'union', 'leader', 'says', 'will', 'fight', 'for', 'plants', 'in', 'tata-thyssen', 'mergeruesseldorf', ',', 'germany', '…', 'URLTOK', '#news', '#reuters', '33950', 'en', 'positive', 'hello', '..', 'everyone', 'shut', 'up', '(', 'fatierza', ')', 'URLTOK', '33951', 'en', 'positive', 'im', 'not', 'ready', 'URLTOK', '33952', 'en', 'negative', 'am', 'i', 'going', 'to', 'have', 'to', 'turn', 'this', 'alarm', 'back', 'on', 'again', ':/', 'URLTOK', '33953', 'en', 'positive', '#ooc', \"you're\", 'welcome', '.', 'URLTOK', '33954', 'en', 'negative', 'USRTOK', 'USRTOK', 'district', 'in', 'aleppo', '33955', 'en', 'positive', 'thats', 'a', 'cool', 'shirt', '!', 'USRTOK', 'URLTOK', '33956', 'en', 'positive', 'i', 'love', 'waking', 'up', 'every', '20', 'minutes', 'throughout', 'the', 'night', '33957', 'en', 'positive', 'wonder', 'if', 'he', 'will', 'be', 'checking', 'with', 'his', 'mates', 'in', 'the', 'farc', 'rebels', '?', 'URLTOK', '33958', 'en', 'positive', 'state', 'to', 'implement', 'therapy', 'cuts', 'for', 'disabled', 'children', 'next', 'month', '-', 'houston', 'chronicleallas', '…', 'URLTOK', '#handicapped', '#love', '33959', 'en', 'negative', 'i', 'want', 'to', 'make', 'art', 'that', 'is', 'not', 'related', 'to', 'school', 'but', 'no', 'time', 'em', 'sad', '33960', 'en', 'negative', 'USRTOK', 'i', 'coukd', 'spend', 'all', 'my', 'day', 'at', 'home', ',', 'resting', 'and', 'still', 'feel', 'tired', '33961', 'en', 'positive', '“', 'opec', 'could', 'lose', 'market', 'share', 'in', 'a', 'world', 'awash', 'in', 'oilon', 'pittis', '”', 'URLTOK', '#energy', '#oil', '33962', 'en', 'positive', 'i', 'just', 'realized', 'that', \"sony's\", 'ffxv-themed', 'headphones', 'are', 'perfect', 'match', 'for', 'the', 'luna', 'edition', 'ps4', '!', '^', '_', '^', 'i', 'hope', 'it', 'comes', 'out', 'in', 'our', 'country', '!', '33963', 'en', 'negative', 'USRTOK', 'maybe', 'it', 'will', 'actually', 'let', 'me', 'post', ',', 'unlike', 'the', 'actual', 'twitter', 'client', '.', '33964', 'en', 'positive', 'love', 'it', 'mayward', 'twogetherinmasbate', 'URLTOK', '33965', 'en', 'positive', 'someone', 'pls', 'shoot', 'me', 'before', 'i', 'have', 'to', 'work', 'this', '6', 'hour', 'cash', 'shift', '33966', 'en', 'positive', 'manrepellerid', 'someone', 'say', 'pink', 'man', 'repeller', 'hats', '!', '?', 'URLTOK', 'URLTOK', '33967', 'en', 'negative', 'shooting', 'tomorrow', '!', '!', '33968', 'en', 'positive', 'USRTOK', 'fight', 'amongst', 'themselves', 'until', 'their', 'enemies', 'get', 'bored', 'of', 'killing', 'the', 'incompetent', 'mess', '?', 'sounds', 'like', 'skaven', 'business', 'as', 'usual', '33969', 'en', 'positive', 'tell', 'him', 'happy', 'birthday', 'too', 'bro', 'and', 'how', 'old', 'is', 'he', 'now', '?', 'URLTOK', '33970', 'en', 'positive', 'i', 'liked', 'a', 'USRTOK', 'video', 'from', 'USRTOK', 'URLTOK', 'quién', 'besa', 'mejor', '?', '-', 'con', 'sayoyyi', 'most', 'likely', 'to', 'tag', '33971', 'en', 'positive', 'USRTOK', 'sure', '!', '!', \"i'll\", 'try', 'to', 'get', 'to', 'it', 'this', 'week', '/', 'maybe', 'weekend', '33972', 'en', 'negative', 'sooooooo', 'USRTOK', 'and', 'i', 'probably', \"can't\", 'get', 'wifi', '/', 'phone', 'plans', 'until', 'after', 'the', 'new', 'year', 'lol', 'rip', 'i', 'hate', 'everything', '33973', 'en', 'positive', 'USRTOK', 'that', 'last', 'photo', 'thank', 'you', 'ave', 'but', 'i', 'love', 'you', 'so', 'much', '!', '!', '\\\\', 'ud83d', '\\\\', 'ude', '0b', '\\\\', 'ud83d', '\\\\', 'ude', '18', '33974', 'en', 'negative', 'fantastic', 'beasts', 'was', 'so', 'good', \"can't\", 'help', 'but', 'cry', 'whilst', 'watching', 'because', 'jk', \"rowling's\", 'imagination', 'is', 'overwhelming', '33975', 'en', 'positive', 'done', '33976', 'en', 'positive', 'thanks', 'for', 'the', 'recent', 'follow', 'USRTOK', 'USRTOK', 'USRTOK', 'happy', 'to', 'connect', 'have', 'a', 'great', 'tuesday', '.', '>', '>', 'URLTOK', '33977', 'en', 'positive', '#9eco', 'brothers', 'pan', 'organizer', 'rack', ',', 'bronzeeco', 'brothers', 'pan', 'organizer', 'rack', ',', 'bronze', 'by', 'deco', 'brothers', '…', 'URLTOK', '#storage', '33978', 'en', 'negative', 'i', 'thought', 'the', 'feeling', 'is', 'mutual', '33979', 'en', 'positive', 'USRTOK', 'the', 'reaper', 'is', 'your', 'friend', '33980', 'en', 'negative', 'had', '200-400', 'viewers', 'constantly', 'the', 'last', '3', 'days', 'i', 'best', 'get', 'a', 'sub', 'button', 'soon', '33981', 'en', 'negative', 'so', 'sad', 'URLTOK', '33982', 'en', 'positive', 'USRTOK', 'freudian', 'mishear', '33983', 'en', 'positive', 'USRTOK', 'USRTOK', '-', 'translation', '...', 'i', 'want', 'you', 'to', 'do', 'the', 'research', 'for', 'me', '...', 'liberalism', 'means', 'knowing', 'even', 'when', 'you', \"don't\", 'know', '33984', 'en', 'positive', 'how', 'cute', 'is', 'growlithe', '!', '!', '!', 'got', 'this', 'nice', 'little', 'poke', 'haul', 'at', 'the', 'kyoto', 'pokemon', 'centre', 'today', '#pokemon', '…', 'URLTOK', '33985', 'en', 'positive', 'thanks', 'for', 'the', 'recent', 'follow', 'USRTOK', 'USRTOK', 'happy', 'to', 'connect', 'have', 'a', 'great', 'tuesday', '.', '>', '>', 'want', 'this', '\\\\', 'ud83c', '\\\\', 'udd', '93', '?', 'URLTOK', '33986', 'en', 'positive', 'its', '6am', ',', 'my', 'classes', 'ends', 'at', '6pm', 'and', 'im', 'already', 'feeling', 'dead', 'inside', '33987', 'en', 'positive', 'stats', 'for', 'the', 'day', 'have', 'arrived', '.', '2', 'new', 'followers', 'and', 'no', 'unfollowers', 'via', 'URLTOK', '33988', 'en', 'negative', 'USRTOK', \"it's\", 'time', 'for', 'kamilah', 'and', 'helen', 'to', 'start', 'working', 'together', 'w', '/', 'o', 'kane', 'please', '33989', 'en', 'positive', 'this', 'is', 'a', 'plan', 'i', 'can', 'support', '.', 'URLTOK', '33990', 'en', 'positive', 'all', 'clean', 'new', 'brakes', 'just', 'wait', 'for', 'new', 'ignition', 'now', 'URLTOK', '33991', 'en', 'negative', 'USRTOK', 'thank', 'you', 'for', 'your', 'words', ',', 'they', 'will', 'never', 'be', 'forgotten', '33992', 'en', 'positive', 'popular', 'on', '500px', 'aily', 'shot', ':', 'the', 'first', 'flood', 'by', 'sergebcd', 'URLTOK', '33993', 'en', 'positive', \"that's\", 'all', 'we', 'need', '!', '#sold', 'URLTOK', '33994', 'en', 'positive', 'USRTOK', 'USRTOK', 'absolutely', '33995', 'en', 'positive', 'USRTOK', 'btw', ',', 'congratulations', 'aisha', '...', 'transition', 'from', 'neem', 'daacter', 'tou', 'complete', 'doctor', 'mubaarak', '.', 'may', 'you', 'get', 'more', 'success', '.', '33996', 'en', 'negative', 'USRTOK', 'USRTOK', '..', 'horrible', '33997', 'en', 'positive', 'rt', 'nythealthespite', 'the', 'problems', 'nightmares', 'can', 'cause', ',', 'sleeping', 'and', 'having', 'them', 'is', 'better', 'than', 'not', 'sleeping', 'URLTOK']\n"
     ]
    }
   ],
   "source": [
    "# TODO Load distant-supervised data\n",
    "# train it with two-layer CNN model\n",
    "# pass the weight to next two layer CNN model\n",
    "print(len(x_test_sentence))\n",
    "x_train_distance, x_test_distance = process_tweet(x_train_distance, x_test_distance, final_embeddings.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To save memory\n",
    "# del x_train_sentence\n",
    "# del x_test_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 2063)\n",
      "(?, 2063, 52, 1)\n",
      "CNN filter (4, 52, 1, 200)\n",
      "CNN filter (3, 1, 200, 200)\n",
      "(?, 1027, 1, 200)\n",
      "h_pool_flat (?, 200)\n",
      "scores (?, 3)\n",
      "predictions (?,)\n",
      "losses (?,)\n"
     ]
    }
   ],
   "source": [
    "# put Word2Vec on 590 million English Tweets using 52 dimensions.\n",
    "vocabulary_size = final_embeddings.shape[0]\n",
    "embedding_size = final_embeddings.shape[1]  # Dimension of the embedding vector.\n",
    "graph = tf.Graph()\n",
    "\n",
    "sequence_length=x_train_distance.shape[1]\n",
    "num_classes=3\n",
    "\n",
    "# filter_sizes: The number of words we want our convolutional filters to cover. \n",
    "# We will have num_filters for each size specified here. \n",
    "# For example, [3, 4, 5] means that we will have filters that slide over 3, 4 and 5 words respectively, for a total of 3 * num_filters filters.\n",
    "first_filter_sizes = [4]\n",
    "first_pool_window_sizes = [4]\n",
    "first_pool_strides = [2]\n",
    "\n",
    "\n",
    "second_filter_window_sizes = [3]\n",
    "num_filters = 200\n",
    "\n",
    "# No L2 norm\n",
    "l2_reg_lambda=0.0\n",
    "\n",
    "with graph.as_default():\n",
    "    input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "    input_y = tf.placeholder(tf.int64, [None], name=\"input_y\")\n",
    "    with tf.device('/cpu:0'):\n",
    "        embeddings = tf.Variable(tf.constant(0.0, shape=[vocabulary_size, embedding_size]),\n",
    "                        trainable=False, name=\"embedding\")\n",
    "\n",
    "        embedding_placeholder = tf.placeholder(tf.float32, [vocabulary_size, embedding_size], name='word_embedding_placeholder')\n",
    "        embedding_init = embeddings.assign(embedding_placeholder)  # assign exist word embeddings\n",
    "\n",
    "        embedded_chars = tf.nn.embedding_lookup(embeddings, input_x)\n",
    "    embedded_chars_expanded = tf.expand_dims(embedded_chars, -1)\n",
    "    print(input_x.shape)\n",
    "    print(embedded_chars_expanded.shape)\n",
    "    # Keeping track of l2 regularization loss (optional)\n",
    "    l2_loss = tf.constant(0.0)\n",
    "    \n",
    "     # Add variable initializer.\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    \n",
    "    # Create first cnn : a convolution + maxpool layer for each filter size    \n",
    "    # 1st Convolution Layer\n",
    "    for i, first_filter_size in enumerate(first_filter_sizes):\n",
    "        with tf.name_scope(\"conv-maxpool-1\"):\n",
    "            # Convolution Layer\n",
    "            filter_shape = [first_filter_size, embedding_size, 1, num_filters]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "            print(\"CNN filter\", W.shape)\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "            conv = tf.nn.conv2d(\n",
    "                embedded_chars_expanded,\n",
    "                W,\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding=\"VALID\",\n",
    "                name=\"conv\")\n",
    "            # Apply nonlinearity\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "            # Maxpooling over the outputs and transform for next layer, so the \"channel\" of convolution \n",
    "            # will become \"input_width\" for next layer\n",
    "#             pooled = tf.transpose(tf.nn.max_pool(\n",
    "#                 h,\n",
    "#                 ksize=[1, first_pool_window_sizes[i], 1, 1],\n",
    "#                 strides=[1, first_pool_strides[i], 1, 1],\n",
    "#                 padding='VALID',\n",
    "#                 name=\"pool\"), perm=[0, 1, 3, 2])\n",
    "            pooled = tf.nn.max_pool(\n",
    "                h,\n",
    "                ksize=[1, first_pool_window_sizes[i], 1, 1],\n",
    "                strides=[1, first_pool_strides[i], 1, 1],\n",
    "                padding='VALID',\n",
    "                name=\"pool\")\n",
    "#     print(\"conv1\", conv.shape)\n",
    "#     print(\"h1\", h.shape)\n",
    "#     print(\"pooled1\", pooled_1.shape)\n",
    "    \n",
    "    # 2nd Convolutional Layer\n",
    "#     for i, second_filter_size in enumerate(second_filter_window_sizes):\n",
    "#         with tf.name_scope(\"conv-maxpool-2\"):\n",
    "#             # Convolution Layer\n",
    "#             filter_shape = [second_filter_size, num_filters, 1, num_filters]\n",
    "#             W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "#             print(\"CNN filter\", W.shape)\n",
    "#             b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "#             conv = tf.nn.conv2d(\n",
    "#                 pooled,\n",
    "#                 W,\n",
    "#                 strides=[1, 1, 1, 1],\n",
    "#                 padding=\"VALID\",\n",
    "#                 name=\"conv\")\n",
    "#             # Apply nonlinearity\n",
    "#             h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "#             # Maxpooling over the outputs and transform for next layer, so the \"channel\" of convolution \n",
    "#             # will become \"input_width\" for next layer\n",
    "#             pooled = tf.nn.max_pool(\n",
    "#                 h,\n",
    "#                 ksize=[1, h.shape[1], 1, 1],\n",
    "#                 strides=[1, 1, 1, 1],\n",
    "#                 padding='VALID',\n",
    "#                 name=\"pool\")\n",
    "    for i, second_filter_size in enumerate(second_filter_window_sizes):\n",
    "        with tf.name_scope(\"conv-maxpool-2\"):\n",
    "            # Convolution Layer\n",
    "            filter_shape = [second_filter_size, 1, num_filters, num_filters]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "            print(\"CNN filter\", W.shape)\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "            conv = tf.nn.conv2d(\n",
    "                pooled,\n",
    "                W,\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding=\"VALID\",\n",
    "                name=\"conv\")\n",
    "            # Apply nonlinearity\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "            print(h.shape)\n",
    "            # Maxpooling over the outputs and transform for next layer, so the \"channel\" of convolution \n",
    "            # will become \"input_width\" for next layer\n",
    "            pooled = tf.nn.max_pool(\n",
    "                h,\n",
    "                ksize=[1, h.shape[1], 1, 1],\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding='VALID',\n",
    "                name=\"pool\")\n",
    " \n",
    "\n",
    "    h_pool_flat = tf.reshape(pooled, [-1, num_filters])  # flatten pooling layers\n",
    "    print(\"h_pool_flat\", h_pool_flat.shape)\n",
    "    \n",
    "    # Add dropout\n",
    "#     with tf.name_scope(\"dropout\"):\n",
    "#         self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "    # Final (unnormalized) scores and predictions\n",
    "    \n",
    "    # Fully connected hidden layer\n",
    "    with tf.name_scope(\"hidden\"):\n",
    "        with tf.variable_scope(\"hidden\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters, num_filters],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            out = tf.nn.relu(tf.nn.xw_plus_b(h_pool_flat, W, b))\n",
    "        \n",
    "    \n",
    "    with tf.name_scope(\"output\"):\n",
    "        with tf.variable_scope(\"output\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            scores = tf.nn.xw_plus_b(out, W, b, name=\"scores\")\n",
    "            print(\"scores\", scores.shape)\n",
    "            predictions = tf.argmax(scores, 1, name=\"predictions\")\n",
    "            print(\"predictions\", predictions.shape)\n",
    "\n",
    "\n",
    "    # Calculate mean cross-entropy loss\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=scores, labels=input_y)\n",
    "        print(\"losses\", losses.shape)\n",
    "        loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "    # Accuracy\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        correct_predictions = tf.equal(predictions, input_y)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-1/W:0/grad/hist is illegal; using conv-maxpool-1/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-1/W:0/grad/sparsity is illegal; using conv-maxpool-1/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-1/b:0/grad/hist is illegal; using conv-maxpool-1/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-1/b:0/grad/sparsity is illegal; using conv-maxpool-1/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/W:0/grad/hist is illegal; using conv-maxpool-2/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/W:0/grad/sparsity is illegal; using conv-maxpool-2/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/b:0/grad/hist is illegal; using conv-maxpool-2/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/b:0/grad/sparsity is illegal; using conv-maxpool-2/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name hidden/W:0/grad/hist is illegal; using hidden/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name hidden/W:0/grad/sparsity is illegal; using hidden/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name hidden/hidden/b:0/grad/hist is illegal; using hidden/hidden/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name hidden/hidden/b:0/grad/sparsity is illegal; using hidden/hidden/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/W:0/grad/hist is illegal; using output/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/W:0/grad/sparsity is illegal; using output/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/output/b:0/grad/hist is illegal; using output/output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/output/b:0/grad/sparsity is illegal; using output/output/b_0/grad/sparsity instead.\n",
      "Writing to /home/phejimlin/Documents/Machine-learning/Milestone2/runs/1511025828\n",
      "\n",
      "Current epoch:  0\n",
      "Current epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phejimlin/anaconda3/envs/tensorflow_3.5/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-19T01:24:24.238155: step 5, loss 0.469793, acc 0.835938, f1 0.761237\n",
      "2017-11-19T01:24:27.531674: step 10, loss 0.452523, acc 0.832031, f1 0.755747\n",
      "2017-11-19T01:24:30.652735: step 15, loss 0.430573, acc 0.849609, f1 0.780528\n",
      "2017-11-19T01:24:33.748674: step 20, loss 0.458488, acc 0.820312, f1 0.739337\n",
      "2017-11-19T01:24:36.863587: step 25, loss 0.417459, acc 0.863281, f1 0.799938\n",
      "2017-11-19T01:24:39.966193: step 30, loss 0.449762, acc 0.820312, f1 0.746475\n",
      "2017-11-19T01:24:43.064461: step 35, loss 0.436679, acc 0.820312, f1 0.746535\n",
      "2017-11-19T01:24:46.197001: step 40, loss 0.417404, acc 0.835938, f1 0.7684\n",
      "2017-11-19T01:24:49.323127: step 45, loss 0.431263, acc 0.830078, f1 0.753006\n",
      "2017-11-19T01:24:52.442455: step 50, loss 0.437058, acc 0.839844, f1 0.784143\n",
      "\n",
      "Evaluation:\n",
      "loss 0.441994, acc 0.831175, f1 0.754614\n",
      "\n",
      "2017-11-19T01:25:04.992139: step 55, loss 0.509859, acc 0.8125, f1 0.728448\n",
      "2017-11-19T01:25:08.146066: step 60, loss 0.392543, acc 0.851562, f1 0.78703\n",
      "2017-11-19T01:25:11.231423: step 65, loss 0.458524, acc 0.822266, f1 0.788314\n",
      "2017-11-19T01:25:14.332001: step 70, loss 0.451271, acc 0.832031, f1 0.755747\n",
      "2017-11-19T01:25:17.447562: step 75, loss 0.398608, acc 0.861328, f1 0.825268\n",
      "2017-11-19T01:25:20.559693: step 80, loss 0.519241, acc 0.804688, f1 0.7176\n",
      "2017-11-19T01:25:23.664936: step 85, loss 0.405958, acc 0.832031, f1 0.757672\n",
      "2017-11-19T01:25:26.832396: step 90, loss 0.391977, acc 0.833984, f1 0.764012\n",
      "2017-11-19T01:25:29.967168: step 95, loss 0.371594, acc 0.845703, f1 0.791195\n",
      "2017-11-19T01:25:33.050638: step 100, loss 0.394035, acc 0.847656, f1 0.777765\n",
      "\n",
      "Evaluation:\n",
      "loss 0.402584, acc 0.845417, f1 0.818545\n",
      "\n",
      "2017-11-19T01:25:45.146592: step 105, loss 0.438951, acc 0.832031, f1 0.821082\n",
      "2017-11-19T01:25:48.240774: step 110, loss 0.402954, acc 0.820312, f1 0.761205\n",
      "2017-11-19T01:25:51.331852: step 115, loss 0.389513, acc 0.833984, f1 0.763941\n",
      "2017-11-19T01:25:54.470537: step 120, loss 0.389864, acc 0.869141, f1 0.840662\n",
      "2017-11-19T01:25:57.596485: step 125, loss 0.418112, acc 0.824219, f1 0.748574\n",
      "2017-11-19T01:26:00.695224: step 130, loss 0.359836, acc 0.861328, f1 0.831149\n",
      "2017-11-19T01:26:03.819931: step 135, loss 0.437036, acc 0.84375, f1 0.774165\n",
      "2017-11-19T01:26:06.925499: step 140, loss 0.412252, acc 0.818359, f1 0.774373\n",
      "2017-11-19T01:26:10.044285: step 145, loss 0.465313, acc 0.822266, f1 0.834444\n",
      "2017-11-19T01:26:13.132767: step 150, loss 0.342504, acc 0.863281, f1 0.816609\n",
      "\n",
      "Evaluation:\n",
      "loss 0.35978, acc 0.843805, f1 0.790278\n",
      "\n",
      "2017-11-19T01:26:25.270316: step 155, loss 0.380592, acc 0.845703, f1 0.780491\n",
      "2017-11-19T01:26:28.379608: step 160, loss 0.354183, acc 0.857422, f1 0.831893\n",
      "2017-11-19T01:26:31.484771: step 165, loss 0.497307, acc 0.826172, f1 0.747531\n",
      "2017-11-19T01:26:34.580203: step 170, loss 0.370119, acc 0.84375, f1 0.811632\n",
      "2017-11-19T01:26:37.669340: step 175, loss 0.388447, acc 0.859375, f1 0.796292\n",
      "2017-11-19T01:26:40.773160: step 180, loss 0.368326, acc 0.869141, f1 0.84947\n",
      "2017-11-19T01:26:43.885113: step 185, loss 0.439117, acc 0.837891, f1 0.763985\n",
      "2017-11-19T01:26:46.992737: step 190, loss 0.36579, acc 0.855469, f1 0.831524\n",
      "2017-11-19T01:26:50.112853: step 195, loss 0.427087, acc 0.837891, f1 0.767642\n",
      "2017-11-19T01:26:53.196686: step 200, loss 0.361229, acc 0.845703, f1 0.815293\n",
      "\n",
      "Evaluation:\n",
      "loss 0.334219, acc 0.862312, f1 0.833446\n",
      "\n",
      "2017-11-19T01:27:05.268343: step 205, loss 0.451994, acc 0.835938, f1 0.764997\n",
      "2017-11-19T01:27:08.405077: step 210, loss 0.329889, acc 0.865234, f1 0.850149\n",
      "2017-11-19T01:27:11.474295: step 215, loss 0.329765, acc 0.865234, f1 0.814158\n",
      "2017-11-19T01:27:14.623927: step 220, loss 0.3098, acc 0.880859, f1 0.863379\n",
      "2017-11-19T01:27:17.713401: step 225, loss 0.344278, acc 0.845703, f1 0.791962\n",
      "2017-11-19T01:27:20.842571: step 230, loss 0.500812, acc 0.826172, f1 0.747531\n",
      "2017-11-19T01:27:23.979706: step 235, loss 0.309776, acc 0.873047, f1 0.851165\n",
      "2017-11-19T01:27:34.330514: step 240, loss 0.353106, acc 0.853516, f1 0.853071\n",
      "2017-11-19T01:27:37.471144: step 245, loss 0.33566, acc 0.855469, f1 0.80308\n",
      "2017-11-19T01:27:40.617935: step 250, loss 0.304459, acc 0.888672, f1 0.872525\n",
      "\n",
      "Evaluation:\n",
      "loss 0.318333, acc 0.865177, f1 0.835655\n",
      "\n",
      "2017-11-19T01:27:53.052332: step 255, loss 0.323202, acc 0.861328, f1 0.849\n",
      "2017-11-19T01:27:56.250573: step 260, loss 0.474432, acc 0.84375, f1 0.775994\n",
      "2017-11-19T01:27:59.365126: step 265, loss 0.280259, acc 0.880859, f1 0.857755\n",
      "2017-11-19T01:28:02.514261: step 270, loss 0.299916, acc 0.880859, f1 0.876815\n",
      "2017-11-19T01:28:05.641150: step 275, loss 0.471283, acc 0.833984, f1 0.760415\n",
      "2017-11-19T01:28:08.785045: step 280, loss 0.312134, acc 0.867188, f1 0.835325\n",
      "2017-11-19T01:28:11.905063: step 285, loss 0.315908, acc 0.884766, f1 0.877266\n",
      "2017-11-19T01:28:15.011978: step 290, loss 0.32707, acc 0.857422, f1 0.829096\n",
      "2017-11-19T01:28:18.128247: step 295, loss 0.370956, acc 0.849609, f1 0.844255\n",
      "2017-11-19T01:28:21.241643: step 300, loss 0.316203, acc 0.865234, f1 0.823845\n",
      "\n",
      "Evaluation:\n",
      "loss 0.307731, acc 0.877028, f1 0.861232\n",
      "\n",
      "2017-11-19T01:28:33.589587: step 305, loss 0.27371, acc 0.904297, f1 0.888554\n",
      "2017-11-19T01:28:36.728076: step 310, loss 0.311411, acc 0.863281, f1 0.845081\n",
      "2017-11-19T01:28:39.844221: step 315, loss 0.42285, acc 0.833984, f1 0.758491\n",
      "2017-11-19T01:28:42.981334: step 320, loss 0.323879, acc 0.880859, f1 0.882079\n",
      "2017-11-19T01:28:46.136488: step 325, loss 0.293303, acc 0.882812, f1 0.865415\n",
      "2017-11-19T01:28:49.315328: step 330, loss 0.422352, acc 0.810547, f1 0.828001\n",
      "2017-11-19T01:28:52.455146: step 335, loss 0.268412, acc 0.900391, f1 0.882654\n",
      "2017-11-19T01:28:55.556153: step 340, loss 0.306759, acc 0.869141, f1 0.863282\n",
      "2017-11-19T01:28:58.700871: step 345, loss 0.302395, acc 0.875, f1 0.838811\n",
      "2017-11-19T01:29:01.818285: step 350, loss 0.296828, acc 0.875, f1 0.853984\n",
      "\n",
      "Evaluation:\n",
      "loss 0.304668, acc 0.881146, f1 0.875717\n",
      "\n",
      "2017-11-19T01:29:14.056750: step 355, loss 0.312914, acc 0.886719, f1 0.878054\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "num_epochs = 1\n",
    "\n",
    "num_checkpoints = 5\n",
    "print_train_every = 5\n",
    "evaluate_every = 50\n",
    "checkpoint_every = 10000000\n",
    "allow_soft_placement=True\n",
    "log_device_placement=False\n",
    "\n",
    "with graph.as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=allow_soft_placement,\n",
    "      log_device_placement=log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        \n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdadeltaOptimizer(1.0)\n",
    "        grads_and_vars = optimizer.compute_gradients(loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=num_checkpoints)\n",
    "\n",
    "#         # Write vocabulary\n",
    "#         vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              input_x: x_batch,\n",
    "              input_y: y_batch,\n",
    "            }\n",
    "            _, step, summaries, cur_loss, cur_accuracy, y_pred = sess.run(\n",
    "                [train_op, global_step, train_summary_op, loss, accuracy, predictions],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "#             print(y_pred)\n",
    "#             print(y_batch)\n",
    "            if step % print_train_every == 0:\n",
    "                f1 = f1_score(y_batch, y_pred, average = 'weighted')\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}, f1 {:g}\".format(time_str, step, cur_loss, cur_accuracy,\n",
    "                                                                     f1))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              input_x: x_batch,\n",
    "              input_y: y_batch,\n",
    "            }\n",
    "            step, summaries, cur_loss, cur_accuracy, y_pred = sess.run(\n",
    "                [global_step, dev_summary_op, loss, accuracy, predictions],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            f1 = f1_score(y_batch, y_pred, average = 'weighted')\n",
    "            print(\"Test\")\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}, f1 {:g}\".format(time_str, step, cur_loss, cur_accuracy,\n",
    "                                                                    f1))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "        \n",
    "        def dev_step_batch(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              input_x: x_batch,\n",
    "              input_y: y_batch,\n",
    "            }\n",
    "            step, summaries, cur_loss, cur_accuracy, y_pred = sess.run(\n",
    "                [global_step, dev_summary_op, loss, accuracy, predictions],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            f1 = f1_score(y_batch, y_pred, average = 'weighted')\n",
    "#             print(\"{}: step {}, loss {:g}, acc {:g}, f1 {:g}\".format(time_str, step, cur_loss, cur_accuracy,\n",
    "#                                                                     f1))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "            return cur_loss, cur_accuracy, f1\n",
    "        \n",
    "        \n",
    "        sess.run(embedding_init, feed_dict={embedding_placeholder: final_embeddings})\n",
    "        # Generate batches\n",
    "        batches = batch_iter(\n",
    "            list(zip(x_train_distance, y_train)), batch_size, num_epochs)\n",
    "        \n",
    "        batches_test = list(batch_iter(\n",
    "            list(zip(x_test_distance, y_test)), batch_size, 1))\n",
    "        \n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            \n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                total_loss=0\n",
    "                total_f1=0\n",
    "                total_accuracy=0\n",
    "                len_of_batch = int(len(batches_test))\n",
    "                for batch_test in batches_test:\n",
    "                    x_batch_test, y_batch_test = zip(*batch_test)\n",
    "                    cur_loss, cur_accuracy, cur_f1 = dev_step_batch(x_batch_test, y_batch_test, writer=dev_summary_writer)\n",
    "                    total_loss+=cur_loss\n",
    "                    total_accuracy+=cur_accuracy\n",
    "                    total_f1+=cur_f1\n",
    "                print(\"loss {:g}, acc {:g}, f1 {:g}\".format(total_loss/len_of_batch, total_accuracy/len_of_batch, total_f1/len_of_batch))\n",
    "                print(\"\")\n",
    "            if current_step % checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "        final_embeddings = embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.056935  ,  0.038956  ,  0.39726499, ..., -0.108269  ,\n",
       "         0.242364  ,  0.238135  ],\n",
       "       [-0.178036  ,  0.165739  ,  0.29103801, ..., -0.206843  ,\n",
       "         0.133855  ,  0.193499  ],\n",
       "       [-0.27868199, -0.081914  ,  0.222532  , ..., -0.30557701,\n",
       "        -0.094965  ,  0.31291401],\n",
       "       ..., \n",
       "       [-0.86243898, -0.106481  ,  0.77118099, ..., -0.72408199,\n",
       "        -0.133205  ,  1.04446006],\n",
       "       [-0.122979  ,  0.053025  ,  1.02682495, ...,  0.41790599,\n",
       "        -0.55734599,  0.42464599],\n",
       "       [-0.95815903,  0.023787  ,  1.03262699, ...,  0.24498899,\n",
       "        -0.36573601,  0.328704  ]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save embeddings\n",
    "np.save('final_embeddings_52', final_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings from previous work\n",
    "final_embeddings = np.load('./final_embeddings_52.npy')\n",
    "word_dict = {}\n",
    "with open('./data/embed_tweets_en_590M_52D_data/vocabulary_dict_52.pickle', 'rb') as myfile:\n",
    "    word_dict = pickle.load(myfile)\n",
    "# with open('./data/embed_tweets_en_200M_200D/vocabulary.pickle', 'rb') as myfile:\n",
    "#     word_dict = pickle.load(myfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20632\n",
      "53\n",
      "[\"there's\", 'a', 'lot', 'of', 'stupid', '$', 'h', '!', 't', 'out', 'there', ',', 'but', 'polling', 'trump', 'v', 'kanye', 'west', 'may', 'take', 'the', 'cake', '.', 'all', 'i', 'can', 'think', 'to', 'say', 'is', ':', '#', '$', '%', '#', '$', '%', '$', '#', '%', '#', '$', '%', '#', '$', '%', '#', '$', '#', '$', '#', '$', '%']\n"
     ]
    }
   ],
   "source": [
    "#Load label data\n",
    "x_train_sentence, y_train, x_test_sentence, y_test = load_data_and_labels('./data/supervised_data/en_full.tsv.txt', './data/supervised_data/en_test.tsv')\n",
    "print(len(x_test_sentence))\n",
    "x_train, x_test = process_tweet(x_train_sentence, x_test_sentence, final_embeddings.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 53)\n",
      "(?, 53, 52, 1)\n",
      "CNN filter (4, 52, 1, 200)\n",
      "CNN filter (3, 1, 200, 200)\n",
      "(?, 22, 1, 200)\n",
      "h_pool_flat (?, 200)\n",
      "scores (?, 3)\n",
      "predictions (?,)\n",
      "losses (?,)\n"
     ]
    }
   ],
   "source": [
    "# put Word2Vec on 590 million English Tweets using 52 dimensions.\n",
    "vocabulary_size = final_embeddings.shape[0]\n",
    "embedding_size = final_embeddings.shape[1]  # Dimension of the embedding vector.\n",
    "graph = tf.Graph()\n",
    "\n",
    "sequence_length=x_train.shape[1]\n",
    "num_classes=3\n",
    "\n",
    "# filter_sizes: The number of words we want our convolutional filters to cover. \n",
    "# We will have num_filters for each size specified here. \n",
    "# For example, [3, 4, 5] means that we will have filters that slide over 3, 4 and 5 words respectively, for a total of 3 * num_filters filters.\n",
    "first_filter_sizes = [4]\n",
    "first_pool_window_sizes = [4]\n",
    "first_pool_strides = [2]\n",
    "\n",
    "\n",
    "second_filter_window_sizes = [3]\n",
    "num_filters = 200\n",
    "\n",
    "# No L2 norm\n",
    "l2_reg_lambda=0.0\n",
    "\n",
    "with graph.as_default():\n",
    "    input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "    input_y = tf.placeholder(tf.int64, [None], name=\"input_y\")\n",
    "    with tf.device('/cpu:0'):\n",
    "        embeddings = tf.Variable(tf.constant(0.0, shape=[vocabulary_size, embedding_size]),\n",
    "                        trainable=False, name=\"embedding\")\n",
    "\n",
    "        embedding_placeholder = tf.placeholder(tf.float32, [vocabulary_size, embedding_size], name='word_embedding_placeholder')\n",
    "        embedding_init = embeddings.assign(embedding_placeholder)  # assign exist word embeddings\n",
    "\n",
    "        embedded_chars = tf.nn.embedding_lookup(embeddings, input_x)\n",
    "    embedded_chars_expanded = tf.expand_dims(embedded_chars, -1)\n",
    "    print(input_x.shape)\n",
    "    print(embedded_chars_expanded.shape)\n",
    "    # Keeping track of l2 regularization loss (optional)\n",
    "    l2_loss = tf.constant(0.0)\n",
    "    \n",
    "     # Add variable initializer.\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    \n",
    "    # Create first cnn : a convolution + maxpool layer for each filter size    \n",
    "    # 1st Convolution Layer\n",
    "    for i, first_filter_size in enumerate(first_filter_sizes):\n",
    "        with tf.name_scope(\"conv-maxpool-1\"):\n",
    "            # Convolution Layer\n",
    "            filter_shape = [first_filter_size, embedding_size, 1, num_filters]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "            print(\"CNN filter\", W.shape)\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "            conv = tf.nn.conv2d(\n",
    "                embedded_chars_expanded,\n",
    "                W,\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding=\"VALID\",\n",
    "                name=\"conv\")\n",
    "            # Apply nonlinearity\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "            # Maxpooling over the outputs and transform for next layer, so the \"channel\" of convolution \n",
    "            # will become \"input_width\" for next layer\n",
    "#             pooled = tf.transpose(tf.nn.max_pool(\n",
    "#                 h,\n",
    "#                 ksize=[1, first_pool_window_sizes[i], 1, 1],\n",
    "#                 strides=[1, first_pool_strides[i], 1, 1],\n",
    "#                 padding='VALID',\n",
    "#                 name=\"pool\"), perm=[0, 1, 3, 2])\n",
    "            pooled = tf.nn.max_pool(\n",
    "                h,\n",
    "                ksize=[1, first_pool_window_sizes[i], 1, 1],\n",
    "                strides=[1, first_pool_strides[i], 1, 1],\n",
    "                padding='VALID',\n",
    "                name=\"pool\")\n",
    "#     print(\"conv1\", conv.shape)\n",
    "#     print(\"h1\", h.shape)\n",
    "#     print(\"pooled1\", pooled_1.shape)\n",
    "    \n",
    "    # 2nd Convolutional Layer\n",
    "#     for i, second_filter_size in enumerate(second_filter_window_sizes):\n",
    "#         with tf.name_scope(\"conv-maxpool-2\"):\n",
    "#             # Convolution Layer\n",
    "#             filter_shape = [second_filter_size, num_filters, 1, num_filters]\n",
    "#             W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "#             print(\"CNN filter\", W.shape)\n",
    "#             b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "#             conv = tf.nn.conv2d(\n",
    "#                 pooled,\n",
    "#                 W,\n",
    "#                 strides=[1, 1, 1, 1],\n",
    "#                 padding=\"VALID\",\n",
    "#                 name=\"conv\")\n",
    "#             # Apply nonlinearity\n",
    "#             h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "#             # Maxpooling over the outputs and transform for next layer, so the \"channel\" of convolution \n",
    "#             # will become \"input_width\" for next layer\n",
    "#             pooled = tf.nn.max_pool(\n",
    "#                 h,\n",
    "#                 ksize=[1, h.shape[1], 1, 1],\n",
    "#                 strides=[1, 1, 1, 1],\n",
    "#                 padding='VALID',\n",
    "#                 name=\"pool\")\n",
    "    for i, second_filter_size in enumerate(second_filter_window_sizes):\n",
    "        with tf.name_scope(\"conv-maxpool-2\"):\n",
    "            # Convolution Layer\n",
    "            filter_shape = [second_filter_size, 1, num_filters, num_filters]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "            print(\"CNN filter\", W.shape)\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "            conv = tf.nn.conv2d(\n",
    "                pooled,\n",
    "                W,\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding=\"VALID\",\n",
    "                name=\"conv\")\n",
    "            # Apply nonlinearity\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "            print(h.shape)\n",
    "            # Maxpooling over the outputs and transform for next layer, so the \"channel\" of convolution \n",
    "            # will become \"input_width\" for next layer\n",
    "            pooled = tf.nn.max_pool(\n",
    "                h,\n",
    "                ksize=[1, h.shape[1], 1, 1],\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding='VALID',\n",
    "                name=\"pool\")\n",
    " \n",
    "\n",
    "    h_pool_flat = tf.reshape(pooled, [-1, num_filters])  # flatten pooling layers\n",
    "    print(\"h_pool_flat\", h_pool_flat.shape)\n",
    "    \n",
    "    # Add dropout\n",
    "#     with tf.name_scope(\"dropout\"):\n",
    "#         self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "    # Final (unnormalized) scores and predictions\n",
    "    \n",
    "    # Fully connected hidden layer\n",
    "    with tf.name_scope(\"hidden\"):\n",
    "        with tf.variable_scope(\"hidden\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters, num_filters],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            out = tf.nn.relu(tf.nn.xw_plus_b(h_pool_flat, W, b))\n",
    "        \n",
    "    \n",
    "    with tf.name_scope(\"output\"):\n",
    "        with tf.variable_scope(\"output\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            scores = tf.nn.xw_plus_b(out, W, b, name=\"scores\")\n",
    "            print(\"scores\", scores.shape)\n",
    "            predictions = tf.argmax(scores, 1, name=\"predictions\")\n",
    "            print(\"predictions\", predictions.shape)\n",
    "\n",
    "\n",
    "    # Calculate mean cross-entropy loss\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=scores, labels=input_y)\n",
    "        print(\"losses\", losses.shape)\n",
    "        loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "    # Accuracy\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        correct_predictions = tf.equal(predictions, input_y)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.6184128163515918&quot;).pbtxt = 'node {\\n  name: &quot;input_x&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: -1\\n        }\\n        dim {\\n          size: 53\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;input_y&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: -1\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Const&quot;\\n  op: &quot;Const&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 6140853\\n          }\\n          dim {\\n            size: 52\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;embedding&quot;\\n  op: &quot;VariableV2&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 6140853\\n        }\\n        dim {\\n          size: 52\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;embedding/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;embedding&quot;\\n  input: &quot;Const&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;embedding/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;embedding&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;word_embedding_placeholder&quot;\\n  op: &quot;Placeholder&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 6140853\\n        }\\n        dim {\\n          size: 52\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;embedding&quot;\\n  input: &quot;word_embedding_placeholder&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;embedding_lookup&quot;\\n  op: &quot;Gather&quot;\\n  input: &quot;embedding/read&quot;\\n  input: &quot;input_x&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;Tindices&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tparams&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@embedding&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_indices&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ExpandDims/dim&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: -1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ExpandDims&quot;\\n  op: &quot;ExpandDims&quot;\\n  input: &quot;embedding_lookup&quot;\\n  input: &quot;ExpandDims/dim&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tdim&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Const_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;init&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^embedding/Assign&quot;\\n  device: &quot;/device:CPU:0&quot;\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/truncated_normal/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\004\\\\000\\\\000\\\\0004\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\310\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/truncated_normal/mean&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/truncated_normal/stddev&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.10000000149011612\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/truncated_normal/TruncatedNormal&quot;\\n  op: &quot;TruncatedNormal&quot;\\n  input: &quot;conv-maxpool-1/truncated_normal/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/truncated_normal/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;conv-maxpool-1/truncated_normal/TruncatedNormal&quot;\\n  input: &quot;conv-maxpool-1/truncated_normal/stddev&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/truncated_normal&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;conv-maxpool-1/truncated_normal/mul&quot;\\n  input: &quot;conv-maxpool-1/truncated_normal/mean&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/W&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 4\\n        }\\n        dim {\\n          size: 52\\n        }\\n        dim {\\n          size: 1\\n        }\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/W/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;conv-maxpool-1/W&quot;\\n  input: &quot;conv-maxpool-1/truncated_normal&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-1/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/W/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;conv-maxpool-1/W&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-1/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 200\\n          }\\n        }\\n        float_val: 0.10000000149011612\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/b&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/b/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;conv-maxpool-1/b&quot;\\n  input: &quot;conv-maxpool-1/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-1/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/b/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;conv-maxpool-1/b&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-1/b&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/conv&quot;\\n  op: &quot;Conv2D&quot;\\n  input: &quot;ExpandDims&quot;\\n  input: &quot;conv-maxpool-1/W/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;VALID&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;conv-maxpool-1/conv&quot;\\n  input: &quot;conv-maxpool-1/b/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;conv-maxpool-1/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-1/pool&quot;\\n  op: &quot;MaxPool&quot;\\n  input: &quot;conv-maxpool-1/relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;ksize&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 4\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;VALID&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 2\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/truncated_normal/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\003\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\310\\\\000\\\\000\\\\000\\\\310\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/truncated_normal/mean&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/truncated_normal/stddev&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.10000000149011612\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/truncated_normal/TruncatedNormal&quot;\\n  op: &quot;TruncatedNormal&quot;\\n  input: &quot;conv-maxpool-2/truncated_normal/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/truncated_normal/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;conv-maxpool-2/truncated_normal/TruncatedNormal&quot;\\n  input: &quot;conv-maxpool-2/truncated_normal/stddev&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/truncated_normal&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;conv-maxpool-2/truncated_normal/mul&quot;\\n  input: &quot;conv-maxpool-2/truncated_normal/mean&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/W&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 3\\n        }\\n        dim {\\n          size: 1\\n        }\\n        dim {\\n          size: 200\\n        }\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/W/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;conv-maxpool-2/W&quot;\\n  input: &quot;conv-maxpool-2/truncated_normal&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-2/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/W/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;conv-maxpool-2/W&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-2/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 200\\n          }\\n        }\\n        float_val: 0.10000000149011612\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/b&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/b/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;conv-maxpool-2/b&quot;\\n  input: &quot;conv-maxpool-2/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-2/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/b/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;conv-maxpool-2/b&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@conv-maxpool-2/b&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/conv&quot;\\n  op: &quot;Conv2D&quot;\\n  input: &quot;conv-maxpool-1/pool&quot;\\n  input: &quot;conv-maxpool-2/W/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;VALID&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;conv-maxpool-2/conv&quot;\\n  input: &quot;conv-maxpool-2/b/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;conv-maxpool-2/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;conv-maxpool-2/pool&quot;\\n  op: &quot;MaxPool&quot;\\n  input: &quot;conv-maxpool-2/relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;ksize&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 22\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;VALID&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\310\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;conv-maxpool-2/pool&quot;\\n  input: &quot;Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\310\\\\000\\\\000\\\\000\\\\310\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.12247448414564133\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.12247448414564133\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform/max&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform/mul&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 200\\n        }\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden/W&quot;\\n  input: &quot;hidden/W/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/W/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden/W&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 200\\n          }\\n        }\\n        float_val: 0.10000000149011612\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/b&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 200\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/b/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden/hidden/b&quot;\\n  input: &quot;hidden/hidden/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/hidden/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/b/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden/hidden/b&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden/hidden/b&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/L2Loss&quot;\\n  op: &quot;L2Loss&quot;\\n  input: &quot;hidden/W/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Const_1&quot;\\n  input: &quot;hidden/hidden/L2Loss&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/L2Loss_1&quot;\\n  op: &quot;L2Loss&quot;\\n  input: &quot;hidden/hidden/b/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/add_1&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden/hidden/add&quot;\\n  input: &quot;hidden/hidden/L2Loss_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/xw_plus_b/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Reshape&quot;\\n  input: &quot;hidden/W/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/xw_plus_b&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;hidden/hidden/xw_plus_b/MatMul&quot;\\n  input: &quot;hidden/hidden/b/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden/hidden/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;hidden/hidden/xw_plus_b&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\310\\\\000\\\\000\\\\000\\\\003\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.17192047834396362\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.17192047834396362\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;output/W/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;output/W/Initializer/random_uniform/max&quot;\\n  input: &quot;output/W/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;output/W/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;output/W/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;output/W/Initializer/random_uniform/mul&quot;\\n  input: &quot;output/W/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 200\\n        }\\n        dim {\\n          size: 3\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;output/W&quot;\\n  input: &quot;output/W/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/W/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;output/W&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        float_val: 0.10000000149011612\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/b&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 3\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/b/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;output/output/b&quot;\\n  input: &quot;output/output/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/output/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/b/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;output/output/b&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output/output/b&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/L2Loss&quot;\\n  op: &quot;L2Loss&quot;\\n  input: &quot;output/W/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden/hidden/add_1&quot;\\n  input: &quot;output/output/L2Loss&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/L2Loss_1&quot;\\n  op: &quot;L2Loss&quot;\\n  input: &quot;output/output/b/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/add_1&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;output/output/add&quot;\\n  input: &quot;output/output/L2Loss_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/scores/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;hidden/hidden/Relu&quot;\\n  input: &quot;output/W/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/scores&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;output/output/scores/MatMul&quot;\\n  input: &quot;output/output/b/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/predictions/dimension&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output/output/predictions&quot;\\n  op: &quot;ArgMax&quot;\\n  input: &quot;output/output/scores&quot;\\n  input: &quot;output/output/predictions/dimension&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;output_type&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;input_y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  op: &quot;SparseSoftmaxCrossEntropyWithLogits&quot;\\n  input: &quot;output/output/scores&quot;\\n  input: &quot;input_y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tlabels&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/Mean&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  input: &quot;loss/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/mul/x&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;loss/mul/x&quot;\\n  input: &quot;output/output/add_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;loss/Mean&quot;\\n  input: &quot;loss/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;accuracy/Equal&quot;\\n  op: &quot;Equal&quot;\\n  input: &quot;output/output/predictions&quot;\\n  input: &quot;input_y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;accuracy/Cast&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;accuracy/Equal&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_BOOL\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;accuracy/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;accuracy/accuracy&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;accuracy/Cast&quot;\\n  input: &quot;accuracy/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nversions {\\n  producer: 24\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.6184128163515918&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_graph(graph.as_graph_def())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-1/W:0/grad/hist is illegal; using conv-maxpool-1/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-1/W:0/grad/sparsity is illegal; using conv-maxpool-1/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-1/b:0/grad/hist is illegal; using conv-maxpool-1/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-1/b:0/grad/sparsity is illegal; using conv-maxpool-1/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/W:0/grad/hist is illegal; using conv-maxpool-2/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/W:0/grad/sparsity is illegal; using conv-maxpool-2/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/b:0/grad/hist is illegal; using conv-maxpool-2/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/b:0/grad/sparsity is illegal; using conv-maxpool-2/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name hidden/W:0/grad/hist is illegal; using hidden/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name hidden/W:0/grad/sparsity is illegal; using hidden/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name hidden/hidden/b:0/grad/hist is illegal; using hidden/hidden/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name hidden/hidden/b:0/grad/sparsity is illegal; using hidden/hidden/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/W:0/grad/hist is illegal; using output/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/W:0/grad/sparsity is illegal; using output/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/output/b:0/grad/hist is illegal; using output/output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/output/b:0/grad/sparsity is illegal; using output/output/b_0/grad/sparsity instead.\n",
      "Writing to /home/phejimlin/Documents/Machine-learning/Milestone2/runs/1511026442\n",
      "\n",
      "Current epoch:  0\n",
      "Current epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phejimlin/anaconda3/envs/tensorflow_3.5/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-19T01:34:08.325129: step 5, loss 1.0872, acc 0.416992, f1 0.275571\n",
      "2017-11-19T01:34:08.620653: step 10, loss 1.01904, acc 0.446289, f1 0.406248\n",
      "2017-11-19T01:34:08.921894: step 15, loss 1.04824, acc 0.410156, f1 0.283891\n",
      "Current epoch:  1\n",
      "2017-11-19T01:34:09.414834: step 20, loss 1.05712, acc 0.432617, f1 0.264631\n",
      "2017-11-19T01:34:09.716680: step 25, loss 1.04282, acc 0.443359, f1 0.401082\n",
      "2017-11-19T01:34:10.021661: step 30, loss 0.999571, acc 0.466797, f1 0.412625\n",
      "2017-11-19T01:34:10.322073: step 35, loss 0.974774, acc 0.491211, f1 0.44286\n",
      "Current epoch:  2\n",
      "2017-11-19T01:34:10.599683: step 40, loss 1.08746, acc 0.415039, f1 0.25506\n",
      "2017-11-19T01:34:10.906662: step 45, loss 1.0668, acc 0.420898, f1 0.253899\n",
      "2017-11-19T01:34:11.237394: step 50, loss 0.972183, acc 0.478516, f1 0.412457\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:34:11.918859: step 50, loss 1.01784, acc 0.427443, f1 0.358912\n",
      "\n",
      "Current epoch:  3\n",
      "2017-11-19T01:34:12.258805: step 55, loss 0.995432, acc 0.445312, f1 0.340421\n",
      "2017-11-19T01:34:12.559148: step 60, loss 0.994544, acc 0.454102, f1 0.350795\n",
      "2017-11-19T01:34:12.854314: step 65, loss 1.03023, acc 0.450195, f1 0.313969\n",
      "2017-11-19T01:34:13.163024: step 70, loss 1.03664, acc 0.427734, f1 0.28608\n",
      "Current epoch:  4\n",
      "2017-11-19T01:34:13.438817: step 75, loss 0.96271, acc 0.500977, f1 0.461667\n",
      "2017-11-19T01:34:13.722521: step 80, loss 0.957309, acc 0.500977, f1 0.452046\n",
      "2017-11-19T01:34:14.041693: step 85, loss 1.07408, acc 0.470703, f1 0.385252\n",
      "2017-11-19T01:34:14.307358: step 90, loss 0.994398, acc 0.47327, f1 0.348616\n",
      "Current epoch:  5\n",
      "2017-11-19T01:34:14.632421: step 95, loss 0.975137, acc 0.524414, f1 0.505917\n",
      "2017-11-19T01:34:14.915306: step 100, loss 0.957915, acc 0.509766, f1 0.469852\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:34:15.168981: step 100, loss 0.948987, acc 0.53068, f1 0.481472\n",
      "\n",
      "2017-11-19T01:34:15.492354: step 105, loss 1.03803, acc 0.412109, f1 0.266984\n",
      "Current epoch:  6\n",
      "2017-11-19T01:34:15.824780: step 110, loss 0.970892, acc 0.484375, f1 0.368283\n",
      "2017-11-19T01:34:16.136821: step 115, loss 0.968908, acc 0.518555, f1 0.51521\n",
      "2017-11-19T01:34:16.462357: step 120, loss 0.979052, acc 0.467773, f1 0.384332\n",
      "2017-11-19T01:34:16.783672: step 125, loss 0.956114, acc 0.473633, f1 0.357732\n",
      "Current epoch:  7\n",
      "2017-11-19T01:34:17.051441: step 130, loss 0.93862, acc 0.558594, f1 0.521179\n",
      "2017-11-19T01:34:17.356705: step 135, loss 0.97221, acc 0.512695, f1 0.455971\n",
      "2017-11-19T01:34:17.657035: step 140, loss 0.926279, acc 0.506836, f1 0.451467\n",
      "Current epoch:  8\n",
      "2017-11-19T01:34:17.953211: step 145, loss 1.06565, acc 0.44043, f1 0.295875\n",
      "2017-11-19T01:34:18.244864: step 150, loss 0.938063, acc 0.513672, f1 0.450928\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:34:18.516775: step 150, loss 0.974008, acc 0.488077, f1 0.46414\n",
      "\n",
      "2017-11-19T01:34:18.858047: step 155, loss 0.93645, acc 0.535156, f1 0.489007\n",
      "2017-11-19T01:34:19.174114: step 160, loss 0.944132, acc 0.538086, f1 0.494622\n",
      "Current epoch:  9\n",
      "2017-11-19T01:34:19.471707: step 165, loss 0.919573, acc 0.529297, f1 0.507432\n",
      "2017-11-19T01:34:19.778154: step 170, loss 1.01241, acc 0.436523, f1 0.328206\n",
      "2017-11-19T01:34:20.105878: step 175, loss 0.905502, acc 0.546875, f1 0.513044\n",
      "2017-11-19T01:34:20.397324: step 180, loss 0.896582, acc 0.581761, f1 0.556957\n",
      "Current epoch:  10\n",
      "2017-11-19T01:34:20.706466: step 185, loss 1.01394, acc 0.455078, f1 0.316549\n",
      "2017-11-19T01:34:21.027294: step 190, loss 0.901596, acc 0.550781, f1 0.511917\n",
      "2017-11-19T01:34:21.299232: step 195, loss 0.926975, acc 0.547852, f1 0.527054\n",
      "Current epoch:  11\n",
      "2017-11-19T01:34:21.646718: step 200, loss 1.11046, acc 0.495117, f1 0.390523\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:34:21.900868: step 200, loss 0.937926, acc 0.529808, f1 0.483317\n",
      "\n",
      "2017-11-19T01:34:22.208326: step 205, loss 0.897322, acc 0.553711, f1 0.512458\n",
      "2017-11-19T01:34:22.544126: step 210, loss 0.901383, acc 0.548828, f1 0.538983\n",
      "2017-11-19T01:34:22.860293: step 215, loss 0.903396, acc 0.538086, f1 0.468095\n",
      "Current epoch:  12\n",
      "2017-11-19T01:34:23.139414: step 220, loss 0.890583, acc 0.570312, f1 0.559781\n",
      "2017-11-19T01:34:23.456476: step 225, loss 0.959911, acc 0.475586, f1 0.36798\n",
      "2017-11-19T01:34:23.778528: step 230, loss 0.921475, acc 0.585938, f1 0.587208\n",
      "Current epoch:  13\n",
      "2017-11-19T01:34:24.073067: step 235, loss 0.940518, acc 0.53125, f1 0.476169\n",
      "2017-11-19T01:34:24.401737: step 240, loss 0.912489, acc 0.5625, f1 0.552701\n",
      "2017-11-19T01:34:24.681270: step 245, loss 0.921463, acc 0.53418, f1 0.492704\n",
      "2017-11-19T01:34:24.981987: step 250, loss 0.912191, acc 0.514648, f1 0.422733\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:34:25.222894: step 250, loss 0.918219, acc 0.533346, f1 0.447167\n",
      "\n",
      "Current epoch:  14\n",
      "2017-11-19T01:34:25.522704: step 255, loss 0.915383, acc 0.545898, f1 0.517283\n",
      "2017-11-19T01:34:25.805098: step 260, loss 0.907496, acc 0.543945, f1 0.498785\n",
      "2017-11-19T01:34:26.235573: step 265, loss 0.925565, acc 0.560547, f1 0.533298\n",
      "2017-11-19T01:34:26.550434: step 270, loss 0.899583, acc 0.573899, f1 0.541317\n",
      "Current epoch:  15\n",
      "2017-11-19T01:34:26.841765: step 275, loss 0.903861, acc 0.532227, f1 0.473636\n",
      "2017-11-19T01:34:27.143423: step 280, loss 0.950176, acc 0.501953, f1 0.428496\n",
      "2017-11-19T01:34:27.495829: step 285, loss 0.897727, acc 0.588867, f1 0.54633\n",
      "Current epoch:  16\n",
      "2017-11-19T01:34:27.808534: step 290, loss 0.919156, acc 0.526367, f1 0.476838\n",
      "2017-11-19T01:34:28.127219: step 295, loss 0.877488, acc 0.55957, f1 0.532517\n",
      "2017-11-19T01:34:28.446380: step 300, loss 0.83865, acc 0.613281, f1 0.592099\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:34:28.685699: step 300, loss 0.915855, acc 0.538871, f1 0.52448\n",
      "\n",
      "2017-11-19T01:34:29.005594: step 305, loss 0.91817, acc 0.538086, f1 0.479417\n",
      "Current epoch:  17\n",
      "2017-11-19T01:34:29.278016: step 310, loss 0.929584, acc 0.491211, f1 0.424782\n",
      "2017-11-19T01:34:29.564252: step 315, loss 0.870546, acc 0.577148, f1 0.55451\n",
      "2017-11-19T01:34:29.848348: step 320, loss 0.875194, acc 0.580078, f1 0.544144\n",
      "Current epoch:  18\n",
      "2017-11-19T01:34:30.122758: step 325, loss 1.03071, acc 0.504883, f1 0.412932\n",
      "2017-11-19T01:34:30.390807: step 330, loss 0.898447, acc 0.547852, f1 0.501978\n",
      "2017-11-19T01:34:30.680698: step 335, loss 0.896672, acc 0.540039, f1 0.487002\n",
      "2017-11-19T01:34:30.971637: step 340, loss 0.858957, acc 0.618164, f1 0.614327\n",
      "Current epoch:  19\n",
      "2017-11-19T01:34:31.245405: step 345, loss 0.891817, acc 0.542969, f1 0.499327\n",
      "2017-11-19T01:34:31.521850: step 350, loss 0.885438, acc 0.579102, f1 0.576477\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:34:31.759712: step 350, loss 0.956056, acc 0.535139, f1 0.493383\n",
      "\n",
      "2017-11-19T01:34:32.064683: step 355, loss 0.970108, acc 0.501953, f1 0.416331\n",
      "2017-11-19T01:34:32.340419: step 360, loss 0.852795, acc 0.603774, f1 0.577207\n",
      "Current epoch:  20\n",
      "2017-11-19T01:34:32.624513: step 365, loss 0.873804, acc 0.582031, f1 0.542106\n",
      "2017-11-19T01:34:32.924298: step 370, loss 0.861592, acc 0.604492, f1 0.599906\n",
      "2017-11-19T01:34:33.204392: step 375, loss 0.968607, acc 0.470703, f1 0.362287\n",
      "Current epoch:  21\n",
      "2017-11-19T01:34:33.482704: step 380, loss 0.868506, acc 0.580078, f1 0.565618\n",
      "2017-11-19T01:34:33.772558: step 385, loss 0.83339, acc 0.598633, f1 0.576194\n",
      "2017-11-19T01:34:34.071255: step 390, loss 0.90462, acc 0.558594, f1 0.551191\n",
      "2017-11-19T01:34:34.377286: step 395, loss 0.974178, acc 0.484375, f1 0.388559\n",
      "Current epoch:  22\n",
      "2017-11-19T01:34:34.706448: step 400, loss 0.873444, acc 0.566406, f1 0.546549\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:34:34.948393: step 400, loss 0.96449, acc 0.485508, f1 0.452044\n",
      "\n",
      "2017-11-19T01:34:35.288823: step 405, loss 0.861603, acc 0.595703, f1 0.580849\n",
      "2017-11-19T01:34:35.594610: step 410, loss 0.870799, acc 0.584961, f1 0.573705\n",
      "Current epoch:  23\n",
      "2017-11-19T01:34:35.876913: step 415, loss 0.917153, acc 0.511719, f1 0.443229\n",
      "2017-11-19T01:34:36.227132: step 420, loss 0.848156, acc 0.571289, f1 0.545682\n",
      "2017-11-19T01:34:36.500589: step 425, loss 0.890504, acc 0.548828, f1 0.501833\n",
      "2017-11-19T01:34:36.809821: step 430, loss 0.837654, acc 0.624023, f1 0.618837\n",
      "Current epoch:  24\n",
      "2017-11-19T01:34:37.120461: step 435, loss 0.8829, acc 0.56543, f1 0.523681\n",
      "2017-11-19T01:34:37.437103: step 440, loss 0.888531, acc 0.549805, f1 0.492788\n",
      "2017-11-19T01:34:37.731837: step 445, loss 0.867189, acc 0.579102, f1 0.571957\n",
      "2017-11-19T01:34:38.000383: step 450, loss 0.891537, acc 0.572327, f1 0.521777\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-19T01:34:38.231304: step 450, loss 0.935649, acc 0.532861, f1 0.53737\n",
      "\n",
      "Current epoch:  25\n",
      "2017-11-19T01:34:38.536918: step 455, loss 0.842692, acc 0.628906, f1 0.627046\n",
      "2017-11-19T01:34:38.829818: step 460, loss 0.849764, acc 0.601562, f1 0.576258\n",
      "2017-11-19T01:34:39.131532: step 465, loss 0.925045, acc 0.532227, f1 0.505834\n",
      "Current epoch:  26\n",
      "2017-11-19T01:34:39.410210: step 470, loss 0.818555, acc 0.62207, f1 0.605461\n",
      "2017-11-19T01:34:39.709668: step 475, loss 0.909168, acc 0.567383, f1 0.519754\n",
      "2017-11-19T01:34:40.027448: step 480, loss 0.84074, acc 0.56543, f1 0.538754\n",
      "2017-11-19T01:34:40.332590: step 485, loss 0.831996, acc 0.601562, f1 0.589298\n",
      "Current epoch:  27\n",
      "2017-11-19T01:34:40.640314: step 490, loss 0.93132, acc 0.530273, f1 0.456798\n",
      "2017-11-19T01:34:40.999655: step 495, loss 0.820706, acc 0.604492, f1 0.590872\n",
      "2017-11-19T01:34:41.320017: step 500, loss 0.858681, acc 0.587891, f1 0.578859\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:34:41.575994: step 500, loss 0.892872, acc 0.579633, f1 0.535039\n",
      "\n",
      "Current epoch:  28\n",
      "2017-11-19T01:34:41.889389: step 505, loss 0.81689, acc 0.616211, f1 0.587382\n",
      "2017-11-19T01:34:42.198577: step 510, loss 0.835295, acc 0.611328, f1 0.586252\n",
      "2017-11-19T01:34:42.487292: step 515, loss 0.868758, acc 0.585938, f1 0.564815\n",
      "2017-11-19T01:34:42.796544: step 520, loss 0.808504, acc 0.616211, f1 0.602551\n",
      "Current epoch:  29\n",
      "2017-11-19T01:34:43.073786: step 525, loss 0.901474, acc 0.524414, f1 0.463363\n",
      "2017-11-19T01:34:43.371473: step 530, loss 0.872914, acc 0.553711, f1 0.507668\n",
      "2017-11-19T01:34:43.666323: step 535, loss 0.84333, acc 0.594727, f1 0.580573\n",
      "2017-11-19T01:34:43.942794: step 540, loss 0.859639, acc 0.583333, f1 0.537937\n",
      "Current epoch:  30\n",
      "2017-11-19T01:34:44.274454: step 545, loss 0.828836, acc 0.59668, f1 0.57519\n",
      "2017-11-19T01:34:44.582252: step 550, loss 0.885333, acc 0.588867, f1 0.54726\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:34:44.842974: step 550, loss 0.920349, acc 0.544445, f1 0.551937\n",
      "\n",
      "2017-11-19T01:34:45.160492: step 555, loss 0.806423, acc 0.641602, f1 0.642161\n",
      "Current epoch:  31\n",
      "2017-11-19T01:34:45.467716: step 560, loss 0.885947, acc 0.539062, f1 0.469905\n",
      "2017-11-19T01:34:45.771140: step 565, loss 0.808338, acc 0.618164, f1 0.582699\n",
      "2017-11-19T01:34:46.087949: step 570, loss 0.881987, acc 0.553711, f1 0.521167\n",
      "2017-11-19T01:34:46.390466: step 575, loss 0.817405, acc 0.620117, f1 0.600273\n",
      "Current epoch:  32\n",
      "2017-11-19T01:34:46.698188: step 580, loss 0.826639, acc 0.56543, f1 0.527192\n",
      "2017-11-19T01:34:46.990915: step 585, loss 0.881128, acc 0.584961, f1 0.543414\n",
      "2017-11-19T01:34:47.318124: step 590, loss 0.807076, acc 0.614258, f1 0.604331\n",
      "Current epoch:  33\n",
      "2017-11-19T01:34:47.629386: step 595, loss 0.807251, acc 0.607422, f1 0.583409\n",
      "2017-11-19T01:34:47.929460: step 600, loss 0.864367, acc 0.529297, f1 0.490774\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:34:48.165310: step 600, loss 0.989709, acc 0.466072, f1 0.415521\n",
      "\n",
      "2017-11-19T01:34:48.452090: step 605, loss 0.831503, acc 0.607422, f1 0.587859\n",
      "2017-11-19T01:34:48.737743: step 610, loss 0.84146, acc 0.601562, f1 0.582316\n",
      "Current epoch:  34\n",
      "2017-11-19T01:34:49.036923: step 615, loss 0.827663, acc 0.594727, f1 0.561922\n",
      "2017-11-19T01:34:49.313236: step 620, loss 0.887191, acc 0.567383, f1 0.552693\n",
      "2017-11-19T01:34:49.632210: step 625, loss 0.815043, acc 0.592773, f1 0.550877\n",
      "2017-11-19T01:34:49.920267: step 630, loss 0.844319, acc 0.619497, f1 0.604317\n",
      "Current epoch:  35\n",
      "2017-11-19T01:34:50.220154: step 635, loss 0.840978, acc 0.605469, f1 0.599509\n",
      "2017-11-19T01:34:50.554033: step 640, loss 0.830982, acc 0.59082, f1 0.554859\n",
      "2017-11-19T01:34:50.863286: step 645, loss 0.792262, acc 0.639648, f1 0.61889\n",
      "Current epoch:  36\n",
      "2017-11-19T01:34:51.144305: step 650, loss 0.785135, acc 0.65625, f1 0.655166\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:34:51.382616: step 650, loss 0.947967, acc 0.520405, f1 0.496302\n",
      "\n",
      "2017-11-19T01:34:51.705974: step 655, loss 0.882936, acc 0.552734, f1 0.500903\n",
      "2017-11-19T01:34:51.987466: step 660, loss 0.785439, acc 0.639648, f1 0.625718\n",
      "2017-11-19T01:34:52.282745: step 665, loss 0.90779, acc 0.542969, f1 0.455329\n",
      "Current epoch:  37\n",
      "2017-11-19T01:34:52.581590: step 670, loss 0.758465, acc 0.654297, f1 0.642606\n",
      "2017-11-19T01:34:52.860301: step 675, loss 0.808516, acc 0.632812, f1 0.603323\n",
      "2017-11-19T01:34:53.166175: step 680, loss 0.969451, acc 0.479492, f1 0.392406\n",
      "Current epoch:  38\n",
      "2017-11-19T01:34:53.471749: step 685, loss 0.77778, acc 0.632812, f1 0.615373\n",
      "2017-11-19T01:34:53.800764: step 690, loss 0.775617, acc 0.661133, f1 0.663422\n",
      "2017-11-19T01:34:54.078704: step 695, loss 0.853819, acc 0.597656, f1 0.547144\n",
      "2017-11-19T01:34:54.370880: step 700, loss 0.805114, acc 0.584961, f1 0.5616\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:34:54.598897: step 700, loss 0.9549, acc 0.500921, f1 0.472402\n",
      "\n",
      "Current epoch:  39\n",
      "2017-11-19T01:34:54.921394: step 705, loss 0.797096, acc 0.625977, f1 0.614902\n",
      "2017-11-19T01:34:55.233104: step 710, loss 0.828767, acc 0.581055, f1 0.539793\n",
      "2017-11-19T01:34:55.548225: step 715, loss 0.823039, acc 0.619141, f1 0.580779\n",
      "2017-11-19T01:34:55.824483: step 720, loss 0.803892, acc 0.616352, f1 0.609864\n",
      "Current epoch:  40\n",
      "2017-11-19T01:34:56.118379: step 725, loss 0.811214, acc 0.611328, f1 0.589233\n",
      "2017-11-19T01:34:56.418021: step 730, loss 0.80104, acc 0.602539, f1 0.573009\n",
      "2017-11-19T01:34:56.702191: step 735, loss 0.769227, acc 0.657227, f1 0.642131\n",
      "Current epoch:  41\n",
      "2017-11-19T01:34:56.981021: step 740, loss 0.806579, acc 0.636719, f1 0.640859\n",
      "2017-11-19T01:34:57.255988: step 745, loss 0.85176, acc 0.59668, f1 0.556707\n",
      "2017-11-19T01:34:57.544280: step 750, loss 0.857098, acc 0.563477, f1 0.524202\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:34:57.774114: step 750, loss 1.01252, acc 0.477801, f1 0.443441\n",
      "\n",
      "2017-11-19T01:34:58.060517: step 755, loss 0.773361, acc 0.649414, f1 0.645141\n",
      "Current epoch:  42\n",
      "2017-11-19T01:34:58.359432: step 760, loss 0.745784, acc 0.670898, f1 0.66196\n",
      "2017-11-19T01:34:58.651888: step 765, loss 0.927224, acc 0.52832, f1 0.457373\n",
      "2017-11-19T01:34:58.962242: step 770, loss 0.776935, acc 0.650391, f1 0.650004\n",
      "Current epoch:  43\n",
      "2017-11-19T01:34:59.265107: step 775, loss 0.741742, acc 0.676758, f1 0.668381\n",
      "2017-11-19T01:34:59.556599: step 780, loss 0.824167, acc 0.592773, f1 0.562989\n",
      "2017-11-19T01:34:59.852010: step 785, loss 0.784734, acc 0.630859, f1 0.618877\n",
      "2017-11-19T01:35:00.156966: step 790, loss 0.77651, acc 0.621094, f1 0.60082\n",
      "Current epoch:  44\n",
      "2017-11-19T01:35:00.461791: step 795, loss 0.835535, acc 0.645508, f1 0.604921\n",
      "2017-11-19T01:35:00.768219: step 800, loss 0.773597, acc 0.642578, f1 0.646627\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:35:01.031659: step 800, loss 0.905675, acc 0.583075, f1 0.545211\n",
      "\n",
      "2017-11-19T01:35:01.305494: step 805, loss 0.772829, acc 0.630859, f1 0.603469\n",
      "2017-11-19T01:35:01.578440: step 810, loss 0.814021, acc 0.600629, f1 0.5757\n",
      "Current epoch:  45\n",
      "2017-11-19T01:35:01.896860: step 815, loss 0.729035, acc 0.669922, f1 0.654429\n",
      "2017-11-19T01:35:02.193521: step 820, loss 0.853223, acc 0.570312, f1 0.54405\n",
      "2017-11-19T01:35:02.459188: step 825, loss 0.748894, acc 0.666016, f1 0.658208\n",
      "Current epoch:  46\n",
      "2017-11-19T01:35:02.753842: step 830, loss 0.770761, acc 0.624023, f1 0.588217\n",
      "2017-11-19T01:35:03.038953: step 835, loss 0.771401, acc 0.65332, f1 0.640142\n",
      "2017-11-19T01:35:03.322868: step 840, loss 0.818336, acc 0.605469, f1 0.585245\n",
      "2017-11-19T01:35:03.600329: step 845, loss 0.75022, acc 0.679688, f1 0.655576\n",
      "Current epoch:  47\n",
      "2017-11-19T01:35:03.868741: step 850, loss 0.896091, acc 0.560547, f1 0.508942\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:35:04.137544: step 850, loss 0.895105, acc 0.584335, f1 0.526259\n",
      "\n",
      "2017-11-19T01:35:04.440871: step 855, loss 0.81038, acc 0.602539, f1 0.584678\n",
      "2017-11-19T01:35:04.748298: step 860, loss 0.757887, acc 0.646484, f1 0.622302\n",
      "Current epoch:  48\n",
      "2017-11-19T01:35:05.055874: step 865, loss 0.816142, acc 0.574219, f1 0.53967\n",
      "2017-11-19T01:35:05.353230: step 870, loss 0.742195, acc 0.68457, f1 0.682029\n",
      "2017-11-19T01:35:05.643728: step 875, loss 0.789306, acc 0.616211, f1 0.595173\n",
      "2017-11-19T01:35:05.928122: step 880, loss 0.76201, acc 0.643555, f1 0.634454\n",
      "Current epoch:  49\n",
      "2017-11-19T01:35:06.226621: step 885, loss 0.732423, acc 0.675781, f1 0.645724\n",
      "2017-11-19T01:35:06.527334: step 890, loss 0.743621, acc 0.685547, f1 0.685003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-19T01:35:06.804668: step 895, loss 0.809379, acc 0.584961, f1 0.546412\n",
      "2017-11-19T01:35:07.083574: step 900, loss 0.757914, acc 0.649371, f1 0.620473\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:35:07.319983: step 900, loss 0.977037, acc 0.502423, f1 0.483578\n",
      "\n",
      "Current epoch:  50\n",
      "2017-11-19T01:35:07.657310: step 905, loss 0.739031, acc 0.677734, f1 0.668053\n",
      "2017-11-19T01:35:07.925771: step 910, loss 0.87332, acc 0.544922, f1 0.50479\n",
      "2017-11-19T01:35:08.225261: step 915, loss 0.823661, acc 0.636719, f1 0.591359\n",
      "Current epoch:  51\n",
      "2017-11-19T01:35:08.522632: step 920, loss 0.747442, acc 0.665039, f1 0.649768\n",
      "2017-11-19T01:35:08.844260: step 925, loss 0.721876, acc 0.683594, f1 0.680462\n",
      "2017-11-19T01:35:09.143141: step 930, loss 0.78185, acc 0.605469, f1 0.571444\n",
      "2017-11-19T01:35:09.460586: step 935, loss 0.781376, acc 0.658203, f1 0.61915\n",
      "Current epoch:  52\n",
      "2017-11-19T01:35:09.732593: step 940, loss 0.734579, acc 0.695312, f1 0.697701\n",
      "2017-11-19T01:35:10.027345: step 945, loss 0.719136, acc 0.666992, f1 0.649764\n",
      "2017-11-19T01:35:10.296687: step 950, loss 0.846495, acc 0.547852, f1 0.49612\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:35:10.525802: step 950, loss 0.916243, acc 0.53131, f1 0.519966\n",
      "\n",
      "Current epoch:  53\n",
      "2017-11-19T01:35:10.813878: step 955, loss 0.728804, acc 0.701172, f1 0.701334\n",
      "2017-11-19T01:35:11.117025: step 960, loss 0.773312, acc 0.595703, f1 0.575332\n",
      "2017-11-19T01:35:11.431206: step 965, loss 0.829821, acc 0.65625, f1 0.608839\n",
      "2017-11-19T01:35:11.741879: step 970, loss 0.72521, acc 0.6875, f1 0.688504\n",
      "Current epoch:  54\n",
      "2017-11-19T01:35:12.032883: step 975, loss 0.69806, acc 0.678711, f1 0.661535\n",
      "2017-11-19T01:35:12.315722: step 980, loss 0.806855, acc 0.583984, f1 0.538948\n",
      "2017-11-19T01:35:12.632369: step 985, loss 0.722154, acc 0.69043, f1 0.68556\n",
      "2017-11-19T01:35:12.918390: step 990, loss 0.717076, acc 0.70283, f1 0.694894\n",
      "Current epoch:  55\n",
      "2017-11-19T01:35:13.245249: step 995, loss 0.785137, acc 0.612305, f1 0.578105\n",
      "2017-11-19T01:35:13.533639: step 1000, loss 0.715871, acc 0.697266, f1 0.694605\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:35:13.781971: step 1000, loss 0.881452, acc 0.584189, f1 0.560465\n",
      "\n",
      "2017-11-19T01:35:14.058680: step 1005, loss 0.728772, acc 0.673828, f1 0.647198\n",
      "Current epoch:  56\n",
      "2017-11-19T01:35:14.332141: step 1010, loss 0.725973, acc 0.675781, f1 0.665901\n",
      "2017-11-19T01:35:14.621808: step 1015, loss 0.807256, acc 0.615234, f1 0.572944\n",
      "2017-11-19T01:35:14.909212: step 1020, loss 0.708463, acc 0.680664, f1 0.678293\n",
      "2017-11-19T01:35:15.193975: step 1025, loss 0.803147, acc 0.645508, f1 0.605787\n",
      "Current epoch:  57\n",
      "2017-11-19T01:35:15.486099: step 1030, loss 0.822023, acc 0.587891, f1 0.541028\n",
      "2017-11-19T01:35:15.761230: step 1035, loss 0.690599, acc 0.688477, f1 0.678698\n",
      "2017-11-19T01:35:16.074584: step 1040, loss 0.744854, acc 0.643555, f1 0.618968\n",
      "Current epoch:  58\n",
      "2017-11-19T01:35:16.365600: step 1045, loss 0.667406, acc 0.732422, f1 0.731392\n",
      "2017-11-19T01:35:16.659388: step 1050, loss 0.765825, acc 0.610352, f1 0.572056\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:35:16.893608: step 1050, loss 1.03226, acc 0.475039, f1 0.430861\n",
      "\n",
      "2017-11-19T01:35:17.240701: step 1055, loss 0.685618, acc 0.733398, f1 0.734248\n",
      "2017-11-19T01:35:17.546319: step 1060, loss 0.755491, acc 0.65918, f1 0.618275\n",
      "Current epoch:  59\n",
      "2017-11-19T01:35:17.878468: step 1065, loss 0.701202, acc 0.681641, f1 0.669179\n",
      "2017-11-19T01:35:18.172676: step 1070, loss 0.740284, acc 0.675781, f1 0.66475\n",
      "2017-11-19T01:35:18.492387: step 1075, loss 0.701234, acc 0.707031, f1 0.694958\n",
      "2017-11-19T01:35:18.773922: step 1080, loss 0.827647, acc 0.558176, f1 0.502469\n",
      "Current epoch:  60\n",
      "2017-11-19T01:35:19.078936: step 1085, loss 0.718889, acc 0.675781, f1 0.660624\n",
      "2017-11-19T01:35:19.381698: step 1090, loss 0.727927, acc 0.68457, f1 0.687076\n",
      "2017-11-19T01:35:19.682185: step 1095, loss 0.722624, acc 0.675781, f1 0.636243\n",
      "Current epoch:  61\n",
      "2017-11-19T01:35:19.994053: step 1100, loss 0.737248, acc 0.663086, f1 0.645786\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:35:20.247928: step 1100, loss 0.907573, acc 0.582251, f1 0.525851\n",
      "\n",
      "2017-11-19T01:35:20.587053: step 1105, loss 0.68874, acc 0.710938, f1 0.705865\n",
      "2017-11-19T01:35:20.899319: step 1110, loss 0.753134, acc 0.657227, f1 0.620567\n",
      "2017-11-19T01:35:21.192612: step 1115, loss 0.792648, acc 0.609375, f1 0.572598\n",
      "Current epoch:  62\n",
      "2017-11-19T01:35:21.485383: step 1120, loss 0.759303, acc 0.662109, f1 0.617902\n",
      "2017-11-19T01:35:21.781311: step 1125, loss 0.6871, acc 0.707031, f1 0.69548\n",
      "2017-11-19T01:35:22.069503: step 1130, loss 0.713865, acc 0.660156, f1 0.623462\n",
      "Current epoch:  63\n",
      "2017-11-19T01:35:22.376199: step 1135, loss 0.65562, acc 0.735352, f1 0.739248\n",
      "2017-11-19T01:35:22.666732: step 1140, loss 0.787623, acc 0.594727, f1 0.560924\n",
      "2017-11-19T01:35:22.950121: step 1145, loss 0.707871, acc 0.678711, f1 0.647351\n",
      "2017-11-19T01:35:23.239659: step 1150, loss 0.721601, acc 0.663086, f1 0.650963\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:35:23.489434: step 1150, loss 0.859684, acc 0.592817, f1 0.568535\n",
      "\n",
      "Current epoch:  64\n",
      "2017-11-19T01:35:23.792937: step 1155, loss 0.678368, acc 0.702148, f1 0.680564\n",
      "2017-11-19T01:35:24.058793: step 1160, loss 0.682054, acc 0.736328, f1 0.735517\n",
      "2017-11-19T01:35:24.350063: step 1165, loss 0.771571, acc 0.641602, f1 0.615968\n",
      "2017-11-19T01:35:24.628539: step 1170, loss 0.656106, acc 0.740566, f1 0.735095\n",
      "Current epoch:  65\n",
      "2017-11-19T01:35:24.915906: step 1175, loss 0.731236, acc 0.660156, f1 0.642783\n",
      "2017-11-19T01:35:25.226628: step 1180, loss 0.734874, acc 0.648438, f1 0.627173\n",
      "2017-11-19T01:35:25.543854: step 1185, loss 0.636669, acc 0.732422, f1 0.722716\n",
      "Current epoch:  66\n",
      "2017-11-19T01:35:25.869363: step 1190, loss 0.758226, acc 0.605469, f1 0.564062\n",
      "2017-11-19T01:35:26.192184: step 1195, loss 0.624247, acc 0.771484, f1 0.772215\n",
      "2017-11-19T01:35:26.491251: step 1200, loss 0.782706, acc 0.635742, f1 0.594241\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:35:26.741480: step 1200, loss 0.999784, acc 0.531068, f1 0.497433\n",
      "\n",
      "2017-11-19T01:35:27.163998: step 1205, loss 0.664295, acc 0.696289, f1 0.690252\n",
      "Current epoch:  67\n",
      "2017-11-19T01:35:27.435665: step 1210, loss 0.747682, acc 0.654297, f1 0.64804\n",
      "2017-11-19T01:35:27.745198: step 1215, loss 0.644742, acc 0.71582, f1 0.700909\n",
      "2017-11-19T01:35:28.048690: step 1220, loss 0.801104, acc 0.600586, f1 0.553684\n",
      "Current epoch:  68\n",
      "2017-11-19T01:35:28.344091: step 1225, loss 0.672937, acc 0.681641, f1 0.663815\n",
      "2017-11-19T01:35:28.675540: step 1230, loss 0.687036, acc 0.65625, f1 0.627568\n",
      "2017-11-19T01:35:28.979007: step 1235, loss 0.66039, acc 0.746094, f1 0.748599\n",
      "2017-11-19T01:35:29.262494: step 1240, loss 0.672107, acc 0.706055, f1 0.681511\n",
      "Current epoch:  69\n",
      "2017-11-19T01:35:29.559749: step 1245, loss 0.641903, acc 0.743164, f1 0.741912\n",
      "2017-11-19T01:35:29.868272: step 1250, loss 0.743985, acc 0.604492, f1 0.57253\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:35:30.142500: step 1250, loss 1.27704, acc 0.420609, f1 0.314795\n",
      "\n",
      "2017-11-19T01:35:30.462599: step 1255, loss 0.740738, acc 0.696289, f1 0.650499\n",
      "2017-11-19T01:35:30.740067: step 1260, loss 0.624049, acc 0.767296, f1 0.764497\n",
      "Current epoch:  70\n",
      "2017-11-19T01:35:31.038474: step 1265, loss 0.729101, acc 0.645508, f1 0.62005\n",
      "2017-11-19T01:35:31.322884: step 1270, loss 0.683094, acc 0.671875, f1 0.646388\n",
      "2017-11-19T01:35:31.636780: step 1275, loss 0.687536, acc 0.692383, f1 0.684574\n",
      "Current epoch:  71\n",
      "2017-11-19T01:35:31.925515: step 1280, loss 0.63603, acc 0.733398, f1 0.705291\n",
      "2017-11-19T01:35:32.242915: step 1285, loss 0.726698, acc 0.643555, f1 0.617793\n",
      "2017-11-19T01:35:32.554378: step 1290, loss 0.657582, acc 0.722656, f1 0.715328\n",
      "2017-11-19T01:35:32.859423: step 1295, loss 0.646351, acc 0.710938, f1 0.686756\n",
      "Current epoch:  72\n",
      "2017-11-19T01:35:33.163118: step 1300, loss 0.786899, acc 0.612305, f1 0.56574\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:35:33.390960: step 1300, loss 0.906132, acc 0.569116, f1 0.538625\n",
      "\n",
      "2017-11-19T01:35:33.712266: step 1305, loss 0.678419, acc 0.700195, f1 0.691192\n",
      "2017-11-19T01:35:34.053217: step 1310, loss 0.66516, acc 0.724609, f1 0.696484\n",
      "Current epoch:  73\n",
      "2017-11-19T01:35:34.363780: step 1315, loss 0.659115, acc 0.709961, f1 0.695589\n",
      "2017-11-19T01:35:34.648582: step 1320, loss 0.659214, acc 0.728516, f1 0.723439\n",
      "2017-11-19T01:35:34.954251: step 1325, loss 0.660862, acc 0.701172, f1 0.663666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-19T01:35:35.263472: step 1330, loss 0.587175, acc 0.776367, f1 0.776648\n",
      "Current epoch:  74\n",
      "2017-11-19T01:35:35.581673: step 1335, loss 0.667921, acc 0.676758, f1 0.657582\n",
      "2017-11-19T01:35:35.867864: step 1340, loss 0.628524, acc 0.734375, f1 0.729418\n",
      "2017-11-19T01:35:36.171344: step 1345, loss 0.62556, acc 0.751953, f1 0.749964\n",
      "2017-11-19T01:35:36.430555: step 1350, loss 0.706811, acc 0.70283, f1 0.658762\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:35:36.665322: step 1350, loss 0.940279, acc 0.538338, f1 0.546844\n",
      "\n",
      "Current epoch:  75\n",
      "2017-11-19T01:35:36.961209: step 1355, loss 0.786698, acc 0.597656, f1 0.561729\n",
      "2017-11-19T01:35:37.295971: step 1360, loss 0.602371, acc 0.771484, f1 0.770523\n",
      "2017-11-19T01:35:37.627467: step 1365, loss 0.658027, acc 0.689453, f1 0.679193\n",
      "Current epoch:  76\n",
      "2017-11-19T01:35:37.933050: step 1370, loss 0.654777, acc 0.695312, f1 0.683078\n",
      "2017-11-19T01:35:38.247836: step 1375, loss 0.617053, acc 0.75, f1 0.750375\n",
      "2017-11-19T01:35:38.544740: step 1380, loss 0.714938, acc 0.704102, f1 0.657459\n",
      "2017-11-19T01:35:38.824062: step 1385, loss 0.682408, acc 0.665039, f1 0.638867\n",
      "Current epoch:  77\n",
      "2017-11-19T01:35:39.110890: step 1390, loss 0.606636, acc 0.75293, f1 0.750349\n",
      "2017-11-19T01:35:39.399817: step 1395, loss 0.687683, acc 0.691406, f1 0.668895\n",
      "2017-11-19T01:35:39.716112: step 1400, loss 0.614271, acc 0.731445, f1 0.725855\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:35:39.943412: step 1400, loss 0.985363, acc 0.513426, f1 0.49286\n",
      "\n",
      "Current epoch:  78\n",
      "2017-11-19T01:35:40.238735: step 1405, loss 0.704057, acc 0.660156, f1 0.634963\n",
      "2017-11-19T01:35:40.534298: step 1410, loss 0.599075, acc 0.764648, f1 0.770114\n",
      "2017-11-19T01:35:40.827333: step 1415, loss 0.6464, acc 0.676758, f1 0.65958\n",
      "2017-11-19T01:35:41.093049: step 1420, loss 0.607272, acc 0.730469, f1 0.719889\n",
      "Current epoch:  79\n",
      "2017-11-19T01:35:41.360905: step 1425, loss 0.625698, acc 0.712891, f1 0.704254\n",
      "2017-11-19T01:35:41.639622: step 1430, loss 0.686673, acc 0.665039, f1 0.631056\n",
      "2017-11-19T01:35:41.959345: step 1435, loss 0.544559, acc 0.795898, f1 0.794461\n",
      "2017-11-19T01:35:42.215430: step 1440, loss 0.785647, acc 0.636792, f1 0.588482\n",
      "Current epoch:  80\n",
      "2017-11-19T01:35:42.531662: step 1445, loss 0.585631, acc 0.743164, f1 0.729854\n",
      "2017-11-19T01:35:42.832070: step 1450, loss 0.631187, acc 0.739258, f1 0.728478\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:35:43.061159: step 1450, loss 0.904679, acc 0.588939, f1 0.562859\n",
      "\n",
      "2017-11-19T01:35:43.378528: step 1455, loss 0.64766, acc 0.71582, f1 0.694553\n",
      "Current epoch:  81\n",
      "2017-11-19T01:35:43.688515: step 1460, loss 0.559225, acc 0.775391, f1 0.772048\n",
      "2017-11-19T01:35:44.008271: step 1465, loss 0.789096, acc 0.620117, f1 0.563202\n",
      "2017-11-19T01:35:44.291443: step 1470, loss 0.566505, acc 0.762695, f1 0.750175\n",
      "2017-11-19T01:35:44.573493: step 1475, loss 0.644075, acc 0.707031, f1 0.695545\n",
      "Current epoch:  82\n",
      "2017-11-19T01:35:44.906188: step 1480, loss 0.546596, acc 0.787109, f1 0.785442\n",
      "2017-11-19T01:35:45.205288: step 1485, loss 0.711631, acc 0.657227, f1 0.629704\n",
      "2017-11-19T01:35:45.515348: step 1490, loss 0.576996, acc 0.748047, f1 0.738481\n",
      "Current epoch:  83\n",
      "2017-11-19T01:35:45.824258: step 1495, loss 0.573686, acc 0.783203, f1 0.78052\n",
      "2017-11-19T01:35:46.130768: step 1500, loss 0.585222, acc 0.723633, f1 0.711035\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:35:46.383463: step 1500, loss 1.05662, acc 0.485799, f1 0.451342\n",
      "\n",
      "2017-11-19T01:35:46.692883: step 1505, loss 0.650868, acc 0.69043, f1 0.678542\n",
      "2017-11-19T01:35:46.982059: step 1510, loss 0.568493, acc 0.764648, f1 0.768259\n",
      "Current epoch:  84\n",
      "2017-11-19T01:35:47.289799: step 1515, loss 0.569944, acc 0.764648, f1 0.737408\n",
      "2017-11-19T01:35:47.558232: step 1520, loss 0.847821, acc 0.59375, f1 0.520575\n",
      "2017-11-19T01:35:47.854638: step 1525, loss 0.533568, acc 0.790039, f1 0.782625\n",
      "2017-11-19T01:35:48.141731: step 1530, loss 0.717567, acc 0.663522, f1 0.623842\n",
      "Current epoch:  85\n",
      "2017-11-19T01:35:48.446337: step 1535, loss 0.577725, acc 0.763672, f1 0.756914\n",
      "2017-11-19T01:35:48.734537: step 1540, loss 0.620554, acc 0.742188, f1 0.719235\n",
      "2017-11-19T01:35:49.023662: step 1545, loss 0.565036, acc 0.772461, f1 0.769987\n",
      "Current epoch:  86\n",
      "2017-11-19T01:35:49.313017: step 1550, loss 0.641594, acc 0.683594, f1 0.660874\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:35:49.561562: step 1550, loss 0.982437, acc 0.567904, f1 0.515214\n",
      "\n",
      "2017-11-19T01:35:49.842725: step 1555, loss 0.569157, acc 0.763672, f1 0.74226\n",
      "2017-11-19T01:35:50.122257: step 1560, loss 0.530994, acc 0.795898, f1 0.794718\n",
      "2017-11-19T01:35:50.449995: step 1565, loss 0.590595, acc 0.732422, f1 0.724118\n",
      "Current epoch:  87\n",
      "2017-11-19T01:35:50.713658: step 1570, loss 0.497597, acc 0.816406, f1 0.815051\n",
      "2017-11-19T01:35:51.000352: step 1575, loss 0.756584, acc 0.700195, f1 0.648718\n",
      "2017-11-19T01:35:51.300982: step 1580, loss 0.589818, acc 0.736328, f1 0.718302\n",
      "Current epoch:  88\n",
      "2017-11-19T01:35:51.567475: step 1585, loss 0.515528, acc 0.820312, f1 0.822927\n",
      "2017-11-19T01:35:51.844520: step 1590, loss 0.527364, acc 0.760742, f1 0.736895\n",
      "2017-11-19T01:35:52.166429: step 1595, loss 0.656354, acc 0.667969, f1 0.635822\n",
      "2017-11-19T01:35:52.494599: step 1600, loss 0.565985, acc 0.72168, f1 0.711841\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:35:52.753868: step 1600, loss 1.09878, acc 0.478092, f1 0.437681\n",
      "\n",
      "Current epoch:  89\n",
      "2017-11-19T01:35:53.096881: step 1605, loss 0.570068, acc 0.753906, f1 0.741686\n",
      "2017-11-19T01:35:53.394786: step 1610, loss 0.621702, acc 0.731445, f1 0.69481\n",
      "2017-11-19T01:35:53.677529: step 1615, loss 0.560328, acc 0.760742, f1 0.7524\n",
      "2017-11-19T01:35:53.969211: step 1620, loss 0.514477, acc 0.83805, f1 0.837838\n",
      "Current epoch:  90\n",
      "2017-11-19T01:35:54.273362: step 1625, loss 0.515878, acc 0.818359, f1 0.818725\n",
      "2017-11-19T01:35:54.580010: step 1630, loss 0.529367, acc 0.764648, f1 0.758835\n",
      "2017-11-19T01:35:54.871506: step 1635, loss 0.690655, acc 0.6875, f1 0.635869\n",
      "Current epoch:  91\n",
      "2017-11-19T01:35:55.144757: step 1640, loss 0.556629, acc 0.779297, f1 0.780437\n",
      "2017-11-19T01:35:55.457722: step 1645, loss 0.569693, acc 0.753906, f1 0.727851\n",
      "2017-11-19T01:35:55.777927: step 1650, loss 0.637703, acc 0.679688, f1 0.6598\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:35:56.020100: step 1650, loss 1.19207, acc 0.454973, f1 0.396913\n",
      "\n",
      "2017-11-19T01:35:56.354929: step 1655, loss 0.510036, acc 0.80957, f1 0.805372\n",
      "Current epoch:  92\n",
      "2017-11-19T01:35:56.640015: step 1660, loss 0.537198, acc 0.758789, f1 0.739945\n",
      "2017-11-19T01:35:56.964014: step 1665, loss 0.533957, acc 0.768555, f1 0.76187\n",
      "2017-11-19T01:35:57.273522: step 1670, loss 0.516566, acc 0.796875, f1 0.794288\n",
      "Current epoch:  93\n",
      "2017-11-19T01:35:57.582950: step 1675, loss 0.696644, acc 0.707031, f1 0.661832\n",
      "2017-11-19T01:35:57.854084: step 1680, loss 0.465334, acc 0.856445, f1 0.858562\n",
      "2017-11-19T01:35:58.171499: step 1685, loss 0.6477, acc 0.667969, f1 0.648377\n",
      "2017-11-19T01:35:58.446561: step 1690, loss 0.483311, acc 0.832031, f1 0.829829\n",
      "Current epoch:  94\n",
      "2017-11-19T01:35:58.734518: step 1695, loss 0.516013, acc 0.775391, f1 0.766212\n",
      "2017-11-19T01:35:59.041941: step 1700, loss 0.566539, acc 0.746094, f1 0.726427\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:35:59.292378: step 1700, loss 0.913719, acc 0.583075, f1 0.566279\n",
      "\n",
      "2017-11-19T01:35:59.653648: step 1705, loss 0.546927, acc 0.737305, f1 0.725663\n",
      "2017-11-19T01:35:59.957596: step 1710, loss 0.482517, acc 0.809748, f1 0.792159\n",
      "Current epoch:  95\n",
      "2017-11-19T01:36:00.261172: step 1715, loss 0.474524, acc 0.84668, f1 0.851512\n",
      "2017-11-19T01:36:00.598220: step 1720, loss 0.708144, acc 0.615234, f1 0.580376\n",
      "2017-11-19T01:36:00.894713: step 1725, loss 0.510163, acc 0.795898, f1 0.779381\n",
      "Current epoch:  96\n",
      "2017-11-19T01:36:01.176536: step 1730, loss 0.536445, acc 0.780273, f1 0.774969\n",
      "2017-11-19T01:36:01.461939: step 1735, loss 0.574219, acc 0.735352, f1 0.722081\n",
      "2017-11-19T01:36:01.762237: step 1740, loss 0.552428, acc 0.735352, f1 0.718728\n",
      "2017-11-19T01:36:02.062422: step 1745, loss 0.478444, acc 0.834961, f1 0.832924\n",
      "Current epoch:  97\n",
      "2017-11-19T01:36:02.365393: step 1750, loss 0.531022, acc 0.775391, f1 0.747506\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:36:02.602194: step 1750, loss 1.05536, acc 0.505622, f1 0.497606\n",
      "\n",
      "2017-11-19T01:36:02.881285: step 1755, loss 0.511443, acc 0.78125, f1 0.773791\n",
      "2017-11-19T01:36:03.171638: step 1760, loss 0.610596, acc 0.710938, f1 0.688714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch:  98\n",
      "2017-11-19T01:36:03.459124: step 1765, loss 0.498652, acc 0.817383, f1 0.796816\n",
      "2017-11-19T01:36:03.753969: step 1770, loss 0.476747, acc 0.833008, f1 0.835228\n",
      "2017-11-19T01:36:04.057348: step 1775, loss 0.762104, acc 0.617188, f1 0.580093\n",
      "2017-11-19T01:36:04.350507: step 1780, loss 0.546726, acc 0.789062, f1 0.755164\n",
      "Current epoch:  99\n",
      "2017-11-19T01:36:04.642998: step 1785, loss 0.468544, acc 0.849609, f1 0.850628\n",
      "2017-11-19T01:36:04.921431: step 1790, loss 0.438679, acc 0.825195, f1 0.820464\n",
      "2017-11-19T01:36:05.195764: step 1795, loss 0.735941, acc 0.640625, f1 0.601642\n",
      "2017-11-19T01:36:05.452227: step 1800, loss 0.503047, acc 0.81761, f1 0.820378\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:36:05.689250: step 1800, loss 1.04654, acc 0.578034, f1 0.537892\n",
      "\n",
      "Current epoch:  100\n",
      "2017-11-19T01:36:06.006089: step 1805, loss 0.483398, acc 0.77832, f1 0.765822\n",
      "2017-11-19T01:36:06.299713: step 1810, loss 0.5389, acc 0.771484, f1 0.760337\n",
      "2017-11-19T01:36:06.584764: step 1815, loss 0.454965, acc 0.820312, f1 0.81904\n",
      "Current epoch:  101\n",
      "2017-11-19T01:36:06.859944: step 1820, loss 0.624531, acc 0.673828, f1 0.652215\n",
      "2017-11-19T01:36:07.157497: step 1825, loss 0.512188, acc 0.803711, f1 0.816046\n",
      "2017-11-19T01:36:07.476851: step 1830, loss 0.394681, acc 0.883789, f1 0.882949\n",
      "2017-11-19T01:36:07.781332: step 1835, loss 0.6287, acc 0.672852, f1 0.64741\n",
      "Current epoch:  102\n",
      "2017-11-19T01:36:08.082672: step 1840, loss 0.421486, acc 0.849609, f1 0.848237\n",
      "2017-11-19T01:36:08.388486: step 1845, loss 0.678999, acc 0.678711, f1 0.661212\n",
      "2017-11-19T01:36:08.701452: step 1850, loss 0.563491, acc 0.732422, f1 0.714921\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:36:08.948412: step 1850, loss 1.04363, acc 0.572363, f1 0.525413\n",
      "\n",
      "Current epoch:  103\n",
      "2017-11-19T01:36:09.259026: step 1855, loss 0.457477, acc 0.828125, f1 0.820068\n",
      "2017-11-19T01:36:09.562859: step 1860, loss 0.43237, acc 0.833984, f1 0.831532\n",
      "2017-11-19T01:36:09.866238: step 1865, loss 0.586125, acc 0.704102, f1 0.694043\n",
      "2017-11-19T01:36:10.193915: step 1870, loss 0.492347, acc 0.798828, f1 0.789852\n",
      "Current epoch:  104\n",
      "2017-11-19T01:36:10.513860: step 1875, loss 0.44607, acc 0.847656, f1 0.85143\n",
      "2017-11-19T01:36:10.838461: step 1880, loss 0.590885, acc 0.699219, f1 0.685258\n",
      "2017-11-19T01:36:11.132781: step 1885, loss 0.404491, acc 0.861328, f1 0.859548\n",
      "2017-11-19T01:36:11.414535: step 1890, loss 0.580148, acc 0.757862, f1 0.759763\n",
      "Current epoch:  105\n",
      "2017-11-19T01:36:11.730325: step 1895, loss 0.473632, acc 0.806641, f1 0.804462\n",
      "2017-11-19T01:36:12.034481: step 1900, loss 0.508556, acc 0.758789, f1 0.754718\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:36:12.277268: step 1900, loss 1.15425, acc 0.477365, f1 0.443862\n",
      "\n",
      "2017-11-19T01:36:12.574737: step 1905, loss 0.406687, acc 0.875977, f1 0.876034\n",
      "Current epoch:  106\n",
      "2017-11-19T01:36:12.867289: step 1910, loss 0.712035, acc 0.762695, f1 0.700923\n",
      "2017-11-19T01:36:13.195189: step 1915, loss 0.581095, acc 0.727539, f1 0.699707\n",
      "2017-11-19T01:36:13.476716: step 1920, loss 0.397567, acc 0.868164, f1 0.867456\n",
      "2017-11-19T01:36:13.798468: step 1925, loss 0.54191, acc 0.739258, f1 0.73589\n",
      "Current epoch:  107\n",
      "2017-11-19T01:36:14.078178: step 1930, loss 0.40165, acc 0.875977, f1 0.875975\n",
      "2017-11-19T01:36:14.375217: step 1935, loss 0.41131, acc 0.831055, f1 0.820637\n",
      "2017-11-19T01:36:14.698418: step 1940, loss 0.46988, acc 0.808594, f1 0.801931\n",
      "Current epoch:  108\n",
      "2017-11-19T01:36:15.007825: step 1945, loss 0.472286, acc 0.806641, f1 0.792821\n",
      "2017-11-19T01:36:15.284979: step 1950, loss 0.371251, acc 0.886719, f1 0.886816\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:36:15.512528: step 1950, loss 1.04384, acc 0.532328, f1 0.52654\n",
      "\n",
      "2017-11-19T01:36:15.826221: step 1955, loss 0.649215, acc 0.675781, f1 0.637075\n",
      "2017-11-19T01:36:16.157883: step 1960, loss 0.410159, acc 0.84375, f1 0.8425\n",
      "Current epoch:  109\n",
      "2017-11-19T01:36:16.440578: step 1965, loss 0.410617, acc 0.854492, f1 0.853661\n",
      "2017-11-19T01:36:16.738945: step 1970, loss 0.49262, acc 0.769531, f1 0.761519\n",
      "2017-11-19T01:36:17.037151: step 1975, loss 0.44364, acc 0.819336, f1 0.816798\n",
      "2017-11-19T01:36:17.354307: step 1980, loss 0.487747, acc 0.779874, f1 0.777647\n",
      "Current epoch:  110\n",
      "2017-11-19T01:36:17.666822: step 1985, loss 0.454237, acc 0.820312, f1 0.813815\n",
      "2017-11-19T01:36:17.974167: step 1990, loss 0.507434, acc 0.787109, f1 0.75162\n",
      "2017-11-19T01:36:18.284512: step 1995, loss 0.492492, acc 0.776367, f1 0.767349\n",
      "Current epoch:  111\n",
      "2017-11-19T01:36:18.561533: step 2000, loss 0.393691, acc 0.84375, f1 0.841795\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:36:18.795700: step 2000, loss 1.0041, acc 0.562718, f1 0.55296\n",
      "\n",
      "2017-11-19T01:36:19.121175: step 2005, loss 0.422343, acc 0.817383, f1 0.815133\n",
      "2017-11-19T01:36:19.460146: step 2010, loss 0.394327, acc 0.84668, f1 0.843559\n",
      "2017-11-19T01:36:19.770285: step 2015, loss 0.454645, acc 0.78125, f1 0.773538\n",
      "Current epoch:  112\n",
      "2017-11-19T01:36:20.098702: step 2020, loss 0.481774, acc 0.805664, f1 0.809239\n",
      "2017-11-19T01:36:20.418466: step 2025, loss 0.354061, acc 0.875977, f1 0.873479\n",
      "2017-11-19T01:36:20.695860: step 2030, loss 0.376317, acc 0.87207, f1 0.870725\n",
      "Current epoch:  113\n",
      "2017-11-19T01:36:21.002547: step 2035, loss 0.55577, acc 0.745117, f1 0.725848\n",
      "2017-11-19T01:36:21.304654: step 2040, loss 0.404851, acc 0.84668, f1 0.830823\n",
      "2017-11-19T01:36:21.585793: step 2045, loss 0.428596, acc 0.831055, f1 0.829166\n",
      "2017-11-19T01:36:21.889303: step 2050, loss 0.424125, acc 0.816406, f1 0.814499\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:36:22.127474: step 2050, loss 1.23238, acc 0.476202, f1 0.441602\n",
      "\n",
      "Current epoch:  114\n",
      "2017-11-19T01:36:22.419933: step 2055, loss 0.541829, acc 0.728516, f1 0.723039\n",
      "2017-11-19T01:36:22.717190: step 2060, loss 0.332132, acc 0.902344, f1 0.902397\n",
      "2017-11-19T01:36:23.032508: step 2065, loss 0.374643, acc 0.868164, f1 0.867444\n",
      "2017-11-19T01:36:23.344697: step 2070, loss 0.52094, acc 0.731132, f1 0.722812\n",
      "Current epoch:  115\n",
      "2017-11-19T01:36:23.649144: step 2075, loss 0.360145, acc 0.893555, f1 0.893673\n",
      "2017-11-19T01:36:23.977694: step 2080, loss 0.399757, acc 0.836914, f1 0.823497\n",
      "2017-11-19T01:36:24.286611: step 2085, loss 0.396985, acc 0.866211, f1 0.867365\n",
      "Current epoch:  116\n",
      "2017-11-19T01:36:24.620774: step 2090, loss 0.533743, acc 0.72168, f1 0.709065\n",
      "2017-11-19T01:36:24.935916: step 2095, loss 0.397533, acc 0.838867, f1 0.837261\n",
      "2017-11-19T01:36:25.262387: step 2100, loss 0.4546, acc 0.821289, f1 0.818511\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:36:25.487698: step 2100, loss 1.04082, acc 0.576047, f1 0.551144\n",
      "\n",
      "2017-11-19T01:36:25.800642: step 2105, loss 0.372935, acc 0.852539, f1 0.851157\n",
      "Current epoch:  117\n",
      "2017-11-19T01:36:26.077897: step 2110, loss 0.531148, acc 0.745117, f1 0.727976\n",
      "2017-11-19T01:36:26.367627: step 2115, loss 0.329044, acc 0.890625, f1 0.889003\n",
      "2017-11-19T01:36:26.652885: step 2120, loss 0.342031, acc 0.889648, f1 0.889226\n",
      "Current epoch:  118\n",
      "2017-11-19T01:36:26.947655: step 2125, loss 0.336618, acc 0.878906, f1 0.877587\n",
      "2017-11-19T01:36:27.236686: step 2130, loss 0.533334, acc 0.748047, f1 0.725076\n",
      "2017-11-19T01:36:27.543907: step 2135, loss 0.370199, acc 0.853516, f1 0.850383\n",
      "2017-11-19T01:36:27.844635: step 2140, loss 0.430653, acc 0.811523, f1 0.801087\n",
      "Current epoch:  119\n",
      "2017-11-19T01:36:28.127677: step 2145, loss 0.421079, acc 0.826172, f1 0.822638\n",
      "2017-11-19T01:36:28.413865: step 2150, loss 0.388799, acc 0.84082, f1 0.823266\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:36:28.651953: step 2150, loss 1.11264, acc 0.519436, f1 0.527428\n",
      "\n",
      "2017-11-19T01:36:29.001759: step 2155, loss 0.324245, acc 0.902344, f1 0.902764\n",
      "2017-11-19T01:36:29.413289: step 2160, loss 0.46938, acc 0.759434, f1 0.753375\n",
      "Current epoch:  120\n",
      "2017-11-19T01:36:29.731050: step 2165, loss 0.333087, acc 0.901367, f1 0.90146\n",
      "2017-11-19T01:36:29.999395: step 2170, loss 0.436034, acc 0.814453, f1 0.807298\n",
      "2017-11-19T01:36:30.302016: step 2175, loss 0.442504, acc 0.826172, f1 0.825293\n",
      "Current epoch:  121\n",
      "2017-11-19T01:36:30.595508: step 2180, loss 0.378326, acc 0.845703, f1 0.83151\n",
      "2017-11-19T01:36:30.891735: step 2185, loss 0.372517, acc 0.866211, f1 0.864465\n",
      "2017-11-19T01:36:31.170559: step 2190, loss 0.593172, acc 0.696289, f1 0.682914\n",
      "2017-11-19T01:36:31.449680: step 2195, loss 0.310912, acc 0.911133, f1 0.910706\n",
      "Current epoch:  122\n",
      "2017-11-19T01:36:31.733013: step 2200, loss 0.325046, acc 0.878906, f1 0.877627\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-19T01:36:31.957604: step 2200, loss 1.23234, acc 0.495686, f1 0.475437\n",
      "\n",
      "2017-11-19T01:36:32.258307: step 2205, loss 0.464677, acc 0.779297, f1 0.77439\n",
      "2017-11-19T01:36:32.552929: step 2210, loss 0.343119, acc 0.879883, f1 0.880471\n",
      "Current epoch:  123\n",
      "2017-11-19T01:36:32.884199: step 2215, loss 0.470476, acc 0.824219, f1 0.782158\n",
      "2017-11-19T01:36:33.194939: step 2220, loss 0.405985, acc 0.833008, f1 0.829954\n",
      "2017-11-19T01:36:33.475442: step 2225, loss 0.352856, acc 0.855469, f1 0.854383\n",
      "2017-11-19T01:36:33.800870: step 2230, loss 0.359514, acc 0.868164, f1 0.869176\n",
      "Current epoch:  124\n",
      "2017-11-19T01:36:34.088506: step 2235, loss 0.308606, acc 0.888672, f1 0.888682\n",
      "2017-11-19T01:36:34.394440: step 2240, loss 0.530755, acc 0.738281, f1 0.731729\n",
      "2017-11-19T01:36:34.686013: step 2245, loss 0.263311, acc 0.941406, f1 0.941399\n",
      "2017-11-19T01:36:34.996898: step 2250, loss 0.282858, acc 0.918239, f1 0.91771\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:36:35.237407: step 2250, loss 1.1042, acc 0.537951, f1 0.542527\n",
      "\n",
      "Current epoch:  125\n",
      "2017-11-19T01:36:35.551614: step 2255, loss 0.26205, acc 0.919922, f1 0.919945\n",
      "2017-11-19T01:36:35.859624: step 2260, loss 0.609844, acc 0.706055, f1 0.665983\n",
      "2017-11-19T01:36:36.137993: step 2265, loss 0.377509, acc 0.858398, f1 0.856321\n",
      "Current epoch:  126\n",
      "2017-11-19T01:36:36.412216: step 2270, loss 0.278797, acc 0.912109, f1 0.908571\n",
      "2017-11-19T01:36:36.724189: step 2275, loss 0.3234, acc 0.896484, f1 0.899375\n",
      "2017-11-19T01:36:37.050780: step 2280, loss 0.491605, acc 0.818359, f1 0.771235\n",
      "2017-11-19T01:36:37.338382: step 2285, loss 0.269797, acc 0.916016, f1 0.91597\n",
      "Current epoch:  127\n",
      "2017-11-19T01:36:37.616292: step 2290, loss 0.457613, acc 0.791992, f1 0.785754\n",
      "2017-11-19T01:36:37.904246: step 2295, loss 0.309448, acc 0.898438, f1 0.898818\n",
      "2017-11-19T01:36:38.214575: step 2300, loss 0.301793, acc 0.901367, f1 0.90084\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:36:38.453048: step 2300, loss 1.13995, acc 0.567274, f1 0.547077\n",
      "\n",
      "Current epoch:  128\n",
      "2017-11-19T01:36:38.773329: step 2305, loss 0.325886, acc 0.866211, f1 0.864116\n",
      "2017-11-19T01:36:39.082203: step 2310, loss 0.305791, acc 0.897461, f1 0.899101\n",
      "2017-11-19T01:36:39.391844: step 2315, loss 0.374288, acc 0.842773, f1 0.820881\n",
      "2017-11-19T01:36:39.675463: step 2320, loss 0.439903, acc 0.787109, f1 0.776869\n",
      "Current epoch:  129\n",
      "2017-11-19T01:36:39.965257: step 2325, loss 0.275394, acc 0.922852, f1 0.922365\n",
      "2017-11-19T01:36:40.249820: step 2330, loss 0.336035, acc 0.853516, f1 0.852964\n",
      "2017-11-19T01:36:40.543052: step 2335, loss 0.34877, acc 0.856445, f1 0.853365\n",
      "2017-11-19T01:36:40.845826: step 2340, loss 0.289112, acc 0.893082, f1 0.893026\n",
      "Current epoch:  130\n",
      "2017-11-19T01:36:41.123898: step 2345, loss 0.227996, acc 0.946289, f1 0.94619\n",
      "2017-11-19T01:36:41.434716: step 2350, loss 0.435241, acc 0.788086, f1 0.780493\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:36:41.672444: step 2350, loss 1.54789, acc 0.446394, f1 0.387296\n",
      "\n",
      "2017-11-19T01:36:42.020671: step 2355, loss 0.278462, acc 0.916016, f1 0.915853\n",
      "Current epoch:  131\n",
      "2017-11-19T01:36:42.313820: step 2360, loss 0.283983, acc 0.888672, f1 0.88765\n",
      "2017-11-19T01:36:42.598809: step 2365, loss 0.345786, acc 0.861328, f1 0.860286\n",
      "2017-11-19T01:36:42.893642: step 2370, loss 0.237462, acc 0.936523, f1 0.936339\n",
      "2017-11-19T01:36:43.163765: step 2375, loss 0.245007, acc 0.927734, f1 0.927648\n",
      "Current epoch:  132\n",
      "2017-11-19T01:36:43.431390: step 2380, loss 0.618231, acc 0.691406, f1 0.648722\n",
      "2017-11-19T01:36:43.728719: step 2385, loss 0.251155, acc 0.927734, f1 0.927525\n",
      "2017-11-19T01:36:44.036561: step 2390, loss 0.233821, acc 0.946289, f1 0.946215\n",
      "Current epoch:  133\n",
      "2017-11-19T01:36:44.314756: step 2395, loss 0.202474, acc 0.945312, f1 0.945276\n",
      "2017-11-19T01:36:44.596146: step 2400, loss 0.383496, acc 0.805664, f1 0.80565\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:36:44.829763: step 2400, loss 1.82797, acc 0.426764, f1 0.349062\n",
      "\n",
      "2017-11-19T01:36:45.102439: step 2405, loss 0.314526, acc 0.892578, f1 0.891142\n",
      "2017-11-19T01:36:45.370546: step 2410, loss 0.233192, acc 0.939453, f1 0.939547\n",
      "Current epoch:  134\n",
      "2017-11-19T01:36:45.688594: step 2415, loss 0.216232, acc 0.94043, f1 0.940379\n",
      "2017-11-19T01:36:46.000273: step 2420, loss 0.364316, acc 0.844727, f1 0.840045\n",
      "2017-11-19T01:36:46.307034: step 2425, loss 0.264005, acc 0.907227, f1 0.905545\n",
      "2017-11-19T01:36:46.572519: step 2430, loss 0.184942, acc 0.965409, f1 0.965362\n",
      "Current epoch:  135\n",
      "2017-11-19T01:36:46.892929: step 2435, loss 0.263176, acc 0.904297, f1 0.897473\n",
      "2017-11-19T01:36:47.196974: step 2440, loss 0.551296, acc 0.733398, f1 0.703488\n",
      "2017-11-19T01:36:47.496714: step 2445, loss 0.37262, acc 0.823242, f1 0.82177\n",
      "Current epoch:  136\n",
      "2017-11-19T01:36:47.778623: step 2450, loss 0.39176, acc 0.834961, f1 0.815048\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:36:48.010688: step 2450, loss 1.26437, acc 0.502617, f1 0.510788\n",
      "\n",
      "2017-11-19T01:36:48.328553: step 2455, loss 0.209734, acc 0.950195, f1 0.950222\n",
      "2017-11-19T01:36:48.611036: step 2460, loss 0.210109, acc 0.945312, f1 0.945254\n",
      "2017-11-19T01:36:48.927781: step 2465, loss 0.215599, acc 0.935547, f1 0.93566\n",
      "Current epoch:  137\n",
      "2017-11-19T01:36:49.235023: step 2470, loss 0.203205, acc 0.945312, f1 0.945293\n",
      "2017-11-19T01:36:49.552849: step 2475, loss 0.556845, acc 0.732422, f1 0.705968\n",
      "2017-11-19T01:36:49.854799: step 2480, loss 0.28629, acc 0.895508, f1 0.894082\n",
      "Current epoch:  138\n",
      "2017-11-19T01:36:50.143744: step 2485, loss 0.196244, acc 0.954102, f1 0.953844\n",
      "2017-11-19T01:36:50.429291: step 2490, loss 0.175791, acc 0.963867, f1 0.963876\n",
      "2017-11-19T01:36:50.700325: step 2495, loss 0.178525, acc 0.953125, f1 0.953185\n",
      "2017-11-19T01:36:50.969453: step 2500, loss 0.190458, acc 0.948242, f1 0.947895\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:36:51.215612: step 2500, loss 1.2181, acc 0.557677, f1 0.549998\n",
      "\n",
      "Current epoch:  139\n",
      "2017-11-19T01:36:51.528069: step 2505, loss 0.215843, acc 0.93457, f1 0.934669\n",
      "2017-11-19T01:36:51.836463: step 2510, loss 0.605696, acc 0.708008, f1 0.666986\n",
      "2017-11-19T01:36:52.164088: step 2515, loss 0.214944, acc 0.941406, f1 0.941416\n",
      "2017-11-19T01:36:52.484218: step 2520, loss 0.185579, acc 0.955975, f1 0.955927\n",
      "Current epoch:  140\n",
      "2017-11-19T01:36:52.806189: step 2525, loss 0.178995, acc 0.958984, f1 0.958928\n",
      "2017-11-19T01:36:53.129194: step 2530, loss 0.175966, acc 0.952148, f1 0.952167\n",
      "2017-11-19T01:36:53.437078: step 2535, loss 0.375101, acc 0.844727, f1 0.857549\n",
      "Current epoch:  141\n",
      "2017-11-19T01:36:53.717473: step 2540, loss 0.171569, acc 0.955078, f1 0.955153\n",
      "2017-11-19T01:36:53.994665: step 2545, loss 0.170353, acc 0.972656, f1 0.972649\n",
      "2017-11-19T01:36:54.292559: step 2550, loss 0.155467, acc 0.970703, f1 0.970578\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:36:54.549945: step 2550, loss 1.26131, acc 0.5364, f1 0.536735\n",
      "\n",
      "2017-11-19T01:36:54.884634: step 2555, loss 0.157281, acc 0.966797, f1 0.966804\n",
      "Current epoch:  142\n",
      "2017-11-19T01:36:55.204743: step 2560, loss 0.191284, acc 0.939453, f1 0.939688\n",
      "2017-11-19T01:36:55.516491: step 2565, loss 0.452312, acc 0.786133, f1 0.774357\n",
      "2017-11-19T01:36:55.832718: step 2570, loss 0.175464, acc 0.96582, f1 0.965848\n",
      "Current epoch:  143\n",
      "2017-11-19T01:36:56.124463: step 2575, loss 0.149863, acc 0.975586, f1 0.975595\n",
      "2017-11-19T01:36:56.432613: step 2580, loss 0.154214, acc 0.963867, f1 0.963917\n",
      "2017-11-19T01:36:56.731435: step 2585, loss 0.157654, acc 0.970703, f1 0.9707\n",
      "2017-11-19T01:36:57.023230: step 2590, loss 0.164378, acc 0.959961, f1 0.959999\n",
      "Current epoch:  144\n",
      "2017-11-19T01:36:57.324929: step 2595, loss 0.160393, acc 0.962891, f1 0.962887\n",
      "2017-11-19T01:36:57.601366: step 2600, loss 0.172017, acc 0.958008, f1 0.957724\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:36:57.834232: step 2600, loss 1.40899, acc 0.505186, f1 0.514176\n",
      "\n",
      "2017-11-19T01:36:58.144875: step 2605, loss 0.423103, acc 0.818359, f1 0.837769\n",
      "2017-11-19T01:36:58.432777: step 2610, loss 0.136683, acc 0.976415, f1 0.976399\n",
      "Current epoch:  145\n",
      "2017-11-19T01:36:58.730845: step 2615, loss 0.150261, acc 0.970703, f1 0.97069\n",
      "2017-11-19T01:36:59.064166: step 2620, loss 0.161924, acc 0.957031, f1 0.957133\n",
      "2017-11-19T01:36:59.368696: step 2625, loss 0.529406, acc 0.733398, f1 0.71744\n",
      "Current epoch:  146\n",
      "2017-11-19T01:36:59.645279: step 2630, loss 0.148821, acc 0.974609, f1 0.974652\n",
      "2017-11-19T01:36:59.930931: step 2635, loss 0.135484, acc 0.974609, f1 0.974578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-19T01:37:00.214529: step 2640, loss 0.144102, acc 0.970703, f1 0.97069\n",
      "2017-11-19T01:37:00.491377: step 2645, loss 0.142054, acc 0.972656, f1 0.972686\n",
      "Current epoch:  147\n",
      "2017-11-19T01:37:00.789558: step 2650, loss 0.124512, acc 0.97168, f1 0.971681\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:37:01.044267: step 2650, loss 1.30822, acc 0.54241, f1 0.541377\n",
      "\n",
      "2017-11-19T01:37:01.379249: step 2655, loss 0.142642, acc 0.970703, f1 0.970617\n",
      "2017-11-19T01:37:01.668228: step 2660, loss 0.14157, acc 0.972656, f1 0.972613\n",
      "Current epoch:  148\n",
      "2017-11-19T01:37:01.956435: step 2665, loss 0.171911, acc 0.955078, f1 0.955131\n",
      "2017-11-19T01:37:02.286114: step 2670, loss 0.429851, acc 0.808594, f1 0.799296\n",
      "2017-11-19T01:37:02.614484: step 2675, loss 0.17276, acc 0.964844, f1 0.964869\n",
      "2017-11-19T01:37:02.886940: step 2680, loss 0.14075, acc 0.974609, f1 0.97463\n",
      "Current epoch:  149\n",
      "2017-11-19T01:37:03.179340: step 2685, loss 0.129859, acc 0.978516, f1 0.97851\n",
      "2017-11-19T01:37:03.451165: step 2690, loss 0.194712, acc 0.929688, f1 0.925722\n",
      "2017-11-19T01:37:03.753029: step 2695, loss 0.204605, acc 0.931641, f1 0.931721\n",
      "2017-11-19T01:37:04.006744: step 2700, loss 0.126404, acc 0.981132, f1 0.981138\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:37:04.238868: step 2700, loss 1.3185, acc 0.539017, f1 0.53938\n",
      "\n",
      "Current epoch:  150\n",
      "2017-11-19T01:37:04.531892: step 2705, loss 0.138487, acc 0.970703, f1 0.970697\n",
      "2017-11-19T01:37:04.821508: step 2710, loss 0.117092, acc 0.982422, f1 0.982438\n",
      "2017-11-19T01:37:05.107356: step 2715, loss 0.130381, acc 0.983398, f1 0.983401\n",
      "Current epoch:  151\n",
      "2017-11-19T01:37:05.389889: step 2720, loss 0.115859, acc 0.983398, f1 0.983377\n",
      "2017-11-19T01:37:05.677940: step 2725, loss 0.133672, acc 0.97168, f1 0.971651\n",
      "2017-11-19T01:37:05.977623: step 2730, loss 0.260051, acc 0.882812, f1 0.882126\n",
      "2017-11-19T01:37:06.289260: step 2735, loss 0.31681, acc 0.847656, f1 0.840947\n",
      "Current epoch:  152\n",
      "2017-11-19T01:37:06.572294: step 2740, loss 0.123884, acc 0.978516, f1 0.978519\n",
      "2017-11-19T01:37:06.897436: step 2745, loss 0.120921, acc 0.983398, f1 0.983407\n",
      "2017-11-19T01:37:07.214104: step 2750, loss 0.127248, acc 0.977539, f1 0.977537\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:37:07.457946: step 2750, loss 1.35132, acc 0.537757, f1 0.538258\n",
      "\n",
      "Current epoch:  153\n",
      "2017-11-19T01:37:07.835536: step 2755, loss 0.107745, acc 0.990234, f1 0.990236\n",
      "2017-11-19T01:37:08.140606: step 2760, loss 0.11843, acc 0.981445, f1 0.981456\n",
      "2017-11-19T01:37:08.440067: step 2765, loss 0.120736, acc 0.976562, f1 0.976561\n",
      "2017-11-19T01:37:08.737635: step 2770, loss 0.108513, acc 0.987305, f1 0.987311\n",
      "Current epoch:  154\n",
      "2017-11-19T01:37:09.053453: step 2775, loss 0.101178, acc 0.987305, f1 0.987325\n",
      "2017-11-19T01:37:09.344760: step 2780, loss 0.121769, acc 0.977539, f1 0.977527\n",
      "2017-11-19T01:37:09.629917: step 2785, loss 0.326177, acc 0.875, f1 0.848153\n",
      "2017-11-19T01:37:09.917340: step 2790, loss 0.193768, acc 0.930818, f1 0.930747\n",
      "Current epoch:  155\n",
      "2017-11-19T01:37:10.196428: step 2795, loss 0.157899, acc 0.956055, f1 0.955931\n",
      "2017-11-19T01:37:10.508597: step 2800, loss 0.182994, acc 0.927734, f1 0.927877\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:37:10.754341: step 2800, loss 1.59937, acc 0.493408, f1 0.480268\n",
      "\n",
      "2017-11-19T01:37:11.039854: step 2805, loss 0.123588, acc 0.972656, f1 0.97263\n",
      "Current epoch:  156\n",
      "2017-11-19T01:37:11.318617: step 2810, loss 0.120329, acc 0.978516, f1 0.978529\n",
      "2017-11-19T01:37:11.611943: step 2815, loss 0.090667, acc 0.990234, f1 0.990235\n",
      "2017-11-19T01:37:11.933953: step 2820, loss 0.0992056, acc 0.984375, f1 0.98437\n",
      "2017-11-19T01:37:12.230838: step 2825, loss 0.122232, acc 0.976562, f1 0.976578\n",
      "Current epoch:  157\n",
      "2017-11-19T01:37:12.524921: step 2830, loss 0.805153, acc 0.65332, f1 0.603039\n",
      "2017-11-19T01:37:12.824326: step 2835, loss 0.186517, acc 0.958984, f1 0.959094\n",
      "2017-11-19T01:37:13.135419: step 2840, loss 0.11064, acc 0.985352, f1 0.98535\n",
      "Current epoch:  158\n",
      "2017-11-19T01:37:13.432517: step 2845, loss 0.111902, acc 0.982422, f1 0.982378\n",
      "2017-11-19T01:37:13.754689: step 2850, loss 0.10888, acc 0.984375, f1 0.98437\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:37:13.980572: step 2850, loss 1.40276, acc 0.530486, f1 0.53162\n",
      "\n",
      "2017-11-19T01:37:14.300666: step 2855, loss 0.100131, acc 0.983398, f1 0.983397\n",
      "2017-11-19T01:37:14.597244: step 2860, loss 0.110914, acc 0.981445, f1 0.981456\n",
      "Current epoch:  159\n",
      "2017-11-19T01:37:14.889425: step 2865, loss 0.100772, acc 0.978516, f1 0.978501\n",
      "2017-11-19T01:37:15.208620: step 2870, loss 0.0922947, acc 0.984375, f1 0.98438\n",
      "2017-11-19T01:37:15.519993: step 2875, loss 0.0912324, acc 0.985352, f1 0.985349\n",
      "2017-11-19T01:37:15.830620: step 2880, loss 0.0817031, acc 0.988994, f1 0.989007\n",
      "Current epoch:  160\n",
      "2017-11-19T01:37:16.153259: step 2885, loss 0.0930558, acc 0.986328, f1 0.986338\n",
      "2017-11-19T01:37:16.476674: step 2890, loss 0.0877749, acc 0.987305, f1 0.987299\n",
      "2017-11-19T01:37:16.765932: step 2895, loss 0.0940624, acc 0.982422, f1 0.982416\n",
      "Current epoch:  161\n",
      "2017-11-19T01:37:17.064216: step 2900, loss 0.632791, acc 0.730469, f1 0.692335\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:37:17.326374: step 2900, loss 2.41691, acc 0.419639, f1 0.343763\n",
      "\n",
      "2017-11-19T01:37:17.687864: step 2905, loss 0.11375, acc 0.985352, f1 0.98536\n",
      "2017-11-19T01:37:17.994821: step 2910, loss 0.0928738, acc 0.990234, f1 0.990242\n",
      "2017-11-19T01:37:18.304531: step 2915, loss 0.0906579, acc 0.988281, f1 0.988284\n",
      "Current epoch:  162\n",
      "2017-11-19T01:37:18.616759: step 2920, loss 0.0884825, acc 0.987305, f1 0.987303\n",
      "2017-11-19T01:37:18.922455: step 2925, loss 0.0885867, acc 0.985352, f1 0.985353\n",
      "2017-11-19T01:37:19.204863: step 2930, loss 0.0914837, acc 0.987305, f1 0.987308\n",
      "Current epoch:  163\n",
      "2017-11-19T01:37:19.490409: step 2935, loss 0.0850045, acc 0.989258, f1 0.989259\n",
      "2017-11-19T01:37:19.758626: step 2940, loss 0.0829777, acc 0.985352, f1 0.985349\n",
      "2017-11-19T01:37:20.083394: step 2945, loss 0.0922335, acc 0.990234, f1 0.990243\n",
      "2017-11-19T01:37:20.386891: step 2950, loss 0.830051, acc 0.814453, f1 0.748632\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:37:20.628110: step 2950, loss 1.65235, acc 0.489434, f1 0.483681\n",
      "\n",
      "Current epoch:  164\n",
      "2017-11-19T01:37:20.982167: step 2955, loss 0.0879375, acc 0.989258, f1 0.989259\n",
      "2017-11-19T01:37:21.300869: step 2960, loss 0.0924052, acc 0.984375, f1 0.984368\n",
      "2017-11-19T01:37:21.598686: step 2965, loss 0.0884633, acc 0.984375, f1 0.984393\n",
      "2017-11-19T01:37:21.867233: step 2970, loss 0.0953855, acc 0.985849, f1 0.985847\n",
      "Current epoch:  165\n",
      "2017-11-19T01:37:22.159546: step 2975, loss 0.0794921, acc 0.992188, f1 0.992189\n",
      "2017-11-19T01:37:22.477023: step 2980, loss 0.0848043, acc 0.985352, f1 0.985354\n",
      "2017-11-19T01:37:22.750345: step 2985, loss 0.105257, acc 0.973633, f1 0.973656\n",
      "Current epoch:  166\n",
      "2017-11-19T01:37:23.049832: step 2990, loss 0.756976, acc 0.685547, f1 0.647056\n",
      "2017-11-19T01:37:23.356475: step 2995, loss 0.0912225, acc 0.991211, f1 0.99121\n",
      "2017-11-19T01:37:23.672626: step 3000, loss 0.0900192, acc 0.987305, f1 0.987302\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:37:23.915631: step 3000, loss 1.47642, acc 0.528742, f1 0.529511\n",
      "\n",
      "2017-11-19T01:37:24.200521: step 3005, loss 0.0865826, acc 0.987305, f1 0.987302\n",
      "Current epoch:  167\n",
      "2017-11-19T01:37:24.474137: step 3010, loss 0.0795214, acc 0.991211, f1 0.991215\n",
      "2017-11-19T01:37:24.775325: step 3015, loss 0.0802108, acc 0.989258, f1 0.989256\n",
      "2017-11-19T01:37:25.076878: step 3020, loss 0.0864947, acc 0.984375, f1 0.984336\n",
      "Current epoch:  168\n",
      "2017-11-19T01:37:25.345899: step 3025, loss 0.0726864, acc 0.991211, f1 0.991212\n",
      "2017-11-19T01:37:25.627558: step 3030, loss 0.0663114, acc 0.995117, f1 0.995119\n",
      "2017-11-19T01:37:25.919384: step 3035, loss 0.0840537, acc 0.989258, f1 0.989248\n",
      "2017-11-19T01:37:26.211458: step 3040, loss 0.0761119, acc 0.988281, f1 0.988282\n",
      "Current epoch:  169\n",
      "2017-11-19T01:37:26.507409: step 3045, loss 0.0780365, acc 0.991211, f1 0.991209\n",
      "2017-11-19T01:37:26.799340: step 3050, loss 0.078509, acc 0.990234, f1 0.990241\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:37:27.119582: step 3050, loss 1.54113, acc 0.537611, f1 0.536357\n",
      "\n",
      "2017-11-19T01:37:27.453846: step 3055, loss 0.0814056, acc 0.983398, f1 0.983419\n",
      "2017-11-19T01:37:27.748141: step 3060, loss 0.62052, acc 0.707547, f1 0.698686\n",
      "Current epoch:  170\n",
      "2017-11-19T01:37:28.063037: step 3065, loss 0.108837, acc 0.981445, f1 0.981349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-19T01:37:28.365029: step 3070, loss 0.0763283, acc 0.992188, f1 0.99219\n",
      "2017-11-19T01:37:28.674905: step 3075, loss 0.0719598, acc 0.993164, f1 0.993163\n",
      "Current epoch:  171\n",
      "2017-11-19T01:37:29.009062: step 3080, loss 0.0723223, acc 0.990234, f1 0.990238\n",
      "2017-11-19T01:37:29.302156: step 3085, loss 0.0674724, acc 0.990234, f1 0.990233\n",
      "2017-11-19T01:37:29.588380: step 3090, loss 0.0730574, acc 0.990234, f1 0.990237\n",
      "2017-11-19T01:37:29.927985: step 3095, loss 0.0736719, acc 0.987305, f1 0.987311\n",
      "Current epoch:  172\n",
      "2017-11-19T01:37:30.222983: step 3100, loss 0.0697884, acc 0.989258, f1 0.989265\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:37:30.453456: step 3100, loss 1.56666, acc 0.536981, f1 0.536481\n",
      "\n",
      "2017-11-19T01:37:30.788945: step 3105, loss 0.071767, acc 0.988281, f1 0.988281\n",
      "2017-11-19T01:37:31.091418: step 3110, loss 0.0784675, acc 0.983398, f1 0.983408\n",
      "Current epoch:  173\n",
      "2017-11-19T01:37:31.491853: step 3115, loss 0.0809828, acc 0.992188, f1 0.99218\n",
      "2017-11-19T01:37:31.752072: step 3120, loss 0.82261, acc 0.717773, f1 0.679149\n",
      "2017-11-19T01:37:32.073587: step 3125, loss 0.0825997, acc 0.991211, f1 0.991212\n",
      "2017-11-19T01:37:32.385459: step 3130, loss 0.0695147, acc 0.994141, f1 0.99414\n",
      "Current epoch:  174\n",
      "2017-11-19T01:37:32.645292: step 3135, loss 0.0605718, acc 0.99707, f1 0.99707\n",
      "2017-11-19T01:37:32.918540: step 3140, loss 0.0606093, acc 0.995117, f1 0.995117\n",
      "2017-11-19T01:37:33.200973: step 3145, loss 0.0687231, acc 0.991211, f1 0.991215\n",
      "2017-11-19T01:37:33.469468: step 3150, loss 0.0651446, acc 0.990566, f1 0.990566\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:37:33.699388: step 3150, loss 1.58656, acc 0.533491, f1 0.533129\n",
      "\n",
      "Current epoch:  175\n",
      "2017-11-19T01:37:34.011492: step 3155, loss 0.0624467, acc 0.996094, f1 0.99609\n",
      "2017-11-19T01:37:34.303554: step 3160, loss 0.0751309, acc 0.986328, f1 0.986328\n",
      "2017-11-19T01:37:34.582870: step 3165, loss 0.0664549, acc 0.990234, f1 0.990239\n",
      "Current epoch:  176\n",
      "2017-11-19T01:37:34.885104: step 3170, loss 0.0625212, acc 0.992188, f1 0.992187\n",
      "2017-11-19T01:37:35.185133: step 3175, loss 0.0558446, acc 0.990234, f1 0.990231\n",
      "2017-11-19T01:37:35.492508: step 3180, loss 0.0633958, acc 0.994141, f1 0.994143\n",
      "2017-11-19T01:37:35.782015: step 3185, loss 0.0984898, acc 0.980469, f1 0.980425\n",
      "Current epoch:  177\n",
      "2017-11-19T01:37:36.059649: step 3190, loss 0.354137, acc 0.829102, f1 0.827587\n",
      "2017-11-19T01:37:36.356513: step 3195, loss 0.080383, acc 0.992188, f1 0.992189\n",
      "2017-11-19T01:37:36.649833: step 3200, loss 0.0693577, acc 0.993164, f1 0.993163\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:37:36.885767: step 3200, loss 1.5804, acc 0.52879, f1 0.529305\n",
      "\n",
      "Current epoch:  178\n",
      "2017-11-19T01:37:37.166641: step 3205, loss 0.0597078, acc 0.993164, f1 0.993165\n",
      "2017-11-19T01:37:37.451491: step 3210, loss 0.0595201, acc 0.993164, f1 0.993164\n",
      "2017-11-19T01:37:37.731966: step 3215, loss 0.0653675, acc 0.988281, f1 0.988284\n",
      "2017-11-19T01:37:38.046457: step 3220, loss 0.0622985, acc 0.991211, f1 0.991202\n",
      "Current epoch:  179\n",
      "2017-11-19T01:37:38.346777: step 3225, loss 0.061412, acc 0.991211, f1 0.991211\n",
      "2017-11-19T01:37:38.664435: step 3230, loss 0.0516836, acc 0.99707, f1 0.99707\n",
      "2017-11-19T01:37:38.997371: step 3235, loss 0.0579553, acc 0.995117, f1 0.995109\n",
      "2017-11-19T01:37:39.301133: step 3240, loss 0.0631402, acc 0.992138, f1 0.992147\n",
      "Current epoch:  180\n",
      "2017-11-19T01:37:39.587233: step 3245, loss 0.0574386, acc 0.992188, f1 0.992192\n",
      "2017-11-19T01:37:39.889855: step 3250, loss 0.0619259, acc 0.989258, f1 0.989252\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:37:40.121872: step 3250, loss 1.71464, acc 0.513329, f1 0.517959\n",
      "\n",
      "2017-11-19T01:37:40.412079: step 3255, loss 0.0811679, acc 0.987305, f1 0.987342\n",
      "Current epoch:  181\n",
      "2017-11-19T01:37:40.710666: step 3260, loss 0.646821, acc 0.716797, f1 0.6929\n",
      "2017-11-19T01:37:41.009430: step 3265, loss 0.0709958, acc 0.993164, f1 0.993163\n",
      "2017-11-19T01:37:41.290076: step 3270, loss 0.0699139, acc 0.993164, f1 0.993151\n",
      "2017-11-19T01:37:41.560209: step 3275, loss 0.0617107, acc 0.994141, f1 0.994142\n",
      "Current epoch:  182\n",
      "2017-11-19T01:37:41.831331: step 3280, loss 0.0633856, acc 0.990234, f1 0.990225\n",
      "2017-11-19T01:37:42.119244: step 3285, loss 0.0545983, acc 0.991211, f1 0.991214\n",
      "2017-11-19T01:37:42.404158: step 3290, loss 0.0588047, acc 0.993164, f1 0.993167\n",
      "Current epoch:  183\n",
      "2017-11-19T01:37:42.690254: step 3295, loss 0.0447713, acc 0.998047, f1 0.998053\n",
      "2017-11-19T01:37:43.022657: step 3300, loss 0.0565075, acc 0.993164, f1 0.993164\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:37:43.284739: step 3300, loss 1.65582, acc 0.529566, f1 0.530124\n",
      "\n",
      "2017-11-19T01:37:43.638029: step 3305, loss 0.0578062, acc 0.995117, f1 0.995117\n",
      "2017-11-19T01:37:43.950973: step 3310, loss 0.0554871, acc 0.992188, f1 0.992191\n",
      "Current epoch:  184\n",
      "2017-11-19T01:37:44.225488: step 3315, loss 0.0553763, acc 0.992188, f1 0.992184\n",
      "2017-11-19T01:37:44.519059: step 3320, loss 0.0556902, acc 0.992188, f1 0.992189\n",
      "2017-11-19T01:37:44.791393: step 3325, loss 0.0497226, acc 0.99707, f1 0.997071\n",
      "2017-11-19T01:37:45.064159: step 3330, loss 0.0733081, acc 0.996855, f1 0.996855\n",
      "Current epoch:  185\n",
      "2017-11-19T01:37:45.362279: step 3335, loss 0.667308, acc 0.6875, f1 0.669968\n",
      "2017-11-19T01:37:45.643879: step 3340, loss 0.0678934, acc 0.989258, f1 0.989258\n",
      "2017-11-19T01:37:45.922771: step 3345, loss 0.0604137, acc 0.993164, f1 0.993163\n",
      "Current epoch:  186\n",
      "2017-11-19T01:37:46.194763: step 3350, loss 0.0523912, acc 0.995117, f1 0.995109\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:37:46.505679: step 3350, loss 1.66667, acc 0.523022, f1 0.524407\n",
      "\n",
      "2017-11-19T01:37:46.847738: step 3355, loss 0.054211, acc 0.995117, f1 0.995117\n",
      "2017-11-19T01:37:47.175648: step 3360, loss 0.0636713, acc 0.987305, f1 0.987307\n",
      "2017-11-19T01:37:47.474951: step 3365, loss 0.0502885, acc 0.994141, f1 0.994141\n",
      "Current epoch:  187\n",
      "2017-11-19T01:37:47.763133: step 3370, loss 0.0499703, acc 0.994141, f1 0.994136\n",
      "2017-11-19T01:37:48.101261: step 3375, loss 0.0468793, acc 0.998047, f1 0.998047\n",
      "2017-11-19T01:37:48.413830: step 3380, loss 0.0468956, acc 0.995117, f1 0.995119\n",
      "Current epoch:  188\n",
      "2017-11-19T01:37:48.713329: step 3385, loss 0.0484773, acc 0.995117, f1 0.995118\n",
      "2017-11-19T01:37:49.012803: step 3390, loss 0.0529431, acc 0.994141, f1 0.994141\n",
      "2017-11-19T01:37:49.332958: step 3395, loss 0.0533322, acc 0.991211, f1 0.991213\n",
      "2017-11-19T01:37:49.628654: step 3400, loss 0.263452, acc 0.895508, f1 0.897518\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:37:49.854356: step 3400, loss 2.60425, acc 0.553412, f1 0.490323\n",
      "\n",
      "Current epoch:  189\n",
      "2017-11-19T01:37:50.152327: step 3405, loss 0.238088, acc 0.905273, f1 0.903879\n",
      "2017-11-19T01:37:50.480562: step 3410, loss 0.0664289, acc 0.991211, f1 0.99121\n",
      "2017-11-19T01:37:50.792463: step 3415, loss 0.0499799, acc 0.999023, f1 0.999023\n",
      "2017-11-19T01:37:51.082907: step 3420, loss 0.0551551, acc 0.990566, f1 0.990565\n",
      "Current epoch:  190\n",
      "2017-11-19T01:37:51.386824: step 3425, loss 0.0510026, acc 0.994141, f1 0.994132\n",
      "2017-11-19T01:37:51.665915: step 3430, loss 0.045505, acc 0.995117, f1 0.995117\n",
      "2017-11-19T01:37:51.964038: step 3435, loss 0.0513392, acc 0.994141, f1 0.99414\n",
      "Current epoch:  191\n",
      "2017-11-19T01:37:52.231486: step 3440, loss 0.0477644, acc 0.993164, f1 0.993169\n",
      "2017-11-19T01:37:52.540460: step 3445, loss 0.0428804, acc 0.994141, f1 0.994141\n",
      "2017-11-19T01:37:52.852835: step 3450, loss 0.0498561, acc 0.993164, f1 0.993163\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:37:53.143385: step 3450, loss 1.72102, acc 0.527869, f1 0.529007\n",
      "\n",
      "2017-11-19T01:37:53.500127: step 3455, loss 0.0469064, acc 0.992188, f1 0.992191\n",
      "Current epoch:  192\n",
      "2017-11-19T01:37:53.768624: step 3460, loss 0.0484377, acc 0.994141, f1 0.99414\n",
      "2017-11-19T01:37:54.068462: step 3465, loss 0.0462962, acc 0.991211, f1 0.99121\n",
      "2017-11-19T01:37:54.403959: step 3470, loss 0.0427397, acc 0.992188, f1 0.992185\n",
      "Current epoch:  193\n",
      "2017-11-19T01:37:54.687593: step 3475, loss 0.048072, acc 0.996094, f1 0.996094\n",
      "2017-11-19T01:37:54.988615: step 3480, loss 0.368262, acc 0.814453, f1 0.811991\n",
      "2017-11-19T01:37:55.299835: step 3485, loss 0.0647177, acc 0.991211, f1 0.991217\n",
      "2017-11-19T01:37:55.601602: step 3490, loss 0.058234, acc 0.993164, f1 0.993169\n",
      "Current epoch:  194\n",
      "2017-11-19T01:37:55.907978: step 3495, loss 0.0504295, acc 0.991211, f1 0.991199\n",
      "2017-11-19T01:37:56.229165: step 3500, loss 0.0454311, acc 0.991211, f1 0.991207\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-19T01:37:56.459367: step 3500, loss 1.71478, acc 0.527966, f1 0.528149\n",
      "\n",
      "2017-11-19T01:37:56.772758: step 3505, loss 0.0487029, acc 0.993164, f1 0.993164\n",
      "2017-11-19T01:37:57.058752: step 3510, loss 0.0438689, acc 0.998428, f1 0.998428\n",
      "Current epoch:  195\n",
      "2017-11-19T01:37:57.372699: step 3515, loss 0.0408479, acc 0.99707, f1 0.997071\n",
      "2017-11-19T01:37:57.697628: step 3520, loss 0.0440775, acc 0.99707, f1 0.997071\n",
      "2017-11-19T01:37:57.995780: step 3525, loss 0.0412815, acc 0.992188, f1 0.992187\n",
      "Current epoch:  196\n",
      "2017-11-19T01:37:58.290905: step 3530, loss 0.0417428, acc 0.995117, f1 0.995118\n",
      "2017-11-19T01:37:58.594788: step 3535, loss 0.0425555, acc 0.996094, f1 0.996093\n",
      "2017-11-19T01:37:58.899757: step 3540, loss 0.038881, acc 0.996094, f1 0.996094\n",
      "2017-11-19T01:37:59.241095: step 3545, loss 0.0447995, acc 0.996094, f1 0.996094\n",
      "Current epoch:  197\n",
      "2017-11-19T01:37:59.543884: step 3550, loss 0.0434915, acc 0.995117, f1 0.995116\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:37:59.797870: step 3550, loss 1.78712, acc 0.526221, f1 0.52771\n",
      "\n",
      "2017-11-19T01:38:00.104682: step 3555, loss 0.043249, acc 0.995117, f1 0.995106\n",
      "2017-11-19T01:38:00.400626: step 3560, loss 0.0555861, acc 0.994141, f1 0.994166\n",
      "Current epoch:  198\n",
      "2017-11-19T01:38:00.732717: step 3565, loss 0.0899318, acc 0.981445, f1 0.981463\n",
      "2017-11-19T01:38:01.009335: step 3570, loss 0.0564251, acc 0.991211, f1 0.991207\n",
      "2017-11-19T01:38:01.301022: step 3575, loss 0.0504354, acc 0.994141, f1 0.994143\n",
      "2017-11-19T01:38:01.588526: step 3580, loss 0.0428019, acc 0.993164, f1 0.993161\n",
      "Current epoch:  199\n",
      "2017-11-19T01:38:01.858621: step 3585, loss 0.0342089, acc 0.998047, f1 0.998047\n",
      "2017-11-19T01:38:02.180177: step 3590, loss 0.0391544, acc 0.995117, f1 0.995114\n",
      "2017-11-19T01:38:02.499451: step 3595, loss 0.0441176, acc 0.994141, f1 0.994136\n",
      "2017-11-19T01:38:02.787161: step 3600, loss 0.0415657, acc 0.993711, f1 0.993711\n",
      "\n",
      "Evaluation:\n",
      "2017-11-19T01:38:03.016864: step 3600, loss 1.80612, acc 0.527384, f1 0.528598\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:45:49.310977: step 28650, loss 2.2659, acc 0.591896, f1 0.590577\n",
      "\n",
      "2017-11-15T23:45:49.825484: step 28655, loss 0.00386814, acc 0.996094, f1 0.996094\n",
      "Current epoch:  1592\n",
      "2017-11-15T23:45:50.310022: step 28660, loss 0.00153759, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:45:50.822744: step 28665, loss 0.00258661, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:45:51.352169: step 28670, loss 0.00137868, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1593\n",
      "2017-11-15T23:45:51.842602: step 28675, loss 0.000639613, acc 1, f1 1\n",
      "2017-11-15T23:45:52.360281: step 28680, loss 0.00294478, acc 0.99707, f1 0.997082\n",
      "2017-11-15T23:45:52.875169: step 28685, loss 0.00427626, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:45:53.398795: step 28690, loss 0.00130105, acc 0.999023, f1 0.999022\n",
      "Current epoch:  1594\n",
      "2017-11-15T23:45:53.891669: step 28695, loss 0.00191334, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:45:54.417607: step 28700, loss 5.42638e-05, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:45:55.096665: step 28700, loss 2.28504, acc 0.584141, f1 0.58365\n",
      "\n",
      "2017-11-15T23:45:55.613080: step 28705, loss 0.00190577, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:45:56.097712: step 28710, loss 0.00352573, acc 0.996855, f1 0.996852\n",
      "Current epoch:  1595\n",
      "2017-11-15T23:45:56.631189: step 28715, loss 0.00353466, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:45:57.172158: step 28720, loss 0.00369558, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:45:57.702745: step 28725, loss 0.00318799, acc 0.99707, f1 0.997067\n",
      "Current epoch:  1596\n",
      "2017-11-15T23:45:58.204540: step 28730, loss 0.000896501, acc 1, f1 1\n",
      "2017-11-15T23:45:58.727528: step 28735, loss 0.00287855, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:45:59.268091: step 28740, loss 0.000817851, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:45:59.803454: step 28745, loss 0.00462742, acc 0.99707, f1 0.997075\n",
      "Current epoch:  1597\n",
      "2017-11-15T23:46:00.310321: step 28750, loss 0.00118373, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:46:00.963777: step 28750, loss 2.33896, acc 0.577598, f1 0.57679\n",
      "\n",
      "2017-11-15T23:46:01.476367: step 28755, loss 0.00136915, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:46:01.991230: step 28760, loss 0.000769768, acc 0.999023, f1 0.999024\n",
      "Current epoch:  1598\n",
      "2017-11-15T23:46:02.511188: step 28765, loss 0.00133187, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:46:03.040044: step 28770, loss 0.000610612, acc 1, f1 1\n",
      "2017-11-15T23:46:03.559971: step 28775, loss 0.00252296, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:46:04.081974: step 28780, loss 0.000598162, acc 1, f1 1\n",
      "Current epoch:  1599\n",
      "2017-11-15T23:46:04.571542: step 28785, loss 0.000851526, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:46:05.117677: step 28790, loss 9.57802e-05, acc 1, f1 1\n",
      "2017-11-15T23:46:05.678565: step 28795, loss 0.00585091, acc 0.995117, f1 0.995114\n",
      "2017-11-15T23:46:06.186913: step 28800, loss 0.00728073, acc 0.993711, f1 0.993706\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:46:06.855731: step 28800, loss 2.33603, acc 0.57246, f1 0.57408\n",
      "\n",
      "Current epoch:  1600\n",
      "2017-11-15T23:46:07.384843: step 28805, loss 0.0025765, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:46:07.914237: step 28810, loss 0.00350469, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:46:08.432157: step 28815, loss 0.00397736, acc 0.99707, f1 0.997066\n",
      "Current epoch:  1601\n",
      "2017-11-15T23:46:08.923942: step 28820, loss 0.00257787, acc 0.998047, f1 0.998051\n",
      "2017-11-15T23:46:09.437911: step 28825, loss 0.00149671, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:46:09.957828: step 28830, loss 0.00197341, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:46:10.479228: step 28835, loss 0.00113885, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1602\n",
      "2017-11-15T23:46:10.979167: step 28840, loss 0.000734356, acc 1, f1 1\n",
      "2017-11-15T23:46:11.496746: step 28845, loss 0.00360641, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:46:12.010633: step 28850, loss 5.95653e-05, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:46:12.650334: step 28850, loss 2.2717, acc 0.587388, f1 0.586073\n",
      "\n",
      "Current epoch:  1603\n",
      "2017-11-15T23:46:13.133851: step 28855, loss 0.00113633, acc 1, f1 1\n",
      "2017-11-15T23:46:13.675590: step 28860, loss 0.00321642, acc 0.998047, f1 0.998052\n",
      "2017-11-15T23:46:14.209256: step 28865, loss 0.000614053, acc 1, f1 1\n",
      "2017-11-15T23:46:14.744903: step 28870, loss 0.00211905, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1604\n",
      "2017-11-15T23:46:15.234811: step 28875, loss 0.0021759, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:46:15.752127: step 28880, loss 0.00187753, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:46:16.270686: step 28885, loss 0.00204933, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:46:16.783637: step 28890, loss 0.00541914, acc 0.995283, f1 0.995277\n",
      "Current epoch:  1605\n",
      "2017-11-15T23:46:17.321679: step 28895, loss 0.0022722, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:46:17.884307: step 28900, loss 0.00248561, acc 0.998047, f1 0.998046\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:46:18.617395: step 28900, loss 2.31449, acc 0.576434, f1 0.579359\n",
      "\n",
      "2017-11-15T23:46:19.202006: step 28905, loss 0.00197378, acc 0.998047, f1 0.998046\n",
      "Current epoch:  1606\n",
      "2017-11-15T23:46:19.721927: step 28910, loss 0.000884493, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:46:20.297501: step 28915, loss 0.00150663, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:46:20.834969: step 28920, loss 0.00125085, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:46:21.357397: step 28925, loss 0.000914062, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1607\n",
      "2017-11-15T23:46:21.847236: step 28930, loss 0.000587061, acc 1, f1 1\n",
      "2017-11-15T23:46:22.370218: step 28935, loss 0.00336769, acc 0.99707, f1 0.997074\n",
      "2017-11-15T23:46:22.926239: step 28940, loss 8.29721e-05, acc 1, f1 1\n",
      "Current epoch:  1608\n",
      "2017-11-15T23:46:23.438637: step 28945, loss 0.0024986, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:46:23.990145: step 28950, loss 0.00286065, acc 0.99707, f1 0.997074\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:46:24.761254: step 28950, loss 2.31169, acc 0.596209, f1 0.590797\n",
      "\n",
      "2017-11-15T23:46:25.321283: step 28955, loss 0.000781407, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:46:25.839198: step 28960, loss 0.00181558, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1609\n",
      "2017-11-15T23:46:26.330040: step 28965, loss 0.000517247, acc 1, f1 1\n",
      "2017-11-15T23:46:26.849962: step 28970, loss 0.00271883, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:46:27.373456: step 28975, loss 0.00412468, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:46:27.860286: step 28980, loss 0.00389721, acc 0.996855, f1 0.996856\n",
      "Current epoch:  1610\n",
      "2017-11-15T23:46:28.381712: step 28985, loss 0.0018714, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:46:28.928417: step 28990, loss 0.00135425, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:46:29.472907: step 28995, loss 0.0033754, acc 0.99707, f1 0.997069\n",
      "Current epoch:  1611\n",
      "2017-11-15T23:46:29.988816: step 29000, loss 0.00270321, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:46:30.640137: step 29000, loss 2.33339, acc 0.591072, f1 0.585668\n",
      "\n",
      "2017-11-15T23:46:31.156046: step 29005, loss 0.00169521, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:46:31.674965: step 29010, loss 0.00201353, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:46:32.184357: step 29015, loss 0.00193158, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1612\n",
      "2017-11-15T23:46:32.671251: step 29020, loss 0.00295453, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:46:33.188664: step 29025, loss 0.00232952, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:46:33.704574: step 29030, loss 0.00196463, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1613\n",
      "2017-11-15T23:46:34.191404: step 29035, loss 0.00128544, acc 1, f1 1\n",
      "2017-11-15T23:46:34.703304: step 29040, loss 0.00386822, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:46:35.223735: step 29045, loss 0.00207276, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:46:35.754185: step 29050, loss 0.00113251, acc 0.999023, f1 0.999024\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:46:36.389928: step 29050, loss 2.28186, acc 0.590975, f1 0.5877\n",
      "\n",
      "Current epoch:  1614\n",
      "2017-11-15T23:46:36.889292: step 29055, loss 6.33895e-05, acc 1, f1 1\n",
      "2017-11-15T23:46:37.418238: step 29060, loss 5.39124e-05, acc 1, f1 1\n",
      "2017-11-15T23:46:37.964231: step 29065, loss 5.69985e-05, acc 1, f1 1\n",
      "2017-11-15T23:46:38.449056: step 29070, loss 0.0042544, acc 0.996855, f1 0.996856\n",
      "Current epoch:  1615\n",
      "2017-11-15T23:46:38.992039: step 29075, loss 0.00348864, acc 0.998047, f1 0.998052\n",
      "2017-11-15T23:46:39.545552: step 29080, loss 0.00318483, acc 0.99707, f1 0.997066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:46:40.067980: step 29085, loss 0.00184323, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1616\n",
      "2017-11-15T23:46:40.556816: step 29090, loss 0.00191371, acc 1, f1 1\n",
      "2017-11-15T23:46:41.092781: step 29095, loss 0.00320513, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:46:41.608201: step 29100, loss 0.00118186, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:46:42.262991: step 29100, loss 2.27078, acc 0.59112, f1 0.590283\n",
      "\n",
      "2017-11-15T23:46:42.781910: step 29105, loss 0.00250555, acc 0.99707, f1 0.997071\n",
      "Current epoch:  1617\n",
      "2017-11-15T23:46:43.274756: step 29110, loss 0.0033572, acc 0.996094, f1 0.996096\n",
      "2017-11-15T23:46:43.793173: step 29115, loss 6.6246e-05, acc 1, f1 1\n",
      "2017-11-15T23:46:44.310588: step 29120, loss 0.00123017, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1618\n",
      "2017-11-15T23:46:44.805440: step 29125, loss 0.000822068, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:46:45.320347: step 29130, loss 0.00328387, acc 0.99707, f1 0.997072\n",
      "2017-11-15T23:46:45.838765: step 29135, loss 0.0010543, acc 1, f1 1\n",
      "2017-11-15T23:46:46.363699: step 29140, loss 0.00220608, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1619\n",
      "2017-11-15T23:46:46.862563: step 29145, loss 0.000667431, acc 1, f1 1\n",
      "2017-11-15T23:46:47.385492: step 29150, loss 0.00231677, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:46:48.042287: step 29150, loss 2.25707, acc 0.588067, f1 0.587913\n",
      "\n",
      "2017-11-15T23:46:48.557937: step 29155, loss 0.00317506, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:46:49.044328: step 29160, loss 0.00144926, acc 0.998428, f1 0.998428\n",
      "Current epoch:  1620\n",
      "2017-11-15T23:46:49.568790: step 29165, loss 0.00168992, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:46:50.082494: step 29170, loss 0.00162832, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:46:50.601601: step 29175, loss 0.00112136, acc 0.999023, f1 0.999022\n",
      "Current epoch:  1621\n",
      "2017-11-15T23:46:51.092431: step 29180, loss 0.00157454, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:46:51.617668: step 29185, loss 0.00159915, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:46:52.156910: step 29190, loss 0.00328798, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:46:52.677842: step 29195, loss 0.00371784, acc 0.99707, f1 0.997072\n",
      "Current epoch:  1622\n",
      "2017-11-15T23:46:53.169758: step 29200, loss 0.000617335, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:46:53.813343: step 29200, loss 2.35635, acc 0.574205, f1 0.573801\n",
      "\n",
      "2017-11-15T23:46:54.356949: step 29205, loss 0.00336526, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:46:54.886465: step 29210, loss 0.00234199, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1623\n",
      "2017-11-15T23:46:55.426214: step 29215, loss 0.00356352, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:46:55.981233: step 29220, loss 0.00148887, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:46:56.518198: step 29225, loss 0.0028256, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:46:57.062687: step 29230, loss 0.00234347, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1624\n",
      "2017-11-15T23:46:57.585120: step 29235, loss 0.0035096, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:46:58.129107: step 29240, loss 0.00217946, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:46:58.658052: step 29245, loss 0.00545671, acc 0.995117, f1 0.995118\n",
      "2017-11-15T23:46:59.175466: step 29250, loss 0.0019423, acc 0.998428, f1 0.998431\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:46:59.839282: step 29250, loss 2.44628, acc 0.568631, f1 0.565616\n",
      "\n",
      "Current epoch:  1625\n",
      "2017-11-15T23:47:00.364216: step 29255, loss 0.00369041, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:47:00.896709: step 29260, loss 0.00282545, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:47:01.417132: step 29265, loss 0.000923748, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1626\n",
      "2017-11-15T23:47:02.010013: step 29270, loss 0.00321189, acc 0.99707, f1 0.997073\n",
      "2017-11-15T23:47:02.526926: step 29275, loss 0.00170796, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:47:03.050858: step 29280, loss 0.00114013, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:47:03.571781: step 29285, loss 0.00215799, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1627\n",
      "2017-11-15T23:47:04.065130: step 29290, loss 7.77965e-05, acc 1, f1 1\n",
      "2017-11-15T23:47:04.575525: step 29295, loss 0.00154255, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:47:05.095446: step 29300, loss 0.00187322, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:47:05.732062: step 29300, loss 2.26418, acc 0.588745, f1 0.588432\n",
      "\n",
      "Current epoch:  1628\n",
      "2017-11-15T23:47:06.265519: step 29305, loss 0.00215498, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:47:06.792597: step 29310, loss 0.00290525, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:47:07.309011: step 29315, loss 0.00164449, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:47:07.822995: step 29320, loss 0.00534744, acc 0.995117, f1 0.995107\n",
      "Current epoch:  1629\n",
      "2017-11-15T23:47:08.321011: step 29325, loss 0.00292516, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:47:08.840897: step 29330, loss 0.0048403, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:47:09.364495: step 29335, loss 0.00151689, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:47:09.859527: step 29340, loss 0.00137108, acc 0.998428, f1 0.998425\n",
      "Current epoch:  1630\n",
      "2017-11-15T23:47:10.385445: step 29345, loss 0.00291826, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:47:10.902997: step 29350, loss 0.00330209, acc 0.99707, f1 0.99707\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:47:11.543058: step 29350, loss 2.27162, acc 0.585013, f1 0.585143\n",
      "\n",
      "2017-11-15T23:47:12.056154: step 29355, loss 0.00329945, acc 0.99707, f1 0.997071\n",
      "Current epoch:  1631\n",
      "2017-11-15T23:47:12.559644: step 29360, loss 0.0014265, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:47:13.089345: step 29365, loss 0.000720258, acc 1, f1 1\n",
      "2017-11-15T23:47:13.629966: step 29370, loss 0.00135828, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:47:14.167194: step 29375, loss 0.00274147, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1632\n",
      "2017-11-15T23:47:14.667610: step 29380, loss 0.00314262, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:47:15.196499: step 29385, loss 0.00344421, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:47:15.717230: step 29390, loss 0.00123032, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1633\n",
      "2017-11-15T23:47:16.204084: step 29395, loss 0.00180935, acc 1, f1 1\n",
      "2017-11-15T23:47:16.729750: step 29400, loss 0.00151687, acc 0.999023, f1 0.999022\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:47:17.389537: step 29400, loss 2.26357, acc 0.584286, f1 0.585445\n",
      "\n",
      "2017-11-15T23:47:17.914768: step 29405, loss 0.00225758, acc 0.998047, f1 0.998052\n",
      "2017-11-15T23:47:18.448768: step 29410, loss 0.00230427, acc 0.998047, f1 0.998043\n",
      "Current epoch:  1634\n",
      "2017-11-15T23:47:18.958750: step 29415, loss 0.00261065, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:47:19.500531: step 29420, loss 0.00313391, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:47:20.018986: step 29425, loss 0.00513042, acc 0.995117, f1 0.99512\n",
      "2017-11-15T23:47:20.504446: step 29430, loss 8.46339e-05, acc 1, f1 1\n",
      "Current epoch:  1635\n",
      "2017-11-15T23:47:21.018544: step 29435, loss 0.000835257, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:47:21.553502: step 29440, loss 0.00194192, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:47:22.093459: step 29445, loss 0.00300792, acc 0.99707, f1 0.997072\n",
      "Current epoch:  1636\n",
      "2017-11-15T23:47:22.589178: step 29450, loss 0.00244137, acc 0.99707, f1 0.997071\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:47:23.229826: step 29450, loss 2.34458, acc 0.577452, f1 0.576171\n",
      "\n",
      "2017-11-15T23:47:23.752630: step 29455, loss 9.34496e-05, acc 1, f1 1\n",
      "2017-11-15T23:47:24.263537: step 29460, loss 0.00276245, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:47:24.808031: step 29465, loss 0.00196869, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1637\n",
      "2017-11-15T23:47:25.319131: step 29470, loss 0.00290729, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:47:25.845317: step 29475, loss 0.00179491, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:47:26.374653: step 29480, loss 0.00150116, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1638\n",
      "2017-11-15T23:47:26.878619: step 29485, loss 0.00155409, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:47:27.401564: step 29490, loss 0.0040738, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:47:27.937820: step 29495, loss 0.00150434, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:47:28.463795: step 29500, loss 0.00209373, acc 0.998047, f1 0.998046\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:47:29.141951: step 29500, loss 2.32703, acc 0.575562, f1 0.57594\n",
      "\n",
      "Current epoch:  1639\n",
      "2017-11-15T23:47:29.643159: step 29505, loss 0.00162885, acc 0.998047, f1 0.998052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:47:30.286891: step 29510, loss 0.00205923, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:47:30.807905: step 29515, loss 0.00250676, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:47:31.302672: step 29520, loss 6.4664e-05, acc 1, f1 1\n",
      "Current epoch:  1640\n",
      "2017-11-15T23:47:31.828511: step 29525, loss 0.000496725, acc 1, f1 1\n",
      "2017-11-15T23:47:32.363044: step 29530, loss 7.05721e-05, acc 1, f1 1\n",
      "2017-11-15T23:47:32.900029: step 29535, loss 0.00557571, acc 0.995117, f1 0.995118\n",
      "Current epoch:  1641\n",
      "2017-11-15T23:47:33.418057: step 29540, loss 0.00222187, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:47:33.948509: step 29545, loss 0.00401678, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:47:34.497008: step 29550, loss 0.00285274, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:47:35.260093: step 29550, loss 2.30217, acc 0.577695, f1 0.579102\n",
      "\n",
      "2017-11-15T23:47:35.792047: step 29555, loss 8.26115e-05, acc 1, f1 1\n",
      "Current epoch:  1642\n",
      "2017-11-15T23:47:36.355086: step 29560, loss 0.00165775, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:47:36.882025: step 29565, loss 0.00125234, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:47:37.403953: step 29570, loss 0.00376067, acc 0.996094, f1 0.996093\n",
      "Current epoch:  1643\n",
      "2017-11-15T23:47:37.901813: step 29575, loss 0.00258502, acc 0.998047, f1 0.99805\n",
      "2017-11-15T23:47:38.413713: step 29580, loss 0.000964757, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:47:38.936140: step 29585, loss 0.00490976, acc 0.996094, f1 0.996093\n",
      "2017-11-15T23:47:39.457065: step 29590, loss 0.000782273, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1644\n",
      "2017-11-15T23:47:39.952919: step 29595, loss 0.00167063, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:47:40.470835: step 29600, loss 5.77095e-05, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:47:41.117187: step 29600, loss 2.3253, acc 0.580409, f1 0.579722\n",
      "\n",
      "2017-11-15T23:47:41.648640: step 29605, loss 0.00203748, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:47:42.142489: step 29610, loss 0.00353227, acc 0.996855, f1 0.996857\n",
      "Current epoch:  1645\n",
      "2017-11-15T23:47:42.670934: step 29615, loss 0.00368773, acc 0.99707, f1 0.997072\n",
      "2017-11-15T23:47:43.189127: step 29620, loss 0.00230618, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:47:43.715584: step 29625, loss 6.00431e-05, acc 1, f1 1\n",
      "Current epoch:  1646\n",
      "2017-11-15T23:47:44.202916: step 29630, loss 0.00220885, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:47:44.725344: step 29635, loss 0.00365282, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:47:45.241255: step 29640, loss 0.00220848, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:47:45.761805: step 29645, loss 0.00455893, acc 0.996094, f1 0.99609\n",
      "Current epoch:  1647\n",
      "2017-11-15T23:47:46.243641: step 29650, loss 0.00072701, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:47:46.892496: step 29650, loss 2.27577, acc 0.58196, f1 0.584037\n",
      "\n",
      "2017-11-15T23:47:47.412606: step 29655, loss 0.00399788, acc 0.99707, f1 0.997081\n",
      "2017-11-15T23:47:47.928651: step 29660, loss 0.00220317, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1648\n",
      "2017-11-15T23:47:48.414151: step 29665, loss 0.00194389, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:47:48.942530: step 29670, loss 0.00449868, acc 0.995117, f1 0.995116\n",
      "2017-11-15T23:47:49.475989: step 29675, loss 0.0017258, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:47:50.000906: step 29680, loss 0.00295449, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1649\n",
      "2017-11-15T23:47:50.491793: step 29685, loss 0.00140587, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:47:51.008421: step 29690, loss 7.417e-05, acc 1, f1 1\n",
      "2017-11-15T23:47:51.518628: step 29695, loss 0.00231939, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:47:52.011608: step 29700, loss 0.00523332, acc 0.995283, f1 0.995288\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:47:52.691764: step 29700, loss 2.53332, acc 0.561409, f1 0.556839\n",
      "\n",
      "Current epoch:  1650\n",
      "2017-11-15T23:47:53.207678: step 29705, loss 0.0025856, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:47:53.735695: step 29710, loss 0.00150672, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:47:54.259780: step 29715, loss 0.000888832, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1651\n",
      "2017-11-15T23:47:54.752652: step 29720, loss 0.00156486, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:47:55.271587: step 29725, loss 0.00366124, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:47:55.796342: step 29730, loss 0.0018743, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:47:56.327344: step 29735, loss 7.79035e-05, acc 1, f1 1\n",
      "Current epoch:  1652\n",
      "2017-11-15T23:47:56.842262: step 29740, loss 0.00229709, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:47:57.367868: step 29745, loss 0.00182747, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:47:57.897901: step 29750, loss 0.0031979, acc 0.99707, f1 0.997071\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:47:58.569727: step 29750, loss 2.31521, acc 0.580457, f1 0.580401\n",
      "\n",
      "Current epoch:  1653\n",
      "2017-11-15T23:47:59.064321: step 29755, loss 0.00186992, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:47:59.582355: step 29760, loss 0.00223528, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:48:00.097295: step 29765, loss 0.00170031, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:48:00.621666: step 29770, loss 0.00386541, acc 0.996094, f1 0.996096\n",
      "Current epoch:  1654\n",
      "2017-11-15T23:48:01.122064: step 29775, loss 0.00129352, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:48:01.642914: step 29780, loss 0.00126737, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:48:02.164692: step 29785, loss 7.42227e-05, acc 1, f1 1\n",
      "2017-11-15T23:48:02.646406: step 29790, loss 0.00175803, acc 0.998428, f1 0.998428\n",
      "Current epoch:  1655\n",
      "2017-11-15T23:48:03.172955: step 29795, loss 0.00427168, acc 0.995117, f1 0.995117\n",
      "2017-11-15T23:48:03.705602: step 29800, loss 0.00200166, acc 0.998047, f1 0.998046\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:48:04.339260: step 29800, loss 2.29064, acc 0.583705, f1 0.583818\n",
      "\n",
      "2017-11-15T23:48:04.854282: step 29805, loss 0.00112932, acc 0.999023, f1 0.999022\n",
      "Current epoch:  1656\n",
      "2017-11-15T23:48:05.350155: step 29810, loss 0.000408565, acc 1, f1 1\n",
      "2017-11-15T23:48:05.875187: step 29815, loss 5.31391e-05, acc 1, f1 1\n",
      "2017-11-15T23:48:06.396773: step 29820, loss 0.00346716, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:48:06.932729: step 29825, loss 0.00441484, acc 0.99707, f1 0.997072\n",
      "Current epoch:  1657\n",
      "2017-11-15T23:48:07.445001: step 29830, loss 0.00363731, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:48:07.979309: step 29835, loss 0.00422572, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:48:08.513023: step 29840, loss 0.00262929, acc 0.998047, f1 0.998052\n",
      "Current epoch:  1658\n",
      "2017-11-15T23:48:09.028551: step 29845, loss 0.00318895, acc 0.99707, f1 0.997075\n",
      "2017-11-15T23:48:09.552988: step 29850, loss 0.00199969, acc 0.998047, f1 0.998043\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:48:10.189089: step 29850, loss 2.33967, acc 0.577355, f1 0.577533\n",
      "\n",
      "2017-11-15T23:48:10.708850: step 29855, loss 0.00103167, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:48:11.217766: step 29860, loss 0.000815477, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1659\n",
      "2017-11-15T23:48:11.705044: step 29865, loss 0.00272254, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:48:12.226487: step 29870, loss 0.00189454, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:48:12.760531: step 29875, loss 0.00125478, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:48:13.257552: step 29880, loss 0.00126221, acc 0.998428, f1 0.998428\n",
      "Current epoch:  1660\n",
      "2017-11-15T23:48:13.772475: step 29885, loss 5.66763e-05, acc 1, f1 1\n",
      "2017-11-15T23:48:14.297476: step 29890, loss 0.00173788, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:48:14.923302: step 29895, loss 0.00224939, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1661\n",
      "2017-11-15T23:48:15.418172: step 29900, loss 0.00085808, acc 0.999023, f1 0.999024\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:48:16.056896: step 29900, loss 2.30258, acc 0.578567, f1 0.580383\n",
      "\n",
      "2017-11-15T23:48:16.589827: step 29905, loss 0.00373147, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:48:17.108757: step 29910, loss 0.00235748, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:48:17.630434: step 29915, loss 0.002736, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1662\n",
      "2017-11-15T23:48:18.125831: step 29920, loss 0.00199039, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:48:18.651048: step 29925, loss 0.00313758, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:48:19.189653: step 29930, loss 0.00215096, acc 0.998047, f1 0.998048\n",
      "Current epoch:  1663\n",
      "2017-11-15T23:48:19.691017: step 29935, loss 0.000618856, acc 1, f1 1\n",
      "2017-11-15T23:48:20.211011: step 29940, loss 0.00293084, acc 0.99707, f1 0.997071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:48:20.732432: step 29945, loss 0.00222861, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:48:21.250221: step 29950, loss 0.00244983, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:48:21.929530: step 29950, loss 2.29064, acc 0.585401, f1 0.585109\n",
      "\n",
      "Current epoch:  1664\n",
      "2017-11-15T23:48:22.421052: step 29955, loss 0.00289479, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:48:22.943479: step 29960, loss 0.00271034, acc 0.998047, f1 0.998043\n",
      "2017-11-15T23:48:23.472147: step 29965, loss 0.00134792, acc 1, f1 1\n",
      "2017-11-15T23:48:23.965653: step 29970, loss 0.00267524, acc 0.998428, f1 0.998427\n",
      "Current epoch:  1665\n",
      "2017-11-15T23:48:24.494298: step 29975, loss 0.00256825, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:48:25.012842: step 29980, loss 0.00324159, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:48:25.547297: step 29985, loss 0.00471571, acc 0.996094, f1 0.996094\n",
      "Current epoch:  1666\n",
      "2017-11-15T23:48:26.052196: step 29990, loss 0.000851692, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:48:26.587744: step 29995, loss 0.001311, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:48:27.115951: step 30000, loss 0.00359264, acc 0.99707, f1 0.997067\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:48:27.756517: step 30000, loss 2.34818, acc 0.574108, f1 0.574224\n",
      "\n",
      "2017-11-15T23:48:28.269672: step 30005, loss 0.00286612, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1667\n",
      "2017-11-15T23:48:28.761374: step 30010, loss 0.00154907, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:48:29.287304: step 30015, loss 0.00481878, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:48:29.817540: step 30020, loss 0.00396202, acc 0.99707, f1 0.997071\n",
      "Current epoch:  1668\n",
      "2017-11-15T23:48:30.308525: step 30025, loss 8.81919e-05, acc 1, f1 1\n",
      "2017-11-15T23:48:30.826216: step 30030, loss 6.05911e-05, acc 1, f1 1\n",
      "2017-11-15T23:48:31.343732: step 30035, loss 0.000755442, acc 1, f1 1\n",
      "2017-11-15T23:48:31.859679: step 30040, loss 0.00504173, acc 0.995117, f1 0.995118\n",
      "Current epoch:  1669\n",
      "2017-11-15T23:48:32.361552: step 30045, loss 0.000878705, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:48:32.894014: step 30050, loss 0.00100622, acc 0.999023, f1 0.999022\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:48:33.542832: step 30050, loss 2.31702, acc 0.579149, f1 0.579516\n",
      "\n",
      "2017-11-15T23:48:34.061454: step 30055, loss 0.00310221, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:48:34.549599: step 30060, loss 0.00138307, acc 0.998428, f1 0.998428\n",
      "Current epoch:  1670\n",
      "2017-11-15T23:48:35.069124: step 30065, loss 0.00150318, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:48:35.593848: step 30070, loss 0.000780885, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:48:36.133499: step 30075, loss 0.00411956, acc 0.996094, f1 0.996093\n",
      "Current epoch:  1671\n",
      "2017-11-15T23:48:36.637127: step 30080, loss 0.00314522, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:48:37.163054: step 30085, loss 0.00209457, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:48:37.684025: step 30090, loss 0.000723121, acc 1, f1 1\n",
      "2017-11-15T23:48:38.207667: step 30095, loss 0.00607299, acc 0.994141, f1 0.99414\n",
      "Current epoch:  1672\n",
      "2017-11-15T23:48:38.693526: step 30100, loss 0.00241455, acc 0.999023, f1 0.999024\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:48:39.344182: step 30100, loss 2.2638, acc 0.591605, f1 0.590059\n",
      "\n",
      "2017-11-15T23:48:39.856097: step 30105, loss 0.00439225, acc 0.995117, f1 0.995117\n",
      "2017-11-15T23:48:40.376663: step 30110, loss 0.00483468, acc 0.996094, f1 0.996097\n",
      "Current epoch:  1673\n",
      "2017-11-15T23:48:40.879538: step 30115, loss 0.00197453, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:48:41.396620: step 30120, loss 0.0007494, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:48:41.935586: step 30125, loss 0.00148511, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:48:42.464439: step 30130, loss 0.0017284, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1674\n",
      "2017-11-15T23:48:42.946346: step 30135, loss 0.00303882, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:48:43.463285: step 30140, loss 0.00185636, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:48:43.980222: step 30145, loss 0.0044634, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:48:44.465079: step 30150, loss 0.00389651, acc 0.996855, f1 0.996855\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:48:45.111250: step 30150, loss 2.33989, acc 0.577549, f1 0.577013\n",
      "\n",
      "Current epoch:  1675\n",
      "2017-11-15T23:48:45.629883: step 30155, loss 0.0024004, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:48:46.155495: step 30160, loss 0.00118896, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:48:46.671274: step 30165, loss 0.00173607, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1676\n",
      "2017-11-15T23:48:47.167201: step 30170, loss 0.00248641, acc 0.998047, f1 0.998053\n",
      "2017-11-15T23:48:47.692145: step 30175, loss 0.00107039, acc 1, f1 1\n",
      "2017-11-15T23:48:48.212057: step 30180, loss 5.08125e-05, acc 1, f1 1\n",
      "2017-11-15T23:48:48.739649: step 30185, loss 0.00280841, acc 0.998047, f1 0.998048\n",
      "Current epoch:  1677\n",
      "2017-11-15T23:48:49.235569: step 30190, loss 0.00783347, acc 0.995117, f1 0.99512\n",
      "2017-11-15T23:48:49.782544: step 30195, loss 0.000628291, acc 1, f1 1\n",
      "2017-11-15T23:48:50.307190: step 30200, loss 0.00269192, acc 0.99707, f1 0.997069\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:48:50.997658: step 30200, loss 2.2912, acc 0.585062, f1 0.584003\n",
      "\n",
      "Current epoch:  1678\n",
      "2017-11-15T23:48:51.532621: step 30205, loss 0.000649904, acc 1, f1 1\n",
      "2017-11-15T23:48:52.068586: step 30210, loss 0.000998976, acc 1, f1 1\n",
      "2017-11-15T23:48:52.594022: step 30215, loss 0.00201133, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:48:53.123469: step 30220, loss 0.00442199, acc 0.996094, f1 0.996091\n",
      "Current epoch:  1679\n",
      "2017-11-15T23:48:53.612807: step 30225, loss 0.000797838, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:48:54.129218: step 30230, loss 0.00284356, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:48:54.644125: step 30235, loss 0.00331155, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:48:55.135972: step 30240, loss 8.10619e-05, acc 1, f1 1\n",
      "Current epoch:  1680\n",
      "2017-11-15T23:48:55.652883: step 30245, loss 0.000805717, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:48:56.172303: step 30250, loss 0.00207383, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:48:56.812571: step 30250, loss 2.30992, acc 0.579391, f1 0.580696\n",
      "\n",
      "2017-11-15T23:48:57.325974: step 30255, loss 0.0031291, acc 0.998047, f1 0.998046\n",
      "Current epoch:  1681\n",
      "2017-11-15T23:48:57.809797: step 30260, loss 0.00310552, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:48:58.334230: step 30265, loss 0.00141864, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:48:58.856658: step 30270, loss 0.0039096, acc 0.99707, f1 0.997068\n",
      "2017-11-15T23:48:59.372593: step 30275, loss 0.00251082, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1682\n",
      "2017-11-15T23:48:59.861930: step 30280, loss 0.00203669, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:49:00.382854: step 30285, loss 0.00150152, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:49:00.911298: step 30290, loss 0.00281908, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1683\n",
      "2017-11-15T23:49:01.415175: step 30295, loss 0.00104684, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:49:01.929080: step 30300, loss 0.00504916, acc 0.99707, f1 0.997069\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:49:02.583870: step 30300, loss 2.31497, acc 0.57944, f1 0.580297\n",
      "\n",
      "2017-11-15T23:49:03.096270: step 30305, loss 0.00382612, acc 0.99707, f1 0.997066\n",
      "2017-11-15T23:49:03.618197: step 30310, loss 0.00123805, acc 0.999023, f1 0.999024\n",
      "Current epoch:  1684\n",
      "2017-11-15T23:49:04.113051: step 30315, loss 0.00155929, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:49:04.630965: step 30320, loss 0.00490291, acc 0.995117, f1 0.995116\n",
      "2017-11-15T23:49:05.149888: step 30325, loss 0.00321479, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:49:05.639227: step 30330, loss 0.00165957, acc 0.998428, f1 0.998428\n",
      "Current epoch:  1685\n",
      "2017-11-15T23:49:06.160652: step 30335, loss 0.00225851, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:49:06.675058: step 30340, loss 0.00193651, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:49:07.194978: step 30345, loss 0.00164268, acc 0.999023, f1 0.999024\n",
      "Current epoch:  1686\n",
      "2017-11-15T23:49:07.681308: step 30350, loss 0.00177922, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:49:08.316545: step 30350, loss 2.34749, acc 0.574883, f1 0.575172\n",
      "\n",
      "2017-11-15T23:49:08.874067: step 30355, loss 0.000740765, acc 1, f1 1\n",
      "2017-11-15T23:49:09.396495: step 30360, loss 0.00156225, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:49:09.919425: step 30365, loss 0.00477859, acc 0.995117, f1 0.995117\n",
      "Current epoch:  1687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:49:10.407259: step 30370, loss 0.00117094, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:49:10.927773: step 30375, loss 0.00201438, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:49:11.448195: step 30380, loss 0.00127567, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1688\n",
      "2017-11-15T23:49:11.940540: step 30385, loss 0.00151738, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:49:12.457955: step 30390, loss 0.00311132, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:49:12.981385: step 30395, loss 0.00214663, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:49:13.509843: step 30400, loss 0.00352712, acc 0.99707, f1 0.99707\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:49:14.168666: step 30400, loss 2.28842, acc 0.590248, f1 0.58809\n",
      "\n",
      "Current epoch:  1689\n",
      "2017-11-15T23:49:14.660511: step 30405, loss 0.00616244, acc 0.995117, f1 0.995115\n",
      "2017-11-15T23:49:15.193500: step 30410, loss 0.002057, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:49:15.719434: step 30415, loss 0.000653468, acc 1, f1 1\n",
      "2017-11-15T23:49:16.217295: step 30420, loss 0.00379385, acc 0.996855, f1 0.996856\n",
      "Current epoch:  1690\n",
      "2017-11-15T23:49:16.745740: step 30425, loss 0.00117572, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:49:17.273682: step 30430, loss 0.00474657, acc 0.99707, f1 0.997066\n",
      "2017-11-15T23:49:17.793102: step 30435, loss 0.00276333, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1691\n",
      "2017-11-15T23:49:18.282439: step 30440, loss 0.000486183, acc 1, f1 1\n",
      "2017-11-15T23:49:18.809380: step 30445, loss 0.00094008, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:49:19.332810: step 30450, loss 0.00425769, acc 0.996094, f1 0.996095\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:49:20.014173: step 30450, loss 2.32322, acc 0.576677, f1 0.579369\n",
      "\n",
      "2017-11-15T23:49:20.535096: step 30455, loss 0.00220337, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1692\n",
      "2017-11-15T23:49:21.020924: step 30460, loss 0.00148778, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:49:21.544858: step 30465, loss 0.00162276, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:49:22.064276: step 30470, loss 0.00263135, acc 0.99707, f1 0.997069\n",
      "Current epoch:  1693\n",
      "2017-11-15T23:49:22.556121: step 30475, loss 0.00177293, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:49:23.079551: step 30480, loss 0.000684066, acc 1, f1 1\n",
      "2017-11-15T23:49:23.603986: step 30485, loss 0.00298949, acc 0.996094, f1 0.996095\n",
      "2017-11-15T23:49:24.121399: step 30490, loss 0.00439157, acc 0.996094, f1 0.996097\n",
      "Current epoch:  1694\n",
      "2017-11-15T23:49:24.607728: step 30495, loss 0.00217985, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:49:25.126647: step 30500, loss 0.00110734, acc 0.999023, f1 0.999024\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:49:25.782963: step 30500, loss 2.35875, acc 0.578179, f1 0.576857\n",
      "\n",
      "2017-11-15T23:49:26.301882: step 30505, loss 0.00368555, acc 0.996094, f1 0.996092\n",
      "2017-11-15T23:49:26.785203: step 30510, loss 0.00354029, acc 0.996855, f1 0.996852\n",
      "Current epoch:  1695\n",
      "2017-11-15T23:49:27.310139: step 30515, loss 0.000744636, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:49:27.836577: step 30520, loss 0.00345298, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:49:28.366024: step 30525, loss 0.00296394, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1696\n",
      "2017-11-15T23:49:28.867395: step 30530, loss 0.00344315, acc 0.99707, f1 0.997072\n",
      "2017-11-15T23:49:29.392877: step 30535, loss 0.00305178, acc 0.99707, f1 0.997072\n",
      "2017-11-15T23:49:29.932876: step 30540, loss 6.35321e-05, acc 1, f1 1\n",
      "2017-11-15T23:49:30.451293: step 30545, loss 0.00399699, acc 0.99707, f1 0.997069\n",
      "Current epoch:  1697\n",
      "2017-11-15T23:49:30.940129: step 30550, loss 0.00238419, acc 0.998047, f1 0.998041\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:49:31.627508: step 30550, loss 2.30652, acc 0.57944, f1 0.580475\n",
      "\n",
      "2017-11-15T23:49:32.147930: step 30555, loss 0.00314304, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:49:32.664855: step 30560, loss 0.00638872, acc 0.993164, f1 0.993164\n",
      "Current epoch:  1698\n",
      "2017-11-15T23:49:33.160209: step 30565, loss 0.000317258, acc 1, f1 1\n",
      "2017-11-15T23:49:33.680631: step 30570, loss 0.00283703, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:49:34.196542: step 30575, loss 5.17486e-05, acc 1, f1 1\n",
      "2017-11-15T23:49:34.716014: step 30580, loss 0.00459323, acc 0.996094, f1 0.996094\n",
      "Current epoch:  1699\n",
      "2017-11-15T23:49:35.208860: step 30585, loss 0.00117515, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:49:35.724770: step 30590, loss 0.00402911, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:49:36.245192: step 30595, loss 0.00210324, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:49:36.746061: step 30600, loss 0.002115, acc 0.998428, f1 0.998431\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:49:37.404411: step 30600, loss 2.28928, acc 0.588939, f1 0.587172\n",
      "\n",
      "Current epoch:  1700\n",
      "2017-11-15T23:49:38.020595: step 30605, loss 0.00151235, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:49:38.535504: step 30610, loss 0.00116054, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:49:39.076483: step 30615, loss 0.00184128, acc 0.998047, f1 0.998048\n",
      "Current epoch:  1701\n",
      "2017-11-15T23:49:39.566322: step 30620, loss 0.00161017, acc 1, f1 1\n",
      "2017-11-15T23:49:40.084237: step 30625, loss 0.00251695, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:49:40.606686: step 30630, loss 5.8085e-05, acc 1, f1 1\n",
      "2017-11-15T23:49:41.126607: step 30635, loss 0.00407812, acc 0.99707, f1 0.997067\n",
      "Current epoch:  1702\n",
      "2017-11-15T23:49:41.619479: step 30640, loss 0.00228783, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:49:42.140403: step 30645, loss 0.00260938, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:49:42.668347: step 30650, loss 7.23501e-05, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:49:43.307093: step 30650, loss 2.36048, acc 0.57784, f1 0.576771\n",
      "\n",
      "Current epoch:  1703\n",
      "2017-11-15T23:49:43.795928: step 30655, loss 0.0026749, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:49:44.315850: step 30660, loss 0.00171932, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:49:44.835287: step 30665, loss 0.00364155, acc 0.99707, f1 0.997068\n",
      "2017-11-15T23:49:45.351698: step 30670, loss 0.00295046, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1704\n",
      "2017-11-15T23:49:45.836524: step 30675, loss 0.00311851, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:49:46.351932: step 30680, loss 0.0023487, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:49:46.867341: step 30685, loss 0.00257497, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:49:47.354673: step 30690, loss 0.0012928, acc 0.998428, f1 0.998428\n",
      "Current epoch:  1705\n",
      "2017-11-15T23:49:47.890640: step 30695, loss 0.000794018, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:49:48.411094: step 30700, loss 0.00206235, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:49:49.058924: step 30700, loss 2.28669, acc 0.588891, f1 0.587627\n",
      "\n",
      "2017-11-15T23:49:49.570336: step 30705, loss 0.0018014, acc 0.998047, f1 0.998046\n",
      "Current epoch:  1706\n",
      "2017-11-15T23:49:50.060676: step 30710, loss 0.000625878, acc 1, f1 1\n",
      "2017-11-15T23:49:50.581113: step 30715, loss 0.0037722, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:49:51.098026: step 30720, loss 0.00360524, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:49:51.612934: step 30725, loss 0.002639, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1707\n",
      "2017-11-15T23:49:52.096755: step 30730, loss 0.00274209, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:49:52.613669: step 30735, loss 0.00256578, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:49:53.130751: step 30740, loss 0.00230167, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1708\n",
      "2017-11-15T23:49:53.634128: step 30745, loss 0.00218478, acc 1, f1 1\n",
      "2017-11-15T23:49:54.151542: step 30750, loss 0.00186502, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:49:54.796305: step 30750, loss 2.27486, acc 0.590345, f1 0.588857\n",
      "\n",
      "2017-11-15T23:49:55.308203: step 30755, loss 0.00267863, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:49:55.821105: step 30760, loss 0.00348773, acc 0.99707, f1 0.997071\n",
      "Current epoch:  1709\n",
      "2017-11-15T23:49:56.305931: step 30765, loss 0.00444378, acc 0.996094, f1 0.996096\n",
      "2017-11-15T23:49:56.823847: step 30770, loss 0.00284205, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:49:57.350786: step 30775, loss 7.05511e-05, acc 1, f1 1\n",
      "2017-11-15T23:49:57.840626: step 30780, loss 4.92942e-05, acc 1, f1 1\n",
      "Current epoch:  1710\n",
      "2017-11-15T23:49:58.362551: step 30785, loss 6.00409e-05, acc 1, f1 1\n",
      "2017-11-15T23:49:58.887988: step 30790, loss 0.00281936, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:49:59.407408: step 30795, loss 0.00105378, acc 0.999023, f1 0.999024\n",
      "Current epoch:  1711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:49:59.898750: step 30800, loss 0.00159446, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:50:00.560059: step 30800, loss 2.37724, acc 0.5807, f1 0.577226\n",
      "\n",
      "2017-11-15T23:50:01.075969: step 30805, loss 0.00206911, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:50:01.603927: step 30810, loss 0.00260104, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:50:02.122846: step 30815, loss 0.000810922, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1712\n",
      "2017-11-15T23:50:02.610178: step 30820, loss 0.00172141, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:50:03.125600: step 30825, loss 0.000596868, acc 1, f1 1\n",
      "2017-11-15T23:50:03.649031: step 30830, loss 0.00309252, acc 0.99707, f1 0.997071\n",
      "Current epoch:  1713\n",
      "2017-11-15T23:50:04.141878: step 30835, loss 0.00235275, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:50:04.672339: step 30840, loss 0.00107542, acc 1, f1 1\n",
      "2017-11-15T23:50:05.190757: step 30845, loss 0.00040276, acc 1, f1 1\n",
      "2017-11-15T23:50:05.705163: step 30850, loss 0.00139275, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:50:06.347418: step 30850, loss 2.34202, acc 0.572945, f1 0.576281\n",
      "\n",
      "Current epoch:  1714\n",
      "2017-11-15T23:50:06.829235: step 30855, loss 0.0029481, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:50:07.352184: step 30860, loss 0.00335364, acc 0.998047, f1 0.998043\n",
      "2017-11-15T23:50:07.873635: step 30865, loss 0.00228694, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:50:08.359965: step 30870, loss 0.00114479, acc 1, f1 1\n",
      "Current epoch:  1715\n",
      "2017-11-15T23:50:08.881891: step 30875, loss 0.00143484, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:50:09.395295: step 30880, loss 0.00391181, acc 0.996094, f1 0.996096\n",
      "2017-11-15T23:50:09.937777: step 30885, loss 0.00269633, acc 0.998047, f1 0.998042\n",
      "Current epoch:  1716\n",
      "2017-11-15T23:50:10.430124: step 30890, loss 0.00405165, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:50:10.953587: step 30895, loss 0.00193431, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:50:11.474511: step 30900, loss 6.00915e-05, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:50:12.124867: step 30900, loss 2.32751, acc 0.576871, f1 0.577949\n",
      "\n",
      "2017-11-15T23:50:12.652364: step 30905, loss 0.00699228, acc 0.994141, f1 0.994144\n",
      "Current epoch:  1717\n",
      "2017-11-15T23:50:13.143708: step 30910, loss 0.0018005, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:50:13.666134: step 30915, loss 0.00276313, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:50:14.181543: step 30920, loss 0.00321788, acc 0.99707, f1 0.997069\n",
      "Current epoch:  1718\n",
      "2017-11-15T23:50:14.681465: step 30925, loss 0.00102189, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:50:15.274103: step 30930, loss 0.00136845, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:50:15.797033: step 30935, loss 0.00358323, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:50:16.318478: step 30940, loss 0.00182967, acc 0.998047, f1 0.998048\n",
      "Current epoch:  1719\n",
      "2017-11-15T23:50:16.808317: step 30945, loss 0.00232285, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:50:17.337780: step 30950, loss 9.0058e-05, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:50:18.027214: step 30950, loss 2.29507, acc 0.587728, f1 0.58625\n",
      "\n",
      "2017-11-15T23:50:18.541621: step 30955, loss 6.7902e-05, acc 1, f1 1\n",
      "2017-11-15T23:50:19.035972: step 30960, loss 0.0041372, acc 0.995283, f1 0.995283\n",
      "Current epoch:  1720\n",
      "2017-11-15T23:50:19.569431: step 30965, loss 0.00103779, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:50:20.103407: step 30970, loss 0.00311466, acc 0.99707, f1 0.997072\n",
      "2017-11-15T23:50:20.620319: step 30975, loss 0.00245737, acc 0.998047, f1 0.998053\n",
      "Current epoch:  1721\n",
      "2017-11-15T23:50:21.118682: step 30980, loss 8.04345e-05, acc 1, f1 1\n",
      "2017-11-15T23:50:21.638102: step 30985, loss 0.00164051, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:50:22.158524: step 30990, loss 0.00180599, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:50:22.683459: step 30995, loss 0.00340768, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1722\n",
      "2017-11-15T23:50:23.173314: step 31000, loss 0.00304178, acc 0.99707, f1 0.997066\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:50:23.832114: step 31000, loss 2.28207, acc 0.58923, f1 0.58892\n",
      "\n",
      "2017-11-15T23:50:24.374296: step 31005, loss 0.00639041, acc 0.994141, f1 0.994138\n",
      "2017-11-15T23:50:24.923798: step 31010, loss 0.00515968, acc 0.995117, f1 0.995113\n",
      "Current epoch:  1723\n",
      "2017-11-15T23:50:25.434194: step 31015, loss 0.00186995, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:50:25.980185: step 31020, loss 0.00313302, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:50:26.542724: step 31025, loss 0.00164353, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:50:27.103757: step 31030, loss 0.00210219, acc 0.999023, f1 0.999022\n",
      "Current epoch:  1724\n",
      "2017-11-15T23:50:27.629194: step 31035, loss 0.000857242, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:50:28.166161: step 31040, loss 0.00344403, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:50:28.698115: step 31045, loss 0.00287235, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:50:29.197479: step 31050, loss 0.00498413, acc 0.995283, f1 0.99528\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:50:29.887867: step 31050, loss 2.3269, acc 0.601347, f1 0.59551\n",
      "\n",
      "Current epoch:  1725\n",
      "2017-11-15T23:50:30.460431: step 31055, loss 0.000844415, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:50:31.000909: step 31060, loss 0.00159033, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:50:31.529354: step 31065, loss 0.00210851, acc 0.998047, f1 0.998046\n",
      "Current epoch:  1726\n",
      "2017-11-15T23:50:32.038266: step 31070, loss 0.000845303, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:50:32.574741: step 31075, loss 0.00213432, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:50:33.105693: step 31080, loss 0.000672645, acc 1, f1 1\n",
      "2017-11-15T23:50:33.630628: step 31085, loss 0.00588582, acc 0.995117, f1 0.995115\n",
      "Current epoch:  1727\n",
      "2017-11-15T23:50:34.129993: step 31090, loss 0.00306927, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:50:34.656432: step 31095, loss 0.00296055, acc 0.998047, f1 0.998051\n",
      "2017-11-15T23:50:35.181868: step 31100, loss 0.00271383, acc 0.99707, f1 0.99707\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:50:35.849807: step 31100, loss 2.28152, acc 0.590151, f1 0.588632\n",
      "\n",
      "Current epoch:  1728\n",
      "2017-11-15T23:50:36.350174: step 31105, loss 0.00255195, acc 0.998047, f1 0.998043\n",
      "2017-11-15T23:50:36.875610: step 31110, loss 0.00136729, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:50:37.415629: step 31115, loss 0.00203755, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:50:37.943614: step 31120, loss 7.85207e-05, acc 1, f1 1\n",
      "Current epoch:  1729\n",
      "2017-11-15T23:50:38.438280: step 31125, loss 0.00260628, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:50:38.965720: step 31130, loss 0.00244451, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:50:39.486143: step 31135, loss 5.75243e-05, acc 1, f1 1\n",
      "2017-11-15T23:50:39.970968: step 31140, loss 0.00666781, acc 0.993711, f1 0.99371\n",
      "Current epoch:  1730\n",
      "2017-11-15T23:50:40.493898: step 31145, loss 0.00338205, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:50:41.004293: step 31150, loss 0.000740478, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:50:41.655573: step 31150, loss 2.29558, acc 0.594077, f1 0.590572\n",
      "\n",
      "2017-11-15T23:50:42.174994: step 31155, loss 0.00169798, acc 0.998047, f1 0.998048\n",
      "Current epoch:  1731\n",
      "2017-11-15T23:50:42.668342: step 31160, loss 0.00236255, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:50:43.200297: step 31165, loss 0.00391161, acc 0.996094, f1 0.996096\n",
      "2017-11-15T23:50:43.715705: step 31170, loss 0.00133152, acc 1, f1 1\n",
      "2017-11-15T23:50:44.240138: step 31175, loss 5.99901e-05, acc 1, f1 1\n",
      "Current epoch:  1732\n",
      "2017-11-15T23:50:44.726992: step 31180, loss 0.00319649, acc 0.998047, f1 0.998043\n",
      "2017-11-15T23:50:45.241900: step 31185, loss 0.0029912, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:50:45.762322: step 31190, loss 0.00103597, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1733\n",
      "2017-11-15T23:50:46.254667: step 31195, loss 8.58615e-05, acc 1, f1 1\n",
      "2017-11-15T23:50:46.770577: step 31200, loss 0.000538643, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:50:47.408320: step 31200, loss 2.35396, acc 0.581378, f1 0.579526\n",
      "\n",
      "2017-11-15T23:50:47.920721: step 31205, loss 0.00612642, acc 0.995117, f1 0.995111\n",
      "2017-11-15T23:50:48.454195: step 31210, loss 0.00224833, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1734\n",
      "2017-11-15T23:50:48.946350: step 31215, loss 0.00127821, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:50:49.475877: step 31220, loss 0.00184529, acc 1, f1 1\n",
      "2017-11-15T23:50:49.998806: step 31225, loss 0.00371526, acc 0.998047, f1 0.998047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:50:50.490649: step 31230, loss 0.0015668, acc 0.998428, f1 0.998428\n",
      "Current epoch:  1735\n",
      "2017-11-15T23:50:51.012576: step 31235, loss 0.00153637, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:50:51.528988: step 31240, loss 0.00087301, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:50:52.055427: step 31245, loss 0.0017022, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1736\n",
      "2017-11-15T23:50:52.555317: step 31250, loss 0.00179216, acc 0.999023, f1 0.999024\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:50:53.218186: step 31250, loss 2.32424, acc 0.576483, f1 0.578607\n",
      "\n",
      "2017-11-15T23:50:53.757224: step 31255, loss 0.00256009, acc 0.99707, f1 0.997067\n",
      "2017-11-15T23:50:54.290786: step 31260, loss 0.00146377, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:50:54.809704: step 31265, loss 0.00167324, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1737\n",
      "2017-11-15T23:50:55.294028: step 31270, loss 0.00256034, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:50:55.810142: step 31275, loss 0.00205763, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:50:56.327557: step 31280, loss 0.00285768, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1738\n",
      "2017-11-15T23:50:56.822411: step 31285, loss 0.00309477, acc 1, f1 1\n",
      "2017-11-15T23:50:57.341839: step 31290, loss 0.00402908, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:50:57.868341: step 31295, loss 0.00186094, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:50:58.393277: step 31300, loss 0.00377889, acc 0.996094, f1 0.996093\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:50:59.067650: step 31300, loss 2.3884, acc 0.568776, f1 0.572792\n",
      "\n",
      "Current epoch:  1739\n",
      "2017-11-15T23:50:59.570021: step 31305, loss 0.00222798, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:51:00.096043: step 31310, loss 0.00227577, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:51:00.619004: step 31315, loss 0.000902967, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:51:01.109845: step 31320, loss 0.00173908, acc 0.998428, f1 0.998425\n",
      "Current epoch:  1740\n",
      "2017-11-15T23:51:01.629766: step 31325, loss 0.0029775, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:51:02.142668: step 31330, loss 0.00290286, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:51:02.658077: step 31335, loss 0.00227998, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1741\n",
      "2017-11-15T23:51:03.145910: step 31340, loss 0.00199249, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:51:03.660817: step 31345, loss 5.0686e-05, acc 1, f1 1\n",
      "2017-11-15T23:51:04.179736: step 31350, loss 0.00265116, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:51:04.863217: step 31350, loss 2.27827, acc 0.586807, f1 0.587157\n",
      "\n",
      "2017-11-15T23:51:05.398297: step 31355, loss 0.00203228, acc 0.998047, f1 0.998043\n",
      "Current epoch:  1742\n",
      "2017-11-15T23:51:05.893167: step 31360, loss 0.000898779, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:51:06.417295: step 31365, loss 0.00188195, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:51:06.940287: step 31370, loss 0.0010423, acc 0.999023, f1 0.999025\n",
      "Current epoch:  1743\n",
      "2017-11-15T23:51:07.437676: step 31375, loss 0.000553781, acc 1, f1 1\n",
      "2017-11-15T23:51:07.949575: step 31380, loss 0.00121644, acc 1, f1 1\n",
      "2017-11-15T23:51:08.468494: step 31385, loss 0.00244045, acc 0.998047, f1 0.998043\n",
      "2017-11-15T23:51:08.989418: step 31390, loss 0.000869923, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1744\n",
      "2017-11-15T23:51:09.486329: step 31395, loss 0.000840168, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:51:10.009759: step 31400, loss 0.00163213, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:51:10.708926: step 31400, loss 2.31109, acc 0.585886, f1 0.584984\n",
      "\n",
      "2017-11-15T23:51:11.251911: step 31405, loss 0.000658389, acc 1, f1 1\n",
      "2017-11-15T23:51:11.750331: step 31410, loss 0.0066479, acc 0.993711, f1 0.993711\n",
      "Current epoch:  1745\n",
      "2017-11-15T23:51:12.268248: step 31415, loss 0.000577052, acc 1, f1 1\n",
      "2017-11-15T23:51:12.790174: step 31420, loss 0.00156809, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:51:13.306585: step 31425, loss 0.00163338, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1746\n",
      "2017-11-15T23:51:13.797427: step 31430, loss 0.00204232, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:51:14.316344: step 31435, loss 0.00438589, acc 0.99707, f1 0.997072\n",
      "2017-11-15T23:51:14.835264: step 31440, loss 0.00395893, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:51:15.349168: step 31445, loss 0.00153049, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1747\n",
      "2017-11-15T23:51:15.846647: step 31450, loss 0.00324665, acc 0.99707, f1 0.997069\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:51:16.518002: step 31450, loss 2.30267, acc 0.583947, f1 0.583787\n",
      "\n",
      "2017-11-15T23:51:17.036418: step 31455, loss 0.000825158, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:51:17.557342: step 31460, loss 0.00264954, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1748\n",
      "2017-11-15T23:51:18.055203: step 31465, loss 0.00279419, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:51:18.575124: step 31470, loss 0.00241526, acc 0.99707, f1 0.997074\n",
      "2017-11-15T23:51:19.092037: step 31475, loss 0.0023294, acc 0.998047, f1 0.998042\n",
      "2017-11-15T23:51:19.623489: step 31480, loss 0.00459442, acc 0.996094, f1 0.996092\n",
      "Current epoch:  1749\n",
      "2017-11-15T23:51:20.117872: step 31485, loss 0.00399825, acc 0.995117, f1 0.995113\n",
      "2017-11-15T23:51:20.642808: step 31490, loss 0.000673738, acc 1, f1 1\n",
      "2017-11-15T23:51:21.169316: step 31495, loss 0.00104948, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:51:21.683742: step 31500, loss 5.10633e-05, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:51:22.347557: step 31500, loss 2.28101, acc 0.590151, f1 0.589676\n",
      "\n",
      "Current epoch:  1750\n",
      "2017-11-15T23:51:22.876502: step 31505, loss 0.00287315, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:51:23.402594: step 31510, loss 0.00334525, acc 0.99707, f1 0.997074\n",
      "2017-11-15T23:51:23.927027: step 31515, loss 6.49901e-05, acc 1, f1 1\n",
      "Current epoch:  1751\n",
      "2017-11-15T23:51:24.410350: step 31520, loss 0.00248038, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:51:24.937288: step 31525, loss 5.01948e-05, acc 1, f1 1\n",
      "2017-11-15T23:51:25.455706: step 31530, loss 0.00287367, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:51:25.982145: step 31535, loss 0.00379624, acc 0.996094, f1 0.996095\n",
      "Current epoch:  1752\n",
      "2017-11-15T23:51:26.468975: step 31540, loss 0.000883589, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:51:26.999964: step 31545, loss 0.00141328, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:51:27.519886: step 31550, loss 0.00188999, acc 0.998047, f1 0.998048\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:51:28.165243: step 31550, loss 2.34651, acc 0.577016, f1 0.576962\n",
      "\n",
      "Current epoch:  1753\n",
      "2017-11-15T23:51:28.648062: step 31555, loss 0.00153588, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:51:29.169989: step 31560, loss 0.00289883, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:51:29.697972: step 31565, loss 0.00361973, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:51:30.220517: step 31570, loss 0.00252508, acc 0.998047, f1 0.998051\n",
      "Current epoch:  1754\n",
      "2017-11-15T23:51:30.716874: step 31575, loss 0.00166285, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:51:31.247457: step 31580, loss 0.00267627, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:51:31.776903: step 31585, loss 0.0014366, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:51:32.272259: step 31590, loss 0.00179563, acc 0.998428, f1 0.998428\n",
      "Current epoch:  1755\n",
      "2017-11-15T23:51:32.797908: step 31595, loss 6.32054e-05, acc 1, f1 1\n",
      "2017-11-15T23:51:33.312314: step 31600, loss 0.00166935, acc 0.999023, f1 0.999024\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:51:34.018745: step 31600, loss 2.39375, acc 0.575659, f1 0.573733\n",
      "\n",
      "2017-11-15T23:51:34.542700: step 31605, loss 0.00335949, acc 0.99707, f1 0.997069\n",
      "Current epoch:  1756\n",
      "2017-11-15T23:51:35.032037: step 31610, loss 0.0011819, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:51:35.553963: step 31615, loss 0.00169279, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:51:36.079475: step 31620, loss 0.00442138, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:51:36.606945: step 31625, loss 0.00501051, acc 0.996094, f1 0.996094\n",
      "Current epoch:  1757\n",
      "2017-11-15T23:51:37.100788: step 31630, loss 0.000682922, acc 1, f1 1\n",
      "2017-11-15T23:51:37.624721: step 31635, loss 0.00254042, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:51:38.153164: step 31640, loss 0.00186507, acc 0.998047, f1 0.998046\n",
      "Current epoch:  1758\n",
      "2017-11-15T23:51:38.648085: step 31645, loss 5.48109e-05, acc 1, f1 1\n",
      "2017-11-15T23:51:39.167022: step 31650, loss 0.00310691, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:51:39.838897: step 31650, loss 2.35451, acc 0.583317, f1 0.580963\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:51:40.364835: step 31655, loss 0.00384289, acc 0.996094, f1 0.996096\n",
      "2017-11-15T23:51:40.893799: step 31660, loss 0.00213699, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1759\n",
      "2017-11-15T23:51:41.389204: step 31665, loss 0.000770643, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:51:41.915656: step 31670, loss 0.0029624, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:51:42.443641: step 31675, loss 0.00173988, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:51:42.933980: step 31680, loss 0.00152629, acc 0.998428, f1 0.998425\n",
      "Current epoch:  1760\n",
      "2017-11-15T23:51:43.475462: step 31685, loss 0.00220178, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:51:43.999894: step 31690, loss 0.00140655, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:51:44.515302: step 31695, loss 0.00366319, acc 0.99707, f1 0.997066\n",
      "Current epoch:  1761\n",
      "2017-11-15T23:51:45.007673: step 31700, loss 0.000468343, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:51:45.646420: step 31700, loss 2.28277, acc 0.59272, f1 0.591959\n",
      "\n",
      "2017-11-15T23:51:46.161327: step 31705, loss 0.00132051, acc 1, f1 1\n",
      "2017-11-15T23:51:46.683273: step 31710, loss 0.00522884, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:51:47.206785: step 31715, loss 0.0025553, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1762\n",
      "2017-11-15T23:51:47.698656: step 31720, loss 0.00083983, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:51:48.215568: step 31725, loss 0.00168671, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:51:48.742511: step 31730, loss 0.000820685, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1763\n",
      "2017-11-15T23:51:49.239869: step 31735, loss 0.00382412, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:51:49.767811: step 31740, loss 7.19737e-05, acc 1, f1 1\n",
      "2017-11-15T23:51:50.295253: step 31745, loss 0.00454093, acc 0.996094, f1 0.996097\n",
      "2017-11-15T23:51:50.820032: step 31750, loss 0.00445958, acc 0.996094, f1 0.996091\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:51:51.486854: step 31750, loss 2.31711, acc 0.582832, f1 0.58303\n",
      "\n",
      "Current epoch:  1764\n",
      "2017-11-15T23:51:51.981725: step 31755, loss 0.00284987, acc 0.99707, f1 0.997074\n",
      "2017-11-15T23:51:52.505193: step 31760, loss 0.00294647, acc 0.998047, f1 0.998042\n",
      "2017-11-15T23:51:53.033136: step 31765, loss 0.00127851, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:51:53.533073: step 31770, loss 0.00625759, acc 0.993711, f1 0.993698\n",
      "Current epoch:  1765\n",
      "2017-11-15T23:51:54.061017: step 31775, loss 0.00170654, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:51:54.602523: step 31780, loss 0.00182503, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:51:55.125473: step 31785, loss 0.00320392, acc 0.99707, f1 0.997069\n",
      "Current epoch:  1766\n",
      "2017-11-15T23:51:55.616315: step 31790, loss 0.00276599, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:51:56.130225: step 31795, loss 4.75148e-05, acc 1, f1 1\n",
      "2017-11-15T23:51:56.652648: step 31800, loss 0.00456333, acc 0.995117, f1 0.995117\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:51:57.306964: step 31800, loss 2.2923, acc 0.595967, f1 0.592698\n",
      "\n",
      "2017-11-15T23:51:57.823875: step 31805, loss 0.00296835, acc 0.99707, f1 0.997071\n",
      "Current epoch:  1767\n",
      "2017-11-15T23:51:58.312211: step 31810, loss 0.000675295, acc 1, f1 1\n",
      "2017-11-15T23:51:58.840654: step 31815, loss 0.00234423, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:51:59.362640: step 31820, loss 0.00330494, acc 0.99707, f1 0.997072\n",
      "Current epoch:  1768\n",
      "2017-11-15T23:51:59.858998: step 31825, loss 0.00227938, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:52:00.390951: step 31830, loss 0.000843381, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:52:00.925646: step 31835, loss 5.06823e-05, acc 1, f1 1\n",
      "2017-11-15T23:52:01.448593: step 31840, loss 0.00537336, acc 0.995117, f1 0.995121\n",
      "Current epoch:  1769\n",
      "2017-11-15T23:52:01.944949: step 31845, loss 0.00247435, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:52:02.473393: step 31850, loss 0.00441581, acc 0.99707, f1 0.99707\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:52:03.157835: step 31850, loss 2.366, acc 0.576531, f1 0.576245\n",
      "\n",
      "2017-11-15T23:52:03.700359: step 31855, loss 0.00300601, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:52:04.192218: step 31860, loss 0.0026506, acc 0.996855, f1 0.996855\n",
      "Current epoch:  1770\n",
      "2017-11-15T23:52:04.723689: step 31865, loss 0.00277074, acc 0.99707, f1 0.997075\n",
      "2017-11-15T23:52:05.250629: step 31870, loss 0.00379555, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:52:05.788188: step 31875, loss 0.00363289, acc 0.996094, f1 0.996094\n",
      "Current epoch:  1771\n",
      "2017-11-15T23:52:06.276521: step 31880, loss 0.00153522, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:52:06.797947: step 31885, loss 0.00352515, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:52:07.308843: step 31890, loss 0.0021399, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:52:07.827762: step 31895, loss 0.00182129, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1772\n",
      "2017-11-15T23:52:08.328630: step 31900, loss 0.00210675, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:52:09.000466: step 31900, loss 2.38235, acc 0.582929, f1 0.579625\n",
      "\n",
      "2017-11-15T23:52:09.518900: step 31905, loss 0.0024315, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:52:10.045339: step 31910, loss 0.00102957, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1773\n",
      "2017-11-15T23:52:10.542856: step 31915, loss 0.00144756, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:52:11.078365: step 31920, loss 0.00348441, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:52:11.608843: step 31925, loss 0.00347878, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:52:12.134783: step 31930, loss 0.00368531, acc 0.996094, f1 0.996095\n",
      "Current epoch:  1774\n",
      "2017-11-15T23:52:12.633145: step 31935, loss 0.000106139, acc 1, f1 1\n",
      "2017-11-15T23:52:13.270952: step 31940, loss 0.00424959, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:52:13.804936: step 31945, loss 0.000729076, acc 1, f1 1\n",
      "2017-11-15T23:52:14.294274: step 31950, loss 0.00128792, acc 0.998428, f1 0.998427\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:52:14.934022: step 31950, loss 2.29204, acc 0.591557, f1 0.591579\n",
      "\n",
      "Current epoch:  1775\n",
      "2017-11-15T23:52:15.510097: step 31955, loss 0.00154827, acc 1, f1 1\n",
      "2017-11-15T23:52:16.036038: step 31960, loss 0.00080082, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:52:16.577017: step 31965, loss 0.003763, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1776\n",
      "2017-11-15T23:52:17.067357: step 31970, loss 0.000863498, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:52:17.591289: step 31975, loss 0.000629467, acc 1, f1 1\n",
      "2017-11-15T23:52:18.118253: step 31980, loss 0.0020379, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:52:18.643717: step 31985, loss 0.00112558, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1777\n",
      "2017-11-15T23:52:19.138068: step 31990, loss 0.00109889, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:52:19.673532: step 31995, loss 0.00123145, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:52:20.192467: step 32000, loss 0.00323123, acc 0.99707, f1 0.997071\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:52:20.860152: step 32000, loss 2.32425, acc 0.586467, f1 0.585468\n",
      "\n",
      "Current epoch:  1778\n",
      "2017-11-15T23:52:21.356508: step 32005, loss 0.0016562, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:52:21.901376: step 32010, loss 0.0042029, acc 0.99707, f1 0.997066\n",
      "2017-11-15T23:52:22.427814: step 32015, loss 0.00166494, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:52:22.960270: step 32020, loss 0.00150429, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1779\n",
      "2017-11-15T23:52:23.461641: step 32025, loss 0.000881699, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:52:23.986575: step 32030, loss 4.70331e-05, acc 1, f1 1\n",
      "2017-11-15T23:52:24.509003: step 32035, loss 0.00104289, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:52:25.015437: step 32040, loss 4.77809e-05, acc 1, f1 1\n",
      "Current epoch:  1780\n",
      "2017-11-15T23:52:25.541033: step 32045, loss 0.000715321, acc 1, f1 1\n",
      "2017-11-15T23:52:26.067985: step 32050, loss 0.00480525, acc 0.995117, f1 0.995117\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:52:26.737832: step 32050, loss 2.29278, acc 0.587437, f1 0.587793\n",
      "\n",
      "2017-11-15T23:52:27.267329: step 32055, loss 0.00299971, acc 0.99707, f1 0.997069\n",
      "Current epoch:  1781\n",
      "2017-11-15T23:52:27.783766: step 32060, loss 0.000668617, acc 1, f1 1\n",
      "2017-11-15T23:52:28.306205: step 32065, loss 0.0019272, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:52:28.837164: step 32070, loss 0.00333921, acc 0.996094, f1 0.996093\n",
      "2017-11-15T23:52:29.370257: step 32075, loss 5.22452e-05, acc 1, f1 1\n",
      "Current epoch:  1782\n",
      "2017-11-15T23:52:29.872184: step 32080, loss 0.00173363, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:52:30.397140: step 32085, loss 0.00313867, acc 0.998047, f1 0.99805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:52:30.934799: step 32090, loss 0.000914494, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1783\n",
      "2017-11-15T23:52:31.434665: step 32095, loss 0.00212047, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:52:31.968625: step 32100, loss 0.00207465, acc 0.999023, f1 0.999022\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:52:32.635585: step 32100, loss 2.29561, acc 0.588455, f1 0.587431\n",
      "\n",
      "2017-11-15T23:52:33.175041: step 32105, loss 5.96303e-05, acc 1, f1 1\n",
      "2017-11-15T23:52:33.701481: step 32110, loss 0.00319382, acc 0.99707, f1 0.997069\n",
      "Current epoch:  1784\n",
      "2017-11-15T23:52:34.201424: step 32115, loss 0.000825372, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:52:34.722348: step 32120, loss 0.00182351, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:52:35.239762: step 32125, loss 0.000595488, acc 1, f1 1\n",
      "2017-11-15T23:52:35.726093: step 32130, loss 0.00153949, acc 0.998428, f1 0.998428\n",
      "Current epoch:  1785\n",
      "2017-11-15T23:52:36.261557: step 32135, loss 4.81049e-05, acc 1, f1 1\n",
      "2017-11-15T23:52:36.786490: step 32140, loss 4.88337e-05, acc 1, f1 1\n",
      "2017-11-15T23:52:37.308918: step 32145, loss 0.00363104, acc 0.996094, f1 0.996096\n",
      "Current epoch:  1786\n",
      "2017-11-15T23:52:37.807304: step 32150, loss 0.00223078, acc 0.999023, f1 0.999024\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:52:38.484225: step 32150, loss 2.30368, acc 0.580603, f1 0.582149\n",
      "\n",
      "2017-11-15T23:52:39.007156: step 32155, loss 0.00303261, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:52:39.539609: step 32160, loss 0.00120082, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:52:40.072566: step 32165, loss 0.00288073, acc 0.99707, f1 0.997069\n",
      "Current epoch:  1787\n",
      "2017-11-15T23:52:40.572450: step 32170, loss 0.0038104, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:52:41.096910: step 32175, loss 0.00159923, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:52:41.621870: step 32180, loss 0.00115507, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1788\n",
      "2017-11-15T23:52:42.119229: step 32185, loss 0.00358468, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:52:42.647213: step 32190, loss 0.000737203, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:52:43.174686: step 32195, loss 0.0023866, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:52:43.709648: step 32200, loss 0.00291924, acc 0.99707, f1 0.99707\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:52:44.402586: step 32200, loss 2.41309, acc 0.575853, f1 0.573791\n",
      "\n",
      "Current epoch:  1789\n",
      "2017-11-15T23:52:44.889449: step 32205, loss 0.00419474, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:52:45.414384: step 32210, loss 0.000982671, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:52:45.944333: step 32215, loss 0.00187988, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:52:46.436237: step 32220, loss 0.00279629, acc 0.996855, f1 0.996852\n",
      "Current epoch:  1790\n",
      "2017-11-15T23:52:46.962174: step 32225, loss 0.00146404, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:52:47.493650: step 32230, loss 0.00590825, acc 0.995117, f1 0.995119\n",
      "2017-11-15T23:52:48.034629: step 32235, loss 0.00275197, acc 0.99707, f1 0.997071\n",
      "Current epoch:  1791\n",
      "2017-11-15T23:52:48.535497: step 32240, loss 0.00136087, acc 1, f1 1\n",
      "2017-11-15T23:52:49.064443: step 32245, loss 0.00235329, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:52:49.609433: step 32250, loss 0.00357751, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:52:50.282811: step 32250, loss 2.32061, acc 0.585013, f1 0.583704\n",
      "\n",
      "2017-11-15T23:52:50.807304: step 32255, loss 0.00232509, acc 0.998047, f1 0.998046\n",
      "Current epoch:  1792\n",
      "2017-11-15T23:52:51.301185: step 32260, loss 0.000519074, acc 1, f1 1\n",
      "2017-11-15T23:52:51.826803: step 32265, loss 0.00207712, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:52:52.351237: step 32270, loss 0.00274145, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1793\n",
      "2017-11-15T23:52:52.850601: step 32275, loss 0.000734845, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:52:53.377533: step 32280, loss 0.00078276, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:52:53.905977: step 32285, loss 0.00380567, acc 0.995117, f1 0.995118\n",
      "2017-11-15T23:52:54.431442: step 32290, loss 0.00279299, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1794\n",
      "2017-11-15T23:52:54.934819: step 32295, loss 0.00082285, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:52:55.465609: step 32300, loss 0.00281242, acc 0.998047, f1 0.998043\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:52:56.143496: step 32300, loss 2.31014, acc 0.582784, f1 0.583608\n",
      "\n",
      "2017-11-15T23:52:56.663920: step 32305, loss 0.00397592, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:52:57.165289: step 32310, loss 0.00509064, acc 0.995283, f1 0.995286\n",
      "Current epoch:  1795\n",
      "2017-11-15T23:52:57.697806: step 32315, loss 0.0015566, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:52:58.221736: step 32320, loss 0.00352283, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:52:58.748176: step 32325, loss 0.00445482, acc 0.996094, f1 0.996098\n",
      "Current epoch:  1796\n",
      "2017-11-15T23:52:59.249045: step 32330, loss 0.00261081, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:52:59.779494: step 32335, loss 0.00138928, acc 1, f1 1\n",
      "2017-11-15T23:53:00.307488: step 32340, loss 0.0015136, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:53:00.839942: step 32345, loss 0.00333462, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1797\n",
      "2017-11-15T23:53:01.341312: step 32350, loss 0.00132609, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:53:02.014676: step 32350, loss 2.35155, acc 0.575223, f1 0.577585\n",
      "\n",
      "2017-11-15T23:53:02.535599: step 32355, loss 0.00179061, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:53:03.067077: step 32360, loss 0.00179051, acc 0.998047, f1 0.998048\n",
      "Current epoch:  1798\n",
      "2017-11-15T23:53:03.566944: step 32365, loss 0.000992897, acc 1, f1 1\n",
      "2017-11-15T23:53:04.090875: step 32370, loss 0.00161059, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:53:04.612353: step 32375, loss 0.00148705, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:53:05.137788: step 32380, loss 0.00197697, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1799\n",
      "2017-11-15T23:53:05.641186: step 32385, loss 0.00198476, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:53:06.179721: step 32390, loss 0.00160327, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:53:06.716750: step 32395, loss 0.00107171, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:53:07.216145: step 32400, loss 0.00519195, acc 0.995283, f1 0.995286\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:53:07.905029: step 32400, loss 2.29302, acc 0.58448, f1 0.585299\n",
      "\n",
      "Current epoch:  1800\n",
      "2017-11-15T23:53:08.441583: step 32405, loss 0.00145468, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:53:08.975089: step 32410, loss 0.00161103, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:53:09.501571: step 32415, loss 0.00314404, acc 0.996094, f1 0.996095\n",
      "Current epoch:  1801\n",
      "2017-11-15T23:53:09.996445: step 32420, loss 0.00323621, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:53:10.519374: step 32425, loss 0.000909297, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:53:11.045375: step 32430, loss 0.00240611, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:53:11.582342: step 32435, loss 0.00199914, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1802\n",
      "2017-11-15T23:53:12.087222: step 32440, loss 0.000666804, acc 1, f1 1\n",
      "2017-11-15T23:53:12.608761: step 32445, loss 0.00471603, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:53:13.138245: step 32450, loss 0.00216165, acc 0.998047, f1 0.998048\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:53:13.810271: step 32450, loss 2.30003, acc 0.584286, f1 0.584824\n",
      "\n",
      "Current epoch:  1803\n",
      "2017-11-15T23:53:14.328878: step 32455, loss 0.00288758, acc 1, f1 1\n",
      "2017-11-15T23:53:14.849301: step 32460, loss 0.00220673, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:53:15.371228: step 32465, loss 0.00361864, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:53:15.897666: step 32470, loss 0.0035685, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1804\n",
      "2017-11-15T23:53:16.398556: step 32475, loss 0.000927892, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:53:16.939662: step 32480, loss 0.00112855, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:53:17.482145: step 32485, loss 5.46924e-05, acc 1, f1 1\n",
      "2017-11-15T23:53:17.981008: step 32490, loss 0.00239864, acc 0.996855, f1 0.996855\n",
      "Current epoch:  1805\n",
      "2017-11-15T23:53:18.508952: step 32495, loss 0.00414337, acc 0.996094, f1 0.996093\n",
      "2017-11-15T23:53:19.043992: step 32500, loss 0.00267358, acc 0.99707, f1 0.99707\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:53:19.718336: step 32500, loss 2.36967, acc 0.577549, f1 0.577136\n",
      "\n",
      "2017-11-15T23:53:20.241784: step 32505, loss 5.77251e-05, acc 1, f1 1\n",
      "Current epoch:  1806\n",
      "2017-11-15T23:53:20.741170: step 32510, loss 0.000838144, acc 0.999023, f1 0.999023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:53:21.264558: step 32515, loss 0.00325604, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:53:21.802626: step 32520, loss 0.00222876, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:53:22.335603: step 32525, loss 0.00281493, acc 0.99707, f1 0.997069\n",
      "Current epoch:  1807\n",
      "2017-11-15T23:53:22.838478: step 32530, loss 0.00275699, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:53:23.376852: step 32535, loss 0.00250701, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:53:23.903792: step 32540, loss 0.00147774, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1808\n",
      "2017-11-15T23:53:24.405779: step 32545, loss 0.00231538, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:53:24.934223: step 32550, loss 0.000838263, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:53:25.609128: step 32550, loss 2.29915, acc 0.584723, f1 0.58574\n",
      "\n",
      "2017-11-15T23:53:26.130555: step 32555, loss 0.00278418, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:53:26.650475: step 32560, loss 0.00239347, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1809\n",
      "2017-11-15T23:53:27.148836: step 32565, loss 0.00315756, acc 0.996094, f1 0.996092\n",
      "2017-11-15T23:53:27.677782: step 32570, loss 0.00291891, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:53:28.214293: step 32575, loss 0.00312969, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:53:28.710666: step 32580, loss 0.00151542, acc 0.998428, f1 0.998428\n",
      "Current epoch:  1810\n",
      "2017-11-15T23:53:29.241660: step 32585, loss 0.0010149, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:53:29.768600: step 32590, loss 0.00136238, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:53:30.292031: step 32595, loss 0.00520011, acc 0.995117, f1 0.995115\n",
      "Current epoch:  1811\n",
      "2017-11-15T23:53:30.790934: step 32600, loss 0.00132501, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:53:31.467799: step 32600, loss 2.35506, acc 0.575514, f1 0.577736\n",
      "\n",
      "2017-11-15T23:53:31.993763: step 32605, loss 0.00293962, acc 0.99707, f1 0.997075\n",
      "2017-11-15T23:53:32.517712: step 32610, loss 0.00255397, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:53:33.043148: step 32615, loss 0.00385571, acc 0.996094, f1 0.996094\n",
      "Current epoch:  1812\n",
      "2017-11-15T23:53:33.556078: step 32620, loss 0.000762007, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:53:34.084049: step 32625, loss 0.00223148, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:53:34.616040: step 32630, loss 0.00298523, acc 0.99707, f1 0.997072\n",
      "Current epoch:  1813\n",
      "2017-11-15T23:53:35.111395: step 32635, loss 5.04607e-05, acc 1, f1 1\n",
      "2017-11-15T23:53:35.641343: step 32640, loss 0.00109043, acc 1, f1 1\n",
      "2017-11-15T23:53:36.169446: step 32645, loss 0.00290664, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:53:36.691874: step 32650, loss 0.00284812, acc 0.99707, f1 0.997071\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:53:37.366215: step 32650, loss 2.3538, acc 0.575756, f1 0.577468\n",
      "\n",
      "Current epoch:  1814\n",
      "2017-11-15T23:53:37.869330: step 32655, loss 0.00169606, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:53:38.400282: step 32660, loss 0.000753136, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:53:38.936748: step 32665, loss 0.0018061, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:53:39.441263: step 32670, loss 0.00150798, acc 0.998428, f1 0.998428\n",
      "Current epoch:  1815\n",
      "2017-11-15T23:53:39.968237: step 32675, loss 0.00144699, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:53:40.499187: step 32680, loss 0.00453099, acc 0.995117, f1 0.99513\n",
      "2017-11-15T23:53:41.026201: step 32685, loss 0.00185583, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1816\n",
      "2017-11-15T23:53:41.524563: step 32690, loss 4.9882e-05, acc 1, f1 1\n",
      "2017-11-15T23:53:42.048496: step 32695, loss 0.000797232, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:53:42.584498: step 32700, loss 0.00127229, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:53:43.251853: step 32700, loss 2.3307, acc 0.587291, f1 0.585393\n",
      "\n",
      "2017-11-15T23:53:43.781321: step 32705, loss 0.00186459, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1817\n",
      "2017-11-15T23:53:44.283224: step 32710, loss 0.00370171, acc 0.99707, f1 0.997072\n",
      "2017-11-15T23:53:44.813674: step 32715, loss 0.00106836, acc 1, f1 1\n",
      "2017-11-15T23:53:45.341617: step 32720, loss 0.00283578, acc 0.99707, f1 0.997074\n",
      "Current epoch:  1818\n",
      "2017-11-15T23:53:45.838475: step 32725, loss 0.00259611, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:53:46.365464: step 32730, loss 0.000665436, acc 1, f1 1\n",
      "2017-11-15T23:53:46.886422: step 32735, loss 0.00194412, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:53:47.411436: step 32740, loss 0.00208571, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1819\n",
      "2017-11-15T23:53:47.910801: step 32745, loss 0.00187024, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:53:48.438797: step 32750, loss 0.00286387, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:53:49.104133: step 32750, loss 2.35185, acc 0.581524, f1 0.581163\n",
      "\n",
      "2017-11-15T23:53:49.633605: step 32755, loss 0.00249451, acc 0.99707, f1 0.997066\n",
      "2017-11-15T23:53:50.146028: step 32760, loss 0.00418112, acc 0.995283, f1 0.995283\n",
      "Current epoch:  1820\n",
      "2017-11-15T23:53:50.679486: step 32765, loss 0.000705334, acc 1, f1 1\n",
      "2017-11-15T23:53:51.205423: step 32770, loss 0.0012745, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:53:51.734391: step 32775, loss 0.00384743, acc 0.99707, f1 0.997069\n",
      "Current epoch:  1821\n",
      "2017-11-15T23:53:52.236264: step 32780, loss 0.000676986, acc 1, f1 1\n",
      "2017-11-15T23:53:52.765733: step 32785, loss 0.00193551, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:53:53.295747: step 32790, loss 0.00438177, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:53:53.818676: step 32795, loss 0.00243285, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1822\n",
      "2017-11-15T23:53:54.312552: step 32800, loss 0.0019118, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:53:54.980877: step 32800, loss 2.388, acc 0.574593, f1 0.573986\n",
      "\n",
      "2017-11-15T23:53:55.516094: step 32805, loss 0.000803951, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:53:56.045548: step 32810, loss 0.00347508, acc 0.996094, f1 0.996095\n",
      "Current epoch:  1823\n",
      "2017-11-15T23:53:56.545988: step 32815, loss 0.00220138, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:53:57.074510: step 32820, loss 0.00321737, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:53:57.599469: step 32825, loss 0.00165959, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:53:58.128414: step 32830, loss 0.00221699, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1824\n",
      "2017-11-15T23:53:58.623267: step 32835, loss 6.68092e-05, acc 1, f1 1\n",
      "2017-11-15T23:53:59.154281: step 32840, loss 0.00180426, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:53:59.679217: step 32845, loss 0.000886563, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:54:00.181087: step 32850, loss 0.00125991, acc 0.998428, f1 0.998428\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:54:00.860945: step 32850, loss 2.31995, acc 0.582445, f1 0.582702\n",
      "\n",
      "Current epoch:  1825\n",
      "2017-11-15T23:54:01.399986: step 32855, loss 0.000745296, acc 1, f1 1\n",
      "2017-11-15T23:54:01.928486: step 32860, loss 0.00420661, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:54:02.454422: step 32865, loss 0.00322291, acc 0.99707, f1 0.997074\n",
      "Current epoch:  1826\n",
      "2017-11-15T23:54:02.952298: step 32870, loss 0.00323173, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:54:03.480260: step 32875, loss 0.00360696, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:54:04.003204: step 32880, loss 0.00160616, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:54:04.532651: step 32885, loss 0.00176128, acc 0.998047, f1 0.998046\n",
      "Current epoch:  1827\n",
      "2017-11-15T23:54:05.026968: step 32890, loss 0.000585735, acc 1, f1 1\n",
      "2017-11-15T23:54:05.543880: step 32895, loss 0.0027815, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:54:06.065324: step 32900, loss 0.00159969, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:54:06.732171: step 32900, loss 2.34977, acc 0.581524, f1 0.580848\n",
      "\n",
      "Current epoch:  1828\n",
      "2017-11-15T23:54:07.225018: step 32905, loss 0.00207139, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:54:07.748449: step 32910, loss 4.70278e-05, acc 1, f1 1\n",
      "2017-11-15T23:54:08.275389: step 32915, loss 5.23055e-05, acc 1, f1 1\n",
      "2017-11-15T23:54:08.800324: step 32920, loss 0.00109339, acc 0.999023, f1 0.999024\n",
      "Current epoch:  1829\n",
      "2017-11-15T23:54:09.292228: step 32925, loss 0.000531223, acc 1, f1 1\n",
      "2017-11-15T23:54:09.821173: step 32930, loss 0.00310409, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:54:10.345607: step 32935, loss 0.00225866, acc 0.998047, f1 0.998052\n",
      "2017-11-15T23:54:10.839960: step 32940, loss 0.000157208, acc 1, f1 1\n",
      "Current epoch:  1830\n",
      "2017-11-15T23:54:11.364912: step 32945, loss 0.00330768, acc 0.996094, f1 0.996095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:54:11.902907: step 32950, loss 0.00196327, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:54:12.589784: step 32950, loss 2.34388, acc 0.579924, f1 0.579687\n",
      "\n",
      "2017-11-15T23:54:13.111041: step 32955, loss 0.00109506, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1831\n",
      "2017-11-15T23:54:13.611408: step 32960, loss 0.00149799, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:54:14.135841: step 32965, loss 0.00099666, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:54:14.666793: step 32970, loss 0.00284557, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:54:15.213788: step 32975, loss 0.00274356, acc 0.99707, f1 0.997071\n",
      "Current epoch:  1832\n",
      "2017-11-15T23:54:15.713830: step 32980, loss 0.00156528, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:54:16.244824: step 32985, loss 0.00153846, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:54:16.774292: step 32990, loss 0.00486877, acc 0.995117, f1 0.995114\n",
      "Current epoch:  1833\n",
      "2017-11-15T23:54:17.278183: step 32995, loss 0.00228159, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:54:17.809134: step 33000, loss 0.000653061, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:54:18.476458: step 33000, loss 2.31238, acc 0.579488, f1 0.581694\n",
      "\n",
      "2017-11-15T23:54:19.000448: step 33005, loss 0.00266636, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:54:19.522393: step 33010, loss 0.00116081, acc 0.999023, f1 0.999024\n",
      "Current epoch:  1834\n",
      "2017-11-15T23:54:20.030281: step 33015, loss 0.00124981, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:54:20.559226: step 33020, loss 0.00218218, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:54:21.082739: step 33025, loss 0.00291006, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:54:21.603801: step 33030, loss 0.00273388, acc 0.996855, f1 0.996856\n",
      "Current epoch:  1835\n",
      "2017-11-15T23:54:22.158316: step 33035, loss 0.0029769, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:54:22.684757: step 33040, loss 7.8186e-05, acc 1, f1 1\n",
      "2017-11-15T23:54:23.221789: step 33045, loss 0.00636195, acc 0.994141, f1 0.994136\n",
      "Current epoch:  1836\n",
      "2017-11-15T23:54:23.718163: step 33050, loss 0.00107926, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:54:24.380974: step 33050, loss 2.31552, acc 0.584819, f1 0.585833\n",
      "\n",
      "2017-11-15T23:54:24.904001: step 33055, loss 0.00307836, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:54:25.430500: step 33060, loss 0.00214278, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:54:25.958186: step 33065, loss 0.00261055, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1837\n",
      "2017-11-15T23:54:26.460073: step 33070, loss 0.00415419, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:54:26.978490: step 33075, loss 0.00182245, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:54:27.507436: step 33080, loss 0.00209861, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1838\n",
      "2017-11-15T23:54:28.003822: step 33085, loss 0.0011522, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:54:28.545302: step 33090, loss 0.000713826, acc 1, f1 1\n",
      "2017-11-15T23:54:29.071239: step 33095, loss 0.00104679, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:54:29.599772: step 33100, loss 0.00235216, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:54:30.277625: step 33100, loss 2.32343, acc 0.581863, f1 0.582058\n",
      "\n",
      "Current epoch:  1839\n",
      "2017-11-15T23:54:30.778132: step 33105, loss 0.000678825, acc 1, f1 1\n",
      "2017-11-15T23:54:31.311616: step 33110, loss 0.00341515, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:54:31.844628: step 33115, loss 7.33278e-05, acc 1, f1 1\n",
      "2017-11-15T23:54:32.344495: step 33120, loss 0.00317872, acc 0.996855, f1 0.996855\n",
      "Current epoch:  1840\n",
      "2017-11-15T23:54:32.882967: step 33125, loss 0.00230107, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:54:33.406424: step 33130, loss 0.00269438, acc 0.99707, f1 0.997072\n",
      "2017-11-15T23:54:33.950536: step 33135, loss 0.00300675, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1841\n",
      "2017-11-15T23:54:34.459444: step 33140, loss 0.00458799, acc 0.995117, f1 0.995119\n",
      "2017-11-15T23:54:34.999420: step 33145, loss 0.000918285, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:54:35.546917: step 33150, loss 4.56571e-05, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:54:36.309503: step 33150, loss 2.30848, acc 0.587146, f1 0.586527\n",
      "\n",
      "2017-11-15T23:54:36.851482: step 33155, loss 0.00188613, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1842\n",
      "2017-11-15T23:54:37.341911: step 33160, loss 0.00162354, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:54:37.869853: step 33165, loss 0.00101905, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:54:38.391814: step 33170, loss 0.00336447, acc 0.99707, f1 0.997069\n",
      "Current epoch:  1843\n",
      "2017-11-15T23:54:38.891210: step 33175, loss 0.000965103, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:54:39.424165: step 33180, loss 0.002914, acc 0.998047, f1 0.998042\n",
      "2017-11-15T23:54:39.959127: step 33185, loss 0.00191667, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:54:40.485594: step 33190, loss 0.00297416, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1844\n",
      "2017-11-15T23:54:40.984050: step 33195, loss 0.00248481, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:54:41.520067: step 33200, loss 0.00353593, acc 0.99707, f1 0.997072\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:54:42.208715: step 33200, loss 2.34014, acc 0.588309, f1 0.585424\n",
      "\n",
      "2017-11-15T23:54:42.729639: step 33205, loss 0.0025748, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:54:43.228548: step 33210, loss 0.0013016, acc 0.998428, f1 0.99843\n",
      "Current epoch:  1845\n",
      "2017-11-15T23:54:43.759027: step 33215, loss 0.000493201, acc 1, f1 1\n",
      "2017-11-15T23:54:44.286363: step 33220, loss 0.00250584, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:54:44.820965: step 33225, loss 0.00339575, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1846\n",
      "2017-11-15T23:54:45.335415: step 33230, loss 4.3411e-05, acc 1, f1 1\n",
      "2017-11-15T23:54:45.859430: step 33235, loss 0.00135587, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:54:46.386399: step 33240, loss 0.00343157, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:54:46.908371: step 33245, loss 4.45226e-05, acc 1, f1 1\n",
      "Current epoch:  1847\n",
      "2017-11-15T23:54:47.395703: step 33250, loss 0.000854189, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:54:48.036957: step 33250, loss 2.37428, acc 0.572896, f1 0.57404\n",
      "\n",
      "2017-11-15T23:54:48.560906: step 33255, loss 0.000941604, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:54:49.089871: step 33260, loss 0.00141445, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1848\n",
      "2017-11-15T23:54:49.588246: step 33265, loss 0.00171252, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:54:50.119777: step 33270, loss 0.0014922, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:54:50.803255: step 33275, loss 0.000750575, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:54:51.337215: step 33280, loss 0.0023445, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1849\n",
      "2017-11-15T23:54:51.834659: step 33285, loss 0.00164942, acc 1, f1 1\n",
      "2017-11-15T23:54:52.365132: step 33290, loss 5.94466e-05, acc 1, f1 1\n",
      "2017-11-15T23:54:52.908207: step 33295, loss 0.00207778, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:54:53.406068: step 33300, loss 0.00805096, acc 0.992138, f1 0.992141\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:54:54.074395: step 33300, loss 2.32835, acc 0.578906, f1 0.58067\n",
      "\n",
      "Current epoch:  1850\n",
      "2017-11-15T23:54:54.600332: step 33305, loss 0.000811175, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:54:55.121757: step 33310, loss 0.000827033, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:54:55.650203: step 33315, loss 7.77707e-05, acc 1, f1 1\n",
      "Current epoch:  1851\n",
      "2017-11-15T23:54:56.165164: step 33320, loss 0.0028695, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:54:56.698133: step 33325, loss 0.00163547, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:54:57.223569: step 33330, loss 0.000949577, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:54:57.746999: step 33335, loss 0.00292487, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1852\n",
      "2017-11-15T23:54:58.243858: step 33340, loss 0.00225451, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:54:58.773325: step 33345, loss 0.00283871, acc 0.998047, f1 0.998051\n",
      "2017-11-15T23:54:59.296757: step 33350, loss 0.00434361, acc 0.99707, f1 0.997074\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:54:59.974669: step 33350, loss 2.31159, acc 0.589036, f1 0.58703\n",
      "\n",
      "Current epoch:  1853\n",
      "2017-11-15T23:55:00.472530: step 33355, loss 0.00111909, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:55:00.997985: step 33360, loss 0.00149064, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:55:01.536014: step 33365, loss 0.00105111, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:55:02.067532: step 33370, loss 0.0037602, acc 0.996094, f1 0.996094\n",
      "Current epoch:  1854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:55:02.559112: step 33375, loss 0.00346669, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:55:03.084074: step 33380, loss 0.00399803, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:55:03.609009: step 33385, loss 0.00363328, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:55:04.101353: step 33390, loss 0.00329851, acc 0.996855, f1 0.996855\n",
      "Current epoch:  1855\n",
      "2017-11-15T23:55:04.631804: step 33395, loss 0.00143517, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:55:05.162856: step 33400, loss 0.00204742, acc 0.999023, f1 0.999022\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:55:05.807115: step 33400, loss 2.39319, acc 0.577695, f1 0.577218\n",
      "\n",
      "2017-11-15T23:55:06.321041: step 33405, loss 0.00204316, acc 0.998047, f1 0.998046\n",
      "Current epoch:  1856\n",
      "2017-11-15T23:55:06.808899: step 33410, loss 0.00185929, acc 1, f1 1\n",
      "2017-11-15T23:55:07.326813: step 33415, loss 0.00179409, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:55:07.850248: step 33420, loss 0.00103028, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:55:08.380196: step 33425, loss 0.00578516, acc 0.995117, f1 0.995115\n",
      "Current epoch:  1857\n",
      "2017-11-15T23:55:08.882069: step 33430, loss 0.00110594, acc 1, f1 1\n",
      "2017-11-15T23:55:09.410513: step 33435, loss 0.00174716, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:55:09.933488: step 33440, loss 0.000901563, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1858\n",
      "2017-11-15T23:55:10.433355: step 33445, loss 0.00205514, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:55:10.957812: step 33450, loss 0.00337376, acc 0.998047, f1 0.998052\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:55:11.625136: step 33450, loss 2.33897, acc 0.582154, f1 0.581561\n",
      "\n",
      "2017-11-15T23:55:12.146561: step 33455, loss 0.00491493, acc 0.996094, f1 0.996095\n",
      "2017-11-15T23:55:12.687040: step 33460, loss 0.000958807, acc 0.999023, f1 0.999024\n",
      "Current epoch:  1859\n",
      "2017-11-15T23:55:13.186418: step 33465, loss 0.00425172, acc 0.996094, f1 0.996095\n",
      "2017-11-15T23:55:13.723385: step 33470, loss 0.00109017, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:55:14.250826: step 33475, loss 0.00334538, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:55:14.744677: step 33480, loss 6.08952e-05, acc 1, f1 1\n",
      "Current epoch:  1860\n",
      "2017-11-15T23:55:15.268609: step 33485, loss 0.00155353, acc 0.998047, f1 0.998042\n",
      "2017-11-15T23:55:15.801088: step 33490, loss 0.00195992, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:55:16.324036: step 33495, loss 0.00183906, acc 0.998047, f1 0.998046\n",
      "Current epoch:  1861\n",
      "2017-11-15T23:55:16.820018: step 33500, loss 0.000700105, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:55:17.489353: step 33500, loss 2.39036, acc 0.57561, f1 0.575216\n",
      "\n",
      "2017-11-15T23:55:18.024811: step 33505, loss 0.00098913, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:55:18.555262: step 33510, loss 0.00160202, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:55:19.081700: step 33515, loss 0.00525585, acc 0.994141, f1 0.994141\n",
      "Current epoch:  1862\n",
      "2017-11-15T23:55:19.578569: step 33520, loss 0.00206868, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:55:20.108016: step 33525, loss 0.00303709, acc 0.99707, f1 0.997072\n",
      "2017-11-15T23:55:20.634456: step 33530, loss 0.000913898, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1863\n",
      "2017-11-15T23:55:21.133319: step 33535, loss 0.0023379, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:55:21.660260: step 33540, loss 0.000920144, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:55:22.185696: step 33545, loss 0.00311562, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:55:22.716647: step 33550, loss 0.00399535, acc 0.996094, f1 0.996094\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:55:23.393063: step 33550, loss 2.3018, acc 0.590054, f1 0.590528\n",
      "\n",
      "Current epoch:  1864\n",
      "2017-11-15T23:55:23.902583: step 33555, loss 0.0027774, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:55:24.427517: step 33560, loss 0.000961428, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:55:24.956001: step 33565, loss 0.00232609, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:55:25.461601: step 33570, loss 6.20372e-05, acc 1, f1 1\n",
      "Current epoch:  1865\n",
      "2017-11-15T23:55:25.997189: step 33575, loss 0.00376971, acc 0.996094, f1 0.996098\n",
      "2017-11-15T23:55:26.517611: step 33580, loss 0.00283477, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:55:27.050065: step 33585, loss 0.00112748, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1866\n",
      "2017-11-15T23:55:27.549932: step 33590, loss 0.00142346, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:55:28.080438: step 33595, loss 0.00353044, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:55:28.611891: step 33600, loss 0.00102803, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:55:29.322862: step 33600, loss 2.33124, acc 0.580603, f1 0.581434\n",
      "\n",
      "2017-11-15T23:55:29.848907: step 33605, loss 5.03708e-05, acc 1, f1 1\n",
      "Current epoch:  1867\n",
      "2017-11-15T23:55:30.344763: step 33610, loss 0.00272319, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:55:30.874483: step 33615, loss 4.47032e-05, acc 1, f1 1\n",
      "2017-11-15T23:55:31.399920: step 33620, loss 0.00260684, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1868\n",
      "2017-11-15T23:55:31.902336: step 33625, loss 0.0027906, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:55:32.427322: step 33630, loss 0.00165261, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:55:32.954785: step 33635, loss 0.00109508, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:55:33.490916: step 33640, loss 0.00305166, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1869\n",
      "2017-11-15T23:55:33.995797: step 33645, loss 0.00127852, acc 1, f1 1\n",
      "2017-11-15T23:55:34.529332: step 33650, loss 0.00218149, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:55:35.220254: step 33650, loss 2.32253, acc 0.586225, f1 0.585595\n",
      "\n",
      "2017-11-15T23:55:35.743704: step 33655, loss 0.00271648, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:55:36.235549: step 33660, loss 0.0027281, acc 0.996855, f1 0.996855\n",
      "Current epoch:  1870\n",
      "2017-11-15T23:55:36.767133: step 33665, loss 0.00270169, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:55:37.292105: step 33670, loss 0.0031671, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:55:37.813054: step 33675, loss 0.0040115, acc 0.996094, f1 0.996094\n",
      "Current epoch:  1871\n",
      "2017-11-15T23:55:38.310413: step 33680, loss 0.00347141, acc 0.99707, f1 0.997072\n",
      "2017-11-15T23:55:38.839394: step 33685, loss 4.54644e-05, acc 1, f1 1\n",
      "2017-11-15T23:55:39.363851: step 33690, loss 0.00262413, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:55:39.893304: step 33695, loss 0.00174225, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1872\n",
      "2017-11-15T23:55:40.396674: step 33700, loss 0.00265654, acc 0.99707, f1 0.997071\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:55:41.077032: step 33700, loss 2.29621, acc 0.589182, f1 0.589158\n",
      "\n",
      "2017-11-15T23:55:41.603904: step 33705, loss 0.000878287, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:55:42.131112: step 33710, loss 0.00218854, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1873\n",
      "2017-11-15T23:55:42.634032: step 33715, loss 0.00286513, acc 0.99707, f1 0.997067\n",
      "2017-11-15T23:55:43.159987: step 33720, loss 0.00136096, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:55:43.688458: step 33725, loss 0.00301651, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:55:44.213893: step 33730, loss 0.00200532, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1874\n",
      "2017-11-15T23:55:44.705765: step 33735, loss 0.0019867, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:55:45.234727: step 33740, loss 0.00153619, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:55:45.779417: step 33745, loss 0.00641271, acc 0.994141, f1 0.994139\n",
      "2017-11-15T23:55:46.268755: step 33750, loss 0.00617802, acc 0.995283, f1 0.995278\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:55:46.910391: step 33750, loss 2.31922, acc 0.581136, f1 0.583809\n",
      "\n",
      "Current epoch:  1875\n",
      "2017-11-15T23:55:47.423810: step 33755, loss 0.00145079, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:55:47.947766: step 33760, loss 0.0019596, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:55:48.486776: step 33765, loss 0.00263168, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1876\n",
      "2017-11-15T23:55:48.983228: step 33770, loss 0.00135524, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:55:49.512426: step 33775, loss 0.00147381, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:55:50.042376: step 33780, loss 0.00235038, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:55:50.569316: step 33785, loss 0.00472491, acc 0.995117, f1 0.995118\n",
      "Current epoch:  1877\n",
      "2017-11-15T23:55:51.081260: step 33790, loss 3.95837e-05, acc 1, f1 1\n",
      "2017-11-15T23:55:51.622740: step 33795, loss 0.00144338, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:55:52.149857: step 33800, loss 0.00249001, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:55:52.823198: step 33800, loss 2.30864, acc 0.585304, f1 0.58531\n",
      "\n",
      "Current epoch:  1878\n",
      "2017-11-15T23:55:53.320055: step 33805, loss 0.000579561, acc 1, f1 1\n",
      "2017-11-15T23:55:53.849048: step 33810, loss 0.000525835, acc 1, f1 1\n",
      "2017-11-15T23:55:54.374985: step 33815, loss 0.00365831, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:55:54.899993: step 33820, loss 0.00395094, acc 0.996094, f1 0.996094\n",
      "Current epoch:  1879\n",
      "2017-11-15T23:55:55.396350: step 33825, loss 0.000656922, acc 1, f1 1\n",
      "2017-11-15T23:55:55.924292: step 33830, loss 0.00214968, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:55:56.459756: step 33835, loss 0.00265433, acc 0.99707, f1 0.997066\n",
      "2017-11-15T23:55:56.967219: step 33840, loss 0.00176286, acc 0.998428, f1 0.99843\n",
      "Current epoch:  1880\n",
      "2017-11-15T23:55:57.496165: step 33845, loss 5.54171e-05, acc 1, f1 1\n",
      "2017-11-15T23:55:58.019595: step 33850, loss 0.00512765, acc 0.995117, f1 0.995117\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:55:58.695068: step 33850, loss 2.30297, acc 0.588067, f1 0.587765\n",
      "\n",
      "2017-11-15T23:55:59.217496: step 33855, loss 0.00277951, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1881\n",
      "2017-11-15T23:55:59.713354: step 33860, loss 0.0038154, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:56:00.244324: step 33865, loss 0.0030812, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:56:00.772266: step 33870, loss 0.00159887, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:56:01.301801: step 33875, loss 0.00142833, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1882\n",
      "2017-11-15T23:56:01.805700: step 33880, loss 0.00137434, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:56:02.345175: step 33885, loss 0.00257248, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:56:02.877680: step 33890, loss 0.00275015, acc 0.99707, f1 0.997069\n",
      "Current epoch:  1883\n",
      "2017-11-15T23:56:03.392153: step 33895, loss 0.00251588, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:56:03.931463: step 33900, loss 5.4075e-05, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:56:04.618468: step 33900, loss 2.3695, acc 0.580457, f1 0.579163\n",
      "\n",
      "2017-11-15T23:56:05.144985: step 33905, loss 0.00125038, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:56:05.670403: step 33910, loss 0.0034335, acc 0.99707, f1 0.997069\n",
      "Current epoch:  1884\n",
      "2017-11-15T23:56:06.169769: step 33915, loss 0.00289577, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:56:06.695706: step 33920, loss 0.00174768, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:56:07.210112: step 33925, loss 0.00121629, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:56:07.717070: step 33930, loss 0.00287186, acc 0.996855, f1 0.996858\n",
      "Current epoch:  1885\n",
      "2017-11-15T23:56:08.248022: step 33935, loss 0.0031637, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:56:08.779999: step 33940, loss 4.44253e-05, acc 1, f1 1\n",
      "2017-11-15T23:56:09.304434: step 33945, loss 0.00486408, acc 0.995117, f1 0.995117\n",
      "Current epoch:  1886\n",
      "2017-11-15T23:56:09.798305: step 33950, loss 0.00274219, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:56:10.473651: step 33950, loss 2.36996, acc 0.577355, f1 0.576897\n",
      "\n",
      "2017-11-15T23:56:11.000590: step 33955, loss 0.0028783, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:56:11.529034: step 33960, loss 0.00286864, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:56:12.054470: step 33965, loss 5.03732e-05, acc 1, f1 1\n",
      "Current epoch:  1887\n",
      "2017-11-15T23:56:12.553400: step 33970, loss 0.00131435, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:56:13.095381: step 33975, loss 0.00155673, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:56:13.631848: step 33980, loss 0.00182621, acc 0.998047, f1 0.998048\n",
      "Current epoch:  1888\n",
      "2017-11-15T23:56:14.129708: step 33985, loss 0.0016139, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:56:14.655646: step 33990, loss 5.103e-05, acc 1, f1 1\n",
      "2017-11-15T23:56:15.180634: step 33995, loss 0.00223921, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:56:15.752197: step 34000, loss 0.00282522, acc 0.99707, f1 0.997076\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:56:16.426081: step 34000, loss 2.32869, acc 0.586613, f1 0.585271\n",
      "\n",
      "Current epoch:  1889\n",
      "2017-11-15T23:56:16.922939: step 34005, loss 0.00312174, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:56:17.450600: step 34010, loss 0.00141185, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:56:17.981049: step 34015, loss 0.00348991, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:56:18.492952: step 34020, loss 4.80259e-05, acc 1, f1 1\n",
      "Current epoch:  1890\n",
      "2017-11-15T23:56:19.024401: step 34025, loss 0.00245555, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:56:19.554511: step 34030, loss 0.000976999, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:56:20.089974: step 34035, loss 9.50308e-05, acc 1, f1 1\n",
      "Current epoch:  1891\n",
      "2017-11-15T23:56:20.589841: step 34040, loss 0.000765965, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:56:21.112269: step 34045, loss 0.00151045, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:56:21.635699: step 34050, loss 0.00198734, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:56:22.306128: step 34050, loss 2.36645, acc 0.578567, f1 0.578248\n",
      "\n",
      "2017-11-15T23:56:22.827570: step 34055, loss 0.00117512, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1892\n",
      "2017-11-15T23:56:23.322939: step 34060, loss 5.26803e-05, acc 1, f1 1\n",
      "2017-11-15T23:56:23.867451: step 34065, loss 0.00430178, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:56:24.401914: step 34070, loss 0.00359075, acc 0.99707, f1 0.997065\n",
      "Current epoch:  1893\n",
      "2017-11-15T23:56:24.905315: step 34075, loss 0.00149213, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:56:25.428757: step 34080, loss 0.000276226, acc 1, f1 1\n",
      "2017-11-15T23:56:25.957222: step 34085, loss 0.00223578, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:56:26.487711: step 34090, loss 0.00133745, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1894\n",
      "2017-11-15T23:56:26.984591: step 34095, loss 0.00275989, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:56:27.515542: step 34100, loss 0.00258702, acc 0.999023, f1 0.999022\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:56:28.194398: step 34100, loss 2.37146, acc 0.573381, f1 0.574875\n",
      "\n",
      "2017-11-15T23:56:28.716324: step 34105, loss 0.00113722, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:56:29.216962: step 34110, loss 0.00175311, acc 0.998428, f1 0.998428\n",
      "Current epoch:  1895\n",
      "2017-11-15T23:56:29.758143: step 34115, loss 0.00327942, acc 0.99707, f1 0.997067\n",
      "2017-11-15T23:56:30.284600: step 34120, loss 0.00128921, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:56:30.825085: step 34125, loss 0.00370286, acc 0.996094, f1 0.996093\n",
      "Current epoch:  1896\n",
      "2017-11-15T23:56:31.335480: step 34130, loss 0.00114939, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:56:31.869975: step 34135, loss 0.00322441, acc 0.99707, f1 0.997075\n",
      "2017-11-15T23:56:32.398420: step 34140, loss 0.00535084, acc 0.994141, f1 0.994142\n",
      "2017-11-15T23:56:32.921406: step 34145, loss 0.000893331, acc 0.999023, f1 0.999024\n",
      "Current epoch:  1897\n",
      "2017-11-15T23:56:33.424784: step 34150, loss 0.000597968, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:56:34.110877: step 34150, loss 2.3072, acc 0.586758, f1 0.586848\n",
      "\n",
      "2017-11-15T23:56:34.651902: step 34155, loss 0.00302527, acc 0.99707, f1 0.997081\n",
      "2017-11-15T23:56:35.207982: step 34160, loss 0.00309356, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1898\n",
      "2017-11-15T23:56:35.712863: step 34165, loss 0.00410674, acc 0.996094, f1 0.996092\n",
      "2017-11-15T23:56:36.241304: step 34170, loss 0.00230944, acc 1, f1 1\n",
      "2017-11-15T23:56:36.773759: step 34175, loss 0.00275165, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:56:37.305269: step 34180, loss 0.00268636, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1899\n",
      "2017-11-15T23:56:37.804194: step 34185, loss 0.00201162, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:56:38.328127: step 34190, loss 0.00274381, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:56:38.860584: step 34195, loss 0.00366857, acc 0.996094, f1 0.996096\n",
      "2017-11-15T23:56:39.352477: step 34200, loss 0.00182834, acc 0.998428, f1 0.998431\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:56:40.032852: step 34200, loss 2.30839, acc 0.592623, f1 0.590509\n",
      "\n",
      "Current epoch:  1900\n",
      "2017-11-15T23:56:40.566892: step 34205, loss 0.00230253, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:56:41.094331: step 34210, loss 4.01393e-05, acc 1, f1 1\n",
      "2017-11-15T23:56:41.618775: step 34215, loss 0.00218849, acc 0.998047, f1 0.998048\n",
      "Current epoch:  1901\n",
      "2017-11-15T23:56:42.114129: step 34220, loss 0.00162608, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:56:42.641185: step 34225, loss 0.00245726, acc 0.998047, f1 0.998044\n",
      "2017-11-15T23:56:43.169125: step 34230, loss 0.00364598, acc 0.996094, f1 0.996094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:56:43.690551: step 34235, loss 0.00275748, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1902\n",
      "2017-11-15T23:56:44.183898: step 34240, loss 0.00204252, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:56:44.708401: step 34245, loss 0.00382085, acc 0.99707, f1 0.997074\n",
      "2017-11-15T23:56:45.232923: step 34250, loss 0.00297496, acc 0.99707, f1 0.997069\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:56:45.912799: step 34250, loss 2.3715, acc 0.59209, f1 0.587197\n",
      "\n",
      "Current epoch:  1903\n",
      "2017-11-15T23:56:46.417224: step 34255, loss 0.000832774, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:56:46.941658: step 34260, loss 0.00109041, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:56:47.476139: step 34265, loss 0.00201358, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:56:48.007591: step 34270, loss 0.00113494, acc 0.999023, f1 0.999025\n",
      "Current epoch:  1904\n",
      "2017-11-15T23:56:48.510465: step 34275, loss 4.54996e-05, acc 1, f1 1\n",
      "2017-11-15T23:56:49.045011: step 34280, loss 0.000774745, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:56:49.574508: step 34285, loss 0.00583132, acc 0.995117, f1 0.995119\n",
      "2017-11-15T23:56:50.067878: step 34290, loss 0.00182116, acc 0.998428, f1 0.998428\n",
      "Current epoch:  1905\n",
      "2017-11-15T23:56:50.594817: step 34295, loss 0.00214291, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:56:51.121263: step 34300, loss 0.00250717, acc 0.998047, f1 0.998051\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:56:51.833228: step 34300, loss 2.31034, acc 0.591266, f1 0.589082\n",
      "\n",
      "2017-11-15T23:56:52.355687: step 34305, loss 0.00146068, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1906\n",
      "2017-11-15T23:56:52.858184: step 34310, loss 0.000559691, acc 1, f1 1\n",
      "2017-11-15T23:56:53.392254: step 34315, loss 0.000756072, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:56:53.923269: step 34320, loss 0.00160676, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:56:54.450820: step 34325, loss 0.00560313, acc 0.996094, f1 0.996091\n",
      "Current epoch:  1907\n",
      "2017-11-15T23:56:54.945229: step 34330, loss 0.000652701, acc 1, f1 1\n",
      "2017-11-15T23:56:55.475227: step 34335, loss 0.00246753, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:56:56.000740: step 34340, loss 0.00138166, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1908\n",
      "2017-11-15T23:56:56.502664: step 34345, loss 0.00105906, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:56:57.032720: step 34350, loss 0.00390517, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:56:57.733367: step 34350, loss 2.30659, acc 0.591944, f1 0.590608\n",
      "\n",
      "2017-11-15T23:56:58.266324: step 34355, loss 0.00106566, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:56:58.796850: step 34360, loss 0.0046976, acc 0.995117, f1 0.995114\n",
      "Current epoch:  1909\n",
      "2017-11-15T23:56:59.291736: step 34365, loss 4.53421e-05, acc 1, f1 1\n",
      "2017-11-15T23:56:59.819207: step 34370, loss 0.00379758, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:57:00.346922: step 34375, loss 0.00237747, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:57:00.844837: step 34380, loss 0.00231482, acc 0.998428, f1 0.998425\n",
      "Current epoch:  1910\n",
      "2017-11-15T23:57:01.374284: step 34385, loss 0.000557958, acc 1, f1 1\n",
      "2017-11-15T23:57:01.898215: step 34390, loss 0.00255106, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:57:02.431673: step 34395, loss 0.00345632, acc 0.99707, f1 0.997072\n",
      "Current epoch:  1911\n",
      "2017-11-15T23:57:02.937221: step 34400, loss 0.00131665, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:57:03.608233: step 34400, loss 2.37224, acc 0.574544, f1 0.576555\n",
      "\n",
      "2017-11-15T23:57:04.133669: step 34405, loss 0.00111195, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:57:04.662112: step 34410, loss 0.0037342, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:57:05.187075: step 34415, loss 0.000866715, acc 0.999023, f1 0.999024\n",
      "Current epoch:  1912\n",
      "2017-11-15T23:57:05.686462: step 34420, loss 0.00215436, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:57:06.210895: step 34425, loss 0.00179595, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:57:06.726304: step 34430, loss 0.0015114, acc 0.999023, f1 0.999024\n",
      "Current epoch:  1913\n",
      "2017-11-15T23:57:07.216644: step 34435, loss 0.00242739, acc 0.998047, f1 0.998043\n",
      "2017-11-15T23:57:07.727039: step 34440, loss 0.00339537, acc 0.996094, f1 0.996093\n",
      "2017-11-15T23:57:08.269021: step 34445, loss 0.00119989, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:57:08.800975: step 34450, loss 0.00382969, acc 0.996094, f1 0.99609\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:57:09.482364: step 34450, loss 2.34588, acc 0.578809, f1 0.581275\n",
      "\n",
      "Current epoch:  1914\n",
      "2017-11-15T23:57:09.975713: step 34455, loss 0.00225099, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:57:10.503673: step 34460, loss 0.00162892, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:57:11.025097: step 34465, loss 0.00127022, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:57:11.520532: step 34470, loss 0.00132004, acc 0.998428, f1 0.998428\n",
      "Current epoch:  1915\n",
      "2017-11-15T23:57:12.049478: step 34475, loss 5.61914e-05, acc 1, f1 1\n",
      "2017-11-15T23:57:12.581934: step 34480, loss 0.00223156, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:57:13.107909: step 34485, loss 0.00549521, acc 0.995117, f1 0.995116\n",
      "Current epoch:  1916\n",
      "2017-11-15T23:57:13.621347: step 34490, loss 0.00180148, acc 1, f1 1\n",
      "2017-11-15T23:57:14.153879: step 34495, loss 0.00188081, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:57:14.682871: step 34500, loss 0.00350374, acc 0.99707, f1 0.997071\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:57:15.347186: step 34500, loss 2.37753, acc 0.576144, f1 0.575663\n",
      "\n",
      "2017-11-15T23:57:15.900699: step 34505, loss 0.00192737, acc 0.998047, f1 0.998048\n",
      "Current epoch:  1917\n",
      "2017-11-15T23:57:16.408105: step 34510, loss 0.00462, acc 0.996094, f1 0.996096\n",
      "2017-11-15T23:57:16.930532: step 34515, loss 0.00104651, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:57:17.469548: step 34520, loss 0.00232723, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1918\n",
      "2017-11-15T23:57:17.969415: step 34525, loss 0.0012636, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:57:18.493847: step 34530, loss 0.00297693, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:57:19.033356: step 34535, loss 0.00294301, acc 0.99707, f1 0.997072\n",
      "2017-11-15T23:57:19.560989: step 34540, loss 0.0022943, acc 0.998047, f1 0.998042\n",
      "Current epoch:  1919\n",
      "2017-11-15T23:57:20.059922: step 34545, loss 0.0016225, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:57:20.591872: step 34550, loss 0.00188119, acc 0.998047, f1 0.998048\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:57:21.261704: step 34550, loss 2.30702, acc 0.585353, f1 0.58608\n",
      "\n",
      "2017-11-15T23:57:21.806192: step 34555, loss 0.00448252, acc 0.995117, f1 0.995125\n",
      "2017-11-15T23:57:22.298562: step 34560, loss 0.00293892, acc 0.996855, f1 0.996858\n",
      "Current epoch:  1920\n",
      "2017-11-15T23:57:22.827020: step 34565, loss 0.00213645, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:57:23.355002: step 34570, loss 0.0005975, acc 1, f1 1\n",
      "2017-11-15T23:57:23.878945: step 34575, loss 0.0045417, acc 0.996094, f1 0.996085\n",
      "Current epoch:  1921\n",
      "2017-11-15T23:57:24.383323: step 34580, loss 0.000853183, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:57:24.930320: step 34585, loss 0.0041348, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:57:25.456308: step 34590, loss 0.00219346, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:57:25.982746: step 34595, loss 0.00253116, acc 0.998047, f1 0.998046\n",
      "Current epoch:  1922\n",
      "2017-11-15T23:57:26.482645: step 34600, loss 0.00453483, acc 0.995117, f1 0.995128\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:57:27.156570: step 34600, loss 2.32666, acc 0.586177, f1 0.585094\n",
      "\n",
      "2017-11-15T23:57:27.673985: step 34605, loss 0.00266149, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:57:28.300697: step 34610, loss 0.00203503, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1923\n",
      "2017-11-15T23:57:28.801566: step 34615, loss 0.000770276, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:57:29.325499: step 34620, loss 0.00253494, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:57:29.861465: step 34625, loss 4.48157e-05, acc 1, f1 1\n",
      "2017-11-15T23:57:30.389044: step 34630, loss 0.00265068, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1924\n",
      "2017-11-15T23:57:30.887928: step 34635, loss 0.00103095, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:57:31.416895: step 34640, loss 0.00236882, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:57:31.952860: step 34645, loss 0.00210962, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:57:32.450765: step 34650, loss 0.001081, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:57:33.119636: step 34650, loss 2.41401, acc 0.572266, f1 0.571955\n",
      "\n",
      "Current epoch:  1925\n",
      "2017-11-15T23:57:33.648612: step 34655, loss 0.00286152, acc 0.998047, f1 0.998047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:57:34.175152: step 34660, loss 0.00188707, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:57:34.702596: step 34665, loss 0.00175806, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1926\n",
      "2017-11-15T23:57:35.195943: step 34670, loss 0.00167867, acc 1, f1 1\n",
      "2017-11-15T23:57:35.740932: step 34675, loss 4.38102e-05, acc 1, f1 1\n",
      "2017-11-15T23:57:36.268894: step 34680, loss 0.00164087, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:57:36.800436: step 34685, loss 0.000974823, acc 0.999023, f1 0.999022\n",
      "Current epoch:  1927\n",
      "2017-11-15T23:57:37.300805: step 34690, loss 0.00263804, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:57:37.824735: step 34695, loss 0.00122397, acc 1, f1 1\n",
      "2017-11-15T23:57:38.346161: step 34700, loss 0.00256147, acc 0.99707, f1 0.997071\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:57:39.016511: step 34700, loss 2.32407, acc 0.584044, f1 0.584504\n",
      "\n",
      "Current epoch:  1928\n",
      "2017-11-15T23:57:39.517441: step 34705, loss 0.00268897, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:57:40.049395: step 34710, loss 4.29851e-05, acc 1, f1 1\n",
      "2017-11-15T23:57:40.577869: step 34715, loss 0.000698911, acc 1, f1 1\n",
      "2017-11-15T23:57:41.123322: step 34720, loss 0.0016841, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1929\n",
      "2017-11-15T23:57:41.636775: step 34725, loss 0.00198114, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:57:42.167266: step 34730, loss 0.00283053, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:57:42.700814: step 34735, loss 0.00140945, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:57:43.190652: step 34740, loss 0.00268956, acc 0.996855, f1 0.996858\n",
      "Current epoch:  1930\n",
      "2017-11-15T23:57:43.722607: step 34745, loss 4.48441e-05, acc 1, f1 1\n",
      "2017-11-15T23:57:44.250076: step 34750, loss 0.000889199, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:57:44.917452: step 34750, loss 2.35744, acc 0.584383, f1 0.583211\n",
      "\n",
      "2017-11-15T23:57:45.446400: step 34755, loss 0.00366017, acc 0.996094, f1 0.996099\n",
      "Current epoch:  1931\n",
      "2017-11-15T23:57:45.949272: step 34760, loss 0.00277743, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:57:46.479722: step 34765, loss 0.00216174, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:57:47.008668: step 34770, loss 0.000846793, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:57:47.533620: step 34775, loss 0.0013733, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1932\n",
      "2017-11-15T23:57:48.029475: step 34780, loss 0.00271525, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:57:48.563434: step 34785, loss 0.00322394, acc 0.996094, f1 0.996093\n",
      "2017-11-15T23:57:49.089899: step 34790, loss 0.00645452, acc 0.993164, f1 0.993163\n",
      "Current epoch:  1933\n",
      "2017-11-15T23:57:49.586790: step 34795, loss 0.00130904, acc 1, f1 1\n",
      "2017-11-15T23:57:50.114733: step 34800, loss 0.00303609, acc 0.99707, f1 0.997069\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:57:50.778046: step 34800, loss 2.3435, acc 0.586952, f1 0.584709\n",
      "\n",
      "2017-11-15T23:57:51.299972: step 34805, loss 0.000797754, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:57:51.828438: step 34810, loss 0.00104556, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1934\n",
      "2017-11-15T23:57:52.338331: step 34815, loss 0.00153569, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:57:52.863768: step 34820, loss 0.00382321, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:57:53.388721: step 34825, loss 5.55062e-05, acc 1, f1 1\n",
      "2017-11-15T23:57:53.881568: step 34830, loss 0.00134137, acc 0.998428, f1 0.998428\n",
      "Current epoch:  1935\n",
      "2017-11-15T23:57:54.413523: step 34835, loss 0.0013648, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:57:54.946079: step 34840, loss 0.00108479, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:57:55.475594: step 34845, loss 0.00296603, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1936\n",
      "2017-11-15T23:57:55.973957: step 34850, loss 0.00268435, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:57:56.649354: step 34850, loss 2.42308, acc 0.568873, f1 0.569323\n",
      "\n",
      "2017-11-15T23:57:57.173285: step 34855, loss 0.0018737, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:57:57.719779: step 34860, loss 5.30224e-05, acc 1, f1 1\n",
      "2017-11-15T23:57:58.249249: step 34865, loss 0.00102637, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1937\n",
      "2017-11-15T23:57:58.750147: step 34870, loss 0.00187733, acc 1, f1 1\n",
      "2017-11-15T23:57:59.285218: step 34875, loss 0.000456541, acc 1, f1 1\n",
      "2017-11-15T23:57:59.808253: step 34880, loss 0.00121409, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1938\n",
      "2017-11-15T23:58:00.313173: step 34885, loss 0.00128708, acc 1, f1 1\n",
      "2017-11-15T23:58:00.841116: step 34890, loss 0.00268085, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:58:01.369822: step 34895, loss 0.0045979, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:58:01.896887: step 34900, loss 0.00289933, acc 0.99707, f1 0.997071\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:58:02.576810: step 34900, loss 2.46751, acc 0.56708, f1 0.567127\n",
      "\n",
      "Current epoch:  1939\n",
      "2017-11-15T23:58:03.088709: step 34905, loss 0.000924665, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:58:03.623808: step 34910, loss 0.00220054, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:58:04.149255: step 34915, loss 0.00374552, acc 0.996094, f1 0.996096\n",
      "2017-11-15T23:58:04.648141: step 34920, loss 0.00471552, acc 0.995283, f1 0.99528\n",
      "Current epoch:  1940\n",
      "2017-11-15T23:58:05.177588: step 34925, loss 0.00139369, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:58:05.709567: step 34930, loss 0.00377315, acc 0.996094, f1 0.996091\n",
      "2017-11-15T23:58:06.239538: step 34935, loss 4.27376e-05, acc 1, f1 1\n",
      "Current epoch:  1941\n",
      "2017-11-15T23:58:06.732383: step 34940, loss 5.06439e-05, acc 1, f1 1\n",
      "2017-11-15T23:58:07.250299: step 34945, loss 0.00203966, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:58:07.765707: step 34950, loss 0.00211372, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:58:08.406637: step 34950, loss 2.32343, acc 0.582202, f1 0.584292\n",
      "\n",
      "2017-11-15T23:58:08.944243: step 34955, loss 0.00326261, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1942\n",
      "2017-11-15T23:58:09.441118: step 34960, loss 0.000876458, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:58:09.972619: step 34965, loss 0.00343995, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:58:10.500598: step 34970, loss 0.000793185, acc 0.999023, f1 0.999025\n",
      "Current epoch:  1943\n",
      "2017-11-15T23:58:10.998960: step 34975, loss 0.00234125, acc 1, f1 1\n",
      "2017-11-15T23:58:11.525594: step 34980, loss 0.000998041, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:58:12.056545: step 34985, loss 0.00152985, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:58:12.589582: step 34990, loss 0.000912334, acc 0.999023, f1 0.999024\n",
      "Current epoch:  1944\n",
      "2017-11-15T23:58:13.085536: step 34995, loss 0.00141144, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:58:13.613554: step 35000, loss 0.0014081, acc 0.999023, f1 0.999022\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:58:14.319987: step 35000, loss 2.3208, acc 0.58448, f1 0.585596\n",
      "\n",
      "2017-11-15T23:58:14.847428: step 35005, loss 0.000987602, acc 0.999023, f1 0.999025\n",
      "2017-11-15T23:58:15.531313: step 35010, loss 0.00324047, acc 0.996855, f1 0.996856\n",
      "Current epoch:  1945\n",
      "2017-11-15T23:58:16.062766: step 35015, loss 0.0046782, acc 0.995117, f1 0.99512\n",
      "2017-11-15T23:58:16.586698: step 35020, loss 0.00291958, acc 0.998047, f1 0.998048\n",
      "2017-11-15T23:58:17.114139: step 35025, loss 0.00304335, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1946\n",
      "2017-11-15T23:58:17.618518: step 35030, loss 0.00080912, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:58:18.147464: step 35035, loss 0.00155189, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:58:18.678415: step 35040, loss 0.00323239, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:58:19.200341: step 35045, loss 0.00333487, acc 0.996094, f1 0.996094\n",
      "Current epoch:  1947\n",
      "2017-11-15T23:58:19.717792: step 35050, loss 0.00093252, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:58:20.398654: step 35050, loss 2.34671, acc 0.580506, f1 0.581281\n",
      "\n",
      "2017-11-15T23:58:20.925593: step 35055, loss 0.00111385, acc 1, f1 1\n",
      "2017-11-15T23:58:21.454037: step 35060, loss 0.00291834, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1948\n",
      "2017-11-15T23:58:21.959419: step 35065, loss 0.000682355, acc 1, f1 1\n",
      "2017-11-15T23:58:22.490370: step 35070, loss 0.0030783, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:58:23.013890: step 35075, loss 0.00108725, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:58:23.540830: step 35080, loss 0.00107118, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1949\n",
      "2017-11-15T23:58:24.044205: step 35085, loss 0.00149219, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:58:24.570666: step 35090, loss 0.00126561, acc 0.999023, f1 0.999022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:58:25.107635: step 35095, loss 0.000995744, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:58:25.610054: step 35100, loss 0.00443643, acc 0.995283, f1 0.995283\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:58:26.287908: step 35100, loss 2.3926, acc 0.574641, f1 0.574447\n",
      "\n",
      "Current epoch:  1950\n",
      "2017-11-15T23:58:26.812356: step 35105, loss 0.00232507, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:58:27.339797: step 35110, loss 0.00133155, acc 1, f1 1\n",
      "2017-11-15T23:58:27.863229: step 35115, loss 0.00213038, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1951\n",
      "2017-11-15T23:58:28.357078: step 35120, loss 0.0039972, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:58:28.888531: step 35125, loss 6.42001e-05, acc 1, f1 1\n",
      "2017-11-15T23:58:29.406948: step 35130, loss 4.24269e-05, acc 1, f1 1\n",
      "2017-11-15T23:58:29.942470: step 35135, loss 0.00151535, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1952\n",
      "2017-11-15T23:58:30.451362: step 35140, loss 0.00317315, acc 0.99707, f1 0.997069\n",
      "2017-11-15T23:58:30.981309: step 35145, loss 0.00167158, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:58:31.507292: step 35150, loss 0.00177122, acc 0.999023, f1 0.999024\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:58:32.189155: step 35150, loss 2.3712, acc 0.58036, f1 0.579143\n",
      "\n",
      "Current epoch:  1953\n",
      "2017-11-15T23:58:32.678493: step 35155, loss 0.00212097, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:58:33.200921: step 35160, loss 0.000750962, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:58:33.729473: step 35165, loss 0.00178606, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:58:34.252930: step 35170, loss 0.00298237, acc 0.99707, f1 0.997073\n",
      "Current epoch:  1954\n",
      "2017-11-15T23:58:34.752295: step 35175, loss 0.000810596, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:58:35.281241: step 35180, loss 0.000757076, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:58:35.816706: step 35185, loss 0.00441176, acc 0.996094, f1 0.996097\n",
      "2017-11-15T23:58:36.325663: step 35190, loss 5.78617e-05, acc 1, f1 1\n",
      "Current epoch:  1955\n",
      "2017-11-15T23:58:36.855726: step 35195, loss 0.00177575, acc 1, f1 1\n",
      "2017-11-15T23:58:37.384170: step 35200, loss 0.000999714, acc 0.999023, f1 0.999024\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:58:38.064554: step 35200, loss 2.32209, acc 0.588842, f1 0.587201\n",
      "\n",
      "2017-11-15T23:58:38.602595: step 35205, loss 0.00173339, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1956\n",
      "2017-11-15T23:58:39.110985: step 35210, loss 0.00117179, acc 1, f1 1\n",
      "2017-11-15T23:58:39.653969: step 35215, loss 0.00229108, acc 0.998047, f1 0.998043\n",
      "2017-11-15T23:58:40.189433: step 35220, loss 0.000905813, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:58:40.715390: step 35225, loss 0.0027595, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1957\n",
      "2017-11-15T23:58:41.212750: step 35230, loss 5.40873e-05, acc 1, f1 1\n",
      "2017-11-15T23:58:41.767363: step 35235, loss 0.003382, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:58:42.295306: step 35240, loss 0.00310653, acc 0.99707, f1 0.997083\n",
      "Current epoch:  1958\n",
      "2017-11-15T23:58:42.797177: step 35245, loss 0.00471552, acc 0.995117, f1 0.995118\n",
      "2017-11-15T23:58:43.317600: step 35250, loss 0.00256252, acc 0.998047, f1 0.998046\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:58:43.994453: step 35250, loss 2.3783, acc 0.573429, f1 0.575076\n",
      "\n",
      "2017-11-15T23:58:44.517398: step 35255, loss 0.0016215, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:58:45.047412: step 35260, loss 6.60683e-05, acc 1, f1 1\n",
      "Current epoch:  1959\n",
      "2017-11-15T23:58:45.556372: step 35265, loss 0.00188008, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:58:46.080302: step 35270, loss 0.000814291, acc 0.999023, f1 0.999022\n",
      "2017-11-15T23:58:46.604234: step 35275, loss 0.0019932, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:58:47.100090: step 35280, loss 0.001868, acc 0.998428, f1 0.998428\n",
      "Current epoch:  1960\n",
      "2017-11-15T23:58:47.619510: step 35285, loss 5.6549e-05, acc 1, f1 1\n",
      "2017-11-15T23:58:48.141937: step 35290, loss 0.000598408, acc 1, f1 1\n",
      "2017-11-15T23:58:48.680410: step 35295, loss 0.00208325, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1961\n",
      "2017-11-15T23:58:49.182786: step 35300, loss 0.000726058, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:58:49.862222: step 35300, loss 2.44128, acc 0.570182, f1 0.569718\n",
      "\n",
      "2017-11-15T23:58:50.388732: step 35305, loss 0.000775883, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:58:50.926702: step 35310, loss 0.00290118, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:58:51.455666: step 35315, loss 0.00191049, acc 0.998047, f1 0.998041\n",
      "Current epoch:  1962\n",
      "2017-11-15T23:58:51.962078: step 35320, loss 0.0027026, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:58:52.501051: step 35325, loss 0.00212249, acc 0.998047, f1 0.998053\n",
      "2017-11-15T23:58:53.038019: step 35330, loss 6.75541e-05, acc 1, f1 1\n",
      "Current epoch:  1963\n",
      "2017-11-15T23:58:53.540926: step 35335, loss 0.00157973, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:58:54.074911: step 35340, loss 0.000627967, acc 1, f1 1\n",
      "2017-11-15T23:58:54.598341: step 35345, loss 0.00104262, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:58:55.122816: step 35350, loss 0.00398592, acc 0.99707, f1 0.997074\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:58:55.791220: step 35350, loss 2.33252, acc 0.592477, f1 0.589658\n",
      "\n",
      "Current epoch:  1964\n",
      "2017-11-15T23:58:56.286573: step 35355, loss 0.00606532, acc 0.994141, f1 0.994141\n",
      "2017-11-15T23:58:56.810518: step 35360, loss 0.000495217, acc 1, f1 1\n",
      "2017-11-15T23:58:57.332947: step 35365, loss 0.00163168, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:58:57.829835: step 35370, loss 0.00159422, acc 0.998428, f1 0.998431\n",
      "Current epoch:  1965\n",
      "2017-11-15T23:58:58.374829: step 35375, loss 0.00105507, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:58:58.901262: step 35380, loss 0.00240448, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:58:59.422710: step 35385, loss 0.00257722, acc 0.99707, f1 0.997071\n",
      "Current epoch:  1966\n",
      "2017-11-15T23:58:59.925604: step 35390, loss 0.00341867, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:59:00.448051: step 35395, loss 0.00278129, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:59:00.981008: step 35400, loss 0.000798315, acc 0.999023, f1 0.999022\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:59:01.648833: step 35400, loss 2.32146, acc 0.581427, f1 0.583286\n",
      "\n",
      "2017-11-15T23:59:02.178782: step 35405, loss 5.2918e-05, acc 1, f1 1\n",
      "Current epoch:  1967\n",
      "2017-11-15T23:59:02.672131: step 35410, loss 0.000895527, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:59:03.196616: step 35415, loss 0.00325102, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:59:03.731143: step 35420, loss 0.00376992, acc 0.996094, f1 0.996095\n",
      "Current epoch:  1968\n",
      "2017-11-15T23:59:04.236069: step 35425, loss 0.00228673, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:59:04.768619: step 35430, loss 0.00284112, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:59:05.298159: step 35435, loss 0.00368197, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:59:05.829166: step 35440, loss 0.000970228, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1969\n",
      "2017-11-15T23:59:06.331038: step 35445, loss 0.00108154, acc 1, f1 1\n",
      "2017-11-15T23:59:06.859535: step 35450, loss 0.00183809, acc 0.998047, f1 0.998048\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:59:07.542984: step 35450, loss 2.31976, acc 0.590442, f1 0.588304\n",
      "\n",
      "2017-11-15T23:59:08.065429: step 35455, loss 0.000849262, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:59:08.567300: step 35460, loss 0.00413988, acc 0.995283, f1 0.995283\n",
      "Current epoch:  1970\n",
      "2017-11-15T23:59:09.113794: step 35465, loss 0.00213571, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:59:09.645776: step 35470, loss 0.00349614, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:59:10.171714: step 35475, loss 4.9437e-05, acc 1, f1 1\n",
      "Current epoch:  1971\n",
      "2017-11-15T23:59:10.674608: step 35480, loss 0.00229492, acc 1, f1 1\n",
      "2017-11-15T23:59:11.198575: step 35485, loss 0.00114122, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:59:11.726538: step 35490, loss 0.00183206, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:59:12.255484: step 35495, loss 0.00351157, acc 0.996094, f1 0.996094\n",
      "Current epoch:  1972\n",
      "2017-11-15T23:59:12.753362: step 35500, loss 0.00306691, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:59:13.423243: step 35500, loss 2.33321, acc 0.589375, f1 0.587746\n",
      "\n",
      "2017-11-15T23:59:13.948203: step 35505, loss 0.000627859, acc 1, f1 1\n",
      "2017-11-15T23:59:14.482164: step 35510, loss 0.00450337, acc 0.996094, f1 0.996095\n",
      "Current epoch:  1973\n",
      "2017-11-15T23:59:14.990052: step 35515, loss 7.36759e-05, acc 1, f1 1\n",
      "2017-11-15T23:59:15.515487: step 35520, loss 0.00211703, acc 0.998047, f1 0.998046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-15T23:59:16.047441: step 35525, loss 0.003397, acc 0.996094, f1 0.996097\n",
      "2017-11-15T23:59:16.573172: step 35530, loss 0.00261285, acc 0.99707, f1 0.99707\n",
      "Current epoch:  1974\n",
      "2017-11-15T23:59:17.070532: step 35535, loss 0.00128627, acc 1, f1 1\n",
      "2017-11-15T23:59:17.598976: step 35540, loss 0.00305306, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:59:18.122408: step 35545, loss 0.00219351, acc 0.998047, f1 0.998051\n",
      "2017-11-15T23:59:18.618262: step 35550, loss 4.03077e-05, acc 1, f1 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:59:19.275559: step 35550, loss 2.32959, acc 0.583268, f1 0.58375\n",
      "\n",
      "Current epoch:  1975\n",
      "2017-11-15T23:59:19.792471: step 35555, loss 0.000779976, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:59:20.317908: step 35560, loss 0.00468215, acc 0.995117, f1 0.995117\n",
      "2017-11-15T23:59:20.836826: step 35565, loss 0.00181298, acc 0.998047, f1 0.998048\n",
      "Current epoch:  1976\n",
      "2017-11-15T23:59:21.328169: step 35570, loss 0.000673392, acc 1, f1 1\n",
      "2017-11-15T23:59:21.843578: step 35575, loss 0.000552767, acc 1, f1 1\n",
      "2017-11-15T23:59:22.364000: step 35580, loss 0.00336294, acc 0.99707, f1 0.997067\n",
      "2017-11-15T23:59:22.879910: step 35585, loss 0.00502302, acc 0.996094, f1 0.996095\n",
      "Current epoch:  1977\n",
      "2017-11-15T23:59:23.363231: step 35590, loss 0.00129325, acc 1, f1 1\n",
      "2017-11-15T23:59:23.881147: step 35595, loss 0.00148303, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:59:24.395051: step 35600, loss 0.00267807, acc 0.99707, f1 0.997072\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:59:25.038811: step 35600, loss 2.31499, acc 0.589763, f1 0.589441\n",
      "\n",
      "Current epoch:  1978\n",
      "2017-11-15T23:59:25.542708: step 35605, loss 0.00202842, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:59:26.064634: step 35610, loss 0.0015925, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:59:26.582049: step 35615, loss 0.00288123, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:59:27.096956: step 35620, loss 0.0035711, acc 0.996094, f1 0.996092\n",
      "Current epoch:  1979\n",
      "2017-11-15T23:59:27.594817: step 35625, loss 0.0028203, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:59:28.106214: step 35630, loss 0.00145705, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:59:28.636163: step 35635, loss 0.00194992, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:59:29.151071: step 35640, loss 0.00148846, acc 0.998428, f1 0.99843\n",
      "Current epoch:  1980\n",
      "2017-11-15T23:59:29.702076: step 35645, loss 0.00150225, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:59:30.233028: step 35650, loss 0.00084378, acc 0.999023, f1 0.999025\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:59:30.885311: step 35650, loss 2.38519, acc 0.575998, f1 0.576106\n",
      "\n",
      "2017-11-15T23:59:31.430301: step 35655, loss 0.000932759, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1981\n",
      "2017-11-15T23:59:31.929665: step 35660, loss 4.26703e-05, acc 1, f1 1\n",
      "2017-11-15T23:59:32.463626: step 35665, loss 0.00222512, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:59:33.004920: step 35670, loss 0.00195957, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:59:33.517320: step 35675, loss 0.00184482, acc 0.998047, f1 0.998047\n",
      "Current epoch:  1982\n",
      "2017-11-15T23:59:34.021699: step 35680, loss 0.00389298, acc 0.996094, f1 0.996094\n",
      "2017-11-15T23:59:34.562678: step 35685, loss 0.00262556, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:59:35.077585: step 35690, loss 4.3752e-05, acc 1, f1 1\n",
      "Current epoch:  1983\n",
      "2017-11-15T23:59:35.578956: step 35695, loss 0.00161748, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:59:36.109906: step 35700, loss 0.00221557, acc 0.998047, f1 0.998047\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:59:36.770713: step 35700, loss 2.36288, acc 0.582881, f1 0.582444\n",
      "\n",
      "2017-11-15T23:59:37.281122: step 35705, loss 0.00142275, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:59:37.799038: step 35710, loss 4.4379e-05, acc 1, f1 1\n",
      "Current epoch:  1984\n",
      "2017-11-15T23:59:38.282860: step 35715, loss 5.06084e-05, acc 1, f1 1\n",
      "2017-11-15T23:59:38.795260: step 35720, loss 0.00210011, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:59:39.310670: step 35725, loss 0.00462013, acc 0.996094, f1 0.996092\n",
      "2017-11-15T23:59:39.795495: step 35730, loss 0.00305737, acc 0.996855, f1 0.996853\n",
      "Current epoch:  1985\n",
      "2017-11-15T23:59:40.310402: step 35735, loss 0.00244526, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:59:40.825310: step 35740, loss 0.00408227, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:59:41.347236: step 35745, loss 0.00184278, acc 0.998047, f1 0.998048\n",
      "Current epoch:  1986\n",
      "2017-11-15T23:59:41.841632: step 35750, loss 0.00286122, acc 0.99707, f1 0.997074\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:59:42.520488: step 35750, loss 2.3487, acc 0.584965, f1 0.584186\n",
      "\n",
      "2017-11-15T23:59:43.040409: step 35755, loss 0.00140015, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:59:43.557321: step 35760, loss 0.0017529, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:59:44.074738: step 35765, loss 0.00496918, acc 0.995117, f1 0.99512\n",
      "Current epoch:  1987\n",
      "2017-11-15T23:59:44.564073: step 35770, loss 0.000512253, acc 1, f1 1\n",
      "2017-11-15T23:59:45.076976: step 35775, loss 0.00157284, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:59:45.593387: step 35780, loss 0.00476758, acc 0.995117, f1 0.995121\n",
      "Current epoch:  1988\n",
      "2017-11-15T23:59:46.094757: step 35785, loss 0.000566853, acc 1, f1 1\n",
      "2017-11-15T23:59:46.615180: step 35790, loss 0.00191455, acc 0.998047, f1 0.998046\n",
      "2017-11-15T23:59:47.138611: step 35795, loss 0.00238966, acc 0.99707, f1 0.997071\n",
      "2017-11-15T23:59:47.659043: step 35800, loss 0.00338139, acc 0.99707, f1 0.99708\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:59:48.311843: step 35800, loss 2.33771, acc 0.590587, f1 0.587683\n",
      "\n",
      "Current epoch:  1989\n",
      "2017-11-15T23:59:48.792156: step 35805, loss 0.00295465, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:59:49.310072: step 35810, loss 0.0016639, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:59:49.838517: step 35815, loss 0.00158441, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:59:50.332366: step 35820, loss 0.00279227, acc 0.996855, f1 0.996856\n",
      "Current epoch:  1990\n",
      "2017-11-15T23:59:50.854794: step 35825, loss 0.00201182, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:59:51.372709: step 35830, loss 0.00224244, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:59:51.905666: step 35835, loss 0.00136817, acc 0.999023, f1 0.999023\n",
      "Current epoch:  1991\n",
      "2017-11-15T23:59:52.394001: step 35840, loss 0.0046503, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:59:52.912419: step 35845, loss 0.00318818, acc 0.998047, f1 0.998047\n",
      "2017-11-15T23:59:53.440361: step 35850, loss 0.00159144, acc 0.999023, f1 0.999023\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:59:54.079608: step 35850, loss 2.32706, acc 0.590442, f1 0.588356\n",
      "\n",
      "2017-11-15T23:59:54.594515: step 35855, loss 0.000920238, acc 0.999023, f1 0.999025\n",
      "Current epoch:  1992\n",
      "2017-11-15T23:59:55.080845: step 35860, loss 8.99382e-05, acc 1, f1 1\n",
      "2017-11-15T23:59:55.597257: step 35865, loss 0.00128128, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:59:56.120186: step 35870, loss 0.00290472, acc 0.99707, f1 0.997069\n",
      "Current epoch:  1993\n",
      "2017-11-15T23:59:56.614036: step 35875, loss 0.00242718, acc 0.99707, f1 0.99707\n",
      "2017-11-15T23:59:57.126437: step 35880, loss 0.00202043, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:59:57.641845: step 35885, loss 0.00231179, acc 0.999023, f1 0.999023\n",
      "2017-11-15T23:59:58.161265: step 35890, loss 4.19756e-05, acc 1, f1 1\n",
      "Current epoch:  1994\n",
      "2017-11-15T23:59:58.657120: step 35895, loss 0.00141233, acc 0.999023, f1 0.999024\n",
      "2017-11-15T23:59:59.176038: step 35900, loss 0.00139582, acc 0.999023, f1 0.999025\n",
      "\n",
      "Evaluation:\n",
      "2017-11-15T23:59:59.813781: step 35900, loss 2.34049, acc 0.587825, f1 0.586053\n",
      "\n",
      "2017-11-16T00:00:00.324176: step 35905, loss 0.00238011, acc 0.998047, f1 0.998047\n",
      "2017-11-16T00:00:00.812009: step 35910, loss 0.00283524, acc 0.996855, f1 0.996856\n",
      "Current epoch:  1995\n",
      "2017-11-16T00:00:01.334439: step 35915, loss 0.00159033, acc 0.999023, f1 0.999024\n",
      "2017-11-16T00:00:01.847339: step 35920, loss 4.1001e-05, acc 1, f1 1\n",
      "2017-11-16T00:00:02.366760: step 35925, loss 0.00170165, acc 0.998047, f1 0.998046\n",
      "Current epoch:  1996\n",
      "2017-11-16T00:00:02.861112: step 35930, loss 0.00185938, acc 0.998047, f1 0.998047\n",
      "2017-11-16T00:00:03.377522: step 35935, loss 0.00110385, acc 0.999023, f1 0.999023\n",
      "2017-11-16T00:00:03.904964: step 35940, loss 0.00286988, acc 0.99707, f1 0.997069\n",
      "2017-11-16T00:00:04.535688: step 35945, loss 0.00302375, acc 0.99707, f1 0.997072\n",
      "Current epoch:  1997\n",
      "2017-11-16T00:00:05.023521: step 35950, loss 0.00166624, acc 0.999023, f1 0.999022\n",
      "\n",
      "Evaluation:\n",
      "2017-11-16T00:00:05.664774: step 35950, loss 2.38322, acc 0.579536, f1 0.579077\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-16T00:00:06.181691: step 35955, loss 0.00277737, acc 0.99707, f1 0.99707\n",
      "2017-11-16T00:00:06.701612: step 35960, loss 4.48798e-05, acc 1, f1 1\n",
      "Current epoch:  1998\n",
      "2017-11-16T00:00:07.179440: step 35965, loss 0.000729787, acc 0.999023, f1 0.999023\n",
      "2017-11-16T00:00:07.696354: step 35970, loss 0.00219639, acc 0.998047, f1 0.998046\n",
      "2017-11-16T00:00:08.209256: step 35975, loss 0.00266371, acc 0.99707, f1 0.997069\n",
      "2017-11-16T00:00:08.727171: step 35980, loss 4.40683e-05, acc 1, f1 1\n",
      "Current epoch:  1999\n",
      "2017-11-16T00:00:09.208988: step 35985, loss 0.00210667, acc 0.998047, f1 0.998047\n",
      "2017-11-16T00:00:09.730914: step 35990, loss 0.00159984, acc 0.998047, f1 0.998048\n",
      "2017-11-16T00:00:10.246324: step 35995, loss 0.00105196, acc 0.999023, f1 0.999023\n",
      "2017-11-16T00:00:10.738167: step 36000, loss 0.00331172, acc 0.996855, f1 0.996856\n",
      "\n",
      "Evaluation:\n",
      "2017-11-16T00:00:11.392957: step 36000, loss 2.36229, acc 0.580215, f1 0.580538\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "num_epochs = 200\n",
    "\n",
    "num_checkpoints = 5\n",
    "print_train_every = 5\n",
    "evaluate_every = 50\n",
    "checkpoint_every = 10000000\n",
    "allow_soft_placement=True\n",
    "log_device_placement=False\n",
    "\n",
    "with graph.as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=allow_soft_placement,\n",
    "      log_device_placement=log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        \n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdadeltaOptimizer(1.0)\n",
    "        grads_and_vars = optimizer.compute_gradients(loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=num_checkpoints)\n",
    "\n",
    "#         # Write vocabulary\n",
    "#         vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              input_x: x_batch,\n",
    "              input_y: y_batch,\n",
    "            }\n",
    "            _, step, summaries, cur_loss, cur_accuracy, y_pred = sess.run(\n",
    "                [train_op, global_step, train_summary_op, loss, accuracy, predictions],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "#             print(y_pred)\n",
    "#             print(y_batch)\n",
    "            if step % print_train_every == 0:\n",
    "                f1 = f1_score(y_batch, y_pred, average = 'weighted')\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}, f1 {:g}\".format(time_str, step, cur_loss, cur_accuracy,\n",
    "                                                                     f1))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              input_x: x_batch,\n",
    "              input_y: y_batch,\n",
    "            }\n",
    "            step, summaries, cur_loss, cur_accuracy, y_pred = sess.run(\n",
    "                [global_step, dev_summary_op, loss, accuracy, predictions],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            f1 = f1_score(y_batch, y_pred, average = 'weighted')\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}, f1 {:g}\".format(time_str, step, cur_loss, cur_accuracy,\n",
    "                                                                    f1))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "                \n",
    "        def dev_step_batch(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              input_x: x_batch,\n",
    "              input_y: y_batch,\n",
    "            }\n",
    "            step, summaries, cur_loss, cur_accuracy, y_pred = sess.run(\n",
    "                [global_step, dev_summary_op, loss, accuracy, predictions],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            f1 = f1_score(y_batch, y_pred, average = 'weighted')\n",
    "#             print(\"{}: step {}, loss {:g}, acc {:g}, f1 {:g}\".format(time_str, step, cur_loss, cur_accuracy,\n",
    "#                                                                     f1))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "            return cur_loss, cur_accuracy, f1\n",
    "\n",
    "        \n",
    "        sess.run(embedding_init, feed_dict={embedding_placeholder: final_embeddings})\n",
    "        # Generate batches\n",
    "        batches = batch_iter(\n",
    "            list(zip(x_train, y_train)), batch_size, num_epochs)\n",
    "        \n",
    "        batches_test = list(batch_iter(\n",
    "            list(zip(x_test, y_test)), batch_size, 1))\n",
    "        \n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            \n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_test, y_test, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_3.5]",
   "language": "python",
   "name": "conda-env-tensorflow_3.5-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
